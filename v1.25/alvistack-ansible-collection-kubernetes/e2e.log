I0729 15:25:32.347994      13 e2e.go:116] Starting e2e run "fdedc122-77f5-414b-b179-f28fd2dcbd5c" on Ginkgo node 1
Jul 29 15:25:32.386: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1690644331 - will randomize all specs

Will run 360 of 7066 specs
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:76
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:76
{"msg":"Test Suite starting","completed":0,"skipped":0,"failed":0}
Jul 29 15:25:32.639: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
Jul 29 15:25:32.647: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Jul 29 15:25:32.688: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Jul 29 15:25:32.753: INFO: 20 / 20 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Jul 29 15:25:32.753: INFO: expected 3 pod replicas in namespace 'kube-system', 3 are Running and Ready.
Jul 29 15:25:32.753: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Jul 29 15:25:32.767: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'cilium' (0 seconds elapsed)
Jul 29 15:25:32.767: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'cilium-node-init' (0 seconds elapsed)
Jul 29 15:25:32.767: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Jul 29 15:25:32.767: INFO: e2e test version: v1.25.12
Jul 29 15:25:32.770: INFO: kube-apiserver version: v1.25.12
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:76
Jul 29 15:25:32.770: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
Jul 29 15:25:32.782: INFO: Cluster IP family: ipv4
------------------------------
[SynchronizedBeforeSuite] PASSED [0.144 seconds]
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:76

  Begin Captured GinkgoWriter Output >>
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:76
    Jul 29 15:25:32.639: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    Jul 29 15:25:32.647: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
    Jul 29 15:25:32.688: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
    Jul 29 15:25:32.753: INFO: 20 / 20 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
    Jul 29 15:25:32.753: INFO: expected 3 pod replicas in namespace 'kube-system', 3 are Running and Ready.
    Jul 29 15:25:32.753: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
    Jul 29 15:25:32.767: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'cilium' (0 seconds elapsed)
    Jul 29 15:25:32.767: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'cilium-node-init' (0 seconds elapsed)
    Jul 29 15:25:32.767: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
    Jul 29 15:25:32.767: INFO: e2e test version: v1.25.12
    Jul 29 15:25:32.770: INFO: kube-apiserver version: v1.25.12
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:76
    Jul 29 15:25:32.770: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    Jul 29 15:25:32.782: INFO: Cluster IP family: ipv4
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:25:32.823
Jul 29 15:25:32.823: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename projected 07/29/23 15:25:32.825
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:25:32.866
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:25:32.871
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77
STEP: Creating projection with secret that has name projected-secret-test-map-b35c7a21-eeed-4b3d-a988-5685f1996a9b 07/29/23 15:25:32.877
STEP: Creating a pod to test consume secrets 07/29/23 15:25:32.888
Jul 29 15:25:32.912: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6fc942d2-635b-428e-a96b-9a3e0ac6170a" in namespace "projected-2504" to be "Succeeded or Failed"
Jul 29 15:25:32.917: INFO: Pod "pod-projected-secrets-6fc942d2-635b-428e-a96b-9a3e0ac6170a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.193402ms
Jul 29 15:25:34.924: INFO: Pod "pod-projected-secrets-6fc942d2-635b-428e-a96b-9a3e0ac6170a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012308592s
Jul 29 15:25:36.930: INFO: Pod "pod-projected-secrets-6fc942d2-635b-428e-a96b-9a3e0ac6170a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018139266s
Jul 29 15:25:38.926: INFO: Pod "pod-projected-secrets-6fc942d2-635b-428e-a96b-9a3e0ac6170a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.014240341s
Jul 29 15:25:40.924: INFO: Pod "pod-projected-secrets-6fc942d2-635b-428e-a96b-9a3e0ac6170a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.011799675s
Jul 29 15:25:42.928: INFO: Pod "pod-projected-secrets-6fc942d2-635b-428e-a96b-9a3e0ac6170a": Phase="Pending", Reason="", readiness=false. Elapsed: 10.016436293s
Jul 29 15:25:44.926: INFO: Pod "pod-projected-secrets-6fc942d2-635b-428e-a96b-9a3e0ac6170a": Phase="Pending", Reason="", readiness=false. Elapsed: 12.014298241s
Jul 29 15:25:46.925: INFO: Pod "pod-projected-secrets-6fc942d2-635b-428e-a96b-9a3e0ac6170a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.013296821s
STEP: Saw pod success 07/29/23 15:25:46.926
Jul 29 15:25:46.926: INFO: Pod "pod-projected-secrets-6fc942d2-635b-428e-a96b-9a3e0ac6170a" satisfied condition "Succeeded or Failed"
Jul 29 15:25:46.933: INFO: Trying to get logs from node wa4quivohpee-3 pod pod-projected-secrets-6fc942d2-635b-428e-a96b-9a3e0ac6170a container projected-secret-volume-test: <nil>
STEP: delete the pod 07/29/23 15:25:46.963
Jul 29 15:25:46.989: INFO: Waiting for pod pod-projected-secrets-6fc942d2-635b-428e-a96b-9a3e0ac6170a to disappear
Jul 29 15:25:46.997: INFO: Pod pod-projected-secrets-6fc942d2-635b-428e-a96b-9a3e0ac6170a no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Jul 29 15:25:46.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2504" for this suite. 07/29/23 15:25:47.005
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":1,"skipped":9,"failed":0}
------------------------------
â€¢ [SLOW TEST] [14.193 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:25:32.823
    Jul 29 15:25:32.823: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename projected 07/29/23 15:25:32.825
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:25:32.866
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:25:32.871
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:77
    STEP: Creating projection with secret that has name projected-secret-test-map-b35c7a21-eeed-4b3d-a988-5685f1996a9b 07/29/23 15:25:32.877
    STEP: Creating a pod to test consume secrets 07/29/23 15:25:32.888
    Jul 29 15:25:32.912: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6fc942d2-635b-428e-a96b-9a3e0ac6170a" in namespace "projected-2504" to be "Succeeded or Failed"
    Jul 29 15:25:32.917: INFO: Pod "pod-projected-secrets-6fc942d2-635b-428e-a96b-9a3e0ac6170a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.193402ms
    Jul 29 15:25:34.924: INFO: Pod "pod-projected-secrets-6fc942d2-635b-428e-a96b-9a3e0ac6170a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012308592s
    Jul 29 15:25:36.930: INFO: Pod "pod-projected-secrets-6fc942d2-635b-428e-a96b-9a3e0ac6170a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018139266s
    Jul 29 15:25:38.926: INFO: Pod "pod-projected-secrets-6fc942d2-635b-428e-a96b-9a3e0ac6170a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.014240341s
    Jul 29 15:25:40.924: INFO: Pod "pod-projected-secrets-6fc942d2-635b-428e-a96b-9a3e0ac6170a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.011799675s
    Jul 29 15:25:42.928: INFO: Pod "pod-projected-secrets-6fc942d2-635b-428e-a96b-9a3e0ac6170a": Phase="Pending", Reason="", readiness=false. Elapsed: 10.016436293s
    Jul 29 15:25:44.926: INFO: Pod "pod-projected-secrets-6fc942d2-635b-428e-a96b-9a3e0ac6170a": Phase="Pending", Reason="", readiness=false. Elapsed: 12.014298241s
    Jul 29 15:25:46.925: INFO: Pod "pod-projected-secrets-6fc942d2-635b-428e-a96b-9a3e0ac6170a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.013296821s
    STEP: Saw pod success 07/29/23 15:25:46.926
    Jul 29 15:25:46.926: INFO: Pod "pod-projected-secrets-6fc942d2-635b-428e-a96b-9a3e0ac6170a" satisfied condition "Succeeded or Failed"
    Jul 29 15:25:46.933: INFO: Trying to get logs from node wa4quivohpee-3 pod pod-projected-secrets-6fc942d2-635b-428e-a96b-9a3e0ac6170a container projected-secret-volume-test: <nil>
    STEP: delete the pod 07/29/23 15:25:46.963
    Jul 29 15:25:46.989: INFO: Waiting for pod pod-projected-secrets-6fc942d2-635b-428e-a96b-9a3e0ac6170a to disappear
    Jul 29 15:25:46.997: INFO: Pod pod-projected-secrets-6fc942d2-635b-428e-a96b-9a3e0ac6170a no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Jul 29 15:25:46.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-2504" for this suite. 07/29/23 15:25:47.005
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:25:47.025
Jul 29 15:25:47.025: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename webhook 07/29/23 15:25:47.028
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:25:47.059
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:25:47.063
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 07/29/23 15:25:47.092
STEP: Create role binding to let webhook read extension-apiserver-authentication 07/29/23 15:25:48.472
STEP: Deploying the webhook pod 07/29/23 15:25:48.484
STEP: Wait for the deployment to be ready 07/29/23 15:25:48.505
Jul 29 15:25:48.546: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 07/29/23 15:25:50.566
STEP: Verifying the service has paired with the endpoint 07/29/23 15:25:50.589
Jul 29 15:25:51.590: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655
STEP: Listing all of the created validation webhooks 07/29/23 15:25:51.705
STEP: Creating a configMap that should be mutated 07/29/23 15:25:51.737
STEP: Deleting the collection of validation webhooks 07/29/23 15:25:51.801
STEP: Creating a configMap that should not be mutated 07/29/23 15:25:51.919
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jul 29 15:25:51.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1067" for this suite. 07/29/23 15:25:51.95
STEP: Destroying namespace "webhook-1067-markers" for this suite. 07/29/23 15:25:51.961
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","completed":2,"skipped":66,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.044 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:25:47.025
    Jul 29 15:25:47.025: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename webhook 07/29/23 15:25:47.028
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:25:47.059
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:25:47.063
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 07/29/23 15:25:47.092
    STEP: Create role binding to let webhook read extension-apiserver-authentication 07/29/23 15:25:48.472
    STEP: Deploying the webhook pod 07/29/23 15:25:48.484
    STEP: Wait for the deployment to be ready 07/29/23 15:25:48.505
    Jul 29 15:25:48.546: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 07/29/23 15:25:50.566
    STEP: Verifying the service has paired with the endpoint 07/29/23 15:25:50.589
    Jul 29 15:25:51.590: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing mutating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:655
    STEP: Listing all of the created validation webhooks 07/29/23 15:25:51.705
    STEP: Creating a configMap that should be mutated 07/29/23 15:25:51.737
    STEP: Deleting the collection of validation webhooks 07/29/23 15:25:51.801
    STEP: Creating a configMap that should not be mutated 07/29/23 15:25:51.919
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jul 29 15:25:51.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-1067" for this suite. 07/29/23 15:25:51.95
    STEP: Destroying namespace "webhook-1067-markers" for this suite. 07/29/23 15:25:51.961
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Kubectl diff
  should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:929
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:25:52.072
Jul 29 15:25:52.072: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename kubectl 07/29/23 15:25:52.076
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:25:52.117
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:25:52.122
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:929
STEP: create deployment with httpd image 07/29/23 15:25:52.128
Jul 29 15:25:52.130: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-2255 create -f -'
Jul 29 15:25:53.288: INFO: stderr: ""
Jul 29 15:25:53.288: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image 07/29/23 15:25:53.288
Jul 29 15:25:53.288: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-2255 diff -f -'
Jul 29 15:25:53.886: INFO: rc: 1
Jul 29 15:25:53.890: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-2255 delete -f -'
Jul 29 15:25:54.146: INFO: stderr: ""
Jul 29 15:25:54.146: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jul 29 15:25:54.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2255" for this suite. 07/29/23 15:25:54.157
{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","completed":3,"skipped":69,"failed":0}
------------------------------
â€¢ [2.098 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl diff
  test/e2e/kubectl/kubectl.go:923
    should check if kubectl diff finds a difference for Deployments [Conformance]
    test/e2e/kubectl/kubectl.go:929

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:25:52.072
    Jul 29 15:25:52.072: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename kubectl 07/29/23 15:25:52.076
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:25:52.117
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:25:52.122
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl diff finds a difference for Deployments [Conformance]
      test/e2e/kubectl/kubectl.go:929
    STEP: create deployment with httpd image 07/29/23 15:25:52.128
    Jul 29 15:25:52.130: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-2255 create -f -'
    Jul 29 15:25:53.288: INFO: stderr: ""
    Jul 29 15:25:53.288: INFO: stdout: "deployment.apps/httpd-deployment created\n"
    STEP: verify diff finds difference between live and declared image 07/29/23 15:25:53.288
    Jul 29 15:25:53.288: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-2255 diff -f -'
    Jul 29 15:25:53.886: INFO: rc: 1
    Jul 29 15:25:53.890: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-2255 delete -f -'
    Jul 29 15:25:54.146: INFO: stderr: ""
    Jul 29 15:25:54.146: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jul 29 15:25:54.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-2255" for this suite. 07/29/23 15:25:54.157
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:25:54.192
Jul 29 15:25:54.192: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename downward-api 07/29/23 15:25:54.194
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:25:54.228
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:25:54.233
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266
STEP: Creating a pod to test downward api env vars 07/29/23 15:25:54.236
Jul 29 15:25:54.250: INFO: Waiting up to 5m0s for pod "downward-api-b3d997f4-d838-48af-bbb7-1aee5261db2a" in namespace "downward-api-2824" to be "Succeeded or Failed"
Jul 29 15:25:54.257: INFO: Pod "downward-api-b3d997f4-d838-48af-bbb7-1aee5261db2a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.005954ms
Jul 29 15:25:56.263: INFO: Pod "downward-api-b3d997f4-d838-48af-bbb7-1aee5261db2a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012436592s
Jul 29 15:25:58.274: INFO: Pod "downward-api-b3d997f4-d838-48af-bbb7-1aee5261db2a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023910129s
Jul 29 15:26:00.268: INFO: Pod "downward-api-b3d997f4-d838-48af-bbb7-1aee5261db2a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.017358713s
Jul 29 15:26:02.267: INFO: Pod "downward-api-b3d997f4-d838-48af-bbb7-1aee5261db2a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.016212887s
Jul 29 15:26:04.266: INFO: Pod "downward-api-b3d997f4-d838-48af-bbb7-1aee5261db2a": Phase="Pending", Reason="", readiness=false. Elapsed: 10.015447676s
Jul 29 15:26:06.265: INFO: Pod "downward-api-b3d997f4-d838-48af-bbb7-1aee5261db2a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.01447292s
STEP: Saw pod success 07/29/23 15:26:06.265
Jul 29 15:26:06.266: INFO: Pod "downward-api-b3d997f4-d838-48af-bbb7-1aee5261db2a" satisfied condition "Succeeded or Failed"
Jul 29 15:26:06.272: INFO: Trying to get logs from node wa4quivohpee-3 pod downward-api-b3d997f4-d838-48af-bbb7-1aee5261db2a container dapi-container: <nil>
STEP: delete the pod 07/29/23 15:26:06.284
Jul 29 15:26:06.310: INFO: Waiting for pod downward-api-b3d997f4-d838-48af-bbb7-1aee5261db2a to disappear
Jul 29 15:26:06.317: INFO: Pod downward-api-b3d997f4-d838-48af-bbb7-1aee5261db2a no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Jul 29 15:26:06.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2824" for this suite. 07/29/23 15:26:06.326
{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","completed":4,"skipped":126,"failed":0}
------------------------------
â€¢ [SLOW TEST] [12.150 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:25:54.192
    Jul 29 15:25:54.192: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename downward-api 07/29/23 15:25:54.194
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:25:54.228
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:25:54.233
    [It] should provide pod UID as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:266
    STEP: Creating a pod to test downward api env vars 07/29/23 15:25:54.236
    Jul 29 15:25:54.250: INFO: Waiting up to 5m0s for pod "downward-api-b3d997f4-d838-48af-bbb7-1aee5261db2a" in namespace "downward-api-2824" to be "Succeeded or Failed"
    Jul 29 15:25:54.257: INFO: Pod "downward-api-b3d997f4-d838-48af-bbb7-1aee5261db2a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.005954ms
    Jul 29 15:25:56.263: INFO: Pod "downward-api-b3d997f4-d838-48af-bbb7-1aee5261db2a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012436592s
    Jul 29 15:25:58.274: INFO: Pod "downward-api-b3d997f4-d838-48af-bbb7-1aee5261db2a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023910129s
    Jul 29 15:26:00.268: INFO: Pod "downward-api-b3d997f4-d838-48af-bbb7-1aee5261db2a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.017358713s
    Jul 29 15:26:02.267: INFO: Pod "downward-api-b3d997f4-d838-48af-bbb7-1aee5261db2a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.016212887s
    Jul 29 15:26:04.266: INFO: Pod "downward-api-b3d997f4-d838-48af-bbb7-1aee5261db2a": Phase="Pending", Reason="", readiness=false. Elapsed: 10.015447676s
    Jul 29 15:26:06.265: INFO: Pod "downward-api-b3d997f4-d838-48af-bbb7-1aee5261db2a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.01447292s
    STEP: Saw pod success 07/29/23 15:26:06.265
    Jul 29 15:26:06.266: INFO: Pod "downward-api-b3d997f4-d838-48af-bbb7-1aee5261db2a" satisfied condition "Succeeded or Failed"
    Jul 29 15:26:06.272: INFO: Trying to get logs from node wa4quivohpee-3 pod downward-api-b3d997f4-d838-48af-bbb7-1aee5261db2a container dapi-container: <nil>
    STEP: delete the pod 07/29/23 15:26:06.284
    Jul 29 15:26:06.310: INFO: Waiting for pod downward-api-b3d997f4-d838-48af-bbb7-1aee5261db2a to disappear
    Jul 29 15:26:06.317: INFO: Pod downward-api-b3d997f4-d838-48af-bbb7-1aee5261db2a no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Jul 29 15:26:06.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-2824" for this suite. 07/29/23 15:26:06.326
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:26:06.347
Jul 29 15:26:06.347: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename projected 07/29/23 15:26:06.35
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:26:06.379
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:26:06.384
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46
STEP: Creating configMap with name projected-configmap-test-volume-95cc7e29-beb6-401f-b3e0-4e787a58f08a 07/29/23 15:26:06.391
STEP: Creating a pod to test consume configMaps 07/29/23 15:26:06.4
Jul 29 15:26:06.417: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c0c50d5a-9b45-4f10-9be4-9b50be00f87c" in namespace "projected-4013" to be "Succeeded or Failed"
Jul 29 15:26:06.424: INFO: Pod "pod-projected-configmaps-c0c50d5a-9b45-4f10-9be4-9b50be00f87c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.582903ms
Jul 29 15:26:08.432: INFO: Pod "pod-projected-configmaps-c0c50d5a-9b45-4f10-9be4-9b50be00f87c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015100365s
Jul 29 15:26:10.432: INFO: Pod "pod-projected-configmaps-c0c50d5a-9b45-4f10-9be4-9b50be00f87c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014700733s
STEP: Saw pod success 07/29/23 15:26:10.432
Jul 29 15:26:10.432: INFO: Pod "pod-projected-configmaps-c0c50d5a-9b45-4f10-9be4-9b50be00f87c" satisfied condition "Succeeded or Failed"
Jul 29 15:26:10.442: INFO: Trying to get logs from node wa4quivohpee-3 pod pod-projected-configmaps-c0c50d5a-9b45-4f10-9be4-9b50be00f87c container agnhost-container: <nil>
STEP: delete the pod 07/29/23 15:26:10.455
Jul 29 15:26:10.480: INFO: Waiting for pod pod-projected-configmaps-c0c50d5a-9b45-4f10-9be4-9b50be00f87c to disappear
Jul 29 15:26:10.486: INFO: Pod pod-projected-configmaps-c0c50d5a-9b45-4f10-9be4-9b50be00f87c no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jul 29 15:26:10.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4013" for this suite. 07/29/23 15:26:10.495
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","completed":5,"skipped":132,"failed":0}
------------------------------
â€¢ [4.164 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:26:06.347
    Jul 29 15:26:06.347: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename projected 07/29/23 15:26:06.35
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:26:06.379
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:26:06.384
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:46
    STEP: Creating configMap with name projected-configmap-test-volume-95cc7e29-beb6-401f-b3e0-4e787a58f08a 07/29/23 15:26:06.391
    STEP: Creating a pod to test consume configMaps 07/29/23 15:26:06.4
    Jul 29 15:26:06.417: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c0c50d5a-9b45-4f10-9be4-9b50be00f87c" in namespace "projected-4013" to be "Succeeded or Failed"
    Jul 29 15:26:06.424: INFO: Pod "pod-projected-configmaps-c0c50d5a-9b45-4f10-9be4-9b50be00f87c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.582903ms
    Jul 29 15:26:08.432: INFO: Pod "pod-projected-configmaps-c0c50d5a-9b45-4f10-9be4-9b50be00f87c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015100365s
    Jul 29 15:26:10.432: INFO: Pod "pod-projected-configmaps-c0c50d5a-9b45-4f10-9be4-9b50be00f87c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014700733s
    STEP: Saw pod success 07/29/23 15:26:10.432
    Jul 29 15:26:10.432: INFO: Pod "pod-projected-configmaps-c0c50d5a-9b45-4f10-9be4-9b50be00f87c" satisfied condition "Succeeded or Failed"
    Jul 29 15:26:10.442: INFO: Trying to get logs from node wa4quivohpee-3 pod pod-projected-configmaps-c0c50d5a-9b45-4f10-9be4-9b50be00f87c container agnhost-container: <nil>
    STEP: delete the pod 07/29/23 15:26:10.455
    Jul 29 15:26:10.480: INFO: Waiting for pod pod-projected-configmaps-c0c50d5a-9b45-4f10-9be4-9b50be00f87c to disappear
    Jul 29 15:26:10.486: INFO: Pod pod-projected-configmaps-c0c50d5a-9b45-4f10-9be4-9b50be00f87c no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jul 29 15:26:10.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4013" for this suite. 07/29/23 15:26:10.495
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:26:10.521
Jul 29 15:26:10.521: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename watch 07/29/23 15:26:10.523
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:26:10.565
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:26:10.571
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
STEP: creating a watch on configmaps with label A 07/29/23 15:26:10.577
STEP: creating a watch on configmaps with label B 07/29/23 15:26:10.579
STEP: creating a watch on configmaps with label A or B 07/29/23 15:26:10.582
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 07/29/23 15:26:10.585
Jul 29 15:26:10.597: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2113  71f14a3f-4992-4ca5-a7df-518d2d93e86f 2338 0 2023-07-29 15:26:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-07-29 15:26:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 29 15:26:10.598: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2113  71f14a3f-4992-4ca5-a7df-518d2d93e86f 2338 0 2023-07-29 15:26:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-07-29 15:26:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification 07/29/23 15:26:10.599
Jul 29 15:26:10.631: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2113  71f14a3f-4992-4ca5-a7df-518d2d93e86f 2339 0 2023-07-29 15:26:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-07-29 15:26:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 29 15:26:10.631: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2113  71f14a3f-4992-4ca5-a7df-518d2d93e86f 2339 0 2023-07-29 15:26:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-07-29 15:26:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification 07/29/23 15:26:10.631
Jul 29 15:26:10.648: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2113  71f14a3f-4992-4ca5-a7df-518d2d93e86f 2340 0 2023-07-29 15:26:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-07-29 15:26:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 29 15:26:10.648: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2113  71f14a3f-4992-4ca5-a7df-518d2d93e86f 2340 0 2023-07-29 15:26:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-07-29 15:26:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification 07/29/23 15:26:10.649
Jul 29 15:26:10.675: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2113  71f14a3f-4992-4ca5-a7df-518d2d93e86f 2341 0 2023-07-29 15:26:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-07-29 15:26:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 29 15:26:10.675: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2113  71f14a3f-4992-4ca5-a7df-518d2d93e86f 2341 0 2023-07-29 15:26:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-07-29 15:26:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 07/29/23 15:26:10.675
Jul 29 15:26:10.707: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2113  12f1bab5-f75b-40e4-a309-eddd1d2613fe 2342 0 2023-07-29 15:26:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-07-29 15:26:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 29 15:26:10.708: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2113  12f1bab5-f75b-40e4-a309-eddd1d2613fe 2342 0 2023-07-29 15:26:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-07-29 15:26:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification 07/29/23 15:26:20.709
Jul 29 15:26:20.724: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2113  12f1bab5-f75b-40e4-a309-eddd1d2613fe 2382 0 2023-07-29 15:26:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-07-29 15:26:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 29 15:26:20.725: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2113  12f1bab5-f75b-40e4-a309-eddd1d2613fe 2382 0 2023-07-29 15:26:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-07-29 15:26:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Jul 29 15:26:30.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2113" for this suite. 07/29/23 15:26:30.738
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","completed":6,"skipped":157,"failed":0}
------------------------------
â€¢ [SLOW TEST] [20.233 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:26:10.521
    Jul 29 15:26:10.521: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename watch 07/29/23 15:26:10.523
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:26:10.565
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:26:10.571
    [It] should observe add, update, and delete watch notifications on configmaps [Conformance]
      test/e2e/apimachinery/watch.go:60
    STEP: creating a watch on configmaps with label A 07/29/23 15:26:10.577
    STEP: creating a watch on configmaps with label B 07/29/23 15:26:10.579
    STEP: creating a watch on configmaps with label A or B 07/29/23 15:26:10.582
    STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 07/29/23 15:26:10.585
    Jul 29 15:26:10.597: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2113  71f14a3f-4992-4ca5-a7df-518d2d93e86f 2338 0 2023-07-29 15:26:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-07-29 15:26:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jul 29 15:26:10.598: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2113  71f14a3f-4992-4ca5-a7df-518d2d93e86f 2338 0 2023-07-29 15:26:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-07-29 15:26:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A and ensuring the correct watchers observe the notification 07/29/23 15:26:10.599
    Jul 29 15:26:10.631: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2113  71f14a3f-4992-4ca5-a7df-518d2d93e86f 2339 0 2023-07-29 15:26:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-07-29 15:26:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jul 29 15:26:10.631: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2113  71f14a3f-4992-4ca5-a7df-518d2d93e86f 2339 0 2023-07-29 15:26:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-07-29 15:26:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A again and ensuring the correct watchers observe the notification 07/29/23 15:26:10.631
    Jul 29 15:26:10.648: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2113  71f14a3f-4992-4ca5-a7df-518d2d93e86f 2340 0 2023-07-29 15:26:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-07-29 15:26:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jul 29 15:26:10.648: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2113  71f14a3f-4992-4ca5-a7df-518d2d93e86f 2340 0 2023-07-29 15:26:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-07-29 15:26:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap A and ensuring the correct watchers observe the notification 07/29/23 15:26:10.649
    Jul 29 15:26:10.675: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2113  71f14a3f-4992-4ca5-a7df-518d2d93e86f 2341 0 2023-07-29 15:26:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-07-29 15:26:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jul 29 15:26:10.675: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2113  71f14a3f-4992-4ca5-a7df-518d2d93e86f 2341 0 2023-07-29 15:26:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-07-29 15:26:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 07/29/23 15:26:10.675
    Jul 29 15:26:10.707: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2113  12f1bab5-f75b-40e4-a309-eddd1d2613fe 2342 0 2023-07-29 15:26:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-07-29 15:26:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jul 29 15:26:10.708: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2113  12f1bab5-f75b-40e4-a309-eddd1d2613fe 2342 0 2023-07-29 15:26:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-07-29 15:26:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap B and ensuring the correct watchers observe the notification 07/29/23 15:26:20.709
    Jul 29 15:26:20.724: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2113  12f1bab5-f75b-40e4-a309-eddd1d2613fe 2382 0 2023-07-29 15:26:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-07-29 15:26:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jul 29 15:26:20.725: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2113  12f1bab5-f75b-40e4-a309-eddd1d2613fe 2382 0 2023-07-29 15:26:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-07-29 15:26:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Jul 29 15:26:30.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-2113" for this suite. 07/29/23 15:26:30.738
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:26:30.759
Jul 29 15:26:30.761: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename services 07/29/23 15:26:30.764
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:26:30.801
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:26:30.806
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404
STEP: creating a service externalname-service with the type=ExternalName in namespace services-9966 07/29/23 15:26:30.811
STEP: changing the ExternalName service to type=ClusterIP 07/29/23 15:26:30.821
STEP: creating replication controller externalname-service in namespace services-9966 07/29/23 15:26:30.854
I0729 15:26:30.871233      13 runners.go:193] Created replication controller with name: externalname-service, namespace: services-9966, replica count: 2
I0729 15:26:33.922905      13 runners.go:193] externalname-service Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0729 15:26:36.925926      13 runners.go:193] externalname-service Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0729 15:26:39.928005      13 runners.go:193] externalname-service Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0729 15:26:42.928814      13 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 29 15:26:42.929: INFO: Creating new exec pod
Jul 29 15:26:42.948: INFO: Waiting up to 5m0s for pod "execpodmlz7d" in namespace "services-9966" to be "running"
Jul 29 15:26:42.963: INFO: Pod "execpodmlz7d": Phase="Pending", Reason="", readiness=false. Elapsed: 13.776148ms
Jul 29 15:26:44.971: INFO: Pod "execpodmlz7d": Phase="Running", Reason="", readiness=true. Elapsed: 2.021215279s
Jul 29 15:26:44.971: INFO: Pod "execpodmlz7d" satisfied condition "running"
Jul 29 15:26:45.974: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=services-9966 exec execpodmlz7d -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jul 29 15:26:46.296: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jul 29 15:26:46.296: INFO: stdout: "externalname-service-gck79"
Jul 29 15:26:46.296: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=services-9966 exec execpodmlz7d -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.62.188 80'
Jul 29 15:26:46.518: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.62.188 80\nConnection to 10.233.62.188 80 port [tcp/http] succeeded!\n"
Jul 29 15:26:46.518: INFO: stdout: "externalname-service-gck79"
Jul 29 15:26:46.518: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jul 29 15:26:46.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9966" for this suite. 07/29/23 15:26:46.564
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","completed":7,"skipped":163,"failed":0}
------------------------------
â€¢ [SLOW TEST] [15.816 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:26:30.759
    Jul 29 15:26:30.761: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename services 07/29/23 15:26:30.764
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:26:30.801
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:26:30.806
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ExternalName to ClusterIP [Conformance]
      test/e2e/network/service.go:1404
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-9966 07/29/23 15:26:30.811
    STEP: changing the ExternalName service to type=ClusterIP 07/29/23 15:26:30.821
    STEP: creating replication controller externalname-service in namespace services-9966 07/29/23 15:26:30.854
    I0729 15:26:30.871233      13 runners.go:193] Created replication controller with name: externalname-service, namespace: services-9966, replica count: 2
    I0729 15:26:33.922905      13 runners.go:193] externalname-service Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0729 15:26:36.925926      13 runners.go:193] externalname-service Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0729 15:26:39.928005      13 runners.go:193] externalname-service Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0729 15:26:42.928814      13 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jul 29 15:26:42.929: INFO: Creating new exec pod
    Jul 29 15:26:42.948: INFO: Waiting up to 5m0s for pod "execpodmlz7d" in namespace "services-9966" to be "running"
    Jul 29 15:26:42.963: INFO: Pod "execpodmlz7d": Phase="Pending", Reason="", readiness=false. Elapsed: 13.776148ms
    Jul 29 15:26:44.971: INFO: Pod "execpodmlz7d": Phase="Running", Reason="", readiness=true. Elapsed: 2.021215279s
    Jul 29 15:26:44.971: INFO: Pod "execpodmlz7d" satisfied condition "running"
    Jul 29 15:26:45.974: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=services-9966 exec execpodmlz7d -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Jul 29 15:26:46.296: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Jul 29 15:26:46.296: INFO: stdout: "externalname-service-gck79"
    Jul 29 15:26:46.296: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=services-9966 exec execpodmlz7d -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.62.188 80'
    Jul 29 15:26:46.518: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.62.188 80\nConnection to 10.233.62.188 80 port [tcp/http] succeeded!\n"
    Jul 29 15:26:46.518: INFO: stdout: "externalname-service-gck79"
    Jul 29 15:26:46.518: INFO: Cleaning up the ExternalName to ClusterIP test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jul 29 15:26:46.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-9966" for this suite. 07/29/23 15:26:46.564
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:26:46.586
Jul 29 15:26:46.586: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename secrets 07/29/23 15:26:46.591
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:26:46.665
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:26:46.669
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98
STEP: Creating secret with name secret-test-38f89fa7-f107-4811-8089-d15a6caab8ff 07/29/23 15:26:46.741
STEP: Creating a pod to test consume secrets 07/29/23 15:26:46.75
Jul 29 15:26:46.762: INFO: Waiting up to 5m0s for pod "pod-secrets-cdfd2c7d-ac3e-4712-a579-7db6ad3f71db" in namespace "secrets-789" to be "Succeeded or Failed"
Jul 29 15:26:46.772: INFO: Pod "pod-secrets-cdfd2c7d-ac3e-4712-a579-7db6ad3f71db": Phase="Pending", Reason="", readiness=false. Elapsed: 10.164185ms
Jul 29 15:26:48.779: INFO: Pod "pod-secrets-cdfd2c7d-ac3e-4712-a579-7db6ad3f71db": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017086351s
Jul 29 15:26:50.780: INFO: Pod "pod-secrets-cdfd2c7d-ac3e-4712-a579-7db6ad3f71db": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018527331s
STEP: Saw pod success 07/29/23 15:26:50.781
Jul 29 15:26:50.781: INFO: Pod "pod-secrets-cdfd2c7d-ac3e-4712-a579-7db6ad3f71db" satisfied condition "Succeeded or Failed"
Jul 29 15:26:50.790: INFO: Trying to get logs from node wa4quivohpee-3 pod pod-secrets-cdfd2c7d-ac3e-4712-a579-7db6ad3f71db container secret-volume-test: <nil>
STEP: delete the pod 07/29/23 15:26:50.809
Jul 29 15:26:50.828: INFO: Waiting for pod pod-secrets-cdfd2c7d-ac3e-4712-a579-7db6ad3f71db to disappear
Jul 29 15:26:50.834: INFO: Pod pod-secrets-cdfd2c7d-ac3e-4712-a579-7db6ad3f71db no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jul 29 15:26:50.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-789" for this suite. 07/29/23 15:26:50.844
STEP: Destroying namespace "secret-namespace-1738" for this suite. 07/29/23 15:26:50.861
{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","completed":8,"skipped":185,"failed":0}
------------------------------
â€¢ [4.288 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:26:46.586
    Jul 29 15:26:46.586: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename secrets 07/29/23 15:26:46.591
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:26:46.665
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:26:46.669
    [It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:98
    STEP: Creating secret with name secret-test-38f89fa7-f107-4811-8089-d15a6caab8ff 07/29/23 15:26:46.741
    STEP: Creating a pod to test consume secrets 07/29/23 15:26:46.75
    Jul 29 15:26:46.762: INFO: Waiting up to 5m0s for pod "pod-secrets-cdfd2c7d-ac3e-4712-a579-7db6ad3f71db" in namespace "secrets-789" to be "Succeeded or Failed"
    Jul 29 15:26:46.772: INFO: Pod "pod-secrets-cdfd2c7d-ac3e-4712-a579-7db6ad3f71db": Phase="Pending", Reason="", readiness=false. Elapsed: 10.164185ms
    Jul 29 15:26:48.779: INFO: Pod "pod-secrets-cdfd2c7d-ac3e-4712-a579-7db6ad3f71db": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017086351s
    Jul 29 15:26:50.780: INFO: Pod "pod-secrets-cdfd2c7d-ac3e-4712-a579-7db6ad3f71db": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018527331s
    STEP: Saw pod success 07/29/23 15:26:50.781
    Jul 29 15:26:50.781: INFO: Pod "pod-secrets-cdfd2c7d-ac3e-4712-a579-7db6ad3f71db" satisfied condition "Succeeded or Failed"
    Jul 29 15:26:50.790: INFO: Trying to get logs from node wa4quivohpee-3 pod pod-secrets-cdfd2c7d-ac3e-4712-a579-7db6ad3f71db container secret-volume-test: <nil>
    STEP: delete the pod 07/29/23 15:26:50.809
    Jul 29 15:26:50.828: INFO: Waiting for pod pod-secrets-cdfd2c7d-ac3e-4712-a579-7db6ad3f71db to disappear
    Jul 29 15:26:50.834: INFO: Pod pod-secrets-cdfd2c7d-ac3e-4712-a579-7db6ad3f71db no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jul 29 15:26:50.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-789" for this suite. 07/29/23 15:26:50.844
    STEP: Destroying namespace "secret-namespace-1738" for this suite. 07/29/23 15:26:50.861
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:26:50.877
Jul 29 15:26:50.877: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename resourcequota 07/29/23 15:26:50.881
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:26:50.911
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:26:50.919
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220
STEP: Counting existing ResourceQuota 07/29/23 15:26:50.924
STEP: Creating a ResourceQuota 07/29/23 15:26:55.94
STEP: Ensuring resource quota status is calculated 07/29/23 15:26:55.952
STEP: Creating a Pod that fits quota 07/29/23 15:26:57.96
STEP: Ensuring ResourceQuota status captures the pod usage 07/29/23 15:26:57.989
STEP: Not allowing a pod to be created that exceeds remaining quota 07/29/23 15:27:00
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 07/29/23 15:27:00.006
STEP: Ensuring a pod cannot update its resource requirements 07/29/23 15:27:00.01
STEP: Ensuring attempts to update pod resource requirements did not change quota usage 07/29/23 15:27:00.022
STEP: Deleting the pod 07/29/23 15:27:02.035
STEP: Ensuring resource quota status released the pod usage 07/29/23 15:27:02.063
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jul 29 15:27:04.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8656" for this suite. 07/29/23 15:27:04.084
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","completed":9,"skipped":191,"failed":0}
------------------------------
â€¢ [SLOW TEST] [13.221 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:26:50.877
    Jul 29 15:26:50.877: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename resourcequota 07/29/23 15:26:50.881
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:26:50.911
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:26:50.919
    [It] should create a ResourceQuota and capture the life of a pod. [Conformance]
      test/e2e/apimachinery/resource_quota.go:220
    STEP: Counting existing ResourceQuota 07/29/23 15:26:50.924
    STEP: Creating a ResourceQuota 07/29/23 15:26:55.94
    STEP: Ensuring resource quota status is calculated 07/29/23 15:26:55.952
    STEP: Creating a Pod that fits quota 07/29/23 15:26:57.96
    STEP: Ensuring ResourceQuota status captures the pod usage 07/29/23 15:26:57.989
    STEP: Not allowing a pod to be created that exceeds remaining quota 07/29/23 15:27:00
    STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 07/29/23 15:27:00.006
    STEP: Ensuring a pod cannot update its resource requirements 07/29/23 15:27:00.01
    STEP: Ensuring attempts to update pod resource requirements did not change quota usage 07/29/23 15:27:00.022
    STEP: Deleting the pod 07/29/23 15:27:02.035
    STEP: Ensuring resource quota status released the pod usage 07/29/23 15:27:02.063
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jul 29 15:27:04.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-8656" for this suite. 07/29/23 15:27:04.084
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:27:04.099
Jul 29 15:27:04.099: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename secrets 07/29/23 15:27:04.104
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:27:04.13
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:27:04.137
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94
STEP: creating secret secrets-3625/secret-test-9320447a-f27d-481c-98fc-4506479b549e 07/29/23 15:27:04.142
STEP: Creating a pod to test consume secrets 07/29/23 15:27:04.149
Jul 29 15:27:04.164: INFO: Waiting up to 5m0s for pod "pod-configmaps-85fdb08f-5fe8-41c4-ac30-238a438eefa6" in namespace "secrets-3625" to be "Succeeded or Failed"
Jul 29 15:27:04.171: INFO: Pod "pod-configmaps-85fdb08f-5fe8-41c4-ac30-238a438eefa6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.782018ms
Jul 29 15:27:06.178: INFO: Pod "pod-configmaps-85fdb08f-5fe8-41c4-ac30-238a438eefa6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013698671s
Jul 29 15:27:08.179: INFO: Pod "pod-configmaps-85fdb08f-5fe8-41c4-ac30-238a438eefa6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014877538s
STEP: Saw pod success 07/29/23 15:27:08.179
Jul 29 15:27:08.180: INFO: Pod "pod-configmaps-85fdb08f-5fe8-41c4-ac30-238a438eefa6" satisfied condition "Succeeded or Failed"
Jul 29 15:27:08.185: INFO: Trying to get logs from node wa4quivohpee-3 pod pod-configmaps-85fdb08f-5fe8-41c4-ac30-238a438eefa6 container env-test: <nil>
STEP: delete the pod 07/29/23 15:27:08.196
Jul 29 15:27:08.215: INFO: Waiting for pod pod-configmaps-85fdb08f-5fe8-41c4-ac30-238a438eefa6 to disappear
Jul 29 15:27:08.223: INFO: Pod pod-configmaps-85fdb08f-5fe8-41c4-ac30-238a438eefa6 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Jul 29 15:27:08.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3625" for this suite. 07/29/23 15:27:08.232
{"msg":"PASSED [sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]","completed":10,"skipped":202,"failed":0}
------------------------------
â€¢ [4.148 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:27:04.099
    Jul 29 15:27:04.099: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename secrets 07/29/23 15:27:04.104
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:27:04.13
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:27:04.137
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:94
    STEP: creating secret secrets-3625/secret-test-9320447a-f27d-481c-98fc-4506479b549e 07/29/23 15:27:04.142
    STEP: Creating a pod to test consume secrets 07/29/23 15:27:04.149
    Jul 29 15:27:04.164: INFO: Waiting up to 5m0s for pod "pod-configmaps-85fdb08f-5fe8-41c4-ac30-238a438eefa6" in namespace "secrets-3625" to be "Succeeded or Failed"
    Jul 29 15:27:04.171: INFO: Pod "pod-configmaps-85fdb08f-5fe8-41c4-ac30-238a438eefa6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.782018ms
    Jul 29 15:27:06.178: INFO: Pod "pod-configmaps-85fdb08f-5fe8-41c4-ac30-238a438eefa6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013698671s
    Jul 29 15:27:08.179: INFO: Pod "pod-configmaps-85fdb08f-5fe8-41c4-ac30-238a438eefa6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014877538s
    STEP: Saw pod success 07/29/23 15:27:08.179
    Jul 29 15:27:08.180: INFO: Pod "pod-configmaps-85fdb08f-5fe8-41c4-ac30-238a438eefa6" satisfied condition "Succeeded or Failed"
    Jul 29 15:27:08.185: INFO: Trying to get logs from node wa4quivohpee-3 pod pod-configmaps-85fdb08f-5fe8-41c4-ac30-238a438eefa6 container env-test: <nil>
    STEP: delete the pod 07/29/23 15:27:08.196
    Jul 29 15:27:08.215: INFO: Waiting for pod pod-configmaps-85fdb08f-5fe8-41c4-ac30-238a438eefa6 to disappear
    Jul 29 15:27:08.223: INFO: Pod pod-configmaps-85fdb08f-5fe8-41c4-ac30-238a438eefa6 no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Jul 29 15:27:08.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-3625" for this suite. 07/29/23 15:27:08.232
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:27:08.249
Jul 29 15:27:08.249: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename secrets 07/29/23 15:27:08.254
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:27:08.289
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:27:08.295
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78
STEP: Creating secret with name secret-test-map-e48784ab-d060-415e-92aa-15d719f3fa41 07/29/23 15:27:08.299
STEP: Creating a pod to test consume secrets 07/29/23 15:27:08.307
Jul 29 15:27:08.319: INFO: Waiting up to 5m0s for pod "pod-secrets-8266e037-cb66-4566-95ef-fc16e4ffa19c" in namespace "secrets-7332" to be "Succeeded or Failed"
Jul 29 15:27:08.333: INFO: Pod "pod-secrets-8266e037-cb66-4566-95ef-fc16e4ffa19c": Phase="Pending", Reason="", readiness=false. Elapsed: 13.128947ms
Jul 29 15:27:10.340: INFO: Pod "pod-secrets-8266e037-cb66-4566-95ef-fc16e4ffa19c": Phase="Running", Reason="", readiness=false. Elapsed: 2.020143489s
Jul 29 15:27:12.345: INFO: Pod "pod-secrets-8266e037-cb66-4566-95ef-fc16e4ffa19c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025892143s
STEP: Saw pod success 07/29/23 15:27:12.345
Jul 29 15:27:12.346: INFO: Pod "pod-secrets-8266e037-cb66-4566-95ef-fc16e4ffa19c" satisfied condition "Succeeded or Failed"
Jul 29 15:27:12.359: INFO: Trying to get logs from node wa4quivohpee-3 pod pod-secrets-8266e037-cb66-4566-95ef-fc16e4ffa19c container secret-volume-test: <nil>
STEP: delete the pod 07/29/23 15:27:12.374
Jul 29 15:27:12.398: INFO: Waiting for pod pod-secrets-8266e037-cb66-4566-95ef-fc16e4ffa19c to disappear
Jul 29 15:27:12.406: INFO: Pod pod-secrets-8266e037-cb66-4566-95ef-fc16e4ffa19c no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jul 29 15:27:12.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7332" for this suite. 07/29/23 15:27:12.414
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":11,"skipped":205,"failed":0}
------------------------------
â€¢ [4.179 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:27:08.249
    Jul 29 15:27:08.249: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename secrets 07/29/23 15:27:08.254
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:27:08.289
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:27:08.295
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:78
    STEP: Creating secret with name secret-test-map-e48784ab-d060-415e-92aa-15d719f3fa41 07/29/23 15:27:08.299
    STEP: Creating a pod to test consume secrets 07/29/23 15:27:08.307
    Jul 29 15:27:08.319: INFO: Waiting up to 5m0s for pod "pod-secrets-8266e037-cb66-4566-95ef-fc16e4ffa19c" in namespace "secrets-7332" to be "Succeeded or Failed"
    Jul 29 15:27:08.333: INFO: Pod "pod-secrets-8266e037-cb66-4566-95ef-fc16e4ffa19c": Phase="Pending", Reason="", readiness=false. Elapsed: 13.128947ms
    Jul 29 15:27:10.340: INFO: Pod "pod-secrets-8266e037-cb66-4566-95ef-fc16e4ffa19c": Phase="Running", Reason="", readiness=false. Elapsed: 2.020143489s
    Jul 29 15:27:12.345: INFO: Pod "pod-secrets-8266e037-cb66-4566-95ef-fc16e4ffa19c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025892143s
    STEP: Saw pod success 07/29/23 15:27:12.345
    Jul 29 15:27:12.346: INFO: Pod "pod-secrets-8266e037-cb66-4566-95ef-fc16e4ffa19c" satisfied condition "Succeeded or Failed"
    Jul 29 15:27:12.359: INFO: Trying to get logs from node wa4quivohpee-3 pod pod-secrets-8266e037-cb66-4566-95ef-fc16e4ffa19c container secret-volume-test: <nil>
    STEP: delete the pod 07/29/23 15:27:12.374
    Jul 29 15:27:12.398: INFO: Waiting for pod pod-secrets-8266e037-cb66-4566-95ef-fc16e4ffa19c to disappear
    Jul 29 15:27:12.406: INFO: Pod pod-secrets-8266e037-cb66-4566-95ef-fc16e4ffa19c no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jul 29 15:27:12.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-7332" for this suite. 07/29/23 15:27:12.414
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Pods
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:27:12.432
Jul 29 15:27:12.433: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename pods 07/29/23 15:27:12.436
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:27:12.473
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:27:12.477
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203
STEP: creating pod 07/29/23 15:27:12.484
Jul 29 15:27:12.532: INFO: Waiting up to 5m0s for pod "pod-hostip-a383c7b5-adf4-47da-b0e4-ae0047eb773a" in namespace "pods-901" to be "running and ready"
Jul 29 15:27:12.544: INFO: Pod "pod-hostip-a383c7b5-adf4-47da-b0e4-ae0047eb773a": Phase="Pending", Reason="", readiness=false. Elapsed: 12.135465ms
Jul 29 15:27:12.544: INFO: The phase of Pod pod-hostip-a383c7b5-adf4-47da-b0e4-ae0047eb773a is Pending, waiting for it to be Running (with Ready = true)
Jul 29 15:27:14.552: INFO: Pod "pod-hostip-a383c7b5-adf4-47da-b0e4-ae0047eb773a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020039432s
Jul 29 15:27:14.552: INFO: The phase of Pod pod-hostip-a383c7b5-adf4-47da-b0e4-ae0047eb773a is Pending, waiting for it to be Running (with Ready = true)
Jul 29 15:27:16.551: INFO: Pod "pod-hostip-a383c7b5-adf4-47da-b0e4-ae0047eb773a": Phase="Running", Reason="", readiness=true. Elapsed: 4.018895945s
Jul 29 15:27:16.551: INFO: The phase of Pod pod-hostip-a383c7b5-adf4-47da-b0e4-ae0047eb773a is Running (Ready = true)
Jul 29 15:27:16.551: INFO: Pod "pod-hostip-a383c7b5-adf4-47da-b0e4-ae0047eb773a" satisfied condition "running and ready"
Jul 29 15:27:16.562: INFO: Pod pod-hostip-a383c7b5-adf4-47da-b0e4-ae0047eb773a has hostIP: 192.168.121.234
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jul 29 15:27:16.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-901" for this suite. 07/29/23 15:27:16.57
{"msg":"PASSED [sig-node] Pods should get a host IP [NodeConformance] [Conformance]","completed":12,"skipped":205,"failed":0}
------------------------------
â€¢ [4.149 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:27:12.432
    Jul 29 15:27:12.433: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename pods 07/29/23 15:27:12.436
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:27:12.473
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:27:12.477
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should get a host IP [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:203
    STEP: creating pod 07/29/23 15:27:12.484
    Jul 29 15:27:12.532: INFO: Waiting up to 5m0s for pod "pod-hostip-a383c7b5-adf4-47da-b0e4-ae0047eb773a" in namespace "pods-901" to be "running and ready"
    Jul 29 15:27:12.544: INFO: Pod "pod-hostip-a383c7b5-adf4-47da-b0e4-ae0047eb773a": Phase="Pending", Reason="", readiness=false. Elapsed: 12.135465ms
    Jul 29 15:27:12.544: INFO: The phase of Pod pod-hostip-a383c7b5-adf4-47da-b0e4-ae0047eb773a is Pending, waiting for it to be Running (with Ready = true)
    Jul 29 15:27:14.552: INFO: Pod "pod-hostip-a383c7b5-adf4-47da-b0e4-ae0047eb773a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020039432s
    Jul 29 15:27:14.552: INFO: The phase of Pod pod-hostip-a383c7b5-adf4-47da-b0e4-ae0047eb773a is Pending, waiting for it to be Running (with Ready = true)
    Jul 29 15:27:16.551: INFO: Pod "pod-hostip-a383c7b5-adf4-47da-b0e4-ae0047eb773a": Phase="Running", Reason="", readiness=true. Elapsed: 4.018895945s
    Jul 29 15:27:16.551: INFO: The phase of Pod pod-hostip-a383c7b5-adf4-47da-b0e4-ae0047eb773a is Running (Ready = true)
    Jul 29 15:27:16.551: INFO: Pod "pod-hostip-a383c7b5-adf4-47da-b0e4-ae0047eb773a" satisfied condition "running and ready"
    Jul 29 15:27:16.562: INFO: Pod pod-hostip-a383c7b5-adf4-47da-b0e4-ae0047eb773a has hostIP: 192.168.121.234
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jul 29 15:27:16.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-901" for this suite. 07/29/23 15:27:16.57
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:27:16.584
Jul 29 15:27:16.584: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename resourcequota 07/29/23 15:27:16.587
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:27:16.681
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:27:16.686
[It] should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933
STEP: Creating a ResourceQuota 07/29/23 15:27:16.693
STEP: Getting a ResourceQuota 07/29/23 15:27:16.703
STEP: Listing all ResourceQuotas with LabelSelector 07/29/23 15:27:16.712
STEP: Patching the ResourceQuota 07/29/23 15:27:16.722
STEP: Deleting a Collection of ResourceQuotas 07/29/23 15:27:16.738
STEP: Verifying the deleted ResourceQuota 07/29/23 15:27:16.762
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jul 29 15:27:16.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8602" for this suite. 07/29/23 15:27:16.774
{"msg":"PASSED [sig-api-machinery] ResourceQuota should manage the lifecycle of a ResourceQuota [Conformance]","completed":13,"skipped":214,"failed":0}
------------------------------
â€¢ [0.206 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:27:16.584
    Jul 29 15:27:16.584: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename resourcequota 07/29/23 15:27:16.587
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:27:16.681
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:27:16.686
    [It] should manage the lifecycle of a ResourceQuota [Conformance]
      test/e2e/apimachinery/resource_quota.go:933
    STEP: Creating a ResourceQuota 07/29/23 15:27:16.693
    STEP: Getting a ResourceQuota 07/29/23 15:27:16.703
    STEP: Listing all ResourceQuotas with LabelSelector 07/29/23 15:27:16.712
    STEP: Patching the ResourceQuota 07/29/23 15:27:16.722
    STEP: Deleting a Collection of ResourceQuotas 07/29/23 15:27:16.738
    STEP: Verifying the deleted ResourceQuota 07/29/23 15:27:16.762
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jul 29 15:27:16.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-8602" for this suite. 07/29/23 15:27:16.774
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:27:16.792
Jul 29 15:27:16.793: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename resourcequota 07/29/23 15:27:16.796
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:27:16.825
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:27:16.831
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150
STEP: Discovering how many secrets are in namespace by default 07/29/23 15:27:16.835
STEP: Counting existing ResourceQuota 07/29/23 15:27:21.842
STEP: Creating a ResourceQuota 07/29/23 15:27:26.851
STEP: Ensuring resource quota status is calculated 07/29/23 15:27:26.861
STEP: Creating a Secret 07/29/23 15:27:28.872
STEP: Ensuring resource quota status captures secret creation 07/29/23 15:27:28.896
STEP: Deleting a secret 07/29/23 15:27:30.904
STEP: Ensuring resource quota status released usage 07/29/23 15:27:30.919
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jul 29 15:27:32.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7378" for this suite. 07/29/23 15:27:32.936
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","completed":14,"skipped":215,"failed":0}
------------------------------
â€¢ [SLOW TEST] [16.153 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:27:16.792
    Jul 29 15:27:16.793: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename resourcequota 07/29/23 15:27:16.796
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:27:16.825
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:27:16.831
    [It] should create a ResourceQuota and capture the life of a secret. [Conformance]
      test/e2e/apimachinery/resource_quota.go:150
    STEP: Discovering how many secrets are in namespace by default 07/29/23 15:27:16.835
    STEP: Counting existing ResourceQuota 07/29/23 15:27:21.842
    STEP: Creating a ResourceQuota 07/29/23 15:27:26.851
    STEP: Ensuring resource quota status is calculated 07/29/23 15:27:26.861
    STEP: Creating a Secret 07/29/23 15:27:28.872
    STEP: Ensuring resource quota status captures secret creation 07/29/23 15:27:28.896
    STEP: Deleting a secret 07/29/23 15:27:30.904
    STEP: Ensuring resource quota status released usage 07/29/23 15:27:30.919
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jul 29 15:27:32.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-7378" for this suite. 07/29/23 15:27:32.936
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:27:32.949
Jul 29 15:27:32.950: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename container-probe 07/29/23 15:27:32.952
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:27:32.986
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:27:32.991
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211
STEP: Creating pod test-webserver-be1c881e-7f4d-4fa7-971b-7e6c097ae84c in namespace container-probe-6973 07/29/23 15:27:32.996
Jul 29 15:27:33.016: INFO: Waiting up to 5m0s for pod "test-webserver-be1c881e-7f4d-4fa7-971b-7e6c097ae84c" in namespace "container-probe-6973" to be "not pending"
Jul 29 15:27:33.031: INFO: Pod "test-webserver-be1c881e-7f4d-4fa7-971b-7e6c097ae84c": Phase="Pending", Reason="", readiness=false. Elapsed: 14.54731ms
Jul 29 15:27:35.042: INFO: Pod "test-webserver-be1c881e-7f4d-4fa7-971b-7e6c097ae84c": Phase="Running", Reason="", readiness=true. Elapsed: 2.02613042s
Jul 29 15:27:35.043: INFO: Pod "test-webserver-be1c881e-7f4d-4fa7-971b-7e6c097ae84c" satisfied condition "not pending"
Jul 29 15:27:35.043: INFO: Started pod test-webserver-be1c881e-7f4d-4fa7-971b-7e6c097ae84c in namespace container-probe-6973
STEP: checking the pod's current state and verifying that restartCount is present 07/29/23 15:27:35.043
Jul 29 15:27:35.049: INFO: Initial restart count of pod test-webserver-be1c881e-7f4d-4fa7-971b-7e6c097ae84c is 0
STEP: deleting the pod 07/29/23 15:31:36.233
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jul 29 15:31:36.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6973" for this suite. 07/29/23 15:31:36.271
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","completed":15,"skipped":251,"failed":0}
------------------------------
â€¢ [SLOW TEST] [243.340 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:27:32.949
    Jul 29 15:27:32.950: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename container-probe 07/29/23 15:27:32.952
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:27:32.986
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:27:32.991
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:211
    STEP: Creating pod test-webserver-be1c881e-7f4d-4fa7-971b-7e6c097ae84c in namespace container-probe-6973 07/29/23 15:27:32.996
    Jul 29 15:27:33.016: INFO: Waiting up to 5m0s for pod "test-webserver-be1c881e-7f4d-4fa7-971b-7e6c097ae84c" in namespace "container-probe-6973" to be "not pending"
    Jul 29 15:27:33.031: INFO: Pod "test-webserver-be1c881e-7f4d-4fa7-971b-7e6c097ae84c": Phase="Pending", Reason="", readiness=false. Elapsed: 14.54731ms
    Jul 29 15:27:35.042: INFO: Pod "test-webserver-be1c881e-7f4d-4fa7-971b-7e6c097ae84c": Phase="Running", Reason="", readiness=true. Elapsed: 2.02613042s
    Jul 29 15:27:35.043: INFO: Pod "test-webserver-be1c881e-7f4d-4fa7-971b-7e6c097ae84c" satisfied condition "not pending"
    Jul 29 15:27:35.043: INFO: Started pod test-webserver-be1c881e-7f4d-4fa7-971b-7e6c097ae84c in namespace container-probe-6973
    STEP: checking the pod's current state and verifying that restartCount is present 07/29/23 15:27:35.043
    Jul 29 15:27:35.049: INFO: Initial restart count of pod test-webserver-be1c881e-7f4d-4fa7-971b-7e6c097ae84c is 0
    STEP: deleting the pod 07/29/23 15:31:36.233
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jul 29 15:31:36.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-6973" for this suite. 07/29/23 15:31:36.271
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:31:36.292
Jul 29 15:31:36.292: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename webhook 07/29/23 15:31:36.297
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:31:36.357
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:31:36.365
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 07/29/23 15:31:36.398
STEP: Create role binding to let webhook read extension-apiserver-authentication 07/29/23 15:31:37.397
STEP: Deploying the webhook pod 07/29/23 15:31:37.415
STEP: Wait for the deployment to be ready 07/29/23 15:31:37.435
Jul 29 15:31:37.458: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 07/29/23 15:31:39.482
STEP: Verifying the service has paired with the endpoint 07/29/23 15:31:39.501
Jul 29 15:31:40.502: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412
STEP: Creating a validating webhook configuration 07/29/23 15:31:40.515
Jul 29 15:31:40.547: INFO: Waiting for webhook configuration to be ready...
STEP: Creating a configMap that does not comply to the validation webhook rules 07/29/23 15:31:40.668
STEP: Updating a validating webhook configuration's rules to not include the create operation 07/29/23 15:31:40.684
STEP: Creating a configMap that does not comply to the validation webhook rules 07/29/23 15:31:40.701
STEP: Patching a validating webhook configuration's rules to include the create operation 07/29/23 15:31:40.727
STEP: Creating a configMap that does not comply to the validation webhook rules 07/29/23 15:31:40.741
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jul 29 15:31:40.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6973" for this suite. 07/29/23 15:31:40.895
STEP: Destroying namespace "webhook-6973-markers" for this suite. 07/29/23 15:31:40.907
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","completed":16,"skipped":268,"failed":0}
------------------------------
â€¢ [4.716 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:31:36.292
    Jul 29 15:31:36.292: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename webhook 07/29/23 15:31:36.297
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:31:36.357
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:31:36.365
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 07/29/23 15:31:36.398
    STEP: Create role binding to let webhook read extension-apiserver-authentication 07/29/23 15:31:37.397
    STEP: Deploying the webhook pod 07/29/23 15:31:37.415
    STEP: Wait for the deployment to be ready 07/29/23 15:31:37.435
    Jul 29 15:31:37.458: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 07/29/23 15:31:39.482
    STEP: Verifying the service has paired with the endpoint 07/29/23 15:31:39.501
    Jul 29 15:31:40.502: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a validating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:412
    STEP: Creating a validating webhook configuration 07/29/23 15:31:40.515
    Jul 29 15:31:40.547: INFO: Waiting for webhook configuration to be ready...
    STEP: Creating a configMap that does not comply to the validation webhook rules 07/29/23 15:31:40.668
    STEP: Updating a validating webhook configuration's rules to not include the create operation 07/29/23 15:31:40.684
    STEP: Creating a configMap that does not comply to the validation webhook rules 07/29/23 15:31:40.701
    STEP: Patching a validating webhook configuration's rules to include the create operation 07/29/23 15:31:40.727
    STEP: Creating a configMap that does not comply to the validation webhook rules 07/29/23 15:31:40.741
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jul 29 15:31:40.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-6973" for this suite. 07/29/23 15:31:40.895
    STEP: Destroying namespace "webhook-6973-markers" for this suite. 07/29/23 15:31:40.907
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Services
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:781
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:31:41.012
Jul 29 15:31:41.012: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename services 07/29/23 15:31:41.016
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:31:41.06
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:31:41.067
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should provide secure master service  [Conformance]
  test/e2e/network/service.go:781
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jul 29 15:31:41.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9258" for this suite. 07/29/23 15:31:41.095
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","completed":17,"skipped":268,"failed":0}
------------------------------
â€¢ [0.099 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:781

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:31:41.012
    Jul 29 15:31:41.012: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename services 07/29/23 15:31:41.016
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:31:41.06
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:31:41.067
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should provide secure master service  [Conformance]
      test/e2e/network/service.go:781
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jul 29 15:31:41.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-9258" for this suite. 07/29/23 15:31:41.095
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:31:41.13
Jul 29 15:31:41.130: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename projected 07/29/23 15:31:41.133
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:31:41.233
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:31:41.239
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66
STEP: Creating projection with secret that has name projected-secret-test-b89fd5bd-00c6-4096-8e0b-8b80d290f5d0 07/29/23 15:31:41.245
STEP: Creating a pod to test consume secrets 07/29/23 15:31:41.265
Jul 29 15:31:41.301: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-3b67bda5-5829-4ed1-aa51-f5cef07f9720" in namespace "projected-6081" to be "Succeeded or Failed"
Jul 29 15:31:41.313: INFO: Pod "pod-projected-secrets-3b67bda5-5829-4ed1-aa51-f5cef07f9720": Phase="Pending", Reason="", readiness=false. Elapsed: 11.586377ms
Jul 29 15:31:43.319: INFO: Pod "pod-projected-secrets-3b67bda5-5829-4ed1-aa51-f5cef07f9720": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017935091s
Jul 29 15:31:45.329: INFO: Pod "pod-projected-secrets-3b67bda5-5829-4ed1-aa51-f5cef07f9720": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028301493s
STEP: Saw pod success 07/29/23 15:31:45.33
Jul 29 15:31:45.330: INFO: Pod "pod-projected-secrets-3b67bda5-5829-4ed1-aa51-f5cef07f9720" satisfied condition "Succeeded or Failed"
Jul 29 15:31:45.337: INFO: Trying to get logs from node wa4quivohpee-3 pod pod-projected-secrets-3b67bda5-5829-4ed1-aa51-f5cef07f9720 container projected-secret-volume-test: <nil>
STEP: delete the pod 07/29/23 15:31:45.374
Jul 29 15:31:45.399: INFO: Waiting for pod pod-projected-secrets-3b67bda5-5829-4ed1-aa51-f5cef07f9720 to disappear
Jul 29 15:31:45.405: INFO: Pod pod-projected-secrets-3b67bda5-5829-4ed1-aa51-f5cef07f9720 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Jul 29 15:31:45.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6081" for this suite. 07/29/23 15:31:45.418
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","completed":18,"skipped":323,"failed":0}
------------------------------
â€¢ [4.299 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:31:41.13
    Jul 29 15:31:41.130: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename projected 07/29/23 15:31:41.133
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:31:41.233
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:31:41.239
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:66
    STEP: Creating projection with secret that has name projected-secret-test-b89fd5bd-00c6-4096-8e0b-8b80d290f5d0 07/29/23 15:31:41.245
    STEP: Creating a pod to test consume secrets 07/29/23 15:31:41.265
    Jul 29 15:31:41.301: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-3b67bda5-5829-4ed1-aa51-f5cef07f9720" in namespace "projected-6081" to be "Succeeded or Failed"
    Jul 29 15:31:41.313: INFO: Pod "pod-projected-secrets-3b67bda5-5829-4ed1-aa51-f5cef07f9720": Phase="Pending", Reason="", readiness=false. Elapsed: 11.586377ms
    Jul 29 15:31:43.319: INFO: Pod "pod-projected-secrets-3b67bda5-5829-4ed1-aa51-f5cef07f9720": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017935091s
    Jul 29 15:31:45.329: INFO: Pod "pod-projected-secrets-3b67bda5-5829-4ed1-aa51-f5cef07f9720": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028301493s
    STEP: Saw pod success 07/29/23 15:31:45.33
    Jul 29 15:31:45.330: INFO: Pod "pod-projected-secrets-3b67bda5-5829-4ed1-aa51-f5cef07f9720" satisfied condition "Succeeded or Failed"
    Jul 29 15:31:45.337: INFO: Trying to get logs from node wa4quivohpee-3 pod pod-projected-secrets-3b67bda5-5829-4ed1-aa51-f5cef07f9720 container projected-secret-volume-test: <nil>
    STEP: delete the pod 07/29/23 15:31:45.374
    Jul 29 15:31:45.399: INFO: Waiting for pod pod-projected-secrets-3b67bda5-5829-4ed1-aa51-f5cef07f9720 to disappear
    Jul 29 15:31:45.405: INFO: Pod pod-projected-secrets-3b67bda5-5829-4ed1-aa51-f5cef07f9720 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Jul 29 15:31:45.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6081" for this suite. 07/29/23 15:31:45.418
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:31:45.434
Jul 29 15:31:45.434: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename watch 07/29/23 15:31:45.436
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:31:45.471
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:31:45.476
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
STEP: creating a watch on configmaps 07/29/23 15:31:45.482
STEP: creating a new configmap 07/29/23 15:31:45.485
STEP: modifying the configmap once 07/29/23 15:31:45.493
STEP: closing the watch once it receives two notifications 07/29/23 15:31:45.513
Jul 29 15:31:45.513: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-239  3bced795-2c72-4bb7-8e80-23114a36b9e8 3425 0 2023-07-29 15:31:45 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-07-29 15:31:45 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 29 15:31:45.515: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-239  3bced795-2c72-4bb7-8e80-23114a36b9e8 3426 0 2023-07-29 15:31:45 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-07-29 15:31:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed 07/29/23 15:31:45.515
STEP: creating a new watch on configmaps from the last resource version observed by the first watch 07/29/23 15:31:45.529
STEP: deleting the configmap 07/29/23 15:31:45.533
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 07/29/23 15:31:45.548
Jul 29 15:31:45.549: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-239  3bced795-2c72-4bb7-8e80-23114a36b9e8 3427 0 2023-07-29 15:31:45 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-07-29 15:31:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 29 15:31:45.550: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-239  3bced795-2c72-4bb7-8e80-23114a36b9e8 3428 0 2023-07-29 15:31:45 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-07-29 15:31:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Jul 29 15:31:45.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-239" for this suite. 07/29/23 15:31:45.56
{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","completed":19,"skipped":333,"failed":0}
------------------------------
â€¢ [0.143 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:31:45.434
    Jul 29 15:31:45.434: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename watch 07/29/23 15:31:45.436
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:31:45.471
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:31:45.476
    [It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
      test/e2e/apimachinery/watch.go:191
    STEP: creating a watch on configmaps 07/29/23 15:31:45.482
    STEP: creating a new configmap 07/29/23 15:31:45.485
    STEP: modifying the configmap once 07/29/23 15:31:45.493
    STEP: closing the watch once it receives two notifications 07/29/23 15:31:45.513
    Jul 29 15:31:45.513: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-239  3bced795-2c72-4bb7-8e80-23114a36b9e8 3425 0 2023-07-29 15:31:45 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-07-29 15:31:45 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jul 29 15:31:45.515: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-239  3bced795-2c72-4bb7-8e80-23114a36b9e8 3426 0 2023-07-29 15:31:45 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-07-29 15:31:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time, while the watch is closed 07/29/23 15:31:45.515
    STEP: creating a new watch on configmaps from the last resource version observed by the first watch 07/29/23 15:31:45.529
    STEP: deleting the configmap 07/29/23 15:31:45.533
    STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 07/29/23 15:31:45.548
    Jul 29 15:31:45.549: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-239  3bced795-2c72-4bb7-8e80-23114a36b9e8 3427 0 2023-07-29 15:31:45 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-07-29 15:31:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jul 29 15:31:45.550: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-239  3bced795-2c72-4bb7-8e80-23114a36b9e8 3428 0 2023-07-29 15:31:45 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-07-29 15:31:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Jul 29 15:31:45.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-239" for this suite. 07/29/23 15:31:45.56
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:31:45.581
Jul 29 15:31:45.581: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename services 07/29/23 15:31:45.583
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:31:45.667
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:31:45.676
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268
STEP: creating service nodeport-test with type=NodePort in namespace services-942 07/29/23 15:31:45.686
STEP: creating replication controller nodeport-test in namespace services-942 07/29/23 15:31:45.713
I0729 15:31:45.729737      13 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-942, replica count: 2
I0729 15:31:48.782382      13 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 29 15:31:48.782: INFO: Creating new exec pod
Jul 29 15:31:48.806: INFO: Waiting up to 5m0s for pod "execpod7wxrv" in namespace "services-942" to be "running"
Jul 29 15:31:48.821: INFO: Pod "execpod7wxrv": Phase="Pending", Reason="", readiness=false. Elapsed: 13.983915ms
Jul 29 15:31:50.833: INFO: Pod "execpod7wxrv": Phase="Running", Reason="", readiness=true. Elapsed: 2.026614304s
Jul 29 15:31:50.833: INFO: Pod "execpod7wxrv" satisfied condition "running"
Jul 29 15:31:51.842: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=services-942 exec execpod7wxrv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jul 29 15:31:52.153: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jul 29 15:31:52.153: INFO: stdout: "nodeport-test-sv4vv"
Jul 29 15:31:52.153: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=services-942 exec execpod7wxrv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.19.175 80'
Jul 29 15:31:52.415: INFO: stderr: "+ echo+  hostNamenc\n -v -t -w 2 10.233.19.175 80\nConnection to 10.233.19.175 80 port [tcp/http] succeeded!\n"
Jul 29 15:31:52.415: INFO: stdout: "nodeport-test-sv4vv"
Jul 29 15:31:52.416: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=services-942 exec execpod7wxrv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.206 30254'
Jul 29 15:31:52.726: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.206 30254\nConnection to 192.168.121.206 30254 port [tcp/*] succeeded!\n"
Jul 29 15:31:52.726: INFO: stdout: "nodeport-test-f5rb6"
Jul 29 15:31:52.727: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=services-942 exec execpod7wxrv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.28 30254'
Jul 29 15:31:52.955: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.28 30254\nConnection to 192.168.121.28 30254 port [tcp/*] succeeded!\n"
Jul 29 15:31:52.956: INFO: stdout: ""
Jul 29 15:31:53.956: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=services-942 exec execpod7wxrv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.28 30254'
Jul 29 15:31:54.240: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.28 30254\nConnection to 192.168.121.28 30254 port [tcp/*] succeeded!\n"
Jul 29 15:31:54.240: INFO: stdout: "nodeport-test-f5rb6"
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jul 29 15:31:54.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-942" for this suite. 07/29/23 15:31:54.249
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","completed":20,"skipped":351,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.685 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:31:45.581
    Jul 29 15:31:45.581: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename services 07/29/23 15:31:45.583
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:31:45.667
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:31:45.676
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to create a functioning NodePort service [Conformance]
      test/e2e/network/service.go:1268
    STEP: creating service nodeport-test with type=NodePort in namespace services-942 07/29/23 15:31:45.686
    STEP: creating replication controller nodeport-test in namespace services-942 07/29/23 15:31:45.713
    I0729 15:31:45.729737      13 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-942, replica count: 2
    I0729 15:31:48.782382      13 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jul 29 15:31:48.782: INFO: Creating new exec pod
    Jul 29 15:31:48.806: INFO: Waiting up to 5m0s for pod "execpod7wxrv" in namespace "services-942" to be "running"
    Jul 29 15:31:48.821: INFO: Pod "execpod7wxrv": Phase="Pending", Reason="", readiness=false. Elapsed: 13.983915ms
    Jul 29 15:31:50.833: INFO: Pod "execpod7wxrv": Phase="Running", Reason="", readiness=true. Elapsed: 2.026614304s
    Jul 29 15:31:50.833: INFO: Pod "execpod7wxrv" satisfied condition "running"
    Jul 29 15:31:51.842: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=services-942 exec execpod7wxrv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jul 29 15:31:52.153: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jul 29 15:31:52.153: INFO: stdout: "nodeport-test-sv4vv"
    Jul 29 15:31:52.153: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=services-942 exec execpod7wxrv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.19.175 80'
    Jul 29 15:31:52.415: INFO: stderr: "+ echo+  hostNamenc\n -v -t -w 2 10.233.19.175 80\nConnection to 10.233.19.175 80 port [tcp/http] succeeded!\n"
    Jul 29 15:31:52.415: INFO: stdout: "nodeport-test-sv4vv"
    Jul 29 15:31:52.416: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=services-942 exec execpod7wxrv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.206 30254'
    Jul 29 15:31:52.726: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.206 30254\nConnection to 192.168.121.206 30254 port [tcp/*] succeeded!\n"
    Jul 29 15:31:52.726: INFO: stdout: "nodeport-test-f5rb6"
    Jul 29 15:31:52.727: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=services-942 exec execpod7wxrv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.28 30254'
    Jul 29 15:31:52.955: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.28 30254\nConnection to 192.168.121.28 30254 port [tcp/*] succeeded!\n"
    Jul 29 15:31:52.956: INFO: stdout: ""
    Jul 29 15:31:53.956: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=services-942 exec execpod7wxrv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.28 30254'
    Jul 29 15:31:54.240: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.28 30254\nConnection to 192.168.121.28 30254 port [tcp/*] succeeded!\n"
    Jul 29 15:31:54.240: INFO: stdout: "nodeport-test-f5rb6"
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jul 29 15:31:54.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-942" for this suite. 07/29/23 15:31:54.249
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:31:54.283
Jul 29 15:31:54.284: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename custom-resource-definition 07/29/23 15:31:54.288
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:31:54.321
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:31:54.327
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
Jul 29 15:31:54.331: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jul 29 15:31:54.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-8346" for this suite. 07/29/23 15:31:54.926
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","completed":21,"skipped":381,"failed":0}
------------------------------
â€¢ [0.657 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:145

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:31:54.283
    Jul 29 15:31:54.284: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename custom-resource-definition 07/29/23 15:31:54.288
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:31:54.321
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:31:54.327
    [It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:145
    Jul 29 15:31:54.331: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jul 29 15:31:54.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-8346" for this suite. 07/29/23 15:31:54.926
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:31:54.944
Jul 29 15:31:54.945: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename emptydir 07/29/23 15:31:54.947
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:31:54.969
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:31:54.973
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106
STEP: Creating a pod to test emptydir 0666 on tmpfs 07/29/23 15:31:54.977
Jul 29 15:31:54.997: INFO: Waiting up to 5m0s for pod "pod-e673c1b7-9993-426e-9768-43585d5fe9f1" in namespace "emptydir-4125" to be "Succeeded or Failed"
Jul 29 15:31:55.012: INFO: Pod "pod-e673c1b7-9993-426e-9768-43585d5fe9f1": Phase="Pending", Reason="", readiness=false. Elapsed: 15.118039ms
Jul 29 15:31:57.021: INFO: Pod "pod-e673c1b7-9993-426e-9768-43585d5fe9f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023937548s
Jul 29 15:31:59.022: INFO: Pod "pod-e673c1b7-9993-426e-9768-43585d5fe9f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025024554s
STEP: Saw pod success 07/29/23 15:31:59.022
Jul 29 15:31:59.023: INFO: Pod "pod-e673c1b7-9993-426e-9768-43585d5fe9f1" satisfied condition "Succeeded or Failed"
Jul 29 15:31:59.031: INFO: Trying to get logs from node wa4quivohpee-3 pod pod-e673c1b7-9993-426e-9768-43585d5fe9f1 container test-container: <nil>
STEP: delete the pod 07/29/23 15:31:59.065
Jul 29 15:31:59.089: INFO: Waiting for pod pod-e673c1b7-9993-426e-9768-43585d5fe9f1 to disappear
Jul 29 15:31:59.095: INFO: Pod pod-e673c1b7-9993-426e-9768-43585d5fe9f1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jul 29 15:31:59.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4125" for this suite. 07/29/23 15:31:59.102
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":22,"skipped":394,"failed":0}
------------------------------
â€¢ [4.170 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:31:54.944
    Jul 29 15:31:54.945: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename emptydir 07/29/23 15:31:54.947
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:31:54.969
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:31:54.973
    [It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:106
    STEP: Creating a pod to test emptydir 0666 on tmpfs 07/29/23 15:31:54.977
    Jul 29 15:31:54.997: INFO: Waiting up to 5m0s for pod "pod-e673c1b7-9993-426e-9768-43585d5fe9f1" in namespace "emptydir-4125" to be "Succeeded or Failed"
    Jul 29 15:31:55.012: INFO: Pod "pod-e673c1b7-9993-426e-9768-43585d5fe9f1": Phase="Pending", Reason="", readiness=false. Elapsed: 15.118039ms
    Jul 29 15:31:57.021: INFO: Pod "pod-e673c1b7-9993-426e-9768-43585d5fe9f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023937548s
    Jul 29 15:31:59.022: INFO: Pod "pod-e673c1b7-9993-426e-9768-43585d5fe9f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025024554s
    STEP: Saw pod success 07/29/23 15:31:59.022
    Jul 29 15:31:59.023: INFO: Pod "pod-e673c1b7-9993-426e-9768-43585d5fe9f1" satisfied condition "Succeeded or Failed"
    Jul 29 15:31:59.031: INFO: Trying to get logs from node wa4quivohpee-3 pod pod-e673c1b7-9993-426e-9768-43585d5fe9f1 container test-container: <nil>
    STEP: delete the pod 07/29/23 15:31:59.065
    Jul 29 15:31:59.089: INFO: Waiting for pod pod-e673c1b7-9993-426e-9768-43585d5fe9f1 to disappear
    Jul 29 15:31:59.095: INFO: Pod pod-e673c1b7-9993-426e-9768-43585d5fe9f1 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jul 29 15:31:59.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-4125" for this suite. 07/29/23 15:31:59.102
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2179
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:31:59.125
Jul 29 15:31:59.125: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename services 07/29/23 15:31:59.128
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:31:59.16
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:31:59.167
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2179
STEP: creating service in namespace services-1278 07/29/23 15:31:59.172
STEP: creating service affinity-clusterip-transition in namespace services-1278 07/29/23 15:31:59.172
STEP: creating replication controller affinity-clusterip-transition in namespace services-1278 07/29/23 15:31:59.191
I0729 15:31:59.205827      13 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-1278, replica count: 3
I0729 15:32:02.258306      13 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0729 15:32:05.259515      13 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0729 15:32:08.260745      13 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0729 15:32:11.261055      13 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0729 15:32:14.261635      13 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 29 15:32:14.273: INFO: Creating new exec pod
Jul 29 15:32:14.291: INFO: Waiting up to 5m0s for pod "execpod-affinityvjph6" in namespace "services-1278" to be "running"
Jul 29 15:32:14.298: INFO: Pod "execpod-affinityvjph6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.288489ms
Jul 29 15:32:16.305: INFO: Pod "execpod-affinityvjph6": Phase="Running", Reason="", readiness=true. Elapsed: 2.013251352s
Jul 29 15:32:16.305: INFO: Pod "execpod-affinityvjph6" satisfied condition "running"
Jul 29 15:32:17.306: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=services-1278 exec execpod-affinityvjph6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
Jul 29 15:32:17.590: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Jul 29 15:32:17.590: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul 29 15:32:17.591: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=services-1278 exec execpod-affinityvjph6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.59.204 80'
Jul 29 15:32:17.831: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.59.204 80\nConnection to 10.233.59.204 80 port [tcp/http] succeeded!\n"
Jul 29 15:32:17.831: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul 29 15:32:17.852: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=services-1278 exec execpod-affinityvjph6 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.59.204:80/ ; done'
Jul 29 15:32:18.382: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.59.204:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.59.204:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.59.204:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.59.204:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.59.204:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.59.204:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.59.204:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.59.204:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.59.204:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.59.204:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.59.204:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.59.204:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.59.204:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.59.204:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.59.204:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.59.204:80/\n"
Jul 29 15:32:18.382: INFO: stdout: "\naffinity-clusterip-transition-tknq4\naffinity-clusterip-transition-q2xcn\naffinity-clusterip-transition-q2xcn\naffinity-clusterip-transition-mpgjg\naffinity-clusterip-transition-tknq4\naffinity-clusterip-transition-mpgjg\naffinity-clusterip-transition-mpgjg\naffinity-clusterip-transition-q2xcn\naffinity-clusterip-transition-mpgjg\naffinity-clusterip-transition-q2xcn\naffinity-clusterip-transition-q2xcn\naffinity-clusterip-transition-tknq4\naffinity-clusterip-transition-mpgjg\naffinity-clusterip-transition-tknq4\naffinity-clusterip-transition-q2xcn\naffinity-clusterip-transition-tknq4"
Jul 29 15:32:18.382: INFO: Received response from host: affinity-clusterip-transition-tknq4
Jul 29 15:32:18.382: INFO: Received response from host: affinity-clusterip-transition-q2xcn
Jul 29 15:32:18.382: INFO: Received response from host: affinity-clusterip-transition-q2xcn
Jul 29 15:32:18.382: INFO: Received response from host: affinity-clusterip-transition-mpgjg
Jul 29 15:32:18.382: INFO: Received response from host: affinity-clusterip-transition-tknq4
Jul 29 15:32:18.382: INFO: Received response from host: affinity-clusterip-transition-mpgjg
Jul 29 15:32:18.382: INFO: Received response from host: affinity-clusterip-transition-mpgjg
Jul 29 15:32:18.382: INFO: Received response from host: affinity-clusterip-transition-q2xcn
Jul 29 15:32:18.382: INFO: Received response from host: affinity-clusterip-transition-mpgjg
Jul 29 15:32:18.382: INFO: Received response from host: affinity-clusterip-transition-q2xcn
Jul 29 15:32:18.382: INFO: Received response from host: affinity-clusterip-transition-q2xcn
Jul 29 15:32:18.382: INFO: Received response from host: affinity-clusterip-transition-tknq4
Jul 29 15:32:18.382: INFO: Received response from host: affinity-clusterip-transition-mpgjg
Jul 29 15:32:18.382: INFO: Received response from host: affinity-clusterip-transition-tknq4
Jul 29 15:32:18.382: INFO: Received response from host: affinity-clusterip-transition-q2xcn
Jul 29 15:32:18.382: INFO: Received response from host: affinity-clusterip-transition-tknq4
Jul 29 15:32:18.405: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=services-1278 exec execpod-affinityvjph6 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.59.204:80/ ; done'
Jul 29 15:32:18.845: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.59.204:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.59.204:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.59.204:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.59.204:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.59.204:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.59.204:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.59.204:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.59.204:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.59.204:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.59.204:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.59.204:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.59.204:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.59.204:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.59.204:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.59.204:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.59.204:80/\n"
Jul 29 15:32:18.845: INFO: stdout: "\naffinity-clusterip-transition-mpgjg\naffinity-clusterip-transition-mpgjg\naffinity-clusterip-transition-mpgjg\naffinity-clusterip-transition-mpgjg\naffinity-clusterip-transition-mpgjg\naffinity-clusterip-transition-mpgjg\naffinity-clusterip-transition-mpgjg\naffinity-clusterip-transition-mpgjg\naffinity-clusterip-transition-mpgjg\naffinity-clusterip-transition-mpgjg\naffinity-clusterip-transition-mpgjg\naffinity-clusterip-transition-mpgjg\naffinity-clusterip-transition-mpgjg\naffinity-clusterip-transition-mpgjg\naffinity-clusterip-transition-mpgjg\naffinity-clusterip-transition-mpgjg"
Jul 29 15:32:18.845: INFO: Received response from host: affinity-clusterip-transition-mpgjg
Jul 29 15:32:18.846: INFO: Received response from host: affinity-clusterip-transition-mpgjg
Jul 29 15:32:18.846: INFO: Received response from host: affinity-clusterip-transition-mpgjg
Jul 29 15:32:18.846: INFO: Received response from host: affinity-clusterip-transition-mpgjg
Jul 29 15:32:18.846: INFO: Received response from host: affinity-clusterip-transition-mpgjg
Jul 29 15:32:18.846: INFO: Received response from host: affinity-clusterip-transition-mpgjg
Jul 29 15:32:18.846: INFO: Received response from host: affinity-clusterip-transition-mpgjg
Jul 29 15:32:18.846: INFO: Received response from host: affinity-clusterip-transition-mpgjg
Jul 29 15:32:18.846: INFO: Received response from host: affinity-clusterip-transition-mpgjg
Jul 29 15:32:18.846: INFO: Received response from host: affinity-clusterip-transition-mpgjg
Jul 29 15:32:18.846: INFO: Received response from host: affinity-clusterip-transition-mpgjg
Jul 29 15:32:18.846: INFO: Received response from host: affinity-clusterip-transition-mpgjg
Jul 29 15:32:18.846: INFO: Received response from host: affinity-clusterip-transition-mpgjg
Jul 29 15:32:18.846: INFO: Received response from host: affinity-clusterip-transition-mpgjg
Jul 29 15:32:18.846: INFO: Received response from host: affinity-clusterip-transition-mpgjg
Jul 29 15:32:18.846: INFO: Received response from host: affinity-clusterip-transition-mpgjg
Jul 29 15:32:18.846: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-1278, will wait for the garbage collector to delete the pods 07/29/23 15:32:18.87
Jul 29 15:32:18.948: INFO: Deleting ReplicationController affinity-clusterip-transition took: 13.149735ms
Jul 29 15:32:19.048: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.404909ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jul 29 15:32:20.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1278" for this suite. 07/29/23 15:32:20.703
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","completed":23,"skipped":463,"failed":0}
------------------------------
â€¢ [SLOW TEST] [21.590 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2179

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:31:59.125
    Jul 29 15:31:59.125: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename services 07/29/23 15:31:59.128
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:31:59.16
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:31:59.167
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2179
    STEP: creating service in namespace services-1278 07/29/23 15:31:59.172
    STEP: creating service affinity-clusterip-transition in namespace services-1278 07/29/23 15:31:59.172
    STEP: creating replication controller affinity-clusterip-transition in namespace services-1278 07/29/23 15:31:59.191
    I0729 15:31:59.205827      13 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-1278, replica count: 3
    I0729 15:32:02.258306      13 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0729 15:32:05.259515      13 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0729 15:32:08.260745      13 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0729 15:32:11.261055      13 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0729 15:32:14.261635      13 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jul 29 15:32:14.273: INFO: Creating new exec pod
    Jul 29 15:32:14.291: INFO: Waiting up to 5m0s for pod "execpod-affinityvjph6" in namespace "services-1278" to be "running"
    Jul 29 15:32:14.298: INFO: Pod "execpod-affinityvjph6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.288489ms
    Jul 29 15:32:16.305: INFO: Pod "execpod-affinityvjph6": Phase="Running", Reason="", readiness=true. Elapsed: 2.013251352s
    Jul 29 15:32:16.305: INFO: Pod "execpod-affinityvjph6" satisfied condition "running"
    Jul 29 15:32:17.306: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=services-1278 exec execpod-affinityvjph6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
    Jul 29 15:32:17.590: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
    Jul 29 15:32:17.590: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jul 29 15:32:17.591: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=services-1278 exec execpod-affinityvjph6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.59.204 80'
    Jul 29 15:32:17.831: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.59.204 80\nConnection to 10.233.59.204 80 port [tcp/http] succeeded!\n"
    Jul 29 15:32:17.831: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jul 29 15:32:17.852: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=services-1278 exec execpod-affinityvjph6 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.59.204:80/ ; done'
    Jul 29 15:32:18.382: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.59.204:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.59.204:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.59.204:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.59.204:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.59.204:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.59.204:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.59.204:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.59.204:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.59.204:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.59.204:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.59.204:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.59.204:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.59.204:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.59.204:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.59.204:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.59.204:80/\n"
    Jul 29 15:32:18.382: INFO: stdout: "\naffinity-clusterip-transition-tknq4\naffinity-clusterip-transition-q2xcn\naffinity-clusterip-transition-q2xcn\naffinity-clusterip-transition-mpgjg\naffinity-clusterip-transition-tknq4\naffinity-clusterip-transition-mpgjg\naffinity-clusterip-transition-mpgjg\naffinity-clusterip-transition-q2xcn\naffinity-clusterip-transition-mpgjg\naffinity-clusterip-transition-q2xcn\naffinity-clusterip-transition-q2xcn\naffinity-clusterip-transition-tknq4\naffinity-clusterip-transition-mpgjg\naffinity-clusterip-transition-tknq4\naffinity-clusterip-transition-q2xcn\naffinity-clusterip-transition-tknq4"
    Jul 29 15:32:18.382: INFO: Received response from host: affinity-clusterip-transition-tknq4
    Jul 29 15:32:18.382: INFO: Received response from host: affinity-clusterip-transition-q2xcn
    Jul 29 15:32:18.382: INFO: Received response from host: affinity-clusterip-transition-q2xcn
    Jul 29 15:32:18.382: INFO: Received response from host: affinity-clusterip-transition-mpgjg
    Jul 29 15:32:18.382: INFO: Received response from host: affinity-clusterip-transition-tknq4
    Jul 29 15:32:18.382: INFO: Received response from host: affinity-clusterip-transition-mpgjg
    Jul 29 15:32:18.382: INFO: Received response from host: affinity-clusterip-transition-mpgjg
    Jul 29 15:32:18.382: INFO: Received response from host: affinity-clusterip-transition-q2xcn
    Jul 29 15:32:18.382: INFO: Received response from host: affinity-clusterip-transition-mpgjg
    Jul 29 15:32:18.382: INFO: Received response from host: affinity-clusterip-transition-q2xcn
    Jul 29 15:32:18.382: INFO: Received response from host: affinity-clusterip-transition-q2xcn
    Jul 29 15:32:18.382: INFO: Received response from host: affinity-clusterip-transition-tknq4
    Jul 29 15:32:18.382: INFO: Received response from host: affinity-clusterip-transition-mpgjg
    Jul 29 15:32:18.382: INFO: Received response from host: affinity-clusterip-transition-tknq4
    Jul 29 15:32:18.382: INFO: Received response from host: affinity-clusterip-transition-q2xcn
    Jul 29 15:32:18.382: INFO: Received response from host: affinity-clusterip-transition-tknq4
    Jul 29 15:32:18.405: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=services-1278 exec execpod-affinityvjph6 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.59.204:80/ ; done'
    Jul 29 15:32:18.845: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.59.204:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.59.204:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.59.204:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.59.204:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.59.204:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.59.204:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.59.204:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.59.204:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.59.204:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.59.204:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.59.204:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.59.204:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.59.204:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.59.204:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.59.204:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.59.204:80/\n"
    Jul 29 15:32:18.845: INFO: stdout: "\naffinity-clusterip-transition-mpgjg\naffinity-clusterip-transition-mpgjg\naffinity-clusterip-transition-mpgjg\naffinity-clusterip-transition-mpgjg\naffinity-clusterip-transition-mpgjg\naffinity-clusterip-transition-mpgjg\naffinity-clusterip-transition-mpgjg\naffinity-clusterip-transition-mpgjg\naffinity-clusterip-transition-mpgjg\naffinity-clusterip-transition-mpgjg\naffinity-clusterip-transition-mpgjg\naffinity-clusterip-transition-mpgjg\naffinity-clusterip-transition-mpgjg\naffinity-clusterip-transition-mpgjg\naffinity-clusterip-transition-mpgjg\naffinity-clusterip-transition-mpgjg"
    Jul 29 15:32:18.845: INFO: Received response from host: affinity-clusterip-transition-mpgjg
    Jul 29 15:32:18.846: INFO: Received response from host: affinity-clusterip-transition-mpgjg
    Jul 29 15:32:18.846: INFO: Received response from host: affinity-clusterip-transition-mpgjg
    Jul 29 15:32:18.846: INFO: Received response from host: affinity-clusterip-transition-mpgjg
    Jul 29 15:32:18.846: INFO: Received response from host: affinity-clusterip-transition-mpgjg
    Jul 29 15:32:18.846: INFO: Received response from host: affinity-clusterip-transition-mpgjg
    Jul 29 15:32:18.846: INFO: Received response from host: affinity-clusterip-transition-mpgjg
    Jul 29 15:32:18.846: INFO: Received response from host: affinity-clusterip-transition-mpgjg
    Jul 29 15:32:18.846: INFO: Received response from host: affinity-clusterip-transition-mpgjg
    Jul 29 15:32:18.846: INFO: Received response from host: affinity-clusterip-transition-mpgjg
    Jul 29 15:32:18.846: INFO: Received response from host: affinity-clusterip-transition-mpgjg
    Jul 29 15:32:18.846: INFO: Received response from host: affinity-clusterip-transition-mpgjg
    Jul 29 15:32:18.846: INFO: Received response from host: affinity-clusterip-transition-mpgjg
    Jul 29 15:32:18.846: INFO: Received response from host: affinity-clusterip-transition-mpgjg
    Jul 29 15:32:18.846: INFO: Received response from host: affinity-clusterip-transition-mpgjg
    Jul 29 15:32:18.846: INFO: Received response from host: affinity-clusterip-transition-mpgjg
    Jul 29 15:32:18.846: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-1278, will wait for the garbage collector to delete the pods 07/29/23 15:32:18.87
    Jul 29 15:32:18.948: INFO: Deleting ReplicationController affinity-clusterip-transition took: 13.149735ms
    Jul 29 15:32:19.048: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.404909ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jul 29 15:32:20.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-1278" for this suite. 07/29/23 15:32:20.703
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:32:20.719
Jul 29 15:32:20.719: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename emptydir 07/29/23 15:32:20.722
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:32:20.754
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:32:20.758
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206
STEP: Creating a pod to test emptydir 0666 on node default medium 07/29/23 15:32:20.763
Jul 29 15:32:20.786: INFO: Waiting up to 5m0s for pod "pod-b2811894-1447-4406-a806-0c1386d08ba0" in namespace "emptydir-3671" to be "Succeeded or Failed"
Jul 29 15:32:20.792: INFO: Pod "pod-b2811894-1447-4406-a806-0c1386d08ba0": Phase="Pending", Reason="", readiness=false. Elapsed: 5.203341ms
Jul 29 15:32:22.805: INFO: Pod "pod-b2811894-1447-4406-a806-0c1386d08ba0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018508926s
Jul 29 15:32:24.799: INFO: Pod "pod-b2811894-1447-4406-a806-0c1386d08ba0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012382853s
STEP: Saw pod success 07/29/23 15:32:24.799
Jul 29 15:32:24.800: INFO: Pod "pod-b2811894-1447-4406-a806-0c1386d08ba0" satisfied condition "Succeeded or Failed"
Jul 29 15:32:24.807: INFO: Trying to get logs from node wa4quivohpee-3 pod pod-b2811894-1447-4406-a806-0c1386d08ba0 container test-container: <nil>
STEP: delete the pod 07/29/23 15:32:24.819
Jul 29 15:32:24.846: INFO: Waiting for pod pod-b2811894-1447-4406-a806-0c1386d08ba0 to disappear
Jul 29 15:32:24.852: INFO: Pod pod-b2811894-1447-4406-a806-0c1386d08ba0 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jul 29 15:32:24.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3671" for this suite. 07/29/23 15:32:24.861
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":24,"skipped":467,"failed":0}
------------------------------
â€¢ [4.152 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:32:20.719
    Jul 29 15:32:20.719: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename emptydir 07/29/23 15:32:20.722
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:32:20.754
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:32:20.758
    [It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:206
    STEP: Creating a pod to test emptydir 0666 on node default medium 07/29/23 15:32:20.763
    Jul 29 15:32:20.786: INFO: Waiting up to 5m0s for pod "pod-b2811894-1447-4406-a806-0c1386d08ba0" in namespace "emptydir-3671" to be "Succeeded or Failed"
    Jul 29 15:32:20.792: INFO: Pod "pod-b2811894-1447-4406-a806-0c1386d08ba0": Phase="Pending", Reason="", readiness=false. Elapsed: 5.203341ms
    Jul 29 15:32:22.805: INFO: Pod "pod-b2811894-1447-4406-a806-0c1386d08ba0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018508926s
    Jul 29 15:32:24.799: INFO: Pod "pod-b2811894-1447-4406-a806-0c1386d08ba0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012382853s
    STEP: Saw pod success 07/29/23 15:32:24.799
    Jul 29 15:32:24.800: INFO: Pod "pod-b2811894-1447-4406-a806-0c1386d08ba0" satisfied condition "Succeeded or Failed"
    Jul 29 15:32:24.807: INFO: Trying to get logs from node wa4quivohpee-3 pod pod-b2811894-1447-4406-a806-0c1386d08ba0 container test-container: <nil>
    STEP: delete the pod 07/29/23 15:32:24.819
    Jul 29 15:32:24.846: INFO: Waiting for pod pod-b2811894-1447-4406-a806-0c1386d08ba0 to disappear
    Jul 29 15:32:24.852: INFO: Pod pod-b2811894-1447-4406-a806-0c1386d08ba0 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jul 29 15:32:24.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-3671" for this suite. 07/29/23 15:32:24.861
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:32:24.881
Jul 29 15:32:24.881: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename container-probe 07/29/23 15:32:24.883
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:32:24.918
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:32:24.923
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131
STEP: Creating pod busybox-3e997ebd-af24-4977-96b2-35add0985173 in namespace container-probe-8526 07/29/23 15:32:24.927
Jul 29 15:32:24.945: INFO: Waiting up to 5m0s for pod "busybox-3e997ebd-af24-4977-96b2-35add0985173" in namespace "container-probe-8526" to be "not pending"
Jul 29 15:32:24.953: INFO: Pod "busybox-3e997ebd-af24-4977-96b2-35add0985173": Phase="Pending", Reason="", readiness=false. Elapsed: 7.434461ms
Jul 29 15:32:26.960: INFO: Pod "busybox-3e997ebd-af24-4977-96b2-35add0985173": Phase="Running", Reason="", readiness=true. Elapsed: 2.015093377s
Jul 29 15:32:26.960: INFO: Pod "busybox-3e997ebd-af24-4977-96b2-35add0985173" satisfied condition "not pending"
Jul 29 15:32:26.960: INFO: Started pod busybox-3e997ebd-af24-4977-96b2-35add0985173 in namespace container-probe-8526
STEP: checking the pod's current state and verifying that restartCount is present 07/29/23 15:32:26.96
Jul 29 15:32:26.969: INFO: Initial restart count of pod busybox-3e997ebd-af24-4977-96b2-35add0985173 is 0
Jul 29 15:33:17.270: INFO: Restart count of pod container-probe-8526/busybox-3e997ebd-af24-4977-96b2-35add0985173 is now 1 (50.300693163s elapsed)
STEP: deleting the pod 07/29/23 15:33:17.27
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jul 29 15:33:17.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8526" for this suite. 07/29/23 15:33:17.311
{"msg":"PASSED [sig-node] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","completed":25,"skipped":500,"failed":0}
------------------------------
â€¢ [SLOW TEST] [52.453 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:32:24.881
    Jul 29 15:32:24.881: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename container-probe 07/29/23 15:32:24.883
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:32:24.918
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:32:24.923
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:131
    STEP: Creating pod busybox-3e997ebd-af24-4977-96b2-35add0985173 in namespace container-probe-8526 07/29/23 15:32:24.927
    Jul 29 15:32:24.945: INFO: Waiting up to 5m0s for pod "busybox-3e997ebd-af24-4977-96b2-35add0985173" in namespace "container-probe-8526" to be "not pending"
    Jul 29 15:32:24.953: INFO: Pod "busybox-3e997ebd-af24-4977-96b2-35add0985173": Phase="Pending", Reason="", readiness=false. Elapsed: 7.434461ms
    Jul 29 15:32:26.960: INFO: Pod "busybox-3e997ebd-af24-4977-96b2-35add0985173": Phase="Running", Reason="", readiness=true. Elapsed: 2.015093377s
    Jul 29 15:32:26.960: INFO: Pod "busybox-3e997ebd-af24-4977-96b2-35add0985173" satisfied condition "not pending"
    Jul 29 15:32:26.960: INFO: Started pod busybox-3e997ebd-af24-4977-96b2-35add0985173 in namespace container-probe-8526
    STEP: checking the pod's current state and verifying that restartCount is present 07/29/23 15:32:26.96
    Jul 29 15:32:26.969: INFO: Initial restart count of pod busybox-3e997ebd-af24-4977-96b2-35add0985173 is 0
    Jul 29 15:33:17.270: INFO: Restart count of pod container-probe-8526/busybox-3e997ebd-af24-4977-96b2-35add0985173 is now 1 (50.300693163s elapsed)
    STEP: deleting the pod 07/29/23 15:33:17.27
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jul 29 15:33:17.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-8526" for this suite. 07/29/23 15:33:17.311
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:33:17.344
Jul 29 15:33:17.345: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename events 07/29/23 15:33:17.347
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:33:17.421
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:33:17.426
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
STEP: creating a test event 07/29/23 15:33:17.431
STEP: listing events in all namespaces 07/29/23 15:33:17.443
STEP: listing events in test namespace 07/29/23 15:33:17.459
STEP: listing events with field selection filtering on source 07/29/23 15:33:17.464
STEP: listing events with field selection filtering on reportingController 07/29/23 15:33:17.469
STEP: getting the test event 07/29/23 15:33:17.474
STEP: patching the test event 07/29/23 15:33:17.478
STEP: getting the test event 07/29/23 15:33:17.501
STEP: updating the test event 07/29/23 15:33:17.51
STEP: getting the test event 07/29/23 15:33:17.528
STEP: deleting the test event 07/29/23 15:33:17.533
STEP: listing events in all namespaces 07/29/23 15:33:17.545
STEP: listing events in test namespace 07/29/23 15:33:17.559
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:187
Jul 29 15:33:17.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-3146" for this suite. 07/29/23 15:33:17.572
{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","completed":26,"skipped":517,"failed":0}
------------------------------
â€¢ [0.240 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:33:17.344
    Jul 29 15:33:17.345: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename events 07/29/23 15:33:17.347
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:33:17.421
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:33:17.426
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
      test/e2e/instrumentation/events.go:98
    STEP: creating a test event 07/29/23 15:33:17.431
    STEP: listing events in all namespaces 07/29/23 15:33:17.443
    STEP: listing events in test namespace 07/29/23 15:33:17.459
    STEP: listing events with field selection filtering on source 07/29/23 15:33:17.464
    STEP: listing events with field selection filtering on reportingController 07/29/23 15:33:17.469
    STEP: getting the test event 07/29/23 15:33:17.474
    STEP: patching the test event 07/29/23 15:33:17.478
    STEP: getting the test event 07/29/23 15:33:17.501
    STEP: updating the test event 07/29/23 15:33:17.51
    STEP: getting the test event 07/29/23 15:33:17.528
    STEP: deleting the test event 07/29/23 15:33:17.533
    STEP: listing events in all namespaces 07/29/23 15:33:17.545
    STEP: listing events in test namespace 07/29/23 15:33:17.559
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:187
    Jul 29 15:33:17.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-3146" for this suite. 07/29/23 15:33:17.572
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:33:17.587
Jul 29 15:33:17.587: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename configmap 07/29/23 15:33:17.589
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:33:17.618
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:33:17.623
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jul 29 15:33:17.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5619" for this suite. 07/29/23 15:33:17.698
{"msg":"PASSED [sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]","completed":27,"skipped":532,"failed":0}
------------------------------
â€¢ [0.123 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:33:17.587
    Jul 29 15:33:17.587: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename configmap 07/29/23 15:33:17.589
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:33:17.618
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:33:17.623
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/configmap_volume.go:503
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jul 29 15:33:17.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-5619" for this suite. 07/29/23 15:33:17.698
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial]
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:33:17.713
Jul 29 15:33:17.713: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename taint-single-pod 07/29/23 15:33:17.717
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:33:17.747
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:33:17.751
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/node/taints.go:166
Jul 29 15:33:17.755: INFO: Waiting up to 1m0s for all nodes to be ready
Jul 29 15:34:17.812: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289
Jul 29 15:34:17.825: INFO: Starting informer...
STEP: Starting pod... 07/29/23 15:34:17.825
Jul 29 15:34:18.060: INFO: Pod is running on wa4quivohpee-3. Tainting Node
STEP: Trying to apply a taint on the Node 07/29/23 15:34:18.06
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 07/29/23 15:34:18.086
STEP: Waiting short time to make sure Pod is queued for deletion 07/29/23 15:34:18.096
Jul 29 15:34:18.097: INFO: Pod wasn't evicted. Proceeding
Jul 29 15:34:18.097: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 07/29/23 15:34:18.156
STEP: Waiting some time to make sure that toleration time passed. 07/29/23 15:34:18.164
Jul 29 15:35:33.165: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/framework.go:187
Jul 29 15:35:33.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-2697" for this suite. 07/29/23 15:35:33.186
{"msg":"PASSED [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","completed":28,"skipped":536,"failed":0}
------------------------------
â€¢ [SLOW TEST] [135.495 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:33:17.713
    Jul 29 15:33:17.713: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename taint-single-pod 07/29/23 15:33:17.717
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:33:17.747
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:33:17.751
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/node/taints.go:166
    Jul 29 15:33:17.755: INFO: Waiting up to 1m0s for all nodes to be ready
    Jul 29 15:34:17.812: INFO: Waiting for terminating namespaces to be deleted...
    [It] removing taint cancels eviction [Disruptive] [Conformance]
      test/e2e/node/taints.go:289
    Jul 29 15:34:17.825: INFO: Starting informer...
    STEP: Starting pod... 07/29/23 15:34:17.825
    Jul 29 15:34:18.060: INFO: Pod is running on wa4quivohpee-3. Tainting Node
    STEP: Trying to apply a taint on the Node 07/29/23 15:34:18.06
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 07/29/23 15:34:18.086
    STEP: Waiting short time to make sure Pod is queued for deletion 07/29/23 15:34:18.096
    Jul 29 15:34:18.097: INFO: Pod wasn't evicted. Proceeding
    Jul 29 15:34:18.097: INFO: Removing taint from Node
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 07/29/23 15:34:18.156
    STEP: Waiting some time to make sure that toleration time passed. 07/29/23 15:34:18.164
    Jul 29 15:35:33.165: INFO: Pod wasn't evicted. Test successful
    [AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/framework.go:187
    Jul 29 15:35:33.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "taint-single-pod-2697" for this suite. 07/29/23 15:35:33.186
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:35:33.215
Jul 29 15:35:33.216: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename projected 07/29/23 15:35:33.219
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:35:33.256
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:35:33.264
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374
STEP: Creating configMap with name projected-configmap-test-volume-a8ae258d-3536-47d4-9692-ccf5f42ae887 07/29/23 15:35:33.27
STEP: Creating a pod to test consume configMaps 07/29/23 15:35:33.279
Jul 29 15:35:33.295: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-64d1fa2f-0d5d-488c-bc97-61c2d064313e" in namespace "projected-9635" to be "Succeeded or Failed"
Jul 29 15:35:33.306: INFO: Pod "pod-projected-configmaps-64d1fa2f-0d5d-488c-bc97-61c2d064313e": Phase="Pending", Reason="", readiness=false. Elapsed: 11.33039ms
Jul 29 15:35:35.314: INFO: Pod "pod-projected-configmaps-64d1fa2f-0d5d-488c-bc97-61c2d064313e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019593628s
Jul 29 15:35:37.315: INFO: Pod "pod-projected-configmaps-64d1fa2f-0d5d-488c-bc97-61c2d064313e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020598181s
STEP: Saw pod success 07/29/23 15:35:37.315
Jul 29 15:35:37.316: INFO: Pod "pod-projected-configmaps-64d1fa2f-0d5d-488c-bc97-61c2d064313e" satisfied condition "Succeeded or Failed"
Jul 29 15:35:37.323: INFO: Trying to get logs from node wa4quivohpee-3 pod pod-projected-configmaps-64d1fa2f-0d5d-488c-bc97-61c2d064313e container projected-configmap-volume-test: <nil>
STEP: delete the pod 07/29/23 15:35:37.351
Jul 29 15:35:37.375: INFO: Waiting for pod pod-projected-configmaps-64d1fa2f-0d5d-488c-bc97-61c2d064313e to disappear
Jul 29 15:35:37.379: INFO: Pod pod-projected-configmaps-64d1fa2f-0d5d-488c-bc97-61c2d064313e no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jul 29 15:35:37.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9635" for this suite. 07/29/23 15:35:37.386
{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","completed":29,"skipped":548,"failed":0}
------------------------------
â€¢ [4.182 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:35:33.215
    Jul 29 15:35:33.216: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename projected 07/29/23 15:35:33.219
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:35:33.256
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:35:33.264
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:374
    STEP: Creating configMap with name projected-configmap-test-volume-a8ae258d-3536-47d4-9692-ccf5f42ae887 07/29/23 15:35:33.27
    STEP: Creating a pod to test consume configMaps 07/29/23 15:35:33.279
    Jul 29 15:35:33.295: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-64d1fa2f-0d5d-488c-bc97-61c2d064313e" in namespace "projected-9635" to be "Succeeded or Failed"
    Jul 29 15:35:33.306: INFO: Pod "pod-projected-configmaps-64d1fa2f-0d5d-488c-bc97-61c2d064313e": Phase="Pending", Reason="", readiness=false. Elapsed: 11.33039ms
    Jul 29 15:35:35.314: INFO: Pod "pod-projected-configmaps-64d1fa2f-0d5d-488c-bc97-61c2d064313e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019593628s
    Jul 29 15:35:37.315: INFO: Pod "pod-projected-configmaps-64d1fa2f-0d5d-488c-bc97-61c2d064313e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020598181s
    STEP: Saw pod success 07/29/23 15:35:37.315
    Jul 29 15:35:37.316: INFO: Pod "pod-projected-configmaps-64d1fa2f-0d5d-488c-bc97-61c2d064313e" satisfied condition "Succeeded or Failed"
    Jul 29 15:35:37.323: INFO: Trying to get logs from node wa4quivohpee-3 pod pod-projected-configmaps-64d1fa2f-0d5d-488c-bc97-61c2d064313e container projected-configmap-volume-test: <nil>
    STEP: delete the pod 07/29/23 15:35:37.351
    Jul 29 15:35:37.375: INFO: Waiting for pod pod-projected-configmaps-64d1fa2f-0d5d-488c-bc97-61c2d064313e to disappear
    Jul 29 15:35:37.379: INFO: Pod pod-projected-configmaps-64d1fa2f-0d5d-488c-bc97-61c2d064313e no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jul 29 15:35:37.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-9635" for this suite. 07/29/23 15:35:37.386
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:35:37.399
Jul 29 15:35:37.399: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename namespaces 07/29/23 15:35:37.402
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:35:37.434
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:35:37.438
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250
STEP: Creating a test namespace 07/29/23 15:35:37.442
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:35:37.474
STEP: Creating a service in the namespace 07/29/23 15:35:37.479
STEP: Deleting the namespace 07/29/23 15:35:37.496
STEP: Waiting for the namespace to be removed. 07/29/23 15:35:37.53
STEP: Recreating the namespace 07/29/23 15:35:43.537
STEP: Verifying there is no service in the namespace 07/29/23 15:35:43.576
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Jul 29 15:35:43.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-9280" for this suite. 07/29/23 15:35:43.59
STEP: Destroying namespace "nsdeletetest-2812" for this suite. 07/29/23 15:35:43.603
Jul 29 15:35:43.609: INFO: Namespace nsdeletetest-2812 was already deleted
STEP: Destroying namespace "nsdeletetest-6962" for this suite. 07/29/23 15:35:43.609
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","completed":30,"skipped":550,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.221 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:35:37.399
    Jul 29 15:35:37.399: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename namespaces 07/29/23 15:35:37.402
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:35:37.434
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:35:37.438
    [It] should ensure that all services are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:250
    STEP: Creating a test namespace 07/29/23 15:35:37.442
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:35:37.474
    STEP: Creating a service in the namespace 07/29/23 15:35:37.479
    STEP: Deleting the namespace 07/29/23 15:35:37.496
    STEP: Waiting for the namespace to be removed. 07/29/23 15:35:37.53
    STEP: Recreating the namespace 07/29/23 15:35:43.537
    STEP: Verifying there is no service in the namespace 07/29/23 15:35:43.576
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Jul 29 15:35:43.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-9280" for this suite. 07/29/23 15:35:43.59
    STEP: Destroying namespace "nsdeletetest-2812" for this suite. 07/29/23 15:35:43.603
    Jul 29 15:35:43.609: INFO: Namespace nsdeletetest-2812 was already deleted
    STEP: Destroying namespace "nsdeletetest-6962" for this suite. 07/29/23 15:35:43.609
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:35:43.634
Jul 29 15:35:43.635: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename projected 07/29/23 15:35:43.639
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:35:43.669
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:35:43.673
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88
STEP: Creating configMap with name projected-configmap-test-volume-map-e532f502-02ad-4212-abf7-5bdc465ae20e 07/29/23 15:35:43.677
STEP: Creating a pod to test consume configMaps 07/29/23 15:35:43.688
Jul 29 15:35:43.701: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-08078ca7-5d46-4525-be8d-9ecf116699b8" in namespace "projected-5684" to be "Succeeded or Failed"
Jul 29 15:35:43.706: INFO: Pod "pod-projected-configmaps-08078ca7-5d46-4525-be8d-9ecf116699b8": Phase="Pending", Reason="", readiness=false. Elapsed: 5.494095ms
Jul 29 15:35:45.722: INFO: Pod "pod-projected-configmaps-08078ca7-5d46-4525-be8d-9ecf116699b8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020632833s
Jul 29 15:35:47.723: INFO: Pod "pod-projected-configmaps-08078ca7-5d46-4525-be8d-9ecf116699b8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021521577s
STEP: Saw pod success 07/29/23 15:35:47.723
Jul 29 15:35:47.724: INFO: Pod "pod-projected-configmaps-08078ca7-5d46-4525-be8d-9ecf116699b8" satisfied condition "Succeeded or Failed"
Jul 29 15:35:47.731: INFO: Trying to get logs from node wa4quivohpee-3 pod pod-projected-configmaps-08078ca7-5d46-4525-be8d-9ecf116699b8 container agnhost-container: <nil>
STEP: delete the pod 07/29/23 15:35:47.742
Jul 29 15:35:47.764: INFO: Waiting for pod pod-projected-configmaps-08078ca7-5d46-4525-be8d-9ecf116699b8 to disappear
Jul 29 15:35:47.769: INFO: Pod pod-projected-configmaps-08078ca7-5d46-4525-be8d-9ecf116699b8 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jul 29 15:35:47.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5684" for this suite. 07/29/23 15:35:47.778
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":31,"skipped":582,"failed":0}
------------------------------
â€¢ [4.155 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:35:43.634
    Jul 29 15:35:43.635: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename projected 07/29/23 15:35:43.639
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:35:43.669
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:35:43.673
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:88
    STEP: Creating configMap with name projected-configmap-test-volume-map-e532f502-02ad-4212-abf7-5bdc465ae20e 07/29/23 15:35:43.677
    STEP: Creating a pod to test consume configMaps 07/29/23 15:35:43.688
    Jul 29 15:35:43.701: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-08078ca7-5d46-4525-be8d-9ecf116699b8" in namespace "projected-5684" to be "Succeeded or Failed"
    Jul 29 15:35:43.706: INFO: Pod "pod-projected-configmaps-08078ca7-5d46-4525-be8d-9ecf116699b8": Phase="Pending", Reason="", readiness=false. Elapsed: 5.494095ms
    Jul 29 15:35:45.722: INFO: Pod "pod-projected-configmaps-08078ca7-5d46-4525-be8d-9ecf116699b8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020632833s
    Jul 29 15:35:47.723: INFO: Pod "pod-projected-configmaps-08078ca7-5d46-4525-be8d-9ecf116699b8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021521577s
    STEP: Saw pod success 07/29/23 15:35:47.723
    Jul 29 15:35:47.724: INFO: Pod "pod-projected-configmaps-08078ca7-5d46-4525-be8d-9ecf116699b8" satisfied condition "Succeeded or Failed"
    Jul 29 15:35:47.731: INFO: Trying to get logs from node wa4quivohpee-3 pod pod-projected-configmaps-08078ca7-5d46-4525-be8d-9ecf116699b8 container agnhost-container: <nil>
    STEP: delete the pod 07/29/23 15:35:47.742
    Jul 29 15:35:47.764: INFO: Waiting for pod pod-projected-configmaps-08078ca7-5d46-4525-be8d-9ecf116699b8 to disappear
    Jul 29 15:35:47.769: INFO: Pod pod-projected-configmaps-08078ca7-5d46-4525-be8d-9ecf116699b8 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jul 29 15:35:47.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5684" for this suite. 07/29/23 15:35:47.778
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:35:47.792
Jul 29 15:35:47.792: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename custom-resource-definition 07/29/23 15:35:47.796
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:35:47.831
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:35:47.835
[It] creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
Jul 29 15:35:47.839: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jul 29 15:35:48.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-4664" for this suite. 07/29/23 15:35:48.917
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","completed":32,"skipped":602,"failed":0}
------------------------------
â€¢ [1.150 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    creating/deleting custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:35:47.792
    Jul 29 15:35:47.792: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename custom-resource-definition 07/29/23 15:35:47.796
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:35:47.831
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:35:47.835
    [It] creating/deleting custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:58
    Jul 29 15:35:47.839: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jul 29 15:35:48.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-4664" for this suite. 07/29/23 15:35:48.917
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:35:48.948
Jul 29 15:35:48.950: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename downward-api 07/29/23 15:35:48.955
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:35:49.011
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:35:49.016
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161
STEP: Creating the pod 07/29/23 15:35:49.061
Jul 29 15:35:49.089: INFO: Waiting up to 5m0s for pod "annotationupdate93e27bd4-c6de-4eaa-938d-2b6b544c7ccc" in namespace "downward-api-9077" to be "running and ready"
Jul 29 15:35:49.197: INFO: Pod "annotationupdate93e27bd4-c6de-4eaa-938d-2b6b544c7ccc": Phase="Pending", Reason="", readiness=false. Elapsed: 108.545555ms
Jul 29 15:35:49.198: INFO: The phase of Pod annotationupdate93e27bd4-c6de-4eaa-938d-2b6b544c7ccc is Pending, waiting for it to be Running (with Ready = true)
Jul 29 15:35:51.205: INFO: Pod "annotationupdate93e27bd4-c6de-4eaa-938d-2b6b544c7ccc": Phase="Running", Reason="", readiness=true. Elapsed: 2.116056832s
Jul 29 15:35:51.205: INFO: The phase of Pod annotationupdate93e27bd4-c6de-4eaa-938d-2b6b544c7ccc is Running (Ready = true)
Jul 29 15:35:51.205: INFO: Pod "annotationupdate93e27bd4-c6de-4eaa-938d-2b6b544c7ccc" satisfied condition "running and ready"
Jul 29 15:35:51.748: INFO: Successfully updated pod "annotationupdate93e27bd4-c6de-4eaa-938d-2b6b544c7ccc"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jul 29 15:35:55.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9077" for this suite. 07/29/23 15:35:55.811
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","completed":33,"skipped":623,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.882 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:35:48.948
    Jul 29 15:35:48.950: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename downward-api 07/29/23 15:35:48.955
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:35:49.011
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:35:49.016
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:161
    STEP: Creating the pod 07/29/23 15:35:49.061
    Jul 29 15:35:49.089: INFO: Waiting up to 5m0s for pod "annotationupdate93e27bd4-c6de-4eaa-938d-2b6b544c7ccc" in namespace "downward-api-9077" to be "running and ready"
    Jul 29 15:35:49.197: INFO: Pod "annotationupdate93e27bd4-c6de-4eaa-938d-2b6b544c7ccc": Phase="Pending", Reason="", readiness=false. Elapsed: 108.545555ms
    Jul 29 15:35:49.198: INFO: The phase of Pod annotationupdate93e27bd4-c6de-4eaa-938d-2b6b544c7ccc is Pending, waiting for it to be Running (with Ready = true)
    Jul 29 15:35:51.205: INFO: Pod "annotationupdate93e27bd4-c6de-4eaa-938d-2b6b544c7ccc": Phase="Running", Reason="", readiness=true. Elapsed: 2.116056832s
    Jul 29 15:35:51.205: INFO: The phase of Pod annotationupdate93e27bd4-c6de-4eaa-938d-2b6b544c7ccc is Running (Ready = true)
    Jul 29 15:35:51.205: INFO: Pod "annotationupdate93e27bd4-c6de-4eaa-938d-2b6b544c7ccc" satisfied condition "running and ready"
    Jul 29 15:35:51.748: INFO: Successfully updated pod "annotationupdate93e27bd4-c6de-4eaa-938d-2b6b544c7ccc"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jul 29 15:35:55.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-9077" for this suite. 07/29/23 15:35:55.811
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:35:55.836
Jul 29 15:35:55.837: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename aggregator 07/29/23 15:35:55.839
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:35:55.877
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:35:55.887
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:78
Jul 29 15:35:55.892: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
STEP: Registering the sample API server. 07/29/23 15:35:55.896
Jul 29 15:35:57.279: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Jul 29 15:35:59.383: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 29 15:36:01.392: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 29 15:36:03.395: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 29 15:36:05.391: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 29 15:36:07.391: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 29 15:36:09.392: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 29 15:36:11.393: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 29 15:36:13.393: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 29 15:36:15.389: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 29 15:36:17.390: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 29 15:36:19.391: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 29 15:36:21.392: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 29 15:36:23.394: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 29 15:36:25.392: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 29 15:36:27.392: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 29 15:36:29.392: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 29 15:36:31.390: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 29 15:36:33.392: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 29 15:36:35.396: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 29 15:36:37.393: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 29 15:36:39.392: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 29 15:36:41.392: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 29 15:36:43.391: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 29 15:36:45.400: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 29 15:36:47.391: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 29 15:36:49.390: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 29 15:36:51.393: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 29 15:36:53.391: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 29 15:36:55.393: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 29 15:36:57.396: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 29 15:36:59.394: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 29 15:37:01.391: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 29 15:37:03.395: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 29 15:37:05.392: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 29 15:37:07.390: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 29 15:37:09.392: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 29 15:37:11.549: INFO: Waited 142.417612ms for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com 07/29/23 15:37:11.645
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 07/29/23 15:37:11.651
STEP: List APIServices 07/29/23 15:37:11.661
Jul 29 15:37:11.681: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/framework/framework.go:187
Jul 29 15:37:12.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-5611" for this suite. 07/29/23 15:37:12.028
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","completed":34,"skipped":662,"failed":0}
------------------------------
â€¢ [SLOW TEST] [76.211 seconds]
[sig-api-machinery] Aggregator
test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:35:55.836
    Jul 29 15:35:55.837: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename aggregator 07/29/23 15:35:55.839
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:35:55.877
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:35:55.887
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:78
    Jul 29 15:35:55.892: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    [It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
      test/e2e/apimachinery/aggregator.go:100
    STEP: Registering the sample API server. 07/29/23 15:35:55.896
    Jul 29 15:35:57.279: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
    Jul 29 15:35:59.383: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jul 29 15:36:01.392: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jul 29 15:36:03.395: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jul 29 15:36:05.391: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jul 29 15:36:07.391: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jul 29 15:36:09.392: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jul 29 15:36:11.393: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jul 29 15:36:13.393: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jul 29 15:36:15.389: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jul 29 15:36:17.390: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jul 29 15:36:19.391: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jul 29 15:36:21.392: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jul 29 15:36:23.394: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jul 29 15:36:25.392: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jul 29 15:36:27.392: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jul 29 15:36:29.392: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jul 29 15:36:31.390: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jul 29 15:36:33.392: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jul 29 15:36:35.396: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jul 29 15:36:37.393: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jul 29 15:36:39.392: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jul 29 15:36:41.392: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jul 29 15:36:43.391: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jul 29 15:36:45.400: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jul 29 15:36:47.391: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jul 29 15:36:49.390: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jul 29 15:36:51.393: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jul 29 15:36:53.391: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jul 29 15:36:55.393: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jul 29 15:36:57.396: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jul 29 15:36:59.394: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jul 29 15:37:01.391: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jul 29 15:37:03.395: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jul 29 15:37:05.392: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jul 29 15:37:07.390: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jul 29 15:37:09.392: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 35, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jul 29 15:37:11.549: INFO: Waited 142.417612ms for the sample-apiserver to be ready to handle requests.
    STEP: Read Status for v1alpha1.wardle.example.com 07/29/23 15:37:11.645
    STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 07/29/23 15:37:11.651
    STEP: List APIServices 07/29/23 15:37:11.661
    Jul 29 15:37:11.681: INFO: Found v1alpha1.wardle.example.com in APIServiceList
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:68
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/framework/framework.go:187
    Jul 29 15:37:12.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "aggregator-5611" for this suite. 07/29/23 15:37:12.028
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:695
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:37:12.051
Jul 29 15:37:12.051: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename statefulset 07/29/23 15:37:12.057
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:37:12.102
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:37:12.108
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-7679 07/29/23 15:37:12.113
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:695
STEP: Creating stateful set ss in namespace statefulset-7679 07/29/23 15:37:12.127
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7679 07/29/23 15:37:12.141
Jul 29 15:37:12.148: INFO: Found 0 stateful pods, waiting for 1
Jul 29 15:37:22.159: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 07/29/23 15:37:22.16
Jul 29 15:37:22.170: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=statefulset-7679 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 29 15:37:22.468: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 29 15:37:22.468: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 29 15:37:22.468: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul 29 15:37:22.475: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jul 29 15:37:32.483: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jul 29 15:37:32.484: INFO: Waiting for statefulset status.replicas updated to 0
Jul 29 15:37:32.567: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Jul 29 15:37:32.567: INFO: ss-0  wa4quivohpee-3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 15:37:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-07-29 15:37:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-07-29 15:37:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 15:37:12 +0000 UTC  }]
Jul 29 15:37:32.567: INFO: 
Jul 29 15:37:32.567: INFO: StatefulSet ss has not reached scale 3, at 1
Jul 29 15:37:33.578: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.986227214s
Jul 29 15:37:34.600: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.975721595s
Jul 29 15:37:35.607: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.953101381s
Jul 29 15:37:36.618: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.946159551s
Jul 29 15:37:37.628: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.934998109s
Jul 29 15:37:38.646: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.924915994s
Jul 29 15:37:39.655: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.907000509s
Jul 29 15:37:40.664: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.898438283s
Jul 29 15:37:41.677: INFO: Verifying statefulset ss doesn't scale past 3 for another 889.235433ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7679 07/29/23 15:37:42.678
Jul 29 15:37:42.690: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=statefulset-7679 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 29 15:37:42.998: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jul 29 15:37:42.998: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul 29 15:37:42.998: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul 29 15:37:42.998: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=statefulset-7679 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 29 15:37:43.221: INFO: rc: 1
Jul 29 15:37:43.222: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=statefulset-7679 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Jul 29 15:37:53.222: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=statefulset-7679 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 29 15:37:53.502: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jul 29 15:37:53.502: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul 29 15:37:53.502: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul 29 15:37:53.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=statefulset-7679 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 29 15:37:53.831: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jul 29 15:37:53.832: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul 29 15:37:53.832: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul 29 15:37:53.840: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 29 15:37:53.840: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 29 15:37:53.841: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod 07/29/23 15:37:53.841
Jul 29 15:37:53.850: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=statefulset-7679 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 29 15:37:54.121: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 29 15:37:54.121: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 29 15:37:54.121: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul 29 15:37:54.122: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=statefulset-7679 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 29 15:37:54.424: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 29 15:37:54.424: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 29 15:37:54.424: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul 29 15:37:54.424: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=statefulset-7679 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 29 15:37:54.780: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 29 15:37:54.780: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 29 15:37:54.780: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul 29 15:37:54.780: INFO: Waiting for statefulset status.replicas updated to 0
Jul 29 15:37:54.786: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Jul 29 15:38:04.805: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jul 29 15:38:04.805: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jul 29 15:38:04.805: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jul 29 15:38:04.843: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Jul 29 15:38:04.843: INFO: ss-0  wa4quivohpee-3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 15:37:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-07-29 15:37:54 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-07-29 15:37:54 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 15:37:12 +0000 UTC  }]
Jul 29 15:38:04.843: INFO: ss-1  wa4quivohpee-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 15:37:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-07-29 15:37:54 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-07-29 15:37:54 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 15:37:32 +0000 UTC  }]
Jul 29 15:38:04.843: INFO: ss-2  wa4quivohpee-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 15:37:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-07-29 15:37:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-07-29 15:37:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 15:37:32 +0000 UTC  }]
Jul 29 15:38:04.843: INFO: 
Jul 29 15:38:04.843: INFO: StatefulSet ss has not reached scale 0, at 3
Jul 29 15:38:05.852: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Jul 29 15:38:05.852: INFO: ss-0  wa4quivohpee-3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 15:37:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-07-29 15:37:54 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-07-29 15:37:54 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 15:37:12 +0000 UTC  }]
Jul 29 15:38:05.852: INFO: ss-1  wa4quivohpee-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 15:37:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-07-29 15:37:54 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-07-29 15:37:54 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 15:37:32 +0000 UTC  }]
Jul 29 15:38:05.853: INFO: ss-2  wa4quivohpee-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 15:37:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-07-29 15:37:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-07-29 15:37:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 15:37:32 +0000 UTC  }]
Jul 29 15:38:05.853: INFO: 
Jul 29 15:38:05.853: INFO: StatefulSet ss has not reached scale 0, at 3
Jul 29 15:38:06.861: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.971388207s
Jul 29 15:38:07.868: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.963065751s
Jul 29 15:38:08.875: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.956118245s
Jul 29 15:38:09.882: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.949382648s
Jul 29 15:38:10.888: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.941997959s
Jul 29 15:38:11.899: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.935910982s
Jul 29 15:38:12.907: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.92545685s
Jul 29 15:38:13.915: INFO: Verifying statefulset ss doesn't scale past 0 for another 917.732305ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7679 07/29/23 15:38:14.915
Jul 29 15:38:14.923: INFO: Scaling statefulset ss to 0
Jul 29 15:38:14.944: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jul 29 15:38:14.950: INFO: Deleting all statefulset in ns statefulset-7679
Jul 29 15:38:14.957: INFO: Scaling statefulset ss to 0
Jul 29 15:38:14.980: INFO: Waiting for statefulset status.replicas updated to 0
Jul 29 15:38:14.985: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jul 29 15:38:15.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7679" for this suite. 07/29/23 15:38:15.018
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","completed":35,"skipped":671,"failed":0}
------------------------------
â€¢ [SLOW TEST] [62.980 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    test/e2e/apps/statefulset.go:695

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:37:12.051
    Jul 29 15:37:12.051: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename statefulset 07/29/23 15:37:12.057
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:37:12.102
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:37:12.108
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-7679 07/29/23 15:37:12.113
    [It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
      test/e2e/apps/statefulset.go:695
    STEP: Creating stateful set ss in namespace statefulset-7679 07/29/23 15:37:12.127
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7679 07/29/23 15:37:12.141
    Jul 29 15:37:12.148: INFO: Found 0 stateful pods, waiting for 1
    Jul 29 15:37:22.159: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 07/29/23 15:37:22.16
    Jul 29 15:37:22.170: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=statefulset-7679 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jul 29 15:37:22.468: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jul 29 15:37:22.468: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jul 29 15:37:22.468: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jul 29 15:37:22.475: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Jul 29 15:37:32.483: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jul 29 15:37:32.484: INFO: Waiting for statefulset status.replicas updated to 0
    Jul 29 15:37:32.567: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
    Jul 29 15:37:32.567: INFO: ss-0  wa4quivohpee-3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 15:37:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-07-29 15:37:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-07-29 15:37:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 15:37:12 +0000 UTC  }]
    Jul 29 15:37:32.567: INFO: 
    Jul 29 15:37:32.567: INFO: StatefulSet ss has not reached scale 3, at 1
    Jul 29 15:37:33.578: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.986227214s
    Jul 29 15:37:34.600: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.975721595s
    Jul 29 15:37:35.607: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.953101381s
    Jul 29 15:37:36.618: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.946159551s
    Jul 29 15:37:37.628: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.934998109s
    Jul 29 15:37:38.646: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.924915994s
    Jul 29 15:37:39.655: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.907000509s
    Jul 29 15:37:40.664: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.898438283s
    Jul 29 15:37:41.677: INFO: Verifying statefulset ss doesn't scale past 3 for another 889.235433ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7679 07/29/23 15:37:42.678
    Jul 29 15:37:42.690: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=statefulset-7679 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jul 29 15:37:42.998: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jul 29 15:37:42.998: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jul 29 15:37:42.998: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jul 29 15:37:42.998: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=statefulset-7679 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jul 29 15:37:43.221: INFO: rc: 1
    Jul 29 15:37:43.222: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=statefulset-7679 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    error: unable to upgrade connection: container not found ("webserver")

    error:
    exit status 1
    Jul 29 15:37:53.222: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=statefulset-7679 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jul 29 15:37:53.502: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Jul 29 15:37:53.502: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jul 29 15:37:53.502: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jul 29 15:37:53.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=statefulset-7679 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jul 29 15:37:53.831: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Jul 29 15:37:53.832: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jul 29 15:37:53.832: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jul 29 15:37:53.840: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Jul 29 15:37:53.840: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Jul 29 15:37:53.841: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Scale down will not halt with unhealthy stateful pod 07/29/23 15:37:53.841
    Jul 29 15:37:53.850: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=statefulset-7679 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jul 29 15:37:54.121: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jul 29 15:37:54.121: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jul 29 15:37:54.121: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jul 29 15:37:54.122: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=statefulset-7679 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jul 29 15:37:54.424: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jul 29 15:37:54.424: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jul 29 15:37:54.424: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jul 29 15:37:54.424: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=statefulset-7679 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jul 29 15:37:54.780: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jul 29 15:37:54.780: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jul 29 15:37:54.780: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jul 29 15:37:54.780: INFO: Waiting for statefulset status.replicas updated to 0
    Jul 29 15:37:54.786: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
    Jul 29 15:38:04.805: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jul 29 15:38:04.805: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Jul 29 15:38:04.805: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Jul 29 15:38:04.843: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
    Jul 29 15:38:04.843: INFO: ss-0  wa4quivohpee-3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 15:37:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-07-29 15:37:54 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-07-29 15:37:54 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 15:37:12 +0000 UTC  }]
    Jul 29 15:38:04.843: INFO: ss-1  wa4quivohpee-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 15:37:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-07-29 15:37:54 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-07-29 15:37:54 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 15:37:32 +0000 UTC  }]
    Jul 29 15:38:04.843: INFO: ss-2  wa4quivohpee-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 15:37:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-07-29 15:37:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-07-29 15:37:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 15:37:32 +0000 UTC  }]
    Jul 29 15:38:04.843: INFO: 
    Jul 29 15:38:04.843: INFO: StatefulSet ss has not reached scale 0, at 3
    Jul 29 15:38:05.852: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
    Jul 29 15:38:05.852: INFO: ss-0  wa4quivohpee-3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 15:37:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-07-29 15:37:54 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-07-29 15:37:54 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 15:37:12 +0000 UTC  }]
    Jul 29 15:38:05.852: INFO: ss-1  wa4quivohpee-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 15:37:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-07-29 15:37:54 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-07-29 15:37:54 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 15:37:32 +0000 UTC  }]
    Jul 29 15:38:05.853: INFO: ss-2  wa4quivohpee-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 15:37:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-07-29 15:37:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-07-29 15:37:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 15:37:32 +0000 UTC  }]
    Jul 29 15:38:05.853: INFO: 
    Jul 29 15:38:05.853: INFO: StatefulSet ss has not reached scale 0, at 3
    Jul 29 15:38:06.861: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.971388207s
    Jul 29 15:38:07.868: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.963065751s
    Jul 29 15:38:08.875: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.956118245s
    Jul 29 15:38:09.882: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.949382648s
    Jul 29 15:38:10.888: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.941997959s
    Jul 29 15:38:11.899: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.935910982s
    Jul 29 15:38:12.907: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.92545685s
    Jul 29 15:38:13.915: INFO: Verifying statefulset ss doesn't scale past 0 for another 917.732305ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7679 07/29/23 15:38:14.915
    Jul 29 15:38:14.923: INFO: Scaling statefulset ss to 0
    Jul 29 15:38:14.944: INFO: Waiting for statefulset status.replicas updated to 0
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jul 29 15:38:14.950: INFO: Deleting all statefulset in ns statefulset-7679
    Jul 29 15:38:14.957: INFO: Scaling statefulset ss to 0
    Jul 29 15:38:14.980: INFO: Waiting for statefulset status.replicas updated to 0
    Jul 29 15:38:14.985: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jul 29 15:38:15.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-7679" for this suite. 07/29/23 15:38:15.018
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
[BeforeEach] [sig-api-machinery] server version
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:38:15.04
Jul 29 15:38:15.040: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename server-version 07/29/23 15:38:15.043
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:38:15.094
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:38:15.099
[It] should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
STEP: Request ServerVersion 07/29/23 15:38:15.102
STEP: Confirm major version 07/29/23 15:38:15.104
Jul 29 15:38:15.105: INFO: Major version: 1
STEP: Confirm minor version 07/29/23 15:38:15.105
Jul 29 15:38:15.105: INFO: cleanMinorVersion: 25
Jul 29 15:38:15.105: INFO: Minor version: 25
[AfterEach] [sig-api-machinery] server version
  test/e2e/framework/framework.go:187
Jul 29 15:38:15.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-1770" for this suite. 07/29/23 15:38:15.115
{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","completed":36,"skipped":689,"failed":0}
------------------------------
â€¢ [0.092 seconds]
[sig-api-machinery] server version
test/e2e/apimachinery/framework.go:23
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] server version
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:38:15.04
    Jul 29 15:38:15.040: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename server-version 07/29/23 15:38:15.043
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:38:15.094
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:38:15.099
    [It] should find the server version [Conformance]
      test/e2e/apimachinery/server_version.go:39
    STEP: Request ServerVersion 07/29/23 15:38:15.102
    STEP: Confirm major version 07/29/23 15:38:15.104
    Jul 29 15:38:15.105: INFO: Major version: 1
    STEP: Confirm minor version 07/29/23 15:38:15.105
    Jul 29 15:38:15.105: INFO: cleanMinorVersion: 25
    Jul 29 15:38:15.105: INFO: Minor version: 25
    [AfterEach] [sig-api-machinery] server version
      test/e2e/framework/framework.go:187
    Jul 29 15:38:15.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "server-version-1770" for this suite. 07/29/23 15:38:15.115
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:38:15.15
Jul 29 15:38:15.150: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename configmap 07/29/23 15:38:15.152
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:38:15.192
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:38:15.199
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73
STEP: Creating configMap with name configmap-test-volume-d05c582a-b79d-437c-b978-46b407fc3a70 07/29/23 15:38:15.204
STEP: Creating a pod to test consume configMaps 07/29/23 15:38:15.215
Jul 29 15:38:15.236: INFO: Waiting up to 5m0s for pod "pod-configmaps-f098a937-d449-47b4-882b-004763bfab29" in namespace "configmap-7239" to be "Succeeded or Failed"
Jul 29 15:38:15.245: INFO: Pod "pod-configmaps-f098a937-d449-47b4-882b-004763bfab29": Phase="Pending", Reason="", readiness=false. Elapsed: 9.389673ms
Jul 29 15:38:17.252: INFO: Pod "pod-configmaps-f098a937-d449-47b4-882b-004763bfab29": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016760339s
Jul 29 15:38:19.253: INFO: Pod "pod-configmaps-f098a937-d449-47b4-882b-004763bfab29": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017432802s
STEP: Saw pod success 07/29/23 15:38:19.253
Jul 29 15:38:19.253: INFO: Pod "pod-configmaps-f098a937-d449-47b4-882b-004763bfab29" satisfied condition "Succeeded or Failed"
Jul 29 15:38:19.260: INFO: Trying to get logs from node wa4quivohpee-3 pod pod-configmaps-f098a937-d449-47b4-882b-004763bfab29 container agnhost-container: <nil>
STEP: delete the pod 07/29/23 15:38:19.301
Jul 29 15:38:19.319: INFO: Waiting for pod pod-configmaps-f098a937-d449-47b4-882b-004763bfab29 to disappear
Jul 29 15:38:19.326: INFO: Pod pod-configmaps-f098a937-d449-47b4-882b-004763bfab29 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jul 29 15:38:19.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7239" for this suite. 07/29/23 15:38:19.335
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","completed":37,"skipped":738,"failed":0}
------------------------------
â€¢ [4.203 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:38:15.15
    Jul 29 15:38:15.150: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename configmap 07/29/23 15:38:15.152
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:38:15.192
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:38:15.199
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:73
    STEP: Creating configMap with name configmap-test-volume-d05c582a-b79d-437c-b978-46b407fc3a70 07/29/23 15:38:15.204
    STEP: Creating a pod to test consume configMaps 07/29/23 15:38:15.215
    Jul 29 15:38:15.236: INFO: Waiting up to 5m0s for pod "pod-configmaps-f098a937-d449-47b4-882b-004763bfab29" in namespace "configmap-7239" to be "Succeeded or Failed"
    Jul 29 15:38:15.245: INFO: Pod "pod-configmaps-f098a937-d449-47b4-882b-004763bfab29": Phase="Pending", Reason="", readiness=false. Elapsed: 9.389673ms
    Jul 29 15:38:17.252: INFO: Pod "pod-configmaps-f098a937-d449-47b4-882b-004763bfab29": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016760339s
    Jul 29 15:38:19.253: INFO: Pod "pod-configmaps-f098a937-d449-47b4-882b-004763bfab29": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017432802s
    STEP: Saw pod success 07/29/23 15:38:19.253
    Jul 29 15:38:19.253: INFO: Pod "pod-configmaps-f098a937-d449-47b4-882b-004763bfab29" satisfied condition "Succeeded or Failed"
    Jul 29 15:38:19.260: INFO: Trying to get logs from node wa4quivohpee-3 pod pod-configmaps-f098a937-d449-47b4-882b-004763bfab29 container agnhost-container: <nil>
    STEP: delete the pod 07/29/23 15:38:19.301
    Jul 29 15:38:19.319: INFO: Waiting for pod pod-configmaps-f098a937-d449-47b4-882b-004763bfab29 to disappear
    Jul 29 15:38:19.326: INFO: Pod pod-configmaps-f098a937-d449-47b4-882b-004763bfab29 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jul 29 15:38:19.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-7239" for this suite. 07/29/23 15:38:19.335
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] EndpointSlice
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:38:19.354
Jul 29 15:38:19.355: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename endpointslice 07/29/23 15:38:19.357
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:38:19.403
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:38:19.407
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352
STEP: getting /apis 07/29/23 15:38:19.415
STEP: getting /apis/discovery.k8s.io 07/29/23 15:38:19.42
STEP: getting /apis/discovery.k8s.iov1 07/29/23 15:38:19.425
STEP: creating 07/29/23 15:38:19.431
STEP: getting 07/29/23 15:38:19.468
STEP: listing 07/29/23 15:38:19.475
STEP: watching 07/29/23 15:38:19.483
Jul 29 15:38:19.484: INFO: starting watch
STEP: cluster-wide listing 07/29/23 15:38:19.487
STEP: cluster-wide watching 07/29/23 15:38:19.492
Jul 29 15:38:19.493: INFO: starting watch
STEP: patching 07/29/23 15:38:19.496
STEP: updating 07/29/23 15:38:19.505
Jul 29 15:38:19.533: INFO: waiting for watch events with expected annotations
Jul 29 15:38:19.534: INFO: saw patched and updated annotations
STEP: deleting 07/29/23 15:38:19.534
STEP: deleting a collection 07/29/23 15:38:19.566
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Jul 29 15:38:19.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-3603" for this suite. 07/29/23 15:38:19.607
{"msg":"PASSED [sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]","completed":38,"skipped":738,"failed":0}
------------------------------
â€¢ [0.272 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:38:19.354
    Jul 29 15:38:19.355: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename endpointslice 07/29/23 15:38:19.357
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:38:19.403
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:38:19.407
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should support creating EndpointSlice API operations [Conformance]
      test/e2e/network/endpointslice.go:352
    STEP: getting /apis 07/29/23 15:38:19.415
    STEP: getting /apis/discovery.k8s.io 07/29/23 15:38:19.42
    STEP: getting /apis/discovery.k8s.iov1 07/29/23 15:38:19.425
    STEP: creating 07/29/23 15:38:19.431
    STEP: getting 07/29/23 15:38:19.468
    STEP: listing 07/29/23 15:38:19.475
    STEP: watching 07/29/23 15:38:19.483
    Jul 29 15:38:19.484: INFO: starting watch
    STEP: cluster-wide listing 07/29/23 15:38:19.487
    STEP: cluster-wide watching 07/29/23 15:38:19.492
    Jul 29 15:38:19.493: INFO: starting watch
    STEP: patching 07/29/23 15:38:19.496
    STEP: updating 07/29/23 15:38:19.505
    Jul 29 15:38:19.533: INFO: waiting for watch events with expected annotations
    Jul 29 15:38:19.534: INFO: saw patched and updated annotations
    STEP: deleting 07/29/23 15:38:19.534
    STEP: deleting a collection 07/29/23 15:38:19.566
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Jul 29 15:38:19.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-3603" for this suite. 07/29/23 15:38:19.607
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:38:19.628
Jul 29 15:38:19.628: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename pods 07/29/23 15:38:19.629
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:38:19.659
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:38:19.665
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082
STEP: Create a pod 07/29/23 15:38:19.67
Jul 29 15:38:19.685: INFO: Waiting up to 5m0s for pod "pod-n7c6r" in namespace "pods-7661" to be "running"
Jul 29 15:38:19.690: INFO: Pod "pod-n7c6r": Phase="Pending", Reason="", readiness=false. Elapsed: 5.580736ms
Jul 29 15:38:21.700: INFO: Pod "pod-n7c6r": Phase="Running", Reason="", readiness=true. Elapsed: 2.015218097s
Jul 29 15:38:21.701: INFO: Pod "pod-n7c6r" satisfied condition "running"
STEP: patching /status 07/29/23 15:38:21.701
Jul 29 15:38:21.729: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jul 29 15:38:21.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7661" for this suite. 07/29/23 15:38:21.737
{"msg":"PASSED [sig-node] Pods should patch a pod status [Conformance]","completed":39,"skipped":761,"failed":0}
------------------------------
â€¢ [2.122 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:38:19.628
    Jul 29 15:38:19.628: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename pods 07/29/23 15:38:19.629
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:38:19.659
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:38:19.665
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should patch a pod status [Conformance]
      test/e2e/common/node/pods.go:1082
    STEP: Create a pod 07/29/23 15:38:19.67
    Jul 29 15:38:19.685: INFO: Waiting up to 5m0s for pod "pod-n7c6r" in namespace "pods-7661" to be "running"
    Jul 29 15:38:19.690: INFO: Pod "pod-n7c6r": Phase="Pending", Reason="", readiness=false. Elapsed: 5.580736ms
    Jul 29 15:38:21.700: INFO: Pod "pod-n7c6r": Phase="Running", Reason="", readiness=true. Elapsed: 2.015218097s
    Jul 29 15:38:21.701: INFO: Pod "pod-n7c6r" satisfied condition "running"
    STEP: patching /status 07/29/23 15:38:21.701
    Jul 29 15:38:21.729: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jul 29 15:38:21.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-7661" for this suite. 07/29/23 15:38:21.737
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:38:21.752
Jul 29 15:38:21.752: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename subpath 07/29/23 15:38:21.755
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:38:21.786
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:38:21.79
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 07/29/23 15:38:21.793
[It] should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
STEP: Creating pod pod-subpath-test-downwardapi-89pg 07/29/23 15:38:21.809
STEP: Creating a pod to test atomic-volume-subpath 07/29/23 15:38:21.809
Jul 29 15:38:21.832: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-89pg" in namespace "subpath-5940" to be "Succeeded or Failed"
Jul 29 15:38:21.844: INFO: Pod "pod-subpath-test-downwardapi-89pg": Phase="Pending", Reason="", readiness=false. Elapsed: 11.337976ms
Jul 29 15:38:23.856: INFO: Pod "pod-subpath-test-downwardapi-89pg": Phase="Running", Reason="", readiness=true. Elapsed: 2.023261284s
Jul 29 15:38:25.852: INFO: Pod "pod-subpath-test-downwardapi-89pg": Phase="Running", Reason="", readiness=true. Elapsed: 4.020240536s
Jul 29 15:38:27.852: INFO: Pod "pod-subpath-test-downwardapi-89pg": Phase="Running", Reason="", readiness=true. Elapsed: 6.020077301s
Jul 29 15:38:29.853: INFO: Pod "pod-subpath-test-downwardapi-89pg": Phase="Running", Reason="", readiness=true. Elapsed: 8.020496878s
Jul 29 15:38:31.853: INFO: Pod "pod-subpath-test-downwardapi-89pg": Phase="Running", Reason="", readiness=true. Elapsed: 10.020832158s
Jul 29 15:38:33.853: INFO: Pod "pod-subpath-test-downwardapi-89pg": Phase="Running", Reason="", readiness=true. Elapsed: 12.020308351s
Jul 29 15:38:35.855: INFO: Pod "pod-subpath-test-downwardapi-89pg": Phase="Running", Reason="", readiness=true. Elapsed: 14.023048303s
Jul 29 15:38:37.854: INFO: Pod "pod-subpath-test-downwardapi-89pg": Phase="Running", Reason="", readiness=true. Elapsed: 16.022088023s
Jul 29 15:38:39.855: INFO: Pod "pod-subpath-test-downwardapi-89pg": Phase="Running", Reason="", readiness=true. Elapsed: 18.02273348s
Jul 29 15:38:41.855: INFO: Pod "pod-subpath-test-downwardapi-89pg": Phase="Running", Reason="", readiness=true. Elapsed: 20.022310618s
Jul 29 15:38:43.854: INFO: Pod "pod-subpath-test-downwardapi-89pg": Phase="Running", Reason="", readiness=false. Elapsed: 22.021326973s
Jul 29 15:38:45.871: INFO: Pod "pod-subpath-test-downwardapi-89pg": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.038615843s
STEP: Saw pod success 07/29/23 15:38:45.872
Jul 29 15:38:45.873: INFO: Pod "pod-subpath-test-downwardapi-89pg" satisfied condition "Succeeded or Failed"
Jul 29 15:38:45.881: INFO: Trying to get logs from node wa4quivohpee-3 pod pod-subpath-test-downwardapi-89pg container test-container-subpath-downwardapi-89pg: <nil>
STEP: delete the pod 07/29/23 15:38:45.897
Jul 29 15:38:45.919: INFO: Waiting for pod pod-subpath-test-downwardapi-89pg to disappear
Jul 29 15:38:45.924: INFO: Pod pod-subpath-test-downwardapi-89pg no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-89pg 07/29/23 15:38:45.925
Jul 29 15:38:45.925: INFO: Deleting pod "pod-subpath-test-downwardapi-89pg" in namespace "subpath-5940"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Jul 29 15:38:45.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5940" for this suite. 07/29/23 15:38:45.938
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [Conformance]","completed":40,"skipped":769,"failed":0}
------------------------------
â€¢ [SLOW TEST] [24.196 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with downward pod [Conformance]
    test/e2e/storage/subpath.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:38:21.752
    Jul 29 15:38:21.752: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename subpath 07/29/23 15:38:21.755
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:38:21.786
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:38:21.79
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 07/29/23 15:38:21.793
    [It] should support subpaths with downward pod [Conformance]
      test/e2e/storage/subpath.go:92
    STEP: Creating pod pod-subpath-test-downwardapi-89pg 07/29/23 15:38:21.809
    STEP: Creating a pod to test atomic-volume-subpath 07/29/23 15:38:21.809
    Jul 29 15:38:21.832: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-89pg" in namespace "subpath-5940" to be "Succeeded or Failed"
    Jul 29 15:38:21.844: INFO: Pod "pod-subpath-test-downwardapi-89pg": Phase="Pending", Reason="", readiness=false. Elapsed: 11.337976ms
    Jul 29 15:38:23.856: INFO: Pod "pod-subpath-test-downwardapi-89pg": Phase="Running", Reason="", readiness=true. Elapsed: 2.023261284s
    Jul 29 15:38:25.852: INFO: Pod "pod-subpath-test-downwardapi-89pg": Phase="Running", Reason="", readiness=true. Elapsed: 4.020240536s
    Jul 29 15:38:27.852: INFO: Pod "pod-subpath-test-downwardapi-89pg": Phase="Running", Reason="", readiness=true. Elapsed: 6.020077301s
    Jul 29 15:38:29.853: INFO: Pod "pod-subpath-test-downwardapi-89pg": Phase="Running", Reason="", readiness=true. Elapsed: 8.020496878s
    Jul 29 15:38:31.853: INFO: Pod "pod-subpath-test-downwardapi-89pg": Phase="Running", Reason="", readiness=true. Elapsed: 10.020832158s
    Jul 29 15:38:33.853: INFO: Pod "pod-subpath-test-downwardapi-89pg": Phase="Running", Reason="", readiness=true. Elapsed: 12.020308351s
    Jul 29 15:38:35.855: INFO: Pod "pod-subpath-test-downwardapi-89pg": Phase="Running", Reason="", readiness=true. Elapsed: 14.023048303s
    Jul 29 15:38:37.854: INFO: Pod "pod-subpath-test-downwardapi-89pg": Phase="Running", Reason="", readiness=true. Elapsed: 16.022088023s
    Jul 29 15:38:39.855: INFO: Pod "pod-subpath-test-downwardapi-89pg": Phase="Running", Reason="", readiness=true. Elapsed: 18.02273348s
    Jul 29 15:38:41.855: INFO: Pod "pod-subpath-test-downwardapi-89pg": Phase="Running", Reason="", readiness=true. Elapsed: 20.022310618s
    Jul 29 15:38:43.854: INFO: Pod "pod-subpath-test-downwardapi-89pg": Phase="Running", Reason="", readiness=false. Elapsed: 22.021326973s
    Jul 29 15:38:45.871: INFO: Pod "pod-subpath-test-downwardapi-89pg": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.038615843s
    STEP: Saw pod success 07/29/23 15:38:45.872
    Jul 29 15:38:45.873: INFO: Pod "pod-subpath-test-downwardapi-89pg" satisfied condition "Succeeded or Failed"
    Jul 29 15:38:45.881: INFO: Trying to get logs from node wa4quivohpee-3 pod pod-subpath-test-downwardapi-89pg container test-container-subpath-downwardapi-89pg: <nil>
    STEP: delete the pod 07/29/23 15:38:45.897
    Jul 29 15:38:45.919: INFO: Waiting for pod pod-subpath-test-downwardapi-89pg to disappear
    Jul 29 15:38:45.924: INFO: Pod pod-subpath-test-downwardapi-89pg no longer exists
    STEP: Deleting pod pod-subpath-test-downwardapi-89pg 07/29/23 15:38:45.925
    Jul 29 15:38:45.925: INFO: Deleting pod "pod-subpath-test-downwardapi-89pg" in namespace "subpath-5940"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Jul 29 15:38:45.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-5940" for this suite. 07/29/23 15:38:45.938
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
[BeforeEach] [sig-node] Pods Extended
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:38:45.952
Jul 29 15:38:45.952: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename pods 07/29/23 15:38:45.955
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:38:45.984
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:38:45.989
[BeforeEach] Pods Set QOS Class
  test/e2e/node/pods.go:152
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
STEP: creating the pod 07/29/23 15:38:45.993
STEP: submitting the pod to kubernetes 07/29/23 15:38:45.994
STEP: verifying QOS class is set on the pod 07/29/23 15:38:46.009
[AfterEach] [sig-node] Pods Extended
  test/e2e/framework/framework.go:187
Jul 29 15:38:46.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6654" for this suite. 07/29/23 15:38:46.037
{"msg":"PASSED [sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","completed":41,"skipped":772,"failed":0}
------------------------------
â€¢ [0.097 seconds]
[sig-node] Pods Extended
test/e2e/node/framework.go:23
  Pods Set QOS Class
  test/e2e/node/pods.go:150
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    test/e2e/node/pods.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods Extended
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:38:45.952
    Jul 29 15:38:45.952: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename pods 07/29/23 15:38:45.955
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:38:45.984
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:38:45.989
    [BeforeEach] Pods Set QOS Class
      test/e2e/node/pods.go:152
    [It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
      test/e2e/node/pods.go:161
    STEP: creating the pod 07/29/23 15:38:45.993
    STEP: submitting the pod to kubernetes 07/29/23 15:38:45.994
    STEP: verifying QOS class is set on the pod 07/29/23 15:38:46.009
    [AfterEach] [sig-node] Pods Extended
      test/e2e/framework/framework.go:187
    Jul 29 15:38:46.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-6654" for this suite. 07/29/23 15:38:46.037
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:38:46.049
Jul 29 15:38:46.049: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename pod-network-test 07/29/23 15:38:46.051
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:38:46.083
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:38:46.087
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
STEP: Performing setup for networking test in namespace pod-network-test-5333 07/29/23 15:38:46.09
STEP: creating a selector 07/29/23 15:38:46.09
STEP: Creating the service pods in kubernetes 07/29/23 15:38:46.091
Jul 29 15:38:46.091: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jul 29 15:38:46.155: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-5333" to be "running and ready"
Jul 29 15:38:46.199: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 43.851861ms
Jul 29 15:38:46.199: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jul 29 15:38:48.207: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.051956033s
Jul 29 15:38:48.207: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jul 29 15:38:50.205: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.04985259s
Jul 29 15:38:50.205: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jul 29 15:38:52.209: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.053737045s
Jul 29 15:38:52.209: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jul 29 15:38:54.207: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.052055525s
Jul 29 15:38:54.207: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jul 29 15:38:56.207: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.052199229s
Jul 29 15:38:56.207: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jul 29 15:38:58.207: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.051892961s
Jul 29 15:38:58.207: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jul 29 15:39:00.208: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.052795352s
Jul 29 15:39:00.208: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jul 29 15:39:02.206: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.05107984s
Jul 29 15:39:02.206: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jul 29 15:39:04.208: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.052900236s
Jul 29 15:39:04.208: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jul 29 15:39:06.210: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.055011734s
Jul 29 15:39:06.210: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jul 29 15:39:08.207: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.052095312s
Jul 29 15:39:08.207: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jul 29 15:39:08.207: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jul 29 15:39:08.215: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-5333" to be "running and ready"
Jul 29 15:39:08.220: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 4.253021ms
Jul 29 15:39:08.220: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jul 29 15:39:08.220: INFO: Pod "netserver-1" satisfied condition "running and ready"
Jul 29 15:39:08.225: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-5333" to be "running and ready"
Jul 29 15:39:08.230: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 4.846567ms
Jul 29 15:39:08.231: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Jul 29 15:39:08.231: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 07/29/23 15:39:08.236
Jul 29 15:39:08.244: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-5333" to be "running"
Jul 29 15:39:08.251: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.972442ms
Jul 29 15:39:10.260: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.015878809s
Jul 29 15:39:10.261: INFO: Pod "test-container-pod" satisfied condition "running"
Jul 29 15:39:10.272: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Jul 29 15:39:10.272: INFO: Breadth first check of 10.233.64.142 on host 192.168.121.28...
Jul 29 15:39:10.279: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.65.117:9080/dial?request=hostname&protocol=http&host=10.233.64.142&port=8083&tries=1'] Namespace:pod-network-test-5333 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 29 15:39:10.280: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
Jul 29 15:39:10.282: INFO: ExecWithOptions: Clientset creation
Jul 29 15:39:10.282: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-5333/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.65.117%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.64.142%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jul 29 15:39:10.438: INFO: Waiting for responses: map[]
Jul 29 15:39:10.439: INFO: reached 10.233.64.142 after 0/1 tries
Jul 29 15:39:10.439: INFO: Breadth first check of 10.233.66.74 on host 192.168.121.206...
Jul 29 15:39:10.448: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.65.117:9080/dial?request=hostname&protocol=http&host=10.233.66.74&port=8083&tries=1'] Namespace:pod-network-test-5333 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 29 15:39:10.448: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
Jul 29 15:39:10.452: INFO: ExecWithOptions: Clientset creation
Jul 29 15:39:10.452: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-5333/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.65.117%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.66.74%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jul 29 15:39:10.603: INFO: Waiting for responses: map[]
Jul 29 15:39:10.603: INFO: reached 10.233.66.74 after 0/1 tries
Jul 29 15:39:10.603: INFO: Breadth first check of 10.233.65.165 on host 192.168.121.234...
Jul 29 15:39:10.623: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.65.117:9080/dial?request=hostname&protocol=http&host=10.233.65.165&port=8083&tries=1'] Namespace:pod-network-test-5333 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 29 15:39:10.623: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
Jul 29 15:39:10.625: INFO: ExecWithOptions: Clientset creation
Jul 29 15:39:10.625: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-5333/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.65.117%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.65.165%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jul 29 15:39:10.770: INFO: Waiting for responses: map[]
Jul 29 15:39:10.770: INFO: reached 10.233.65.165 after 0/1 tries
Jul 29 15:39:10.770: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Jul 29 15:39:10.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5333" for this suite. 07/29/23 15:39:10.778
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","completed":42,"skipped":772,"failed":0}
------------------------------
â€¢ [SLOW TEST] [24.745 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:38:46.049
    Jul 29 15:38:46.049: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename pod-network-test 07/29/23 15:38:46.051
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:38:46.083
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:38:46.087
    [It] should function for intra-pod communication: http [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:82
    STEP: Performing setup for networking test in namespace pod-network-test-5333 07/29/23 15:38:46.09
    STEP: creating a selector 07/29/23 15:38:46.09
    STEP: Creating the service pods in kubernetes 07/29/23 15:38:46.091
    Jul 29 15:38:46.091: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jul 29 15:38:46.155: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-5333" to be "running and ready"
    Jul 29 15:38:46.199: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 43.851861ms
    Jul 29 15:38:46.199: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jul 29 15:38:48.207: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.051956033s
    Jul 29 15:38:48.207: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jul 29 15:38:50.205: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.04985259s
    Jul 29 15:38:50.205: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jul 29 15:38:52.209: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.053737045s
    Jul 29 15:38:52.209: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jul 29 15:38:54.207: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.052055525s
    Jul 29 15:38:54.207: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jul 29 15:38:56.207: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.052199229s
    Jul 29 15:38:56.207: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jul 29 15:38:58.207: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.051892961s
    Jul 29 15:38:58.207: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jul 29 15:39:00.208: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.052795352s
    Jul 29 15:39:00.208: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jul 29 15:39:02.206: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.05107984s
    Jul 29 15:39:02.206: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jul 29 15:39:04.208: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.052900236s
    Jul 29 15:39:04.208: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jul 29 15:39:06.210: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.055011734s
    Jul 29 15:39:06.210: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jul 29 15:39:08.207: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.052095312s
    Jul 29 15:39:08.207: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jul 29 15:39:08.207: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jul 29 15:39:08.215: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-5333" to be "running and ready"
    Jul 29 15:39:08.220: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 4.253021ms
    Jul 29 15:39:08.220: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jul 29 15:39:08.220: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Jul 29 15:39:08.225: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-5333" to be "running and ready"
    Jul 29 15:39:08.230: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 4.846567ms
    Jul 29 15:39:08.231: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Jul 29 15:39:08.231: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 07/29/23 15:39:08.236
    Jul 29 15:39:08.244: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-5333" to be "running"
    Jul 29 15:39:08.251: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.972442ms
    Jul 29 15:39:10.260: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.015878809s
    Jul 29 15:39:10.261: INFO: Pod "test-container-pod" satisfied condition "running"
    Jul 29 15:39:10.272: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Jul 29 15:39:10.272: INFO: Breadth first check of 10.233.64.142 on host 192.168.121.28...
    Jul 29 15:39:10.279: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.65.117:9080/dial?request=hostname&protocol=http&host=10.233.64.142&port=8083&tries=1'] Namespace:pod-network-test-5333 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jul 29 15:39:10.280: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    Jul 29 15:39:10.282: INFO: ExecWithOptions: Clientset creation
    Jul 29 15:39:10.282: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-5333/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.65.117%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.64.142%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jul 29 15:39:10.438: INFO: Waiting for responses: map[]
    Jul 29 15:39:10.439: INFO: reached 10.233.64.142 after 0/1 tries
    Jul 29 15:39:10.439: INFO: Breadth first check of 10.233.66.74 on host 192.168.121.206...
    Jul 29 15:39:10.448: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.65.117:9080/dial?request=hostname&protocol=http&host=10.233.66.74&port=8083&tries=1'] Namespace:pod-network-test-5333 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jul 29 15:39:10.448: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    Jul 29 15:39:10.452: INFO: ExecWithOptions: Clientset creation
    Jul 29 15:39:10.452: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-5333/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.65.117%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.66.74%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jul 29 15:39:10.603: INFO: Waiting for responses: map[]
    Jul 29 15:39:10.603: INFO: reached 10.233.66.74 after 0/1 tries
    Jul 29 15:39:10.603: INFO: Breadth first check of 10.233.65.165 on host 192.168.121.234...
    Jul 29 15:39:10.623: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.65.117:9080/dial?request=hostname&protocol=http&host=10.233.65.165&port=8083&tries=1'] Namespace:pod-network-test-5333 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jul 29 15:39:10.623: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    Jul 29 15:39:10.625: INFO: ExecWithOptions: Clientset creation
    Jul 29 15:39:10.625: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-5333/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.65.117%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.65.165%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jul 29 15:39:10.770: INFO: Waiting for responses: map[]
    Jul 29 15:39:10.770: INFO: reached 10.233.65.165 after 0/1 tries
    Jul 29 15:39:10.770: INFO: Going to retry 0 out of 3 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Jul 29 15:39:10.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-5333" for this suite. 07/29/23 15:39:10.778
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:39:10.797
Jul 29 15:39:10.797: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename runtimeclass 07/29/23 15:39:10.799
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:39:10.839
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:39:10.845
[It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
STEP: Deleting RuntimeClass runtimeclass-8997-delete-me 07/29/23 15:39:10.859
STEP: Waiting for the RuntimeClass to disappear 07/29/23 15:39:10.874
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Jul 29 15:39:10.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-8997" for this suite. 07/29/23 15:39:10.908
{"msg":"PASSED [sig-node] RuntimeClass should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]","completed":43,"skipped":781,"failed":0}
------------------------------
â€¢ [0.125 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:39:10.797
    Jul 29 15:39:10.797: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename runtimeclass 07/29/23 15:39:10.799
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:39:10.839
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:39:10.845
    [It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:156
    STEP: Deleting RuntimeClass runtimeclass-8997-delete-me 07/29/23 15:39:10.859
    STEP: Waiting for the RuntimeClass to disappear 07/29/23 15:39:10.874
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Jul 29 15:39:10.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-8997" for this suite. 07/29/23 15:39:10.908
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:39:10.925
Jul 29 15:39:10.926: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename crd-webhook 07/29/23 15:39:10.928
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:39:10.967
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:39:10.979
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 07/29/23 15:39:10.989
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 07/29/23 15:39:11.683
STEP: Deploying the custom resource conversion webhook pod 07/29/23 15:39:11.704
STEP: Wait for the deployment to be ready 07/29/23 15:39:11.737
Jul 29 15:39:11.762: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 07/29/23 15:39:13.786
STEP: Verifying the service has paired with the endpoint 07/29/23 15:39:13.818
Jul 29 15:39:14.820: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
Jul 29 15:39:14.830: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Creating a v1 custom resource 07/29/23 15:39:17.639
STEP: v2 custom resource should be converted 07/29/23 15:39:17.653
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jul 29 15:39:18.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-8855" for this suite. 07/29/23 15:39:18.235
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","completed":44,"skipped":783,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.410 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:39:10.925
    Jul 29 15:39:10.926: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename crd-webhook 07/29/23 15:39:10.928
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:39:10.967
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:39:10.979
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 07/29/23 15:39:10.989
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 07/29/23 15:39:11.683
    STEP: Deploying the custom resource conversion webhook pod 07/29/23 15:39:11.704
    STEP: Wait for the deployment to be ready 07/29/23 15:39:11.737
    Jul 29 15:39:11.762: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 07/29/23 15:39:13.786
    STEP: Verifying the service has paired with the endpoint 07/29/23 15:39:13.818
    Jul 29 15:39:14.820: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert from CR v1 to CR v2 [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:149
    Jul 29 15:39:14.830: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Creating a v1 custom resource 07/29/23 15:39:17.639
    STEP: v2 custom resource should be converted 07/29/23 15:39:17.653
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jul 29 15:39:18.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-webhook-8855" for this suite. 07/29/23 15:39:18.235
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] ReplicationController
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:39:18.342
Jul 29 15:39:18.342: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename replication-controller 07/29/23 15:39:18.345
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:39:18.806
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:39:18.838
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66
STEP: Creating replication controller my-hostname-basic-9cc15cac-b2ed-4881-877e-7b2040dbfe62 07/29/23 15:39:18.865
Jul 29 15:39:18.897: INFO: Pod name my-hostname-basic-9cc15cac-b2ed-4881-877e-7b2040dbfe62: Found 0 pods out of 1
Jul 29 15:39:23.908: INFO: Pod name my-hostname-basic-9cc15cac-b2ed-4881-877e-7b2040dbfe62: Found 1 pods out of 1
Jul 29 15:39:23.909: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-9cc15cac-b2ed-4881-877e-7b2040dbfe62" are running
Jul 29 15:39:23.909: INFO: Waiting up to 5m0s for pod "my-hostname-basic-9cc15cac-b2ed-4881-877e-7b2040dbfe62-ftbpc" in namespace "replication-controller-9454" to be "running"
Jul 29 15:39:23.919: INFO: Pod "my-hostname-basic-9cc15cac-b2ed-4881-877e-7b2040dbfe62-ftbpc": Phase="Running", Reason="", readiness=true. Elapsed: 9.60921ms
Jul 29 15:39:23.919: INFO: Pod "my-hostname-basic-9cc15cac-b2ed-4881-877e-7b2040dbfe62-ftbpc" satisfied condition "running"
Jul 29 15:39:23.919: INFO: Pod "my-hostname-basic-9cc15cac-b2ed-4881-877e-7b2040dbfe62-ftbpc" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-07-29 15:39:18 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-07-29 15:39:20 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-07-29 15:39:20 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-07-29 15:39:18 +0000 UTC Reason: Message:}])
Jul 29 15:39:23.919: INFO: Trying to dial the pod
Jul 29 15:39:28.946: INFO: Controller my-hostname-basic-9cc15cac-b2ed-4881-877e-7b2040dbfe62: Got expected result from replica 1 [my-hostname-basic-9cc15cac-b2ed-4881-877e-7b2040dbfe62-ftbpc]: "my-hostname-basic-9cc15cac-b2ed-4881-877e-7b2040dbfe62-ftbpc", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Jul 29 15:39:28.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9454" for this suite. 07/29/23 15:39:28.956
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","completed":45,"skipped":789,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.631 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:39:18.342
    Jul 29 15:39:18.342: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename replication-controller 07/29/23 15:39:18.345
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:39:18.806
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:39:18.838
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/rc.go:66
    STEP: Creating replication controller my-hostname-basic-9cc15cac-b2ed-4881-877e-7b2040dbfe62 07/29/23 15:39:18.865
    Jul 29 15:39:18.897: INFO: Pod name my-hostname-basic-9cc15cac-b2ed-4881-877e-7b2040dbfe62: Found 0 pods out of 1
    Jul 29 15:39:23.908: INFO: Pod name my-hostname-basic-9cc15cac-b2ed-4881-877e-7b2040dbfe62: Found 1 pods out of 1
    Jul 29 15:39:23.909: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-9cc15cac-b2ed-4881-877e-7b2040dbfe62" are running
    Jul 29 15:39:23.909: INFO: Waiting up to 5m0s for pod "my-hostname-basic-9cc15cac-b2ed-4881-877e-7b2040dbfe62-ftbpc" in namespace "replication-controller-9454" to be "running"
    Jul 29 15:39:23.919: INFO: Pod "my-hostname-basic-9cc15cac-b2ed-4881-877e-7b2040dbfe62-ftbpc": Phase="Running", Reason="", readiness=true. Elapsed: 9.60921ms
    Jul 29 15:39:23.919: INFO: Pod "my-hostname-basic-9cc15cac-b2ed-4881-877e-7b2040dbfe62-ftbpc" satisfied condition "running"
    Jul 29 15:39:23.919: INFO: Pod "my-hostname-basic-9cc15cac-b2ed-4881-877e-7b2040dbfe62-ftbpc" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-07-29 15:39:18 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-07-29 15:39:20 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-07-29 15:39:20 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-07-29 15:39:18 +0000 UTC Reason: Message:}])
    Jul 29 15:39:23.919: INFO: Trying to dial the pod
    Jul 29 15:39:28.946: INFO: Controller my-hostname-basic-9cc15cac-b2ed-4881-877e-7b2040dbfe62: Got expected result from replica 1 [my-hostname-basic-9cc15cac-b2ed-4881-877e-7b2040dbfe62-ftbpc]: "my-hostname-basic-9cc15cac-b2ed-4881-877e-7b2040dbfe62-ftbpc", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Jul 29 15:39:28.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-9454" for this suite. 07/29/23 15:39:28.956
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:39:28.974
Jul 29 15:39:28.974: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename resourcequota 07/29/23 15:39:28.98
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:39:29.015
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:39:29.02
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680
STEP: Creating a ResourceQuota with terminating scope 07/29/23 15:39:29.026
STEP: Ensuring ResourceQuota status is calculated 07/29/23 15:39:29.037
STEP: Creating a ResourceQuota with not terminating scope 07/29/23 15:39:31.048
STEP: Ensuring ResourceQuota status is calculated 07/29/23 15:39:31.06
STEP: Creating a long running pod 07/29/23 15:39:33.07
STEP: Ensuring resource quota with not terminating scope captures the pod usage 07/29/23 15:39:33.108
STEP: Ensuring resource quota with terminating scope ignored the pod usage 07/29/23 15:39:35.122
STEP: Deleting the pod 07/29/23 15:39:37.153
STEP: Ensuring resource quota status released the pod usage 07/29/23 15:39:37.195
STEP: Creating a terminating pod 07/29/23 15:39:39.204
STEP: Ensuring resource quota with terminating scope captures the pod usage 07/29/23 15:39:39.228
STEP: Ensuring resource quota with not terminating scope ignored the pod usage 07/29/23 15:39:41.236
STEP: Deleting the pod 07/29/23 15:39:43.244
STEP: Ensuring resource quota status released the pod usage 07/29/23 15:39:43.281
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jul 29 15:39:45.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9977" for this suite. 07/29/23 15:39:45.299
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","completed":46,"skipped":795,"failed":0}
------------------------------
â€¢ [SLOW TEST] [16.338 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:39:28.974
    Jul 29 15:39:28.974: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename resourcequota 07/29/23 15:39:28.98
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:39:29.015
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:39:29.02
    [It] should verify ResourceQuota with terminating scopes. [Conformance]
      test/e2e/apimachinery/resource_quota.go:680
    STEP: Creating a ResourceQuota with terminating scope 07/29/23 15:39:29.026
    STEP: Ensuring ResourceQuota status is calculated 07/29/23 15:39:29.037
    STEP: Creating a ResourceQuota with not terminating scope 07/29/23 15:39:31.048
    STEP: Ensuring ResourceQuota status is calculated 07/29/23 15:39:31.06
    STEP: Creating a long running pod 07/29/23 15:39:33.07
    STEP: Ensuring resource quota with not terminating scope captures the pod usage 07/29/23 15:39:33.108
    STEP: Ensuring resource quota with terminating scope ignored the pod usage 07/29/23 15:39:35.122
    STEP: Deleting the pod 07/29/23 15:39:37.153
    STEP: Ensuring resource quota status released the pod usage 07/29/23 15:39:37.195
    STEP: Creating a terminating pod 07/29/23 15:39:39.204
    STEP: Ensuring resource quota with terminating scope captures the pod usage 07/29/23 15:39:39.228
    STEP: Ensuring resource quota with not terminating scope ignored the pod usage 07/29/23 15:39:41.236
    STEP: Deleting the pod 07/29/23 15:39:43.244
    STEP: Ensuring resource quota status released the pod usage 07/29/23 15:39:43.281
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jul 29 15:39:45.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-9977" for this suite. 07/29/23 15:39:45.299
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:39:45.321
Jul 29 15:39:45.321: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename dns 07/29/23 15:39:45.327
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:39:45.378
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:39:45.383
[It] should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
STEP: Creating a test headless service 07/29/23 15:39:45.391
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-4164.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-4164.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
 07/29/23 15:39:45.403
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-4164.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-4164.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
 07/29/23 15:39:45.404
STEP: creating a pod to probe DNS 07/29/23 15:39:45.404
STEP: submitting the pod to kubernetes 07/29/23 15:39:45.405
Jul 29 15:39:45.431: INFO: Waiting up to 15m0s for pod "dns-test-be313e7d-8cad-4b30-88ee-773430d44eed" in namespace "dns-4164" to be "running"
Jul 29 15:39:45.444: INFO: Pod "dns-test-be313e7d-8cad-4b30-88ee-773430d44eed": Phase="Pending", Reason="", readiness=false. Elapsed: 12.740935ms
Jul 29 15:39:47.452: INFO: Pod "dns-test-be313e7d-8cad-4b30-88ee-773430d44eed": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020864006s
Jul 29 15:39:49.453: INFO: Pod "dns-test-be313e7d-8cad-4b30-88ee-773430d44eed": Phase="Pending", Reason="", readiness=false. Elapsed: 4.02188675s
Jul 29 15:39:51.452: INFO: Pod "dns-test-be313e7d-8cad-4b30-88ee-773430d44eed": Phase="Pending", Reason="", readiness=false. Elapsed: 6.020756825s
Jul 29 15:39:53.453: INFO: Pod "dns-test-be313e7d-8cad-4b30-88ee-773430d44eed": Phase="Pending", Reason="", readiness=false. Elapsed: 8.021977862s
Jul 29 15:39:55.452: INFO: Pod "dns-test-be313e7d-8cad-4b30-88ee-773430d44eed": Phase="Pending", Reason="", readiness=false. Elapsed: 10.020636391s
Jul 29 15:39:57.452: INFO: Pod "dns-test-be313e7d-8cad-4b30-88ee-773430d44eed": Phase="Pending", Reason="", readiness=false. Elapsed: 12.021073777s
Jul 29 15:39:59.451: INFO: Pod "dns-test-be313e7d-8cad-4b30-88ee-773430d44eed": Phase="Pending", Reason="", readiness=false. Elapsed: 14.019813038s
Jul 29 15:40:01.457: INFO: Pod "dns-test-be313e7d-8cad-4b30-88ee-773430d44eed": Phase="Pending", Reason="", readiness=false. Elapsed: 16.025215997s
Jul 29 15:40:03.451: INFO: Pod "dns-test-be313e7d-8cad-4b30-88ee-773430d44eed": Phase="Pending", Reason="", readiness=false. Elapsed: 18.01984182s
Jul 29 15:40:05.452: INFO: Pod "dns-test-be313e7d-8cad-4b30-88ee-773430d44eed": Phase="Pending", Reason="", readiness=false. Elapsed: 20.020318222s
Jul 29 15:40:07.451: INFO: Pod "dns-test-be313e7d-8cad-4b30-88ee-773430d44eed": Phase="Pending", Reason="", readiness=false. Elapsed: 22.019583493s
Jul 29 15:40:09.453: INFO: Pod "dns-test-be313e7d-8cad-4b30-88ee-773430d44eed": Phase="Pending", Reason="", readiness=false. Elapsed: 24.022097086s
Jul 29 15:40:11.458: INFO: Pod "dns-test-be313e7d-8cad-4b30-88ee-773430d44eed": Phase="Pending", Reason="", readiness=false. Elapsed: 26.026625072s
Jul 29 15:40:13.458: INFO: Pod "dns-test-be313e7d-8cad-4b30-88ee-773430d44eed": Phase="Pending", Reason="", readiness=false. Elapsed: 28.026333073s
Jul 29 15:40:15.454: INFO: Pod "dns-test-be313e7d-8cad-4b30-88ee-773430d44eed": Phase="Pending", Reason="", readiness=false. Elapsed: 30.022751731s
Jul 29 15:40:17.451: INFO: Pod "dns-test-be313e7d-8cad-4b30-88ee-773430d44eed": Phase="Pending", Reason="", readiness=false. Elapsed: 32.019170929s
Jul 29 15:40:19.453: INFO: Pod "dns-test-be313e7d-8cad-4b30-88ee-773430d44eed": Phase="Pending", Reason="", readiness=false. Elapsed: 34.021842834s
Jul 29 15:40:21.459: INFO: Pod "dns-test-be313e7d-8cad-4b30-88ee-773430d44eed": Phase="Pending", Reason="", readiness=false. Elapsed: 36.02745338s
Jul 29 15:40:23.456: INFO: Pod "dns-test-be313e7d-8cad-4b30-88ee-773430d44eed": Phase="Pending", Reason="", readiness=false. Elapsed: 38.024871685s
Jul 29 15:40:25.452: INFO: Pod "dns-test-be313e7d-8cad-4b30-88ee-773430d44eed": Phase="Pending", Reason="", readiness=false. Elapsed: 40.020301725s
Jul 29 15:40:27.458: INFO: Pod "dns-test-be313e7d-8cad-4b30-88ee-773430d44eed": Phase="Running", Reason="", readiness=true. Elapsed: 42.026288539s
Jul 29 15:40:27.458: INFO: Pod "dns-test-be313e7d-8cad-4b30-88ee-773430d44eed" satisfied condition "running"
STEP: retrieving the pod 07/29/23 15:40:27.458
STEP: looking for the results for each expected name from probers 07/29/23 15:40:27.466
Jul 29 15:40:27.509: INFO: DNS probes using dns-4164/dns-test-be313e7d-8cad-4b30-88ee-773430d44eed succeeded

STEP: deleting the pod 07/29/23 15:40:27.509
STEP: deleting the test headless service 07/29/23 15:40:27.533
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jul 29 15:40:27.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4164" for this suite. 07/29/23 15:40:27.572
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [Conformance]","completed":47,"skipped":805,"failed":0}
------------------------------
â€¢ [SLOW TEST] [42.269 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:39:45.321
    Jul 29 15:39:45.321: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename dns 07/29/23 15:39:45.327
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:39:45.378
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:39:45.383
    [It] should provide DNS for pods for Hostname [Conformance]
      test/e2e/network/dns.go:248
    STEP: Creating a test headless service 07/29/23 15:39:45.391
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-4164.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-4164.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
     07/29/23 15:39:45.403
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-4164.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-4164.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
     07/29/23 15:39:45.404
    STEP: creating a pod to probe DNS 07/29/23 15:39:45.404
    STEP: submitting the pod to kubernetes 07/29/23 15:39:45.405
    Jul 29 15:39:45.431: INFO: Waiting up to 15m0s for pod "dns-test-be313e7d-8cad-4b30-88ee-773430d44eed" in namespace "dns-4164" to be "running"
    Jul 29 15:39:45.444: INFO: Pod "dns-test-be313e7d-8cad-4b30-88ee-773430d44eed": Phase="Pending", Reason="", readiness=false. Elapsed: 12.740935ms
    Jul 29 15:39:47.452: INFO: Pod "dns-test-be313e7d-8cad-4b30-88ee-773430d44eed": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020864006s
    Jul 29 15:39:49.453: INFO: Pod "dns-test-be313e7d-8cad-4b30-88ee-773430d44eed": Phase="Pending", Reason="", readiness=false. Elapsed: 4.02188675s
    Jul 29 15:39:51.452: INFO: Pod "dns-test-be313e7d-8cad-4b30-88ee-773430d44eed": Phase="Pending", Reason="", readiness=false. Elapsed: 6.020756825s
    Jul 29 15:39:53.453: INFO: Pod "dns-test-be313e7d-8cad-4b30-88ee-773430d44eed": Phase="Pending", Reason="", readiness=false. Elapsed: 8.021977862s
    Jul 29 15:39:55.452: INFO: Pod "dns-test-be313e7d-8cad-4b30-88ee-773430d44eed": Phase="Pending", Reason="", readiness=false. Elapsed: 10.020636391s
    Jul 29 15:39:57.452: INFO: Pod "dns-test-be313e7d-8cad-4b30-88ee-773430d44eed": Phase="Pending", Reason="", readiness=false. Elapsed: 12.021073777s
    Jul 29 15:39:59.451: INFO: Pod "dns-test-be313e7d-8cad-4b30-88ee-773430d44eed": Phase="Pending", Reason="", readiness=false. Elapsed: 14.019813038s
    Jul 29 15:40:01.457: INFO: Pod "dns-test-be313e7d-8cad-4b30-88ee-773430d44eed": Phase="Pending", Reason="", readiness=false. Elapsed: 16.025215997s
    Jul 29 15:40:03.451: INFO: Pod "dns-test-be313e7d-8cad-4b30-88ee-773430d44eed": Phase="Pending", Reason="", readiness=false. Elapsed: 18.01984182s
    Jul 29 15:40:05.452: INFO: Pod "dns-test-be313e7d-8cad-4b30-88ee-773430d44eed": Phase="Pending", Reason="", readiness=false. Elapsed: 20.020318222s
    Jul 29 15:40:07.451: INFO: Pod "dns-test-be313e7d-8cad-4b30-88ee-773430d44eed": Phase="Pending", Reason="", readiness=false. Elapsed: 22.019583493s
    Jul 29 15:40:09.453: INFO: Pod "dns-test-be313e7d-8cad-4b30-88ee-773430d44eed": Phase="Pending", Reason="", readiness=false. Elapsed: 24.022097086s
    Jul 29 15:40:11.458: INFO: Pod "dns-test-be313e7d-8cad-4b30-88ee-773430d44eed": Phase="Pending", Reason="", readiness=false. Elapsed: 26.026625072s
    Jul 29 15:40:13.458: INFO: Pod "dns-test-be313e7d-8cad-4b30-88ee-773430d44eed": Phase="Pending", Reason="", readiness=false. Elapsed: 28.026333073s
    Jul 29 15:40:15.454: INFO: Pod "dns-test-be313e7d-8cad-4b30-88ee-773430d44eed": Phase="Pending", Reason="", readiness=false. Elapsed: 30.022751731s
    Jul 29 15:40:17.451: INFO: Pod "dns-test-be313e7d-8cad-4b30-88ee-773430d44eed": Phase="Pending", Reason="", readiness=false. Elapsed: 32.019170929s
    Jul 29 15:40:19.453: INFO: Pod "dns-test-be313e7d-8cad-4b30-88ee-773430d44eed": Phase="Pending", Reason="", readiness=false. Elapsed: 34.021842834s
    Jul 29 15:40:21.459: INFO: Pod "dns-test-be313e7d-8cad-4b30-88ee-773430d44eed": Phase="Pending", Reason="", readiness=false. Elapsed: 36.02745338s
    Jul 29 15:40:23.456: INFO: Pod "dns-test-be313e7d-8cad-4b30-88ee-773430d44eed": Phase="Pending", Reason="", readiness=false. Elapsed: 38.024871685s
    Jul 29 15:40:25.452: INFO: Pod "dns-test-be313e7d-8cad-4b30-88ee-773430d44eed": Phase="Pending", Reason="", readiness=false. Elapsed: 40.020301725s
    Jul 29 15:40:27.458: INFO: Pod "dns-test-be313e7d-8cad-4b30-88ee-773430d44eed": Phase="Running", Reason="", readiness=true. Elapsed: 42.026288539s
    Jul 29 15:40:27.458: INFO: Pod "dns-test-be313e7d-8cad-4b30-88ee-773430d44eed" satisfied condition "running"
    STEP: retrieving the pod 07/29/23 15:40:27.458
    STEP: looking for the results for each expected name from probers 07/29/23 15:40:27.466
    Jul 29 15:40:27.509: INFO: DNS probes using dns-4164/dns-test-be313e7d-8cad-4b30-88ee-773430d44eed succeeded

    STEP: deleting the pod 07/29/23 15:40:27.509
    STEP: deleting the test headless service 07/29/23 15:40:27.533
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jul 29 15:40:27.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-4164" for this suite. 07/29/23 15:40:27.572
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:40:27.625
Jul 29 15:40:27.625: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename subpath 07/29/23 15:40:27.643
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:40:27.678
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:40:27.683
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 07/29/23 15:40:27.688
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
STEP: Creating pod pod-subpath-test-configmap-glh5 07/29/23 15:40:27.704
STEP: Creating a pod to test atomic-volume-subpath 07/29/23 15:40:27.704
Jul 29 15:40:27.725: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-glh5" in namespace "subpath-2518" to be "Succeeded or Failed"
Jul 29 15:40:27.731: INFO: Pod "pod-subpath-test-configmap-glh5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.175843ms
Jul 29 15:40:29.740: INFO: Pod "pod-subpath-test-configmap-glh5": Phase="Running", Reason="", readiness=true. Elapsed: 2.015023577s
Jul 29 15:40:31.740: INFO: Pod "pod-subpath-test-configmap-glh5": Phase="Running", Reason="", readiness=true. Elapsed: 4.014518136s
Jul 29 15:40:33.745: INFO: Pod "pod-subpath-test-configmap-glh5": Phase="Running", Reason="", readiness=true. Elapsed: 6.020376953s
Jul 29 15:40:35.737: INFO: Pod "pod-subpath-test-configmap-glh5": Phase="Running", Reason="", readiness=true. Elapsed: 8.01241659s
Jul 29 15:40:37.738: INFO: Pod "pod-subpath-test-configmap-glh5": Phase="Running", Reason="", readiness=true. Elapsed: 10.013113954s
Jul 29 15:40:39.743: INFO: Pod "pod-subpath-test-configmap-glh5": Phase="Running", Reason="", readiness=true. Elapsed: 12.018076505s
Jul 29 15:40:41.739: INFO: Pod "pod-subpath-test-configmap-glh5": Phase="Running", Reason="", readiness=true. Elapsed: 14.014297621s
Jul 29 15:40:43.740: INFO: Pod "pod-subpath-test-configmap-glh5": Phase="Running", Reason="", readiness=true. Elapsed: 16.014761648s
Jul 29 15:40:45.738: INFO: Pod "pod-subpath-test-configmap-glh5": Phase="Running", Reason="", readiness=true. Elapsed: 18.013425094s
Jul 29 15:40:47.746: INFO: Pod "pod-subpath-test-configmap-glh5": Phase="Running", Reason="", readiness=true. Elapsed: 20.02125603s
Jul 29 15:40:49.737: INFO: Pod "pod-subpath-test-configmap-glh5": Phase="Running", Reason="", readiness=false. Elapsed: 22.012222895s
Jul 29 15:40:51.739: INFO: Pod "pod-subpath-test-configmap-glh5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.014316241s
STEP: Saw pod success 07/29/23 15:40:51.74
Jul 29 15:40:51.740: INFO: Pod "pod-subpath-test-configmap-glh5" satisfied condition "Succeeded or Failed"
Jul 29 15:40:51.749: INFO: Trying to get logs from node wa4quivohpee-3 pod pod-subpath-test-configmap-glh5 container test-container-subpath-configmap-glh5: <nil>
STEP: delete the pod 07/29/23 15:40:51.78
Jul 29 15:40:51.800: INFO: Waiting for pod pod-subpath-test-configmap-glh5 to disappear
Jul 29 15:40:51.806: INFO: Pod pod-subpath-test-configmap-glh5 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-glh5 07/29/23 15:40:51.806
Jul 29 15:40:51.806: INFO: Deleting pod "pod-subpath-test-configmap-glh5" in namespace "subpath-2518"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Jul 29 15:40:51.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2518" for this suite. 07/29/23 15:40:51.831
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [Conformance]","completed":48,"skipped":837,"failed":0}
------------------------------
â€¢ [SLOW TEST] [24.224 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    test/e2e/storage/subpath.go:80

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:40:27.625
    Jul 29 15:40:27.625: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename subpath 07/29/23 15:40:27.643
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:40:27.678
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:40:27.683
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 07/29/23 15:40:27.688
    [It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
      test/e2e/storage/subpath.go:80
    STEP: Creating pod pod-subpath-test-configmap-glh5 07/29/23 15:40:27.704
    STEP: Creating a pod to test atomic-volume-subpath 07/29/23 15:40:27.704
    Jul 29 15:40:27.725: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-glh5" in namespace "subpath-2518" to be "Succeeded or Failed"
    Jul 29 15:40:27.731: INFO: Pod "pod-subpath-test-configmap-glh5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.175843ms
    Jul 29 15:40:29.740: INFO: Pod "pod-subpath-test-configmap-glh5": Phase="Running", Reason="", readiness=true. Elapsed: 2.015023577s
    Jul 29 15:40:31.740: INFO: Pod "pod-subpath-test-configmap-glh5": Phase="Running", Reason="", readiness=true. Elapsed: 4.014518136s
    Jul 29 15:40:33.745: INFO: Pod "pod-subpath-test-configmap-glh5": Phase="Running", Reason="", readiness=true. Elapsed: 6.020376953s
    Jul 29 15:40:35.737: INFO: Pod "pod-subpath-test-configmap-glh5": Phase="Running", Reason="", readiness=true. Elapsed: 8.01241659s
    Jul 29 15:40:37.738: INFO: Pod "pod-subpath-test-configmap-glh5": Phase="Running", Reason="", readiness=true. Elapsed: 10.013113954s
    Jul 29 15:40:39.743: INFO: Pod "pod-subpath-test-configmap-glh5": Phase="Running", Reason="", readiness=true. Elapsed: 12.018076505s
    Jul 29 15:40:41.739: INFO: Pod "pod-subpath-test-configmap-glh5": Phase="Running", Reason="", readiness=true. Elapsed: 14.014297621s
    Jul 29 15:40:43.740: INFO: Pod "pod-subpath-test-configmap-glh5": Phase="Running", Reason="", readiness=true. Elapsed: 16.014761648s
    Jul 29 15:40:45.738: INFO: Pod "pod-subpath-test-configmap-glh5": Phase="Running", Reason="", readiness=true. Elapsed: 18.013425094s
    Jul 29 15:40:47.746: INFO: Pod "pod-subpath-test-configmap-glh5": Phase="Running", Reason="", readiness=true. Elapsed: 20.02125603s
    Jul 29 15:40:49.737: INFO: Pod "pod-subpath-test-configmap-glh5": Phase="Running", Reason="", readiness=false. Elapsed: 22.012222895s
    Jul 29 15:40:51.739: INFO: Pod "pod-subpath-test-configmap-glh5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.014316241s
    STEP: Saw pod success 07/29/23 15:40:51.74
    Jul 29 15:40:51.740: INFO: Pod "pod-subpath-test-configmap-glh5" satisfied condition "Succeeded or Failed"
    Jul 29 15:40:51.749: INFO: Trying to get logs from node wa4quivohpee-3 pod pod-subpath-test-configmap-glh5 container test-container-subpath-configmap-glh5: <nil>
    STEP: delete the pod 07/29/23 15:40:51.78
    Jul 29 15:40:51.800: INFO: Waiting for pod pod-subpath-test-configmap-glh5 to disappear
    Jul 29 15:40:51.806: INFO: Pod pod-subpath-test-configmap-glh5 no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-glh5 07/29/23 15:40:51.806
    Jul 29 15:40:51.806: INFO: Deleting pod "pod-subpath-test-configmap-glh5" in namespace "subpath-2518"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Jul 29 15:40:51.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-2518" for this suite. 07/29/23 15:40:51.831
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:40:51.852
Jul 29 15:40:51.852: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename configmap 07/29/23 15:40:51.855
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:40:51.884
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:40:51.889
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108
STEP: Creating configMap with name configmap-test-volume-map-07b30992-d119-4bd6-ada2-b4d23bc0ecf6 07/29/23 15:40:51.896
STEP: Creating a pod to test consume configMaps 07/29/23 15:40:51.904
Jul 29 15:40:51.920: INFO: Waiting up to 5m0s for pod "pod-configmaps-0bb4d31e-6d7b-40d6-9660-464a6f553d07" in namespace "configmap-7535" to be "Succeeded or Failed"
Jul 29 15:40:51.925: INFO: Pod "pod-configmaps-0bb4d31e-6d7b-40d6-9660-464a6f553d07": Phase="Pending", Reason="", readiness=false. Elapsed: 4.663203ms
Jul 29 15:40:53.936: INFO: Pod "pod-configmaps-0bb4d31e-6d7b-40d6-9660-464a6f553d07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015633542s
Jul 29 15:40:55.933: INFO: Pod "pod-configmaps-0bb4d31e-6d7b-40d6-9660-464a6f553d07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012579126s
STEP: Saw pod success 07/29/23 15:40:55.933
Jul 29 15:40:55.933: INFO: Pod "pod-configmaps-0bb4d31e-6d7b-40d6-9660-464a6f553d07" satisfied condition "Succeeded or Failed"
Jul 29 15:40:55.940: INFO: Trying to get logs from node wa4quivohpee-3 pod pod-configmaps-0bb4d31e-6d7b-40d6-9660-464a6f553d07 container agnhost-container: <nil>
STEP: delete the pod 07/29/23 15:40:55.953
Jul 29 15:40:55.989: INFO: Waiting for pod pod-configmaps-0bb4d31e-6d7b-40d6-9660-464a6f553d07 to disappear
Jul 29 15:40:55.995: INFO: Pod pod-configmaps-0bb4d31e-6d7b-40d6-9660-464a6f553d07 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jul 29 15:40:55.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7535" for this suite. 07/29/23 15:40:56.004
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","completed":49,"skipped":849,"failed":0}
------------------------------
â€¢ [4.167 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:40:51.852
    Jul 29 15:40:51.852: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename configmap 07/29/23 15:40:51.855
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:40:51.884
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:40:51.889
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:108
    STEP: Creating configMap with name configmap-test-volume-map-07b30992-d119-4bd6-ada2-b4d23bc0ecf6 07/29/23 15:40:51.896
    STEP: Creating a pod to test consume configMaps 07/29/23 15:40:51.904
    Jul 29 15:40:51.920: INFO: Waiting up to 5m0s for pod "pod-configmaps-0bb4d31e-6d7b-40d6-9660-464a6f553d07" in namespace "configmap-7535" to be "Succeeded or Failed"
    Jul 29 15:40:51.925: INFO: Pod "pod-configmaps-0bb4d31e-6d7b-40d6-9660-464a6f553d07": Phase="Pending", Reason="", readiness=false. Elapsed: 4.663203ms
    Jul 29 15:40:53.936: INFO: Pod "pod-configmaps-0bb4d31e-6d7b-40d6-9660-464a6f553d07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015633542s
    Jul 29 15:40:55.933: INFO: Pod "pod-configmaps-0bb4d31e-6d7b-40d6-9660-464a6f553d07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012579126s
    STEP: Saw pod success 07/29/23 15:40:55.933
    Jul 29 15:40:55.933: INFO: Pod "pod-configmaps-0bb4d31e-6d7b-40d6-9660-464a6f553d07" satisfied condition "Succeeded or Failed"
    Jul 29 15:40:55.940: INFO: Trying to get logs from node wa4quivohpee-3 pod pod-configmaps-0bb4d31e-6d7b-40d6-9660-464a6f553d07 container agnhost-container: <nil>
    STEP: delete the pod 07/29/23 15:40:55.953
    Jul 29 15:40:55.989: INFO: Waiting for pod pod-configmaps-0bb4d31e-6d7b-40d6-9660-464a6f553d07 to disappear
    Jul 29 15:40:55.995: INFO: Pod pod-configmaps-0bb4d31e-6d7b-40d6-9660-464a6f553d07 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jul 29 15:40:55.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-7535" for this suite. 07/29/23 15:40:56.004
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] CronJob
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:40:56.022
Jul 29 15:40:56.023: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename cronjob 07/29/23 15:40:56.026
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:40:56.067
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:40:56.073
[It] should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
STEP: Creating a suspended cronjob 07/29/23 15:40:56.078
STEP: Ensuring no jobs are scheduled 07/29/23 15:40:56.09
STEP: Ensuring no job exists by listing jobs explicitly 07/29/23 15:45:56.104
STEP: Removing cronjob 07/29/23 15:45:56.113
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Jul 29 15:45:56.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-1374" for this suite. 07/29/23 15:45:56.136
{"msg":"PASSED [sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]","completed":50,"skipped":851,"failed":0}
------------------------------
â€¢ [SLOW TEST] [300.126 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:40:56.022
    Jul 29 15:40:56.023: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename cronjob 07/29/23 15:40:56.026
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:40:56.067
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:40:56.073
    [It] should not schedule jobs when suspended [Slow] [Conformance]
      test/e2e/apps/cronjob.go:96
    STEP: Creating a suspended cronjob 07/29/23 15:40:56.078
    STEP: Ensuring no jobs are scheduled 07/29/23 15:40:56.09
    STEP: Ensuring no job exists by listing jobs explicitly 07/29/23 15:45:56.104
    STEP: Removing cronjob 07/29/23 15:45:56.113
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Jul 29 15:45:56.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-1374" for this suite. 07/29/23 15:45:56.136
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] IngressClass API
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
[BeforeEach] [sig-network] IngressClass API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:45:56.155
Jul 29 15:45:56.155: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename ingressclass 07/29/23 15:45:56.159
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:45:56.195
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:45:56.201
[BeforeEach] [sig-network] IngressClass API
  test/e2e/network/ingressclass.go:211
[It]  should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
STEP: getting /apis 07/29/23 15:45:56.205
STEP: getting /apis/networking.k8s.io 07/29/23 15:45:56.21
STEP: getting /apis/networking.k8s.iov1 07/29/23 15:45:56.212
STEP: creating 07/29/23 15:45:56.215
STEP: getting 07/29/23 15:45:56.248
STEP: listing 07/29/23 15:45:56.253
STEP: watching 07/29/23 15:45:56.26
Jul 29 15:45:56.261: INFO: starting watch
STEP: patching 07/29/23 15:45:56.262
STEP: updating 07/29/23 15:45:56.273
Jul 29 15:45:56.282: INFO: waiting for watch events with expected annotations
Jul 29 15:45:56.282: INFO: saw patched and updated annotations
STEP: deleting 07/29/23 15:45:56.282
STEP: deleting a collection 07/29/23 15:45:56.303
[AfterEach] [sig-network] IngressClass API
  test/e2e/framework/framework.go:187
Jul 29 15:45:56.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-877" for this suite. 07/29/23 15:45:56.336
{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","completed":51,"skipped":891,"failed":0}
------------------------------
â€¢ [0.196 seconds]
[sig-network] IngressClass API
test/e2e/network/common/framework.go:23
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:45:56.155
    Jul 29 15:45:56.155: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename ingressclass 07/29/23 15:45:56.159
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:45:56.195
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:45:56.201
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/network/ingressclass.go:211
    [It]  should support creating IngressClass API operations [Conformance]
      test/e2e/network/ingressclass.go:223
    STEP: getting /apis 07/29/23 15:45:56.205
    STEP: getting /apis/networking.k8s.io 07/29/23 15:45:56.21
    STEP: getting /apis/networking.k8s.iov1 07/29/23 15:45:56.212
    STEP: creating 07/29/23 15:45:56.215
    STEP: getting 07/29/23 15:45:56.248
    STEP: listing 07/29/23 15:45:56.253
    STEP: watching 07/29/23 15:45:56.26
    Jul 29 15:45:56.261: INFO: starting watch
    STEP: patching 07/29/23 15:45:56.262
    STEP: updating 07/29/23 15:45:56.273
    Jul 29 15:45:56.282: INFO: waiting for watch events with expected annotations
    Jul 29 15:45:56.282: INFO: saw patched and updated annotations
    STEP: deleting 07/29/23 15:45:56.282
    STEP: deleting a collection 07/29/23 15:45:56.303
    [AfterEach] [sig-network] IngressClass API
      test/e2e/framework/framework.go:187
    Jul 29 15:45:56.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ingressclass-877" for this suite. 07/29/23 15:45:56.336
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:45:56.351
Jul 29 15:45:56.351: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename kubelet-test 07/29/23 15:45:56.353
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:45:56.386
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:45:56.391
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
Jul 29 15:45:56.412: INFO: Waiting up to 5m0s for pod "busybox-readonly-fsce0e7921-496c-4cf8-8d36-750392f01ec5" in namespace "kubelet-test-4430" to be "running and ready"
Jul 29 15:45:56.422: INFO: Pod "busybox-readonly-fsce0e7921-496c-4cf8-8d36-750392f01ec5": Phase="Pending", Reason="", readiness=false. Elapsed: 9.942882ms
Jul 29 15:45:56.422: INFO: The phase of Pod busybox-readonly-fsce0e7921-496c-4cf8-8d36-750392f01ec5 is Pending, waiting for it to be Running (with Ready = true)
Jul 29 15:45:58.433: INFO: Pod "busybox-readonly-fsce0e7921-496c-4cf8-8d36-750392f01ec5": Phase="Running", Reason="", readiness=true. Elapsed: 2.020573609s
Jul 29 15:45:58.433: INFO: The phase of Pod busybox-readonly-fsce0e7921-496c-4cf8-8d36-750392f01ec5 is Running (Ready = true)
Jul 29 15:45:58.433: INFO: Pod "busybox-readonly-fsce0e7921-496c-4cf8-8d36-750392f01ec5" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Jul 29 15:45:58.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4430" for this suite. 07/29/23 15:45:58.482
{"msg":"PASSED [sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","completed":52,"skipped":894,"failed":0}
------------------------------
â€¢ [2.144 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a read only busybox container
  test/e2e/common/node/kubelet.go:175
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:45:56.351
    Jul 29 15:45:56.351: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename kubelet-test 07/29/23 15:45:56.353
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:45:56.386
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:45:56.391
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:184
    Jul 29 15:45:56.412: INFO: Waiting up to 5m0s for pod "busybox-readonly-fsce0e7921-496c-4cf8-8d36-750392f01ec5" in namespace "kubelet-test-4430" to be "running and ready"
    Jul 29 15:45:56.422: INFO: Pod "busybox-readonly-fsce0e7921-496c-4cf8-8d36-750392f01ec5": Phase="Pending", Reason="", readiness=false. Elapsed: 9.942882ms
    Jul 29 15:45:56.422: INFO: The phase of Pod busybox-readonly-fsce0e7921-496c-4cf8-8d36-750392f01ec5 is Pending, waiting for it to be Running (with Ready = true)
    Jul 29 15:45:58.433: INFO: Pod "busybox-readonly-fsce0e7921-496c-4cf8-8d36-750392f01ec5": Phase="Running", Reason="", readiness=true. Elapsed: 2.020573609s
    Jul 29 15:45:58.433: INFO: The phase of Pod busybox-readonly-fsce0e7921-496c-4cf8-8d36-750392f01ec5 is Running (Ready = true)
    Jul 29 15:45:58.433: INFO: Pod "busybox-readonly-fsce0e7921-496c-4cf8-8d36-750392f01ec5" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Jul 29 15:45:58.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-4430" for this suite. 07/29/23 15:45:58.482
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:45:58.499
Jul 29 15:45:58.499: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename init-container 07/29/23 15:45:58.502
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:45:58.534
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:45:58.539
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254
STEP: creating the pod 07/29/23 15:45:58.545
Jul 29 15:45:58.545: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Jul 29 15:46:01.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5774" for this suite. 07/29/23 15:46:01.734
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","completed":53,"skipped":894,"failed":0}
------------------------------
â€¢ [3.250 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:45:58.499
    Jul 29 15:45:58.499: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename init-container 07/29/23 15:45:58.502
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:45:58.534
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:45:58.539
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should invoke init containers on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:254
    STEP: creating the pod 07/29/23 15:45:58.545
    Jul 29 15:45:58.545: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Jul 29 15:46:01.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-5774" for this suite. 07/29/23 15:46:01.734
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Services
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:46:01.751
Jul 29 15:46:01.752: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename services 07/29/23 15:46:01.755
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:46:01.794
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:46:01.799
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-3330 07/29/23 15:46:01.808
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 07/29/23 15:46:01.835
STEP: creating service externalsvc in namespace services-3330 07/29/23 15:46:01.835
STEP: creating replication controller externalsvc in namespace services-3330 07/29/23 15:46:01.858
I0729 15:46:01.871361      13 runners.go:193] Created replication controller with name: externalsvc, namespace: services-3330, replica count: 2
I0729 15:46:04.923934      13 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName 07/29/23 15:46:04.931
Jul 29 15:46:04.952: INFO: Creating new exec pod
Jul 29 15:46:04.984: INFO: Waiting up to 5m0s for pod "execpodtklhg" in namespace "services-3330" to be "running"
Jul 29 15:46:04.999: INFO: Pod "execpodtklhg": Phase="Pending", Reason="", readiness=false. Elapsed: 14.65598ms
Jul 29 15:46:07.017: INFO: Pod "execpodtklhg": Phase="Running", Reason="", readiness=true. Elapsed: 2.033356035s
Jul 29 15:46:07.018: INFO: Pod "execpodtklhg" satisfied condition "running"
Jul 29 15:46:07.018: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=services-3330 exec execpodtklhg -- /bin/sh -x -c nslookup clusterip-service.services-3330.svc.cluster.local'
Jul 29 15:46:07.491: INFO: stderr: "+ nslookup clusterip-service.services-3330.svc.cluster.local\n"
Jul 29 15:46:07.492: INFO: stdout: "Server:\t\t10.233.0.10\nAddress:\t10.233.0.10#53\n\nclusterip-service.services-3330.svc.cluster.local\tcanonical name = externalsvc.services-3330.svc.cluster.local.\nName:\texternalsvc.services-3330.svc.cluster.local\nAddress: 10.233.49.87\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-3330, will wait for the garbage collector to delete the pods 07/29/23 15:46:07.492
Jul 29 15:46:07.565: INFO: Deleting ReplicationController externalsvc took: 14.650476ms
Jul 29 15:46:07.666: INFO: Terminating ReplicationController externalsvc pods took: 100.578455ms
Jul 29 15:46:09.903: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jul 29 15:46:09.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3330" for this suite. 07/29/23 15:46:09.952
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","completed":54,"skipped":897,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.217 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:46:01.751
    Jul 29 15:46:01.752: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename services 07/29/23 15:46:01.755
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:46:01.794
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:46:01.799
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ClusterIP to ExternalName [Conformance]
      test/e2e/network/service.go:1481
    STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-3330 07/29/23 15:46:01.808
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 07/29/23 15:46:01.835
    STEP: creating service externalsvc in namespace services-3330 07/29/23 15:46:01.835
    STEP: creating replication controller externalsvc in namespace services-3330 07/29/23 15:46:01.858
    I0729 15:46:01.871361      13 runners.go:193] Created replication controller with name: externalsvc, namespace: services-3330, replica count: 2
    I0729 15:46:04.923934      13 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the ClusterIP service to type=ExternalName 07/29/23 15:46:04.931
    Jul 29 15:46:04.952: INFO: Creating new exec pod
    Jul 29 15:46:04.984: INFO: Waiting up to 5m0s for pod "execpodtklhg" in namespace "services-3330" to be "running"
    Jul 29 15:46:04.999: INFO: Pod "execpodtklhg": Phase="Pending", Reason="", readiness=false. Elapsed: 14.65598ms
    Jul 29 15:46:07.017: INFO: Pod "execpodtklhg": Phase="Running", Reason="", readiness=true. Elapsed: 2.033356035s
    Jul 29 15:46:07.018: INFO: Pod "execpodtklhg" satisfied condition "running"
    Jul 29 15:46:07.018: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=services-3330 exec execpodtklhg -- /bin/sh -x -c nslookup clusterip-service.services-3330.svc.cluster.local'
    Jul 29 15:46:07.491: INFO: stderr: "+ nslookup clusterip-service.services-3330.svc.cluster.local\n"
    Jul 29 15:46:07.492: INFO: stdout: "Server:\t\t10.233.0.10\nAddress:\t10.233.0.10#53\n\nclusterip-service.services-3330.svc.cluster.local\tcanonical name = externalsvc.services-3330.svc.cluster.local.\nName:\texternalsvc.services-3330.svc.cluster.local\nAddress: 10.233.49.87\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-3330, will wait for the garbage collector to delete the pods 07/29/23 15:46:07.492
    Jul 29 15:46:07.565: INFO: Deleting ReplicationController externalsvc took: 14.650476ms
    Jul 29 15:46:07.666: INFO: Terminating ReplicationController externalsvc pods took: 100.578455ms
    Jul 29 15:46:09.903: INFO: Cleaning up the ClusterIP to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jul 29 15:46:09.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-3330" for this suite. 07/29/23 15:46:09.952
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:46:09.977
Jul 29 15:46:09.978: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename subpath 07/29/23 15:46:09.987
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:46:10.029
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:46:10.038
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 07/29/23 15:46:10.043
[It] should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
STEP: Creating pod pod-subpath-test-projected-p452 07/29/23 15:46:10.067
STEP: Creating a pod to test atomic-volume-subpath 07/29/23 15:46:10.067
Jul 29 15:46:10.086: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-p452" in namespace "subpath-6699" to be "Succeeded or Failed"
Jul 29 15:46:10.095: INFO: Pod "pod-subpath-test-projected-p452": Phase="Pending", Reason="", readiness=false. Elapsed: 8.434392ms
Jul 29 15:46:12.105: INFO: Pod "pod-subpath-test-projected-p452": Phase="Running", Reason="", readiness=true. Elapsed: 2.018340142s
Jul 29 15:46:14.102: INFO: Pod "pod-subpath-test-projected-p452": Phase="Running", Reason="", readiness=true. Elapsed: 4.015824356s
Jul 29 15:46:16.105: INFO: Pod "pod-subpath-test-projected-p452": Phase="Running", Reason="", readiness=true. Elapsed: 6.0183635s
Jul 29 15:46:18.104: INFO: Pod "pod-subpath-test-projected-p452": Phase="Running", Reason="", readiness=true. Elapsed: 8.016941431s
Jul 29 15:46:20.105: INFO: Pod "pod-subpath-test-projected-p452": Phase="Running", Reason="", readiness=true. Elapsed: 10.018174362s
Jul 29 15:46:22.102: INFO: Pod "pod-subpath-test-projected-p452": Phase="Running", Reason="", readiness=true. Elapsed: 12.014867997s
Jul 29 15:46:24.107: INFO: Pod "pod-subpath-test-projected-p452": Phase="Running", Reason="", readiness=true. Elapsed: 14.019874782s
Jul 29 15:46:26.103: INFO: Pod "pod-subpath-test-projected-p452": Phase="Running", Reason="", readiness=true. Elapsed: 16.016652444s
Jul 29 15:46:28.102: INFO: Pod "pod-subpath-test-projected-p452": Phase="Running", Reason="", readiness=true. Elapsed: 18.015256436s
Jul 29 15:46:30.105: INFO: Pod "pod-subpath-test-projected-p452": Phase="Running", Reason="", readiness=true. Elapsed: 20.018551745s
Jul 29 15:46:32.104: INFO: Pod "pod-subpath-test-projected-p452": Phase="Running", Reason="", readiness=false. Elapsed: 22.017164849s
Jul 29 15:46:34.105: INFO: Pod "pod-subpath-test-projected-p452": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.018708874s
STEP: Saw pod success 07/29/23 15:46:34.105
Jul 29 15:46:34.106: INFO: Pod "pod-subpath-test-projected-p452" satisfied condition "Succeeded or Failed"
Jul 29 15:46:34.114: INFO: Trying to get logs from node wa4quivohpee-3 pod pod-subpath-test-projected-p452 container test-container-subpath-projected-p452: <nil>
STEP: delete the pod 07/29/23 15:46:34.135
Jul 29 15:46:34.162: INFO: Waiting for pod pod-subpath-test-projected-p452 to disappear
Jul 29 15:46:34.169: INFO: Pod pod-subpath-test-projected-p452 no longer exists
STEP: Deleting pod pod-subpath-test-projected-p452 07/29/23 15:46:34.169
Jul 29 15:46:34.170: INFO: Deleting pod "pod-subpath-test-projected-p452" in namespace "subpath-6699"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Jul 29 15:46:34.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6699" for this suite. 07/29/23 15:46:34.185
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [Conformance]","completed":55,"skipped":908,"failed":0}
------------------------------
â€¢ [SLOW TEST] [24.226 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with projected pod [Conformance]
    test/e2e/storage/subpath.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:46:09.977
    Jul 29 15:46:09.978: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename subpath 07/29/23 15:46:09.987
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:46:10.029
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:46:10.038
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 07/29/23 15:46:10.043
    [It] should support subpaths with projected pod [Conformance]
      test/e2e/storage/subpath.go:106
    STEP: Creating pod pod-subpath-test-projected-p452 07/29/23 15:46:10.067
    STEP: Creating a pod to test atomic-volume-subpath 07/29/23 15:46:10.067
    Jul 29 15:46:10.086: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-p452" in namespace "subpath-6699" to be "Succeeded or Failed"
    Jul 29 15:46:10.095: INFO: Pod "pod-subpath-test-projected-p452": Phase="Pending", Reason="", readiness=false. Elapsed: 8.434392ms
    Jul 29 15:46:12.105: INFO: Pod "pod-subpath-test-projected-p452": Phase="Running", Reason="", readiness=true. Elapsed: 2.018340142s
    Jul 29 15:46:14.102: INFO: Pod "pod-subpath-test-projected-p452": Phase="Running", Reason="", readiness=true. Elapsed: 4.015824356s
    Jul 29 15:46:16.105: INFO: Pod "pod-subpath-test-projected-p452": Phase="Running", Reason="", readiness=true. Elapsed: 6.0183635s
    Jul 29 15:46:18.104: INFO: Pod "pod-subpath-test-projected-p452": Phase="Running", Reason="", readiness=true. Elapsed: 8.016941431s
    Jul 29 15:46:20.105: INFO: Pod "pod-subpath-test-projected-p452": Phase="Running", Reason="", readiness=true. Elapsed: 10.018174362s
    Jul 29 15:46:22.102: INFO: Pod "pod-subpath-test-projected-p452": Phase="Running", Reason="", readiness=true. Elapsed: 12.014867997s
    Jul 29 15:46:24.107: INFO: Pod "pod-subpath-test-projected-p452": Phase="Running", Reason="", readiness=true. Elapsed: 14.019874782s
    Jul 29 15:46:26.103: INFO: Pod "pod-subpath-test-projected-p452": Phase="Running", Reason="", readiness=true. Elapsed: 16.016652444s
    Jul 29 15:46:28.102: INFO: Pod "pod-subpath-test-projected-p452": Phase="Running", Reason="", readiness=true. Elapsed: 18.015256436s
    Jul 29 15:46:30.105: INFO: Pod "pod-subpath-test-projected-p452": Phase="Running", Reason="", readiness=true. Elapsed: 20.018551745s
    Jul 29 15:46:32.104: INFO: Pod "pod-subpath-test-projected-p452": Phase="Running", Reason="", readiness=false. Elapsed: 22.017164849s
    Jul 29 15:46:34.105: INFO: Pod "pod-subpath-test-projected-p452": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.018708874s
    STEP: Saw pod success 07/29/23 15:46:34.105
    Jul 29 15:46:34.106: INFO: Pod "pod-subpath-test-projected-p452" satisfied condition "Succeeded or Failed"
    Jul 29 15:46:34.114: INFO: Trying to get logs from node wa4quivohpee-3 pod pod-subpath-test-projected-p452 container test-container-subpath-projected-p452: <nil>
    STEP: delete the pod 07/29/23 15:46:34.135
    Jul 29 15:46:34.162: INFO: Waiting for pod pod-subpath-test-projected-p452 to disappear
    Jul 29 15:46:34.169: INFO: Pod pod-subpath-test-projected-p452 no longer exists
    STEP: Deleting pod pod-subpath-test-projected-p452 07/29/23 15:46:34.169
    Jul 29 15:46:34.170: INFO: Deleting pod "pod-subpath-test-projected-p452" in namespace "subpath-6699"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Jul 29 15:46:34.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-6699" for this suite. 07/29/23 15:46:34.185
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:46:34.207
Jul 29 15:46:34.208: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename container-probe 07/29/23 15:46:34.209
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:46:34.241
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:46:34.247
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148
STEP: Creating pod busybox-92d37e05-1acf-4e6b-af4c-9c2bdc5f3f35 in namespace container-probe-9057 07/29/23 15:46:34.253
Jul 29 15:46:34.270: INFO: Waiting up to 5m0s for pod "busybox-92d37e05-1acf-4e6b-af4c-9c2bdc5f3f35" in namespace "container-probe-9057" to be "not pending"
Jul 29 15:46:34.277: INFO: Pod "busybox-92d37e05-1acf-4e6b-af4c-9c2bdc5f3f35": Phase="Pending", Reason="", readiness=false. Elapsed: 6.286107ms
Jul 29 15:46:36.286: INFO: Pod "busybox-92d37e05-1acf-4e6b-af4c-9c2bdc5f3f35": Phase="Running", Reason="", readiness=true. Elapsed: 2.015439553s
Jul 29 15:46:36.286: INFO: Pod "busybox-92d37e05-1acf-4e6b-af4c-9c2bdc5f3f35" satisfied condition "not pending"
Jul 29 15:46:36.286: INFO: Started pod busybox-92d37e05-1acf-4e6b-af4c-9c2bdc5f3f35 in namespace container-probe-9057
STEP: checking the pod's current state and verifying that restartCount is present 07/29/23 15:46:36.286
Jul 29 15:46:36.294: INFO: Initial restart count of pod busybox-92d37e05-1acf-4e6b-af4c-9c2bdc5f3f35 is 0
STEP: deleting the pod 07/29/23 15:50:37.468
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jul 29 15:50:37.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9057" for this suite. 07/29/23 15:50:37.532
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","completed":56,"skipped":917,"failed":0}
------------------------------
â€¢ [SLOW TEST] [243.338 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:46:34.207
    Jul 29 15:46:34.208: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename container-probe 07/29/23 15:46:34.209
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:46:34.241
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:46:34.247
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:148
    STEP: Creating pod busybox-92d37e05-1acf-4e6b-af4c-9c2bdc5f3f35 in namespace container-probe-9057 07/29/23 15:46:34.253
    Jul 29 15:46:34.270: INFO: Waiting up to 5m0s for pod "busybox-92d37e05-1acf-4e6b-af4c-9c2bdc5f3f35" in namespace "container-probe-9057" to be "not pending"
    Jul 29 15:46:34.277: INFO: Pod "busybox-92d37e05-1acf-4e6b-af4c-9c2bdc5f3f35": Phase="Pending", Reason="", readiness=false. Elapsed: 6.286107ms
    Jul 29 15:46:36.286: INFO: Pod "busybox-92d37e05-1acf-4e6b-af4c-9c2bdc5f3f35": Phase="Running", Reason="", readiness=true. Elapsed: 2.015439553s
    Jul 29 15:46:36.286: INFO: Pod "busybox-92d37e05-1acf-4e6b-af4c-9c2bdc5f3f35" satisfied condition "not pending"
    Jul 29 15:46:36.286: INFO: Started pod busybox-92d37e05-1acf-4e6b-af4c-9c2bdc5f3f35 in namespace container-probe-9057
    STEP: checking the pod's current state and verifying that restartCount is present 07/29/23 15:46:36.286
    Jul 29 15:46:36.294: INFO: Initial restart count of pod busybox-92d37e05-1acf-4e6b-af4c-9c2bdc5f3f35 is 0
    STEP: deleting the pod 07/29/23 15:50:37.468
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jul 29 15:50:37.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-9057" for this suite. 07/29/23 15:50:37.532
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:50:37.551
Jul 29 15:50:37.551: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename pods 07/29/23 15:50:37.553
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:50:37.587
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:50:37.595
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844
STEP: Create set of pods 07/29/23 15:50:37.601
Jul 29 15:50:37.616: INFO: created test-pod-1
Jul 29 15:50:37.626: INFO: created test-pod-2
Jul 29 15:50:37.639: INFO: created test-pod-3
STEP: waiting for all 3 pods to be running 07/29/23 15:50:37.64
Jul 29 15:50:37.640: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-198' to be running and ready
Jul 29 15:50:37.667: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jul 29 15:50:37.667: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jul 29 15:50:37.667: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jul 29 15:50:37.667: INFO: 0 / 3 pods in namespace 'pods-198' are running and ready (0 seconds elapsed)
Jul 29 15:50:37.667: INFO: expected 0 pod replicas in namespace 'pods-198', 0 are Running and Ready.
Jul 29 15:50:37.667: INFO: POD         NODE            PHASE    GRACE  CONDITIONS
Jul 29 15:50:37.667: INFO: test-pod-1  wa4quivohpee-3  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 15:50:37 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-07-29 15:50:37 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-07-29 15:50:37 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 15:50:37 +0000 UTC  }]
Jul 29 15:50:37.667: INFO: test-pod-2  wa4quivohpee-3  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 15:50:37 +0000 UTC  }]
Jul 29 15:50:37.667: INFO: test-pod-3  wa4quivohpee-3  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 15:50:37 +0000 UTC  }]
Jul 29 15:50:37.668: INFO: 
Jul 29 15:50:39.700: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jul 29 15:50:39.700: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jul 29 15:50:39.700: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jul 29 15:50:39.700: INFO: 0 / 3 pods in namespace 'pods-198' are running and ready (2 seconds elapsed)
Jul 29 15:50:39.700: INFO: expected 0 pod replicas in namespace 'pods-198', 0 are Running and Ready.
Jul 29 15:50:39.700: INFO: POD         NODE            PHASE    GRACE  CONDITIONS
Jul 29 15:50:39.700: INFO: test-pod-1  wa4quivohpee-3  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 15:50:37 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-07-29 15:50:37 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-07-29 15:50:37 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 15:50:37 +0000 UTC  }]
Jul 29 15:50:39.700: INFO: test-pod-2  wa4quivohpee-3  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 15:50:37 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-07-29 15:50:37 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-07-29 15:50:37 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 15:50:37 +0000 UTC  }]
Jul 29 15:50:39.700: INFO: test-pod-3  wa4quivohpee-3  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 15:50:37 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-07-29 15:50:37 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-07-29 15:50:37 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 15:50:37 +0000 UTC  }]
Jul 29 15:50:39.701: INFO: 
Jul 29 15:50:41.684: INFO: 3 / 3 pods in namespace 'pods-198' are running and ready (4 seconds elapsed)
Jul 29 15:50:41.684: INFO: expected 0 pod replicas in namespace 'pods-198', 0 are Running and Ready.
STEP: waiting for all pods to be deleted 07/29/23 15:50:41.726
Jul 29 15:50:41.737: INFO: Pod quantity 3 is different from expected quantity 0
Jul 29 15:50:42.798: INFO: Pod quantity 3 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jul 29 15:50:43.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-198" for this suite. 07/29/23 15:50:43.756
{"msg":"PASSED [sig-node] Pods should delete a collection of pods [Conformance]","completed":57,"skipped":1005,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.223 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:50:37.551
    Jul 29 15:50:37.551: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename pods 07/29/23 15:50:37.553
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:50:37.587
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:50:37.595
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should delete a collection of pods [Conformance]
      test/e2e/common/node/pods.go:844
    STEP: Create set of pods 07/29/23 15:50:37.601
    Jul 29 15:50:37.616: INFO: created test-pod-1
    Jul 29 15:50:37.626: INFO: created test-pod-2
    Jul 29 15:50:37.639: INFO: created test-pod-3
    STEP: waiting for all 3 pods to be running 07/29/23 15:50:37.64
    Jul 29 15:50:37.640: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-198' to be running and ready
    Jul 29 15:50:37.667: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jul 29 15:50:37.667: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jul 29 15:50:37.667: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jul 29 15:50:37.667: INFO: 0 / 3 pods in namespace 'pods-198' are running and ready (0 seconds elapsed)
    Jul 29 15:50:37.667: INFO: expected 0 pod replicas in namespace 'pods-198', 0 are Running and Ready.
    Jul 29 15:50:37.667: INFO: POD         NODE            PHASE    GRACE  CONDITIONS
    Jul 29 15:50:37.667: INFO: test-pod-1  wa4quivohpee-3  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 15:50:37 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-07-29 15:50:37 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-07-29 15:50:37 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 15:50:37 +0000 UTC  }]
    Jul 29 15:50:37.667: INFO: test-pod-2  wa4quivohpee-3  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 15:50:37 +0000 UTC  }]
    Jul 29 15:50:37.667: INFO: test-pod-3  wa4quivohpee-3  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 15:50:37 +0000 UTC  }]
    Jul 29 15:50:37.668: INFO: 
    Jul 29 15:50:39.700: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jul 29 15:50:39.700: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jul 29 15:50:39.700: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jul 29 15:50:39.700: INFO: 0 / 3 pods in namespace 'pods-198' are running and ready (2 seconds elapsed)
    Jul 29 15:50:39.700: INFO: expected 0 pod replicas in namespace 'pods-198', 0 are Running and Ready.
    Jul 29 15:50:39.700: INFO: POD         NODE            PHASE    GRACE  CONDITIONS
    Jul 29 15:50:39.700: INFO: test-pod-1  wa4quivohpee-3  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 15:50:37 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-07-29 15:50:37 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-07-29 15:50:37 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 15:50:37 +0000 UTC  }]
    Jul 29 15:50:39.700: INFO: test-pod-2  wa4quivohpee-3  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 15:50:37 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-07-29 15:50:37 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-07-29 15:50:37 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 15:50:37 +0000 UTC  }]
    Jul 29 15:50:39.700: INFO: test-pod-3  wa4quivohpee-3  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 15:50:37 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-07-29 15:50:37 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-07-29 15:50:37 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 15:50:37 +0000 UTC  }]
    Jul 29 15:50:39.701: INFO: 
    Jul 29 15:50:41.684: INFO: 3 / 3 pods in namespace 'pods-198' are running and ready (4 seconds elapsed)
    Jul 29 15:50:41.684: INFO: expected 0 pod replicas in namespace 'pods-198', 0 are Running and Ready.
    STEP: waiting for all pods to be deleted 07/29/23 15:50:41.726
    Jul 29 15:50:41.737: INFO: Pod quantity 3 is different from expected quantity 0
    Jul 29 15:50:42.798: INFO: Pod quantity 3 is different from expected quantity 0
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jul 29 15:50:43.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-198" for this suite. 07/29/23 15:50:43.756
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:50:43.784
Jul 29 15:50:43.785: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename kubelet-test 07/29/23 15:50:43.787
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:50:43.824
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:50:43.829
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Jul 29 15:50:43.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5411" for this suite. 07/29/23 15:50:43.892
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","completed":58,"skipped":1033,"failed":0}
------------------------------
â€¢ [0.121 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should be possible to delete [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:50:43.784
    Jul 29 15:50:43.785: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename kubelet-test 07/29/23 15:50:43.787
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:50:43.824
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:50:43.829
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should be possible to delete [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:135
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Jul 29 15:50:43.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-5411" for this suite. 07/29/23 15:50:43.892
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:337
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:50:43.909
Jul 29 15:50:43.910: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename kubectl 07/29/23 15:50:43.913
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:50:43.946
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:50:43.951
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:324
[It] should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:337
STEP: creating a replication controller 07/29/23 15:50:43.956
Jul 29 15:50:43.957: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-9089 create -f -'
Jul 29 15:50:44.699: INFO: stderr: ""
Jul 29 15:50:44.699: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 07/29/23 15:50:44.699
Jul 29 15:50:44.700: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-9089 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jul 29 15:50:44.932: INFO: stderr: ""
Jul 29 15:50:44.932: INFO: stdout: "update-demo-nautilus-5w2p4 update-demo-nautilus-7f72n "
Jul 29 15:50:44.932: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-9089 get pods update-demo-nautilus-5w2p4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jul 29 15:50:45.088: INFO: stderr: ""
Jul 29 15:50:45.088: INFO: stdout: ""
Jul 29 15:50:45.088: INFO: update-demo-nautilus-5w2p4 is created but not running
Jul 29 15:50:50.089: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-9089 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jul 29 15:50:50.262: INFO: stderr: ""
Jul 29 15:50:50.262: INFO: stdout: "update-demo-nautilus-5w2p4 update-demo-nautilus-7f72n "
Jul 29 15:50:50.264: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-9089 get pods update-demo-nautilus-5w2p4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jul 29 15:50:50.408: INFO: stderr: ""
Jul 29 15:50:50.408: INFO: stdout: ""
Jul 29 15:50:50.408: INFO: update-demo-nautilus-5w2p4 is created but not running
Jul 29 15:50:55.409: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-9089 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jul 29 15:50:55.603: INFO: stderr: ""
Jul 29 15:50:55.605: INFO: stdout: "update-demo-nautilus-5w2p4 update-demo-nautilus-7f72n "
Jul 29 15:50:55.607: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-9089 get pods update-demo-nautilus-5w2p4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jul 29 15:50:55.812: INFO: stderr: ""
Jul 29 15:50:55.812: INFO: stdout: ""
Jul 29 15:50:55.812: INFO: update-demo-nautilus-5w2p4 is created but not running
Jul 29 15:51:00.815: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-9089 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jul 29 15:51:00.998: INFO: stderr: ""
Jul 29 15:51:00.998: INFO: stdout: "update-demo-nautilus-5w2p4 update-demo-nautilus-7f72n "
Jul 29 15:51:00.998: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-9089 get pods update-demo-nautilus-5w2p4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jul 29 15:51:01.190: INFO: stderr: ""
Jul 29 15:51:01.190: INFO: stdout: "true"
Jul 29 15:51:01.190: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-9089 get pods update-demo-nautilus-5w2p4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jul 29 15:51:01.313: INFO: stderr: ""
Jul 29 15:51:01.313: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jul 29 15:51:01.313: INFO: validating pod update-demo-nautilus-5w2p4
Jul 29 15:51:01.333: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 29 15:51:01.333: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 29 15:51:01.333: INFO: update-demo-nautilus-5w2p4 is verified up and running
Jul 29 15:51:01.334: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-9089 get pods update-demo-nautilus-7f72n -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jul 29 15:51:01.490: INFO: stderr: ""
Jul 29 15:51:01.490: INFO: stdout: "true"
Jul 29 15:51:01.490: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-9089 get pods update-demo-nautilus-7f72n -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jul 29 15:51:01.628: INFO: stderr: ""
Jul 29 15:51:01.628: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jul 29 15:51:01.628: INFO: validating pod update-demo-nautilus-7f72n
Jul 29 15:51:01.643: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 29 15:51:01.643: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 29 15:51:01.643: INFO: update-demo-nautilus-7f72n is verified up and running
STEP: using delete to clean up resources 07/29/23 15:51:01.643
Jul 29 15:51:01.644: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-9089 delete --grace-period=0 --force -f -'
Jul 29 15:51:01.799: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 29 15:51:01.799: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jul 29 15:51:01.799: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-9089 get rc,svc -l name=update-demo --no-headers'
Jul 29 15:51:02.032: INFO: stderr: "No resources found in kubectl-9089 namespace.\n"
Jul 29 15:51:02.032: INFO: stdout: ""
Jul 29 15:51:02.034: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-9089 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul 29 15:51:02.238: INFO: stderr: ""
Jul 29 15:51:02.238: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jul 29 15:51:02.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9089" for this suite. 07/29/23 15:51:02.288
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","completed":59,"skipped":1040,"failed":0}
------------------------------
â€¢ [SLOW TEST] [18.396 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:322
    should create and stop a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:337

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:50:43.909
    Jul 29 15:50:43.910: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename kubectl 07/29/23 15:50:43.913
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:50:43.946
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:50:43.951
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:324
    [It] should create and stop a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:337
    STEP: creating a replication controller 07/29/23 15:50:43.956
    Jul 29 15:50:43.957: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-9089 create -f -'
    Jul 29 15:50:44.699: INFO: stderr: ""
    Jul 29 15:50:44.699: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 07/29/23 15:50:44.699
    Jul 29 15:50:44.700: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-9089 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jul 29 15:50:44.932: INFO: stderr: ""
    Jul 29 15:50:44.932: INFO: stdout: "update-demo-nautilus-5w2p4 update-demo-nautilus-7f72n "
    Jul 29 15:50:44.932: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-9089 get pods update-demo-nautilus-5w2p4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jul 29 15:50:45.088: INFO: stderr: ""
    Jul 29 15:50:45.088: INFO: stdout: ""
    Jul 29 15:50:45.088: INFO: update-demo-nautilus-5w2p4 is created but not running
    Jul 29 15:50:50.089: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-9089 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jul 29 15:50:50.262: INFO: stderr: ""
    Jul 29 15:50:50.262: INFO: stdout: "update-demo-nautilus-5w2p4 update-demo-nautilus-7f72n "
    Jul 29 15:50:50.264: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-9089 get pods update-demo-nautilus-5w2p4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jul 29 15:50:50.408: INFO: stderr: ""
    Jul 29 15:50:50.408: INFO: stdout: ""
    Jul 29 15:50:50.408: INFO: update-demo-nautilus-5w2p4 is created but not running
    Jul 29 15:50:55.409: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-9089 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jul 29 15:50:55.603: INFO: stderr: ""
    Jul 29 15:50:55.605: INFO: stdout: "update-demo-nautilus-5w2p4 update-demo-nautilus-7f72n "
    Jul 29 15:50:55.607: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-9089 get pods update-demo-nautilus-5w2p4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jul 29 15:50:55.812: INFO: stderr: ""
    Jul 29 15:50:55.812: INFO: stdout: ""
    Jul 29 15:50:55.812: INFO: update-demo-nautilus-5w2p4 is created but not running
    Jul 29 15:51:00.815: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-9089 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jul 29 15:51:00.998: INFO: stderr: ""
    Jul 29 15:51:00.998: INFO: stdout: "update-demo-nautilus-5w2p4 update-demo-nautilus-7f72n "
    Jul 29 15:51:00.998: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-9089 get pods update-demo-nautilus-5w2p4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jul 29 15:51:01.190: INFO: stderr: ""
    Jul 29 15:51:01.190: INFO: stdout: "true"
    Jul 29 15:51:01.190: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-9089 get pods update-demo-nautilus-5w2p4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jul 29 15:51:01.313: INFO: stderr: ""
    Jul 29 15:51:01.313: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jul 29 15:51:01.313: INFO: validating pod update-demo-nautilus-5w2p4
    Jul 29 15:51:01.333: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jul 29 15:51:01.333: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jul 29 15:51:01.333: INFO: update-demo-nautilus-5w2p4 is verified up and running
    Jul 29 15:51:01.334: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-9089 get pods update-demo-nautilus-7f72n -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jul 29 15:51:01.490: INFO: stderr: ""
    Jul 29 15:51:01.490: INFO: stdout: "true"
    Jul 29 15:51:01.490: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-9089 get pods update-demo-nautilus-7f72n -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jul 29 15:51:01.628: INFO: stderr: ""
    Jul 29 15:51:01.628: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jul 29 15:51:01.628: INFO: validating pod update-demo-nautilus-7f72n
    Jul 29 15:51:01.643: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jul 29 15:51:01.643: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jul 29 15:51:01.643: INFO: update-demo-nautilus-7f72n is verified up and running
    STEP: using delete to clean up resources 07/29/23 15:51:01.643
    Jul 29 15:51:01.644: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-9089 delete --grace-period=0 --force -f -'
    Jul 29 15:51:01.799: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jul 29 15:51:01.799: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Jul 29 15:51:01.799: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-9089 get rc,svc -l name=update-demo --no-headers'
    Jul 29 15:51:02.032: INFO: stderr: "No resources found in kubectl-9089 namespace.\n"
    Jul 29 15:51:02.032: INFO: stdout: ""
    Jul 29 15:51:02.034: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-9089 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Jul 29 15:51:02.238: INFO: stderr: ""
    Jul 29 15:51:02.238: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jul 29 15:51:02.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-9089" for this suite. 07/29/23 15:51:02.288
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:51:02.316
Jul 29 15:51:02.317: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename emptydir 07/29/23 15:51:02.325
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:51:02.376
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:51:02.383
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86
STEP: Creating a pod to test emptydir volume type on tmpfs 07/29/23 15:51:02.387
Jul 29 15:51:02.408: INFO: Waiting up to 5m0s for pod "pod-2ea88227-d018-4522-a9b4-a9830facf1d2" in namespace "emptydir-2594" to be "Succeeded or Failed"
Jul 29 15:51:02.418: INFO: Pod "pod-2ea88227-d018-4522-a9b4-a9830facf1d2": Phase="Pending", Reason="", readiness=false. Elapsed: 10.505104ms
Jul 29 15:51:04.426: INFO: Pod "pod-2ea88227-d018-4522-a9b4-a9830facf1d2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018438012s
Jul 29 15:51:06.429: INFO: Pod "pod-2ea88227-d018-4522-a9b4-a9830facf1d2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021024553s
STEP: Saw pod success 07/29/23 15:51:06.429
Jul 29 15:51:06.429: INFO: Pod "pod-2ea88227-d018-4522-a9b4-a9830facf1d2" satisfied condition "Succeeded or Failed"
Jul 29 15:51:06.448: INFO: Trying to get logs from node wa4quivohpee-3 pod pod-2ea88227-d018-4522-a9b4-a9830facf1d2 container test-container: <nil>
STEP: delete the pod 07/29/23 15:51:06.485
Jul 29 15:51:06.509: INFO: Waiting for pod pod-2ea88227-d018-4522-a9b4-a9830facf1d2 to disappear
Jul 29 15:51:06.515: INFO: Pod pod-2ea88227-d018-4522-a9b4-a9830facf1d2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jul 29 15:51:06.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2594" for this suite. 07/29/23 15:51:06.525
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","completed":60,"skipped":1063,"failed":0}
------------------------------
â€¢ [4.220 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:51:02.316
    Jul 29 15:51:02.317: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename emptydir 07/29/23 15:51:02.325
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:51:02.376
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:51:02.383
    [It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:86
    STEP: Creating a pod to test emptydir volume type on tmpfs 07/29/23 15:51:02.387
    Jul 29 15:51:02.408: INFO: Waiting up to 5m0s for pod "pod-2ea88227-d018-4522-a9b4-a9830facf1d2" in namespace "emptydir-2594" to be "Succeeded or Failed"
    Jul 29 15:51:02.418: INFO: Pod "pod-2ea88227-d018-4522-a9b4-a9830facf1d2": Phase="Pending", Reason="", readiness=false. Elapsed: 10.505104ms
    Jul 29 15:51:04.426: INFO: Pod "pod-2ea88227-d018-4522-a9b4-a9830facf1d2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018438012s
    Jul 29 15:51:06.429: INFO: Pod "pod-2ea88227-d018-4522-a9b4-a9830facf1d2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021024553s
    STEP: Saw pod success 07/29/23 15:51:06.429
    Jul 29 15:51:06.429: INFO: Pod "pod-2ea88227-d018-4522-a9b4-a9830facf1d2" satisfied condition "Succeeded or Failed"
    Jul 29 15:51:06.448: INFO: Trying to get logs from node wa4quivohpee-3 pod pod-2ea88227-d018-4522-a9b4-a9830facf1d2 container test-container: <nil>
    STEP: delete the pod 07/29/23 15:51:06.485
    Jul 29 15:51:06.509: INFO: Waiting for pod pod-2ea88227-d018-4522-a9b4-a9830facf1d2 to disappear
    Jul 29 15:51:06.515: INFO: Pod pod-2ea88227-d018-4522-a9b4-a9830facf1d2 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jul 29 15:51:06.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-2594" for this suite. 07/29/23 15:51:06.525
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application
  should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:392
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:51:06.549
Jul 29 15:51:06.549: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename kubectl 07/29/23 15:51:06.551
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:51:06.592
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:51:06.597
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:392
STEP: creating all guestbook components 07/29/23 15:51:06.602
Jul 29 15:51:06.602: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Jul 29 15:51:06.603: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-5771 create -f -'
Jul 29 15:51:08.686: INFO: stderr: ""
Jul 29 15:51:08.686: INFO: stdout: "service/agnhost-replica created\n"
Jul 29 15:51:08.686: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Jul 29 15:51:08.686: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-5771 create -f -'
Jul 29 15:51:09.276: INFO: stderr: ""
Jul 29 15:51:09.276: INFO: stdout: "service/agnhost-primary created\n"
Jul 29 15:51:09.276: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Jul 29 15:51:09.276: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-5771 create -f -'
Jul 29 15:51:09.710: INFO: stderr: ""
Jul 29 15:51:09.710: INFO: stdout: "service/frontend created\n"
Jul 29 15:51:09.711: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Jul 29 15:51:09.711: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-5771 create -f -'
Jul 29 15:51:10.180: INFO: stderr: ""
Jul 29 15:51:10.180: INFO: stdout: "deployment.apps/frontend created\n"
Jul 29 15:51:10.180: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jul 29 15:51:10.180: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-5771 create -f -'
Jul 29 15:51:10.861: INFO: stderr: ""
Jul 29 15:51:10.861: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Jul 29 15:51:10.861: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jul 29 15:51:10.862: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-5771 create -f -'
Jul 29 15:51:11.772: INFO: stderr: ""
Jul 29 15:51:11.772: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app 07/29/23 15:51:11.772
Jul 29 15:51:11.772: INFO: Waiting for all frontend pods to be Running.
Jul 29 15:51:16.870: INFO: Waiting for frontend to serve content.
Jul 29 15:51:16.905: INFO: Trying to add a new entry to the guestbook.
Jul 29 15:51:16.933: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources 07/29/23 15:51:16.951
Jul 29 15:51:16.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-5771 delete --grace-period=0 --force -f -'
Jul 29 15:51:17.119: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 29 15:51:17.119: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources 07/29/23 15:51:17.119
Jul 29 15:51:17.120: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-5771 delete --grace-period=0 --force -f -'
Jul 29 15:51:17.298: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 29 15:51:17.298: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 07/29/23 15:51:17.298
Jul 29 15:51:17.299: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-5771 delete --grace-period=0 --force -f -'
Jul 29 15:51:17.490: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 29 15:51:17.490: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources 07/29/23 15:51:17.49
Jul 29 15:51:17.490: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-5771 delete --grace-period=0 --force -f -'
Jul 29 15:51:17.632: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 29 15:51:17.632: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources 07/29/23 15:51:17.632
Jul 29 15:51:17.633: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-5771 delete --grace-period=0 --force -f -'
Jul 29 15:51:17.841: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 29 15:51:17.841: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 07/29/23 15:51:17.841
Jul 29 15:51:17.842: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-5771 delete --grace-period=0 --force -f -'
Jul 29 15:51:18.145: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 29 15:51:18.145: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jul 29 15:51:18.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5771" for this suite. 07/29/23 15:51:18.164
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","completed":61,"skipped":1086,"failed":0}
------------------------------
â€¢ [SLOW TEST] [11.651 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Guestbook application
  test/e2e/kubectl/kubectl.go:367
    should create and stop a working application  [Conformance]
    test/e2e/kubectl/kubectl.go:392

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:51:06.549
    Jul 29 15:51:06.549: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename kubectl 07/29/23 15:51:06.551
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:51:06.592
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:51:06.597
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should create and stop a working application  [Conformance]
      test/e2e/kubectl/kubectl.go:392
    STEP: creating all guestbook components 07/29/23 15:51:06.602
    Jul 29 15:51:06.602: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-replica
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      ports:
      - port: 6379
      selector:
        app: agnhost
        role: replica
        tier: backend

    Jul 29 15:51:06.603: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-5771 create -f -'
    Jul 29 15:51:08.686: INFO: stderr: ""
    Jul 29 15:51:08.686: INFO: stdout: "service/agnhost-replica created\n"
    Jul 29 15:51:08.686: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-primary
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      ports:
      - port: 6379
        targetPort: 6379
      selector:
        app: agnhost
        role: primary
        tier: backend

    Jul 29 15:51:08.686: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-5771 create -f -'
    Jul 29 15:51:09.276: INFO: stderr: ""
    Jul 29 15:51:09.276: INFO: stdout: "service/agnhost-primary created\n"
    Jul 29 15:51:09.276: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: frontend
      labels:
        app: guestbook
        tier: frontend
    spec:
      # if your cluster supports it, uncomment the following to automatically create
      # an external load-balanced IP for the frontend service.
      # type: LoadBalancer
      ports:
      - port: 80
      selector:
        app: guestbook
        tier: frontend

    Jul 29 15:51:09.276: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-5771 create -f -'
    Jul 29 15:51:09.710: INFO: stderr: ""
    Jul 29 15:51:09.710: INFO: stdout: "service/frontend created\n"
    Jul 29 15:51:09.711: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: frontend
    spec:
      replicas: 3
      selector:
        matchLabels:
          app: guestbook
          tier: frontend
      template:
        metadata:
          labels:
            app: guestbook
            tier: frontend
        spec:
          containers:
          - name: guestbook-frontend
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--backend-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 80

    Jul 29 15:51:09.711: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-5771 create -f -'
    Jul 29 15:51:10.180: INFO: stderr: ""
    Jul 29 15:51:10.180: INFO: stdout: "deployment.apps/frontend created\n"
    Jul 29 15:51:10.180: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-primary
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: agnhost
          role: primary
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: primary
            tier: backend
        spec:
          containers:
          - name: primary
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Jul 29 15:51:10.180: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-5771 create -f -'
    Jul 29 15:51:10.861: INFO: stderr: ""
    Jul 29 15:51:10.861: INFO: stdout: "deployment.apps/agnhost-primary created\n"
    Jul 29 15:51:10.861: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-replica
    spec:
      replicas: 2
      selector:
        matchLabels:
          app: agnhost
          role: replica
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: replica
            tier: backend
        spec:
          containers:
          - name: replica
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Jul 29 15:51:10.862: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-5771 create -f -'
    Jul 29 15:51:11.772: INFO: stderr: ""
    Jul 29 15:51:11.772: INFO: stdout: "deployment.apps/agnhost-replica created\n"
    STEP: validating guestbook app 07/29/23 15:51:11.772
    Jul 29 15:51:11.772: INFO: Waiting for all frontend pods to be Running.
    Jul 29 15:51:16.870: INFO: Waiting for frontend to serve content.
    Jul 29 15:51:16.905: INFO: Trying to add a new entry to the guestbook.
    Jul 29 15:51:16.933: INFO: Verifying that added entry can be retrieved.
    STEP: using delete to clean up resources 07/29/23 15:51:16.951
    Jul 29 15:51:16.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-5771 delete --grace-period=0 --force -f -'
    Jul 29 15:51:17.119: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jul 29 15:51:17.119: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
    STEP: using delete to clean up resources 07/29/23 15:51:17.119
    Jul 29 15:51:17.120: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-5771 delete --grace-period=0 --force -f -'
    Jul 29 15:51:17.298: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jul 29 15:51:17.298: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 07/29/23 15:51:17.298
    Jul 29 15:51:17.299: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-5771 delete --grace-period=0 --force -f -'
    Jul 29 15:51:17.490: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jul 29 15:51:17.490: INFO: stdout: "service \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 07/29/23 15:51:17.49
    Jul 29 15:51:17.490: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-5771 delete --grace-period=0 --force -f -'
    Jul 29 15:51:17.632: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jul 29 15:51:17.632: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 07/29/23 15:51:17.632
    Jul 29 15:51:17.633: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-5771 delete --grace-period=0 --force -f -'
    Jul 29 15:51:17.841: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jul 29 15:51:17.841: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 07/29/23 15:51:17.841
    Jul 29 15:51:17.842: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-5771 delete --grace-period=0 --force -f -'
    Jul 29 15:51:18.145: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jul 29 15:51:18.145: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jul 29 15:51:18.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-5771" for this suite. 07/29/23 15:51:18.164
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:51:18.202
Jul 29 15:51:18.202: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename resourcequota 07/29/23 15:51:18.208
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:51:18.312
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:51:18.317
[It] should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874
STEP: Creating a ResourceQuota 07/29/23 15:51:18.322
STEP: Getting a ResourceQuota 07/29/23 15:51:18.333
STEP: Updating a ResourceQuota 07/29/23 15:51:18.345
STEP: Verifying a ResourceQuota was modified 07/29/23 15:51:18.359
STEP: Deleting a ResourceQuota 07/29/23 15:51:18.371
STEP: Verifying the deleted ResourceQuota 07/29/23 15:51:18.386
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jul 29 15:51:18.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6128" for this suite. 07/29/23 15:51:18.42
{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","completed":62,"skipped":1110,"failed":0}
------------------------------
â€¢ [0.282 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:51:18.202
    Jul 29 15:51:18.202: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename resourcequota 07/29/23 15:51:18.208
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:51:18.312
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:51:18.317
    [It] should be able to update and delete ResourceQuota. [Conformance]
      test/e2e/apimachinery/resource_quota.go:874
    STEP: Creating a ResourceQuota 07/29/23 15:51:18.322
    STEP: Getting a ResourceQuota 07/29/23 15:51:18.333
    STEP: Updating a ResourceQuota 07/29/23 15:51:18.345
    STEP: Verifying a ResourceQuota was modified 07/29/23 15:51:18.359
    STEP: Deleting a ResourceQuota 07/29/23 15:51:18.371
    STEP: Verifying the deleted ResourceQuota 07/29/23 15:51:18.386
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jul 29 15:51:18.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-6128" for this suite. 07/29/23 15:51:18.42
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:51:18.487
Jul 29 15:51:18.487: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename secrets 07/29/23 15:51:18.49
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:51:18.543
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:51:18.551
[It] should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139
STEP: Creating projection with secret that has name secret-emptykey-test-9fd61910-cfa9-487c-90f9-282c860a7ccc 07/29/23 15:51:18.558
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Jul 29 15:51:18.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2914" for this suite. 07/29/23 15:51:18.583
{"msg":"PASSED [sig-node] Secrets should fail to create secret due to empty secret key [Conformance]","completed":63,"skipped":1128,"failed":0}
------------------------------
â€¢ [0.110 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:51:18.487
    Jul 29 15:51:18.487: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename secrets 07/29/23 15:51:18.49
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:51:18.543
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:51:18.551
    [It] should fail to create secret due to empty secret key [Conformance]
      test/e2e/common/node/secrets.go:139
    STEP: Creating projection with secret that has name secret-emptykey-test-9fd61910-cfa9-487c-90f9-282c860a7ccc 07/29/23 15:51:18.558
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Jul 29 15:51:18.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-2914" for this suite. 07/29/23 15:51:18.583
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:51:18.599
Jul 29 15:51:18.599: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename namespaces 07/29/23 15:51:18.602
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:51:18.703
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:51:18.714
[It] should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267
STEP: creating a Namespace 07/29/23 15:51:18.726
STEP: patching the Namespace 07/29/23 15:51:18.767
STEP: get the Namespace and ensuring it has the label 07/29/23 15:51:18.782
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Jul 29 15:51:18.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-7130" for this suite. 07/29/23 15:51:18.796
STEP: Destroying namespace "nspatchtest-caaec362-3999-4f76-a9d9-9de586134c61-1854" for this suite. 07/29/23 15:51:18.808
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","completed":64,"skipped":1134,"failed":0}
------------------------------
â€¢ [0.223 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:51:18.599
    Jul 29 15:51:18.599: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename namespaces 07/29/23 15:51:18.602
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:51:18.703
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:51:18.714
    [It] should patch a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:267
    STEP: creating a Namespace 07/29/23 15:51:18.726
    STEP: patching the Namespace 07/29/23 15:51:18.767
    STEP: get the Namespace and ensuring it has the label 07/29/23 15:51:18.782
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Jul 29 15:51:18.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-7130" for this suite. 07/29/23 15:51:18.796
    STEP: Destroying namespace "nspatchtest-caaec362-3999-4f76-a9d9-9de586134c61-1854" for this suite. 07/29/23 15:51:18.808
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:51:18.823
Jul 29 15:51:18.823: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename resourcequota 07/29/23 15:51:18.826
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:51:18.859
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:51:18.866
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316
STEP: Counting existing ResourceQuota 07/29/23 15:51:35.879
STEP: Creating a ResourceQuota 07/29/23 15:51:40.887
STEP: Ensuring resource quota status is calculated 07/29/23 15:51:40.9
STEP: Creating a ConfigMap 07/29/23 15:51:42.909
STEP: Ensuring resource quota status captures configMap creation 07/29/23 15:51:42.929
STEP: Deleting a ConfigMap 07/29/23 15:51:44.94
STEP: Ensuring resource quota status released usage 07/29/23 15:51:44.955
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jul 29 15:51:46.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6463" for this suite. 07/29/23 15:51:46.974
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","completed":65,"skipped":1142,"failed":0}
------------------------------
â€¢ [SLOW TEST] [28.168 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:51:18.823
    Jul 29 15:51:18.823: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename resourcequota 07/29/23 15:51:18.826
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:51:18.859
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:51:18.866
    [It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
      test/e2e/apimachinery/resource_quota.go:316
    STEP: Counting existing ResourceQuota 07/29/23 15:51:35.879
    STEP: Creating a ResourceQuota 07/29/23 15:51:40.887
    STEP: Ensuring resource quota status is calculated 07/29/23 15:51:40.9
    STEP: Creating a ConfigMap 07/29/23 15:51:42.909
    STEP: Ensuring resource quota status captures configMap creation 07/29/23 15:51:42.929
    STEP: Deleting a ConfigMap 07/29/23 15:51:44.94
    STEP: Ensuring resource quota status released usage 07/29/23 15:51:44.955
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jul 29 15:51:46.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-6463" for this suite. 07/29/23 15:51:46.974
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:51:47.015
Jul 29 15:51:47.015: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename replication-controller 07/29/23 15:51:47.017
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:51:47.064
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:51:47.069
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100
STEP: Given a ReplicationController is created 07/29/23 15:51:47.074
STEP: When the matched label of one of its pods change 07/29/23 15:51:47.088
Jul 29 15:51:47.099: INFO: Pod name pod-release: Found 0 pods out of 1
Jul 29 15:51:52.129: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released 07/29/23 15:51:52.157
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Jul 29 15:51:53.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9373" for this suite. 07/29/23 15:51:53.208
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","completed":66,"skipped":1196,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.214 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:51:47.015
    Jul 29 15:51:47.015: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename replication-controller 07/29/23 15:51:47.017
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:51:47.064
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:51:47.069
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should release no longer matching pods [Conformance]
      test/e2e/apps/rc.go:100
    STEP: Given a ReplicationController is created 07/29/23 15:51:47.074
    STEP: When the matched label of one of its pods change 07/29/23 15:51:47.088
    Jul 29 15:51:47.099: INFO: Pod name pod-release: Found 0 pods out of 1
    Jul 29 15:51:52.129: INFO: Pod name pod-release: Found 1 pods out of 1
    STEP: Then the pod is released 07/29/23 15:51:52.157
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Jul 29 15:51:53.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-9373" for this suite. 07/29/23 15:51:53.208
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] Deployment
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:51:53.23
Jul 29 15:51:53.230: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename deployment 07/29/23 15:51:53.233
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:51:53.271
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:51:53.277
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
Jul 29 15:51:53.286: INFO: Creating deployment "test-recreate-deployment"
Jul 29 15:51:53.298: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Jul 29 15:51:53.317: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Jul 29 15:51:55.337: INFO: Waiting deployment "test-recreate-deployment" to complete
Jul 29 15:51:55.343: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Jul 29 15:51:55.363: INFO: Updating deployment test-recreate-deployment
Jul 29 15:51:55.363: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jul 29 15:51:55.550: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-2805  dc7b22c5-8d63-44ac-8042-bd9e53fd231e 8011 2 2023-07-29 15:51:53 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-07-29 15:51:55 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-29 15:51:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003569168 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-07-29 15:51:55 +0000 UTC,LastTransitionTime:2023-07-29 15:51:55 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-9d58999df" is progressing.,LastUpdateTime:2023-07-29 15:51:55 +0000 UTC,LastTransitionTime:2023-07-29 15:51:53 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Jul 29 15:51:55.559: INFO: New ReplicaSet "test-recreate-deployment-9d58999df" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-9d58999df  deployment-2805  1e223fe0-4234-42d4-819b-4bc5c6b1dbc6 8008 1 2023-07-29 15:51:55 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment dc7b22c5-8d63-44ac-8042-bd9e53fd231e 0xc0035695f0 0xc0035695f1}] [] [{kube-controller-manager Update apps/v1 2023-07-29 15:51:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dc7b22c5-8d63-44ac-8042-bd9e53fd231e\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-29 15:51:55 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 9d58999df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003569688 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jul 29 15:51:55.559: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Jul 29 15:51:55.560: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-7d8b6f647f  deployment-2805  a4700721-c521-422e-9cba-803530223f9d 7998 2 2023-07-29 15:51:53 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment dc7b22c5-8d63-44ac-8042-bd9e53fd231e 0xc0035694e7 0xc0035694e8}] [] [{kube-controller-manager Update apps/v1 2023-07-29 15:51:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dc7b22c5-8d63-44ac-8042-bd9e53fd231e\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-29 15:51:55 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d8b6f647f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003569598 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jul 29 15:51:55.568: INFO: Pod "test-recreate-deployment-9d58999df-bwvtk" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-9d58999df-bwvtk test-recreate-deployment-9d58999df- deployment-2805  a7adfda2-6106-4c39-b0b4-becefbcf48f3 8009 0 2023-07-29 15:51:55 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [{apps/v1 ReplicaSet test-recreate-deployment-9d58999df 1e223fe0-4234-42d4-819b-4bc5c6b1dbc6 0xc003569b00 0xc003569b01}] [] [{kube-controller-manager Update v1 2023-07-29 15:51:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1e223fe0-4234-42d4-819b-4bc5c6b1dbc6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 15:51:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-b9p5q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-b9p5q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wa4quivohpee-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 15:51:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 15:51:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 15:51:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 15:51:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.234,PodIP:,StartTime:2023-07-29 15:51:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jul 29 15:51:55.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2805" for this suite. 07/29/23 15:51:55.578
{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","completed":67,"skipped":1196,"failed":0}
------------------------------
â€¢ [2.362 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:51:53.23
    Jul 29 15:51:53.230: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename deployment 07/29/23 15:51:53.233
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:51:53.271
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:51:53.277
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RecreateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:113
    Jul 29 15:51:53.286: INFO: Creating deployment "test-recreate-deployment"
    Jul 29 15:51:53.298: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
    Jul 29 15:51:53.317: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
    Jul 29 15:51:55.337: INFO: Waiting deployment "test-recreate-deployment" to complete
    Jul 29 15:51:55.343: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
    Jul 29 15:51:55.363: INFO: Updating deployment test-recreate-deployment
    Jul 29 15:51:55.363: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jul 29 15:51:55.550: INFO: Deployment "test-recreate-deployment":
    &Deployment{ObjectMeta:{test-recreate-deployment  deployment-2805  dc7b22c5-8d63-44ac-8042-bd9e53fd231e 8011 2 2023-07-29 15:51:53 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-07-29 15:51:55 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-29 15:51:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003569168 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-07-29 15:51:55 +0000 UTC,LastTransitionTime:2023-07-29 15:51:55 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-9d58999df" is progressing.,LastUpdateTime:2023-07-29 15:51:55 +0000 UTC,LastTransitionTime:2023-07-29 15:51:53 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

    Jul 29 15:51:55.559: INFO: New ReplicaSet "test-recreate-deployment-9d58999df" of Deployment "test-recreate-deployment":
    &ReplicaSet{ObjectMeta:{test-recreate-deployment-9d58999df  deployment-2805  1e223fe0-4234-42d4-819b-4bc5c6b1dbc6 8008 1 2023-07-29 15:51:55 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment dc7b22c5-8d63-44ac-8042-bd9e53fd231e 0xc0035695f0 0xc0035695f1}] [] [{kube-controller-manager Update apps/v1 2023-07-29 15:51:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dc7b22c5-8d63-44ac-8042-bd9e53fd231e\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-29 15:51:55 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 9d58999df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003569688 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jul 29 15:51:55.559: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
    Jul 29 15:51:55.560: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-7d8b6f647f  deployment-2805  a4700721-c521-422e-9cba-803530223f9d 7998 2 2023-07-29 15:51:53 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment dc7b22c5-8d63-44ac-8042-bd9e53fd231e 0xc0035694e7 0xc0035694e8}] [] [{kube-controller-manager Update apps/v1 2023-07-29 15:51:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dc7b22c5-8d63-44ac-8042-bd9e53fd231e\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-29 15:51:55 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d8b6f647f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003569598 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jul 29 15:51:55.568: INFO: Pod "test-recreate-deployment-9d58999df-bwvtk" is not available:
    &Pod{ObjectMeta:{test-recreate-deployment-9d58999df-bwvtk test-recreate-deployment-9d58999df- deployment-2805  a7adfda2-6106-4c39-b0b4-becefbcf48f3 8009 0 2023-07-29 15:51:55 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [{apps/v1 ReplicaSet test-recreate-deployment-9d58999df 1e223fe0-4234-42d4-819b-4bc5c6b1dbc6 0xc003569b00 0xc003569b01}] [] [{kube-controller-manager Update v1 2023-07-29 15:51:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1e223fe0-4234-42d4-819b-4bc5c6b1dbc6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 15:51:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-b9p5q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-b9p5q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wa4quivohpee-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 15:51:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 15:51:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 15:51:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 15:51:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.234,PodIP:,StartTime:2023-07-29 15:51:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jul 29 15:51:55.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-2805" for this suite. 07/29/23 15:51:55.578
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] CronJob
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:51:55.593
Jul 29 15:51:55.593: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename cronjob 07/29/23 15:51:55.597
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:51:55.628
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:51:55.633
[It] should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
STEP: Creating a cronjob 07/29/23 15:51:55.638
STEP: Ensuring more than one job is running at a time 07/29/23 15:51:55.649
STEP: Ensuring at least two running jobs exists by listing jobs explicitly 07/29/23 15:53:01.658
STEP: Removing cronjob 07/29/23 15:53:01.665
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Jul 29 15:53:01.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-2931" for this suite. 07/29/23 15:53:01.687
{"msg":"PASSED [sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]","completed":68,"skipped":1196,"failed":0}
------------------------------
â€¢ [SLOW TEST] [66.102 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:51:55.593
    Jul 29 15:51:55.593: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename cronjob 07/29/23 15:51:55.597
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:51:55.628
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:51:55.633
    [It] should schedule multiple jobs concurrently [Conformance]
      test/e2e/apps/cronjob.go:69
    STEP: Creating a cronjob 07/29/23 15:51:55.638
    STEP: Ensuring more than one job is running at a time 07/29/23 15:51:55.649
    STEP: Ensuring at least two running jobs exists by listing jobs explicitly 07/29/23 15:53:01.658
    STEP: Removing cronjob 07/29/23 15:53:01.665
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Jul 29 15:53:01.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-2931" for this suite. 07/29/23 15:53:01.687
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:53:01.696
Jul 29 15:53:01.696: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename downward-api 07/29/23 15:53:01.698
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:53:01.73
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:53:01.738
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220
STEP: Creating a pod to test downward API volume plugin 07/29/23 15:53:01.741
Jul 29 15:53:01.787: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a1a2ad33-6ab4-4fd6-8269-c283d02544bb" in namespace "downward-api-2934" to be "Succeeded or Failed"
Jul 29 15:53:01.800: INFO: Pod "downwardapi-volume-a1a2ad33-6ab4-4fd6-8269-c283d02544bb": Phase="Pending", Reason="", readiness=false. Elapsed: 12.88441ms
Jul 29 15:53:03.808: INFO: Pod "downwardapi-volume-a1a2ad33-6ab4-4fd6-8269-c283d02544bb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020920907s
Jul 29 15:53:05.807: INFO: Pod "downwardapi-volume-a1a2ad33-6ab4-4fd6-8269-c283d02544bb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019885733s
STEP: Saw pod success 07/29/23 15:53:05.807
Jul 29 15:53:05.807: INFO: Pod "downwardapi-volume-a1a2ad33-6ab4-4fd6-8269-c283d02544bb" satisfied condition "Succeeded or Failed"
Jul 29 15:53:05.813: INFO: Trying to get logs from node wa4quivohpee-3 pod downwardapi-volume-a1a2ad33-6ab4-4fd6-8269-c283d02544bb container client-container: <nil>
STEP: delete the pod 07/29/23 15:53:05.843
Jul 29 15:53:05.867: INFO: Waiting for pod downwardapi-volume-a1a2ad33-6ab4-4fd6-8269-c283d02544bb to disappear
Jul 29 15:53:05.872: INFO: Pod downwardapi-volume-a1a2ad33-6ab4-4fd6-8269-c283d02544bb no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jul 29 15:53:05.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2934" for this suite. 07/29/23 15:53:05.881
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","completed":69,"skipped":1197,"failed":0}
------------------------------
â€¢ [4.193 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:53:01.696
    Jul 29 15:53:01.696: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename downward-api 07/29/23 15:53:01.698
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:53:01.73
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:53:01.738
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:220
    STEP: Creating a pod to test downward API volume plugin 07/29/23 15:53:01.741
    Jul 29 15:53:01.787: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a1a2ad33-6ab4-4fd6-8269-c283d02544bb" in namespace "downward-api-2934" to be "Succeeded or Failed"
    Jul 29 15:53:01.800: INFO: Pod "downwardapi-volume-a1a2ad33-6ab4-4fd6-8269-c283d02544bb": Phase="Pending", Reason="", readiness=false. Elapsed: 12.88441ms
    Jul 29 15:53:03.808: INFO: Pod "downwardapi-volume-a1a2ad33-6ab4-4fd6-8269-c283d02544bb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020920907s
    Jul 29 15:53:05.807: INFO: Pod "downwardapi-volume-a1a2ad33-6ab4-4fd6-8269-c283d02544bb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019885733s
    STEP: Saw pod success 07/29/23 15:53:05.807
    Jul 29 15:53:05.807: INFO: Pod "downwardapi-volume-a1a2ad33-6ab4-4fd6-8269-c283d02544bb" satisfied condition "Succeeded or Failed"
    Jul 29 15:53:05.813: INFO: Trying to get logs from node wa4quivohpee-3 pod downwardapi-volume-a1a2ad33-6ab4-4fd6-8269-c283d02544bb container client-container: <nil>
    STEP: delete the pod 07/29/23 15:53:05.843
    Jul 29 15:53:05.867: INFO: Waiting for pod downwardapi-volume-a1a2ad33-6ab4-4fd6-8269-c283d02544bb to disappear
    Jul 29 15:53:05.872: INFO: Pod downwardapi-volume-a1a2ad33-6ab4-4fd6-8269-c283d02544bb no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jul 29 15:53:05.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-2934" for this suite. 07/29/23 15:53:05.881
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance]
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:53:05.899
Jul 29 15:53:05.899: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename ephemeral-containers-test 07/29/23 15:53:05.901
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:53:05.929
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:53:05.934
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/common/node/ephemeral_containers.go:38
[It] will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
STEP: creating a target pod 07/29/23 15:53:05.938
Jul 29 15:53:05.956: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-9042" to be "running and ready"
Jul 29 15:53:05.973: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 17.08122ms
Jul 29 15:53:05.973: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
Jul 29 15:53:07.982: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.025877439s
Jul 29 15:53:07.982: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
Jul 29 15:53:07.982: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
STEP: adding an ephemeral container 07/29/23 15:53:07.995
Jul 29 15:53:08.034: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-9042" to be "container debugger running"
Jul 29 15:53:08.040: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 5.801458ms
Jul 29 15:53:10.050: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.016185486s
Jul 29 15:53:12.049: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.014575526s
Jul 29 15:53:12.049: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
STEP: checking pod container endpoints 07/29/23 15:53:12.049
Jul 29 15:53:12.049: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-9042 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 29 15:53:12.049: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
Jul 29 15:53:12.051: INFO: ExecWithOptions: Clientset creation
Jul 29 15:53:12.051: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/ephemeral-containers-test-9042/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
Jul 29 15:53:12.154: INFO: Exec stderr: ""
[AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/framework.go:187
Jul 29 15:53:12.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ephemeral-containers-test-9042" for this suite. 07/29/23 15:53:12.173
{"msg":"PASSED [sig-node] Ephemeral Containers [NodeConformance] will start an ephemeral container in an existing pod [Conformance]","completed":70,"skipped":1225,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.286 seconds]
[sig-node] Ephemeral Containers [NodeConformance]
test/e2e/common/node/framework.go:23
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:53:05.899
    Jul 29 15:53:05.899: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename ephemeral-containers-test 07/29/23 15:53:05.901
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:53:05.929
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:53:05.934
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/common/node/ephemeral_containers.go:38
    [It] will start an ephemeral container in an existing pod [Conformance]
      test/e2e/common/node/ephemeral_containers.go:45
    STEP: creating a target pod 07/29/23 15:53:05.938
    Jul 29 15:53:05.956: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-9042" to be "running and ready"
    Jul 29 15:53:05.973: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 17.08122ms
    Jul 29 15:53:05.973: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
    Jul 29 15:53:07.982: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.025877439s
    Jul 29 15:53:07.982: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
    Jul 29 15:53:07.982: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
    STEP: adding an ephemeral container 07/29/23 15:53:07.995
    Jul 29 15:53:08.034: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-9042" to be "container debugger running"
    Jul 29 15:53:08.040: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 5.801458ms
    Jul 29 15:53:10.050: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.016185486s
    Jul 29 15:53:12.049: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.014575526s
    Jul 29 15:53:12.049: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
    STEP: checking pod container endpoints 07/29/23 15:53:12.049
    Jul 29 15:53:12.049: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-9042 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jul 29 15:53:12.049: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    Jul 29 15:53:12.051: INFO: ExecWithOptions: Clientset creation
    Jul 29 15:53:12.051: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/ephemeral-containers-test-9042/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
    Jul 29 15:53:12.154: INFO: Exec stderr: ""
    [AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/framework.go:187
    Jul 29 15:53:12.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ephemeral-containers-test-9042" for this suite. 07/29/23 15:53:12.173
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Kubectl version
  should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1683
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:53:12.187
Jul 29 15:53:12.187: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename kubectl 07/29/23 15:53:12.189
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:53:12.217
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:53:12.223
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1683
Jul 29 15:53:12.228: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-4501 version'
Jul 29 15:53:12.361: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
Jul 29 15:53:12.361: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.12\", GitCommit:\"ba490f01df1945d0567348b271c79a2aece7f623\", GitTreeState:\"clean\", BuildDate:\"2023-07-19T12:23:43Z\", GoVersion:\"go1.20.6\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.12\", GitCommit:\"ba490f01df1945d0567348b271c79a2aece7f623\", GitTreeState:\"clean\", BuildDate:\"2023-07-19T12:17:23Z\", GoVersion:\"go1.20.6\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jul 29 15:53:12.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4501" for this suite. 07/29/23 15:53:12.371
{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","completed":71,"skipped":1228,"failed":0}
------------------------------
â€¢ [0.196 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl version
  test/e2e/kubectl/kubectl.go:1677
    should check is all data is printed  [Conformance]
    test/e2e/kubectl/kubectl.go:1683

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:53:12.187
    Jul 29 15:53:12.187: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename kubectl 07/29/23 15:53:12.189
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:53:12.217
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:53:12.223
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check is all data is printed  [Conformance]
      test/e2e/kubectl/kubectl.go:1683
    Jul 29 15:53:12.228: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-4501 version'
    Jul 29 15:53:12.361: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
    Jul 29 15:53:12.361: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.12\", GitCommit:\"ba490f01df1945d0567348b271c79a2aece7f623\", GitTreeState:\"clean\", BuildDate:\"2023-07-19T12:23:43Z\", GoVersion:\"go1.20.6\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.12\", GitCommit:\"ba490f01df1945d0567348b271c79a2aece7f623\", GitTreeState:\"clean\", BuildDate:\"2023-07-19T12:17:23Z\", GoVersion:\"go1.20.6\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jul 29 15:53:12.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-4501" for this suite. 07/29/23 15:53:12.371
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:53:12.384
Jul 29 15:53:12.385: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename emptydir 07/29/23 15:53:12.386
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:53:12.419
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:53:12.423
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116
STEP: Creating a pod to test emptydir 0777 on tmpfs 07/29/23 15:53:12.429
Jul 29 15:53:12.455: INFO: Waiting up to 5m0s for pod "pod-6ca19f69-7ea1-4461-89e4-57113c6af92b" in namespace "emptydir-6235" to be "Succeeded or Failed"
Jul 29 15:53:12.461: INFO: Pod "pod-6ca19f69-7ea1-4461-89e4-57113c6af92b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.064984ms
Jul 29 15:53:14.470: INFO: Pod "pod-6ca19f69-7ea1-4461-89e4-57113c6af92b": Phase="Running", Reason="", readiness=true. Elapsed: 2.014913071s
Jul 29 15:53:16.470: INFO: Pod "pod-6ca19f69-7ea1-4461-89e4-57113c6af92b": Phase="Running", Reason="", readiness=false. Elapsed: 4.0145775s
Jul 29 15:53:18.471: INFO: Pod "pod-6ca19f69-7ea1-4461-89e4-57113c6af92b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.01545671s
STEP: Saw pod success 07/29/23 15:53:18.471
Jul 29 15:53:18.471: INFO: Pod "pod-6ca19f69-7ea1-4461-89e4-57113c6af92b" satisfied condition "Succeeded or Failed"
Jul 29 15:53:18.477: INFO: Trying to get logs from node wa4quivohpee-3 pod pod-6ca19f69-7ea1-4461-89e4-57113c6af92b container test-container: <nil>
STEP: delete the pod 07/29/23 15:53:18.489
Jul 29 15:53:18.514: INFO: Waiting for pod pod-6ca19f69-7ea1-4461-89e4-57113c6af92b to disappear
Jul 29 15:53:18.525: INFO: Pod pod-6ca19f69-7ea1-4461-89e4-57113c6af92b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jul 29 15:53:18.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6235" for this suite. 07/29/23 15:53:18.537
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":72,"skipped":1261,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.165 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:53:12.384
    Jul 29 15:53:12.385: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename emptydir 07/29/23 15:53:12.386
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:53:12.419
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:53:12.423
    [It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:116
    STEP: Creating a pod to test emptydir 0777 on tmpfs 07/29/23 15:53:12.429
    Jul 29 15:53:12.455: INFO: Waiting up to 5m0s for pod "pod-6ca19f69-7ea1-4461-89e4-57113c6af92b" in namespace "emptydir-6235" to be "Succeeded or Failed"
    Jul 29 15:53:12.461: INFO: Pod "pod-6ca19f69-7ea1-4461-89e4-57113c6af92b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.064984ms
    Jul 29 15:53:14.470: INFO: Pod "pod-6ca19f69-7ea1-4461-89e4-57113c6af92b": Phase="Running", Reason="", readiness=true. Elapsed: 2.014913071s
    Jul 29 15:53:16.470: INFO: Pod "pod-6ca19f69-7ea1-4461-89e4-57113c6af92b": Phase="Running", Reason="", readiness=false. Elapsed: 4.0145775s
    Jul 29 15:53:18.471: INFO: Pod "pod-6ca19f69-7ea1-4461-89e4-57113c6af92b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.01545671s
    STEP: Saw pod success 07/29/23 15:53:18.471
    Jul 29 15:53:18.471: INFO: Pod "pod-6ca19f69-7ea1-4461-89e4-57113c6af92b" satisfied condition "Succeeded or Failed"
    Jul 29 15:53:18.477: INFO: Trying to get logs from node wa4quivohpee-3 pod pod-6ca19f69-7ea1-4461-89e4-57113c6af92b container test-container: <nil>
    STEP: delete the pod 07/29/23 15:53:18.489
    Jul 29 15:53:18.514: INFO: Waiting for pod pod-6ca19f69-7ea1-4461-89e4-57113c6af92b to disappear
    Jul 29 15:53:18.525: INFO: Pod pod-6ca19f69-7ea1-4461-89e4-57113c6af92b no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jul 29 15:53:18.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-6235" for this suite. 07/29/23 15:53:18.537
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:53:18.553
Jul 29 15:53:18.553: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename projected 07/29/23 15:53:18.555
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:53:18.581
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:53:18.586
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55
STEP: Creating projection with secret that has name projected-secret-test-16b8ddd4-fa51-4a29-b026-c5227ff1aa19 07/29/23 15:53:18.59
STEP: Creating a pod to test consume secrets 07/29/23 15:53:18.601
Jul 29 15:53:18.632: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-af618c11-5996-49c4-8f7c-2873c78fe053" in namespace "projected-3755" to be "Succeeded or Failed"
Jul 29 15:53:18.639: INFO: Pod "pod-projected-secrets-af618c11-5996-49c4-8f7c-2873c78fe053": Phase="Pending", Reason="", readiness=false. Elapsed: 7.32573ms
Jul 29 15:53:20.647: INFO: Pod "pod-projected-secrets-af618c11-5996-49c4-8f7c-2873c78fe053": Phase="Running", Reason="", readiness=false. Elapsed: 2.01477226s
Jul 29 15:53:22.648: INFO: Pod "pod-projected-secrets-af618c11-5996-49c4-8f7c-2873c78fe053": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016411733s
STEP: Saw pod success 07/29/23 15:53:22.649
Jul 29 15:53:22.649: INFO: Pod "pod-projected-secrets-af618c11-5996-49c4-8f7c-2873c78fe053" satisfied condition "Succeeded or Failed"
Jul 29 15:53:22.655: INFO: Trying to get logs from node wa4quivohpee-3 pod pod-projected-secrets-af618c11-5996-49c4-8f7c-2873c78fe053 container projected-secret-volume-test: <nil>
STEP: delete the pod 07/29/23 15:53:22.726
Jul 29 15:53:22.756: INFO: Waiting for pod pod-projected-secrets-af618c11-5996-49c4-8f7c-2873c78fe053 to disappear
Jul 29 15:53:22.762: INFO: Pod pod-projected-secrets-af618c11-5996-49c4-8f7c-2873c78fe053 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Jul 29 15:53:22.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3755" for this suite. 07/29/23 15:53:22.774
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":73,"skipped":1265,"failed":0}
------------------------------
â€¢ [4.237 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:53:18.553
    Jul 29 15:53:18.553: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename projected 07/29/23 15:53:18.555
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:53:18.581
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:53:18.586
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:55
    STEP: Creating projection with secret that has name projected-secret-test-16b8ddd4-fa51-4a29-b026-c5227ff1aa19 07/29/23 15:53:18.59
    STEP: Creating a pod to test consume secrets 07/29/23 15:53:18.601
    Jul 29 15:53:18.632: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-af618c11-5996-49c4-8f7c-2873c78fe053" in namespace "projected-3755" to be "Succeeded or Failed"
    Jul 29 15:53:18.639: INFO: Pod "pod-projected-secrets-af618c11-5996-49c4-8f7c-2873c78fe053": Phase="Pending", Reason="", readiness=false. Elapsed: 7.32573ms
    Jul 29 15:53:20.647: INFO: Pod "pod-projected-secrets-af618c11-5996-49c4-8f7c-2873c78fe053": Phase="Running", Reason="", readiness=false. Elapsed: 2.01477226s
    Jul 29 15:53:22.648: INFO: Pod "pod-projected-secrets-af618c11-5996-49c4-8f7c-2873c78fe053": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016411733s
    STEP: Saw pod success 07/29/23 15:53:22.649
    Jul 29 15:53:22.649: INFO: Pod "pod-projected-secrets-af618c11-5996-49c4-8f7c-2873c78fe053" satisfied condition "Succeeded or Failed"
    Jul 29 15:53:22.655: INFO: Trying to get logs from node wa4quivohpee-3 pod pod-projected-secrets-af618c11-5996-49c4-8f7c-2873c78fe053 container projected-secret-volume-test: <nil>
    STEP: delete the pod 07/29/23 15:53:22.726
    Jul 29 15:53:22.756: INFO: Waiting for pod pod-projected-secrets-af618c11-5996-49c4-8f7c-2873c78fe053 to disappear
    Jul 29 15:53:22.762: INFO: Pod pod-projected-secrets-af618c11-5996-49c4-8f7c-2873c78fe053 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Jul 29 15:53:22.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3755" for this suite. 07/29/23 15:53:22.774
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] Watchers
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:53:22.794
Jul 29 15:53:22.794: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename watch 07/29/23 15:53:22.797
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:53:22.834
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:53:22.84
[It] should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
STEP: creating a new configmap 07/29/23 15:53:22.845
STEP: modifying the configmap once 07/29/23 15:53:22.855
STEP: modifying the configmap a second time 07/29/23 15:53:22.874
STEP: deleting the configmap 07/29/23 15:53:22.889
STEP: creating a watch on configmaps from the resource version returned by the first update 07/29/23 15:53:22.902
STEP: Expecting to observe notifications for all changes to the configmap after the first update 07/29/23 15:53:22.905
Jul 29 15:53:22.906: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-5592  5441efe4-80a1-483b-a3ff-53c93a87b72c 8406 0 2023-07-29 15:53:22 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-07-29 15:53:22 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 29 15:53:22.906: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-5592  5441efe4-80a1-483b-a3ff-53c93a87b72c 8407 0 2023-07-29 15:53:22 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-07-29 15:53:22 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Jul 29 15:53:22.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5592" for this suite. 07/29/23 15:53:22.916
{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","completed":74,"skipped":1268,"failed":0}
------------------------------
â€¢ [0.138 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:53:22.794
    Jul 29 15:53:22.794: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename watch 07/29/23 15:53:22.797
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:53:22.834
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:53:22.84
    [It] should be able to start watching from a specific resource version [Conformance]
      test/e2e/apimachinery/watch.go:142
    STEP: creating a new configmap 07/29/23 15:53:22.845
    STEP: modifying the configmap once 07/29/23 15:53:22.855
    STEP: modifying the configmap a second time 07/29/23 15:53:22.874
    STEP: deleting the configmap 07/29/23 15:53:22.889
    STEP: creating a watch on configmaps from the resource version returned by the first update 07/29/23 15:53:22.902
    STEP: Expecting to observe notifications for all changes to the configmap after the first update 07/29/23 15:53:22.905
    Jul 29 15:53:22.906: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-5592  5441efe4-80a1-483b-a3ff-53c93a87b72c 8406 0 2023-07-29 15:53:22 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-07-29 15:53:22 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jul 29 15:53:22.906: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-5592  5441efe4-80a1-483b-a3ff-53c93a87b72c 8407 0 2023-07-29 15:53:22 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-07-29 15:53:22 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Jul 29 15:53:22.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-5592" for this suite. 07/29/23 15:53:22.916
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:53:22.941
Jul 29 15:53:22.942: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename emptydir 07/29/23 15:53:22.944
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:53:22.978
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:53:22.983
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216
STEP: Creating a pod to test emptydir 0777 on node default medium 07/29/23 15:53:22.989
Jul 29 15:53:23.008: INFO: Waiting up to 5m0s for pod "pod-e0635eae-ad69-4429-bc02-556f226478f3" in namespace "emptydir-5065" to be "Succeeded or Failed"
Jul 29 15:53:23.015: INFO: Pod "pod-e0635eae-ad69-4429-bc02-556f226478f3": Phase="Pending", Reason="", readiness=false. Elapsed: 7.057461ms
Jul 29 15:53:25.027: INFO: Pod "pod-e0635eae-ad69-4429-bc02-556f226478f3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019191223s
Jul 29 15:53:27.026: INFO: Pod "pod-e0635eae-ad69-4429-bc02-556f226478f3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017993284s
STEP: Saw pod success 07/29/23 15:53:27.027
Jul 29 15:53:27.028: INFO: Pod "pod-e0635eae-ad69-4429-bc02-556f226478f3" satisfied condition "Succeeded or Failed"
Jul 29 15:53:27.035: INFO: Trying to get logs from node wa4quivohpee-3 pod pod-e0635eae-ad69-4429-bc02-556f226478f3 container test-container: <nil>
STEP: delete the pod 07/29/23 15:53:27.046
Jul 29 15:53:27.070: INFO: Waiting for pod pod-e0635eae-ad69-4429-bc02-556f226478f3 to disappear
Jul 29 15:53:27.077: INFO: Pod pod-e0635eae-ad69-4429-bc02-556f226478f3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jul 29 15:53:27.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5065" for this suite. 07/29/23 15:53:27.084
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":75,"skipped":1298,"failed":0}
------------------------------
â€¢ [4.156 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:53:22.941
    Jul 29 15:53:22.942: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename emptydir 07/29/23 15:53:22.944
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:53:22.978
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:53:22.983
    [It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:216
    STEP: Creating a pod to test emptydir 0777 on node default medium 07/29/23 15:53:22.989
    Jul 29 15:53:23.008: INFO: Waiting up to 5m0s for pod "pod-e0635eae-ad69-4429-bc02-556f226478f3" in namespace "emptydir-5065" to be "Succeeded or Failed"
    Jul 29 15:53:23.015: INFO: Pod "pod-e0635eae-ad69-4429-bc02-556f226478f3": Phase="Pending", Reason="", readiness=false. Elapsed: 7.057461ms
    Jul 29 15:53:25.027: INFO: Pod "pod-e0635eae-ad69-4429-bc02-556f226478f3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019191223s
    Jul 29 15:53:27.026: INFO: Pod "pod-e0635eae-ad69-4429-bc02-556f226478f3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017993284s
    STEP: Saw pod success 07/29/23 15:53:27.027
    Jul 29 15:53:27.028: INFO: Pod "pod-e0635eae-ad69-4429-bc02-556f226478f3" satisfied condition "Succeeded or Failed"
    Jul 29 15:53:27.035: INFO: Trying to get logs from node wa4quivohpee-3 pod pod-e0635eae-ad69-4429-bc02-556f226478f3 container test-container: <nil>
    STEP: delete the pod 07/29/23 15:53:27.046
    Jul 29 15:53:27.070: INFO: Waiting for pod pod-e0635eae-ad69-4429-bc02-556f226478f3 to disappear
    Jul 29 15:53:27.077: INFO: Pod pod-e0635eae-ad69-4429-bc02-556f226478f3 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jul 29 15:53:27.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-5065" for this suite. 07/29/23 15:53:27.084
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Services
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2194
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:53:27.098
Jul 29 15:53:27.098: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename services 07/29/23 15:53:27.103
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:53:27.138
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:53:27.147
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2194
STEP: creating service in namespace services-1892 07/29/23 15:53:27.152
STEP: creating service affinity-nodeport in namespace services-1892 07/29/23 15:53:27.153
STEP: creating replication controller affinity-nodeport in namespace services-1892 07/29/23 15:53:27.184
I0729 15:53:27.198458      13 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-1892, replica count: 3
I0729 15:53:30.250355      13 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 29 15:53:30.275: INFO: Creating new exec pod
Jul 29 15:53:30.293: INFO: Waiting up to 5m0s for pod "execpod-affinity4w55f" in namespace "services-1892" to be "running"
Jul 29 15:53:30.300: INFO: Pod "execpod-affinity4w55f": Phase="Pending", Reason="", readiness=false. Elapsed: 7.05841ms
Jul 29 15:53:32.307: INFO: Pod "execpod-affinity4w55f": Phase="Running", Reason="", readiness=true. Elapsed: 2.013870668s
Jul 29 15:53:32.307: INFO: Pod "execpod-affinity4w55f" satisfied condition "running"
Jul 29 15:53:33.315: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=services-1892 exec execpod-affinity4w55f -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
Jul 29 15:53:33.647: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Jul 29 15:53:33.647: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul 29 15:53:33.649: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=services-1892 exec execpod-affinity4w55f -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.24.243 80'
Jul 29 15:53:33.925: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.24.243 80\nConnection to 10.233.24.243 80 port [tcp/http] succeeded!\n"
Jul 29 15:53:33.925: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul 29 15:53:33.926: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=services-1892 exec execpod-affinity4w55f -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.28 32292'
Jul 29 15:53:34.149: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.28 32292\nConnection to 192.168.121.28 32292 port [tcp/*] succeeded!\n"
Jul 29 15:53:34.149: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul 29 15:53:34.151: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=services-1892 exec execpod-affinity4w55f -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.234 32292'
Jul 29 15:53:34.375: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.234 32292\nConnection to 192.168.121.234 32292 port [tcp/*] succeeded!\n"
Jul 29 15:53:34.375: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul 29 15:53:34.375: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=services-1892 exec execpod-affinity4w55f -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.121.28:32292/ ; done'
Jul 29 15:53:34.812: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.28:32292/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.28:32292/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.28:32292/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.28:32292/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.28:32292/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.28:32292/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.28:32292/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.28:32292/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.28:32292/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.28:32292/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.28:32292/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.28:32292/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.28:32292/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.28:32292/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.28:32292/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.28:32292/\n"
Jul 29 15:53:34.812: INFO: stdout: "\naffinity-nodeport-8nbds\naffinity-nodeport-8nbds\naffinity-nodeport-8nbds\naffinity-nodeport-8nbds\naffinity-nodeport-8nbds\naffinity-nodeport-8nbds\naffinity-nodeport-8nbds\naffinity-nodeport-8nbds\naffinity-nodeport-8nbds\naffinity-nodeport-8nbds\naffinity-nodeport-8nbds\naffinity-nodeport-8nbds\naffinity-nodeport-8nbds\naffinity-nodeport-8nbds\naffinity-nodeport-8nbds\naffinity-nodeport-8nbds"
Jul 29 15:53:34.812: INFO: Received response from host: affinity-nodeport-8nbds
Jul 29 15:53:34.812: INFO: Received response from host: affinity-nodeport-8nbds
Jul 29 15:53:34.812: INFO: Received response from host: affinity-nodeport-8nbds
Jul 29 15:53:34.812: INFO: Received response from host: affinity-nodeport-8nbds
Jul 29 15:53:34.812: INFO: Received response from host: affinity-nodeport-8nbds
Jul 29 15:53:34.812: INFO: Received response from host: affinity-nodeport-8nbds
Jul 29 15:53:34.812: INFO: Received response from host: affinity-nodeport-8nbds
Jul 29 15:53:34.812: INFO: Received response from host: affinity-nodeport-8nbds
Jul 29 15:53:34.812: INFO: Received response from host: affinity-nodeport-8nbds
Jul 29 15:53:34.812: INFO: Received response from host: affinity-nodeport-8nbds
Jul 29 15:53:34.812: INFO: Received response from host: affinity-nodeport-8nbds
Jul 29 15:53:34.812: INFO: Received response from host: affinity-nodeport-8nbds
Jul 29 15:53:34.812: INFO: Received response from host: affinity-nodeport-8nbds
Jul 29 15:53:34.812: INFO: Received response from host: affinity-nodeport-8nbds
Jul 29 15:53:34.812: INFO: Received response from host: affinity-nodeport-8nbds
Jul 29 15:53:34.812: INFO: Received response from host: affinity-nodeport-8nbds
Jul 29 15:53:34.812: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-1892, will wait for the garbage collector to delete the pods 07/29/23 15:53:34.838
Jul 29 15:53:34.906: INFO: Deleting ReplicationController affinity-nodeport took: 10.485272ms
Jul 29 15:53:35.007: INFO: Terminating ReplicationController affinity-nodeport pods took: 101.401073ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jul 29 15:53:36.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1892" for this suite. 07/29/23 15:53:36.864
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","completed":76,"skipped":1298,"failed":0}
------------------------------
â€¢ [SLOW TEST] [9.778 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:53:27.098
    Jul 29 15:53:27.098: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename services 07/29/23 15:53:27.103
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:53:27.138
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:53:27.147
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2194
    STEP: creating service in namespace services-1892 07/29/23 15:53:27.152
    STEP: creating service affinity-nodeport in namespace services-1892 07/29/23 15:53:27.153
    STEP: creating replication controller affinity-nodeport in namespace services-1892 07/29/23 15:53:27.184
    I0729 15:53:27.198458      13 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-1892, replica count: 3
    I0729 15:53:30.250355      13 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jul 29 15:53:30.275: INFO: Creating new exec pod
    Jul 29 15:53:30.293: INFO: Waiting up to 5m0s for pod "execpod-affinity4w55f" in namespace "services-1892" to be "running"
    Jul 29 15:53:30.300: INFO: Pod "execpod-affinity4w55f": Phase="Pending", Reason="", readiness=false. Elapsed: 7.05841ms
    Jul 29 15:53:32.307: INFO: Pod "execpod-affinity4w55f": Phase="Running", Reason="", readiness=true. Elapsed: 2.013870668s
    Jul 29 15:53:32.307: INFO: Pod "execpod-affinity4w55f" satisfied condition "running"
    Jul 29 15:53:33.315: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=services-1892 exec execpod-affinity4w55f -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
    Jul 29 15:53:33.647: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
    Jul 29 15:53:33.647: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jul 29 15:53:33.649: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=services-1892 exec execpod-affinity4w55f -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.24.243 80'
    Jul 29 15:53:33.925: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.24.243 80\nConnection to 10.233.24.243 80 port [tcp/http] succeeded!\n"
    Jul 29 15:53:33.925: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jul 29 15:53:33.926: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=services-1892 exec execpod-affinity4w55f -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.28 32292'
    Jul 29 15:53:34.149: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.28 32292\nConnection to 192.168.121.28 32292 port [tcp/*] succeeded!\n"
    Jul 29 15:53:34.149: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jul 29 15:53:34.151: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=services-1892 exec execpod-affinity4w55f -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.234 32292'
    Jul 29 15:53:34.375: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.234 32292\nConnection to 192.168.121.234 32292 port [tcp/*] succeeded!\n"
    Jul 29 15:53:34.375: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jul 29 15:53:34.375: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=services-1892 exec execpod-affinity4w55f -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.121.28:32292/ ; done'
    Jul 29 15:53:34.812: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.28:32292/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.28:32292/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.28:32292/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.28:32292/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.28:32292/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.28:32292/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.28:32292/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.28:32292/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.28:32292/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.28:32292/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.28:32292/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.28:32292/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.28:32292/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.28:32292/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.28:32292/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.28:32292/\n"
    Jul 29 15:53:34.812: INFO: stdout: "\naffinity-nodeport-8nbds\naffinity-nodeport-8nbds\naffinity-nodeport-8nbds\naffinity-nodeport-8nbds\naffinity-nodeport-8nbds\naffinity-nodeport-8nbds\naffinity-nodeport-8nbds\naffinity-nodeport-8nbds\naffinity-nodeport-8nbds\naffinity-nodeport-8nbds\naffinity-nodeport-8nbds\naffinity-nodeport-8nbds\naffinity-nodeport-8nbds\naffinity-nodeport-8nbds\naffinity-nodeport-8nbds\naffinity-nodeport-8nbds"
    Jul 29 15:53:34.812: INFO: Received response from host: affinity-nodeport-8nbds
    Jul 29 15:53:34.812: INFO: Received response from host: affinity-nodeport-8nbds
    Jul 29 15:53:34.812: INFO: Received response from host: affinity-nodeport-8nbds
    Jul 29 15:53:34.812: INFO: Received response from host: affinity-nodeport-8nbds
    Jul 29 15:53:34.812: INFO: Received response from host: affinity-nodeport-8nbds
    Jul 29 15:53:34.812: INFO: Received response from host: affinity-nodeport-8nbds
    Jul 29 15:53:34.812: INFO: Received response from host: affinity-nodeport-8nbds
    Jul 29 15:53:34.812: INFO: Received response from host: affinity-nodeport-8nbds
    Jul 29 15:53:34.812: INFO: Received response from host: affinity-nodeport-8nbds
    Jul 29 15:53:34.812: INFO: Received response from host: affinity-nodeport-8nbds
    Jul 29 15:53:34.812: INFO: Received response from host: affinity-nodeport-8nbds
    Jul 29 15:53:34.812: INFO: Received response from host: affinity-nodeport-8nbds
    Jul 29 15:53:34.812: INFO: Received response from host: affinity-nodeport-8nbds
    Jul 29 15:53:34.812: INFO: Received response from host: affinity-nodeport-8nbds
    Jul 29 15:53:34.812: INFO: Received response from host: affinity-nodeport-8nbds
    Jul 29 15:53:34.812: INFO: Received response from host: affinity-nodeport-8nbds
    Jul 29 15:53:34.812: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport in namespace services-1892, will wait for the garbage collector to delete the pods 07/29/23 15:53:34.838
    Jul 29 15:53:34.906: INFO: Deleting ReplicationController affinity-nodeport took: 10.485272ms
    Jul 29 15:53:35.007: INFO: Terminating ReplicationController affinity-nodeport pods took: 101.401073ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jul 29 15:53:36.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-1892" for this suite. 07/29/23 15:53:36.864
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-instrumentation] Events
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:53:36.877
Jul 29 15:53:36.877: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename events 07/29/23 15:53:36.881
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:53:36.913
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:53:36.917
[It] should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
STEP: creating a test event 07/29/23 15:53:36.921
STEP: listing all events in all namespaces 07/29/23 15:53:36.93
STEP: patching the test event 07/29/23 15:53:36.947
STEP: fetching the test event 07/29/23 15:53:36.965
STEP: updating the test event 07/29/23 15:53:36.971
STEP: getting the test event 07/29/23 15:53:36.99
STEP: deleting the test event 07/29/23 15:53:37.014
STEP: listing all events in all namespaces 07/29/23 15:53:37.037
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:187
Jul 29 15:53:37.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-7219" for this suite. 07/29/23 15:53:37.072
{"msg":"PASSED [sig-instrumentation] Events should manage the lifecycle of an event [Conformance]","completed":77,"skipped":1305,"failed":0}
------------------------------
â€¢ [0.207 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:53:36.877
    Jul 29 15:53:36.877: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename events 07/29/23 15:53:36.881
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:53:36.913
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:53:36.917
    [It] should manage the lifecycle of an event [Conformance]
      test/e2e/instrumentation/core_events.go:57
    STEP: creating a test event 07/29/23 15:53:36.921
    STEP: listing all events in all namespaces 07/29/23 15:53:36.93
    STEP: patching the test event 07/29/23 15:53:36.947
    STEP: fetching the test event 07/29/23 15:53:36.965
    STEP: updating the test event 07/29/23 15:53:36.971
    STEP: getting the test event 07/29/23 15:53:36.99
    STEP: deleting the test event 07/29/23 15:53:37.014
    STEP: listing all events in all namespaces 07/29/23 15:53:37.037
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:187
    Jul 29 15:53:37.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-7219" for this suite. 07/29/23 15:53:37.072
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:53:37.087
Jul 29 15:53:37.087: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename projected 07/29/23 15:53:37.09
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:53:37.18
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:53:37.184
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206
STEP: Creating a pod to test downward API volume plugin 07/29/23 15:53:37.188
Jul 29 15:53:37.210: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fbcf5d70-9276-4b9d-9564-8f414147dfff" in namespace "projected-8746" to be "Succeeded or Failed"
Jul 29 15:53:37.218: INFO: Pod "downwardapi-volume-fbcf5d70-9276-4b9d-9564-8f414147dfff": Phase="Pending", Reason="", readiness=false. Elapsed: 7.68843ms
Jul 29 15:53:39.225: INFO: Pod "downwardapi-volume-fbcf5d70-9276-4b9d-9564-8f414147dfff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015094772s
Jul 29 15:53:41.229: INFO: Pod "downwardapi-volume-fbcf5d70-9276-4b9d-9564-8f414147dfff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018742189s
STEP: Saw pod success 07/29/23 15:53:41.229
Jul 29 15:53:41.230: INFO: Pod "downwardapi-volume-fbcf5d70-9276-4b9d-9564-8f414147dfff" satisfied condition "Succeeded or Failed"
Jul 29 15:53:41.237: INFO: Trying to get logs from node wa4quivohpee-3 pod downwardapi-volume-fbcf5d70-9276-4b9d-9564-8f414147dfff container client-container: <nil>
STEP: delete the pod 07/29/23 15:53:41.25
Jul 29 15:53:41.279: INFO: Waiting for pod downwardapi-volume-fbcf5d70-9276-4b9d-9564-8f414147dfff to disappear
Jul 29 15:53:41.284: INFO: Pod downwardapi-volume-fbcf5d70-9276-4b9d-9564-8f414147dfff no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jul 29 15:53:41.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8746" for this suite. 07/29/23 15:53:41.296
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","completed":78,"skipped":1306,"failed":0}
------------------------------
â€¢ [4.224 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:53:37.087
    Jul 29 15:53:37.087: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename projected 07/29/23 15:53:37.09
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:53:37.18
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:53:37.184
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:206
    STEP: Creating a pod to test downward API volume plugin 07/29/23 15:53:37.188
    Jul 29 15:53:37.210: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fbcf5d70-9276-4b9d-9564-8f414147dfff" in namespace "projected-8746" to be "Succeeded or Failed"
    Jul 29 15:53:37.218: INFO: Pod "downwardapi-volume-fbcf5d70-9276-4b9d-9564-8f414147dfff": Phase="Pending", Reason="", readiness=false. Elapsed: 7.68843ms
    Jul 29 15:53:39.225: INFO: Pod "downwardapi-volume-fbcf5d70-9276-4b9d-9564-8f414147dfff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015094772s
    Jul 29 15:53:41.229: INFO: Pod "downwardapi-volume-fbcf5d70-9276-4b9d-9564-8f414147dfff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018742189s
    STEP: Saw pod success 07/29/23 15:53:41.229
    Jul 29 15:53:41.230: INFO: Pod "downwardapi-volume-fbcf5d70-9276-4b9d-9564-8f414147dfff" satisfied condition "Succeeded or Failed"
    Jul 29 15:53:41.237: INFO: Trying to get logs from node wa4quivohpee-3 pod downwardapi-volume-fbcf5d70-9276-4b9d-9564-8f414147dfff container client-container: <nil>
    STEP: delete the pod 07/29/23 15:53:41.25
    Jul 29 15:53:41.279: INFO: Waiting for pod downwardapi-volume-fbcf5d70-9276-4b9d-9564-8f414147dfff to disappear
    Jul 29 15:53:41.284: INFO: Pod downwardapi-volume-fbcf5d70-9276-4b9d-9564-8f414147dfff no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jul 29 15:53:41.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-8746" for this suite. 07/29/23 15:53:41.296
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Pods
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:53:41.316
Jul 29 15:53:41.317: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename pods 07/29/23 15:53:41.319
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:53:41.365
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:53:41.371
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535
Jul 29 15:53:41.385: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: creating the pod 07/29/23 15:53:41.387
STEP: submitting the pod to kubernetes 07/29/23 15:53:41.388
Jul 29 15:53:41.409: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-30af4dbe-d77c-4574-ba66-d1991fa756cd" in namespace "pods-1810" to be "running and ready"
Jul 29 15:53:41.420: INFO: Pod "pod-exec-websocket-30af4dbe-d77c-4574-ba66-d1991fa756cd": Phase="Pending", Reason="", readiness=false. Elapsed: 10.292783ms
Jul 29 15:53:41.420: INFO: The phase of Pod pod-exec-websocket-30af4dbe-d77c-4574-ba66-d1991fa756cd is Pending, waiting for it to be Running (with Ready = true)
Jul 29 15:53:43.430: INFO: Pod "pod-exec-websocket-30af4dbe-d77c-4574-ba66-d1991fa756cd": Phase="Running", Reason="", readiness=true. Elapsed: 2.020994071s
Jul 29 15:53:43.430: INFO: The phase of Pod pod-exec-websocket-30af4dbe-d77c-4574-ba66-d1991fa756cd is Running (Ready = true)
Jul 29 15:53:43.430: INFO: Pod "pod-exec-websocket-30af4dbe-d77c-4574-ba66-d1991fa756cd" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jul 29 15:53:43.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1810" for this suite. 07/29/23 15:53:43.531
{"msg":"PASSED [sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","completed":79,"skipped":1308,"failed":0}
------------------------------
â€¢ [2.229 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:53:41.316
    Jul 29 15:53:41.317: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename pods 07/29/23 15:53:41.319
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:53:41.365
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:53:41.371
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should support remote command execution over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:535
    Jul 29 15:53:41.385: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: creating the pod 07/29/23 15:53:41.387
    STEP: submitting the pod to kubernetes 07/29/23 15:53:41.388
    Jul 29 15:53:41.409: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-30af4dbe-d77c-4574-ba66-d1991fa756cd" in namespace "pods-1810" to be "running and ready"
    Jul 29 15:53:41.420: INFO: Pod "pod-exec-websocket-30af4dbe-d77c-4574-ba66-d1991fa756cd": Phase="Pending", Reason="", readiness=false. Elapsed: 10.292783ms
    Jul 29 15:53:41.420: INFO: The phase of Pod pod-exec-websocket-30af4dbe-d77c-4574-ba66-d1991fa756cd is Pending, waiting for it to be Running (with Ready = true)
    Jul 29 15:53:43.430: INFO: Pod "pod-exec-websocket-30af4dbe-d77c-4574-ba66-d1991fa756cd": Phase="Running", Reason="", readiness=true. Elapsed: 2.020994071s
    Jul 29 15:53:43.430: INFO: The phase of Pod pod-exec-websocket-30af4dbe-d77c-4574-ba66-d1991fa756cd is Running (Ready = true)
    Jul 29 15:53:43.430: INFO: Pod "pod-exec-websocket-30af4dbe-d77c-4574-ba66-d1991fa756cd" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jul 29 15:53:43.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-1810" for this suite. 07/29/23 15:53:43.531
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:53:43.55
Jul 29 15:53:43.550: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename configmap 07/29/23 15:53:43.552
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:53:43.589
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:53:43.595
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92
STEP: Creating configMap configmap-1139/configmap-test-d642a50b-b1bb-4c04-9cb2-c4966529ec4f 07/29/23 15:53:43.6
STEP: Creating a pod to test consume configMaps 07/29/23 15:53:43.609
Jul 29 15:53:43.624: INFO: Waiting up to 5m0s for pod "pod-configmaps-b67ed516-f917-4971-a5e7-4f7e848fec0a" in namespace "configmap-1139" to be "Succeeded or Failed"
Jul 29 15:53:43.630: INFO: Pod "pod-configmaps-b67ed516-f917-4971-a5e7-4f7e848fec0a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.015947ms
Jul 29 15:53:45.639: INFO: Pod "pod-configmaps-b67ed516-f917-4971-a5e7-4f7e848fec0a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014152852s
Jul 29 15:53:47.639: INFO: Pod "pod-configmaps-b67ed516-f917-4971-a5e7-4f7e848fec0a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014647845s
STEP: Saw pod success 07/29/23 15:53:47.639
Jul 29 15:53:47.640: INFO: Pod "pod-configmaps-b67ed516-f917-4971-a5e7-4f7e848fec0a" satisfied condition "Succeeded or Failed"
Jul 29 15:53:47.680: INFO: Trying to get logs from node wa4quivohpee-3 pod pod-configmaps-b67ed516-f917-4971-a5e7-4f7e848fec0a container env-test: <nil>
STEP: delete the pod 07/29/23 15:53:47.693
Jul 29 15:53:47.714: INFO: Waiting for pod pod-configmaps-b67ed516-f917-4971-a5e7-4f7e848fec0a to disappear
Jul 29 15:53:47.719: INFO: Pod pod-configmaps-b67ed516-f917-4971-a5e7-4f7e848fec0a no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Jul 29 15:53:47.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1139" for this suite. 07/29/23 15:53:47.725
{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","completed":80,"skipped":1320,"failed":0}
------------------------------
â€¢ [4.193 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:53:43.55
    Jul 29 15:53:43.550: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename configmap 07/29/23 15:53:43.552
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:53:43.589
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:53:43.595
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:92
    STEP: Creating configMap configmap-1139/configmap-test-d642a50b-b1bb-4c04-9cb2-c4966529ec4f 07/29/23 15:53:43.6
    STEP: Creating a pod to test consume configMaps 07/29/23 15:53:43.609
    Jul 29 15:53:43.624: INFO: Waiting up to 5m0s for pod "pod-configmaps-b67ed516-f917-4971-a5e7-4f7e848fec0a" in namespace "configmap-1139" to be "Succeeded or Failed"
    Jul 29 15:53:43.630: INFO: Pod "pod-configmaps-b67ed516-f917-4971-a5e7-4f7e848fec0a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.015947ms
    Jul 29 15:53:45.639: INFO: Pod "pod-configmaps-b67ed516-f917-4971-a5e7-4f7e848fec0a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014152852s
    Jul 29 15:53:47.639: INFO: Pod "pod-configmaps-b67ed516-f917-4971-a5e7-4f7e848fec0a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014647845s
    STEP: Saw pod success 07/29/23 15:53:47.639
    Jul 29 15:53:47.640: INFO: Pod "pod-configmaps-b67ed516-f917-4971-a5e7-4f7e848fec0a" satisfied condition "Succeeded or Failed"
    Jul 29 15:53:47.680: INFO: Trying to get logs from node wa4quivohpee-3 pod pod-configmaps-b67ed516-f917-4971-a5e7-4f7e848fec0a container env-test: <nil>
    STEP: delete the pod 07/29/23 15:53:47.693
    Jul 29 15:53:47.714: INFO: Waiting for pod pod-configmaps-b67ed516-f917-4971-a5e7-4f7e848fec0a to disappear
    Jul 29 15:53:47.719: INFO: Pod pod-configmaps-b67ed516-f917-4971-a5e7-4f7e848fec0a no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Jul 29 15:53:47.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-1139" for this suite. 07/29/23 15:53:47.725
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:53:47.749
Jul 29 15:53:47.749: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename namespaces 07/29/23 15:53:47.751
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:53:47.788
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:53:47.792
[It] should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298
STEP: Read namespace status 07/29/23 15:53:47.796
Jul 29 15:53:47.803: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
STEP: Patch namespace status 07/29/23 15:53:47.803
Jul 29 15:53:47.812: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
STEP: Update namespace status 07/29/23 15:53:47.812
Jul 29 15:53:47.828: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Jul 29 15:53:47.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-8256" for this suite. 07/29/23 15:53:47.84
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should apply changes to a namespace status [Conformance]","completed":81,"skipped":1340,"failed":0}
------------------------------
â€¢ [0.106 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:53:47.749
    Jul 29 15:53:47.749: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename namespaces 07/29/23 15:53:47.751
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:53:47.788
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:53:47.792
    [It] should apply changes to a namespace status [Conformance]
      test/e2e/apimachinery/namespace.go:298
    STEP: Read namespace status 07/29/23 15:53:47.796
    Jul 29 15:53:47.803: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
    STEP: Patch namespace status 07/29/23 15:53:47.803
    Jul 29 15:53:47.812: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
    STEP: Update namespace status 07/29/23 15:53:47.812
    Jul 29 15:53:47.828: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Jul 29 15:53:47.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-8256" for this suite. 07/29/23 15:53:47.84
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:53:47.857
Jul 29 15:53:47.857: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename projected 07/29/23 15:53:47.867
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:53:47.918
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:53:47.922
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108
STEP: Creating configMap with name projected-configmap-test-volume-map-0afe29d2-0fa4-4fcf-bcea-ef5133bd2db0 07/29/23 15:53:47.927
STEP: Creating a pod to test consume configMaps 07/29/23 15:53:47.941
Jul 29 15:53:47.964: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3ad969c4-7321-445a-a18d-e224c60445a3" in namespace "projected-6959" to be "Succeeded or Failed"
Jul 29 15:53:47.977: INFO: Pod "pod-projected-configmaps-3ad969c4-7321-445a-a18d-e224c60445a3": Phase="Pending", Reason="", readiness=false. Elapsed: 12.810818ms
Jul 29 15:53:49.986: INFO: Pod "pod-projected-configmaps-3ad969c4-7321-445a-a18d-e224c60445a3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022446652s
Jul 29 15:53:51.986: INFO: Pod "pod-projected-configmaps-3ad969c4-7321-445a-a18d-e224c60445a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021944786s
STEP: Saw pod success 07/29/23 15:53:51.986
Jul 29 15:53:51.986: INFO: Pod "pod-projected-configmaps-3ad969c4-7321-445a-a18d-e224c60445a3" satisfied condition "Succeeded or Failed"
Jul 29 15:53:51.992: INFO: Trying to get logs from node wa4quivohpee-3 pod pod-projected-configmaps-3ad969c4-7321-445a-a18d-e224c60445a3 container agnhost-container: <nil>
STEP: delete the pod 07/29/23 15:53:52.005
Jul 29 15:53:52.026: INFO: Waiting for pod pod-projected-configmaps-3ad969c4-7321-445a-a18d-e224c60445a3 to disappear
Jul 29 15:53:52.031: INFO: Pod pod-projected-configmaps-3ad969c4-7321-445a-a18d-e224c60445a3 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jul 29 15:53:52.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6959" for this suite. 07/29/23 15:53:52.04
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","completed":82,"skipped":1369,"failed":0}
------------------------------
â€¢ [4.194 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:53:47.857
    Jul 29 15:53:47.857: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename projected 07/29/23 15:53:47.867
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:53:47.918
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:53:47.922
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:108
    STEP: Creating configMap with name projected-configmap-test-volume-map-0afe29d2-0fa4-4fcf-bcea-ef5133bd2db0 07/29/23 15:53:47.927
    STEP: Creating a pod to test consume configMaps 07/29/23 15:53:47.941
    Jul 29 15:53:47.964: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3ad969c4-7321-445a-a18d-e224c60445a3" in namespace "projected-6959" to be "Succeeded or Failed"
    Jul 29 15:53:47.977: INFO: Pod "pod-projected-configmaps-3ad969c4-7321-445a-a18d-e224c60445a3": Phase="Pending", Reason="", readiness=false. Elapsed: 12.810818ms
    Jul 29 15:53:49.986: INFO: Pod "pod-projected-configmaps-3ad969c4-7321-445a-a18d-e224c60445a3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022446652s
    Jul 29 15:53:51.986: INFO: Pod "pod-projected-configmaps-3ad969c4-7321-445a-a18d-e224c60445a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021944786s
    STEP: Saw pod success 07/29/23 15:53:51.986
    Jul 29 15:53:51.986: INFO: Pod "pod-projected-configmaps-3ad969c4-7321-445a-a18d-e224c60445a3" satisfied condition "Succeeded or Failed"
    Jul 29 15:53:51.992: INFO: Trying to get logs from node wa4quivohpee-3 pod pod-projected-configmaps-3ad969c4-7321-445a-a18d-e224c60445a3 container agnhost-container: <nil>
    STEP: delete the pod 07/29/23 15:53:52.005
    Jul 29 15:53:52.026: INFO: Waiting for pod pod-projected-configmaps-3ad969c4-7321-445a-a18d-e224c60445a3 to disappear
    Jul 29 15:53:52.031: INFO: Pod pod-projected-configmaps-3ad969c4-7321-445a-a18d-e224c60445a3 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jul 29 15:53:52.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6959" for this suite. 07/29/23 15:53:52.04
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:53:52.055
Jul 29 15:53:52.056: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename downward-api 07/29/23 15:53:52.059
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:53:52.092
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:53:52.096
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52
STEP: Creating a pod to test downward API volume plugin 07/29/23 15:53:52.101
Jul 29 15:53:52.122: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4ef06dc5-9318-40e1-876b-e09ca4d61925" in namespace "downward-api-4151" to be "Succeeded or Failed"
Jul 29 15:53:52.133: INFO: Pod "downwardapi-volume-4ef06dc5-9318-40e1-876b-e09ca4d61925": Phase="Pending", Reason="", readiness=false. Elapsed: 10.805569ms
Jul 29 15:53:54.143: INFO: Pod "downwardapi-volume-4ef06dc5-9318-40e1-876b-e09ca4d61925": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020819508s
Jul 29 15:53:56.144: INFO: Pod "downwardapi-volume-4ef06dc5-9318-40e1-876b-e09ca4d61925": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021345699s
STEP: Saw pod success 07/29/23 15:53:56.144
Jul 29 15:53:56.144: INFO: Pod "downwardapi-volume-4ef06dc5-9318-40e1-876b-e09ca4d61925" satisfied condition "Succeeded or Failed"
Jul 29 15:53:56.151: INFO: Trying to get logs from node wa4quivohpee-3 pod downwardapi-volume-4ef06dc5-9318-40e1-876b-e09ca4d61925 container client-container: <nil>
STEP: delete the pod 07/29/23 15:53:56.166
Jul 29 15:53:56.207: INFO: Waiting for pod downwardapi-volume-4ef06dc5-9318-40e1-876b-e09ca4d61925 to disappear
Jul 29 15:53:56.215: INFO: Pod downwardapi-volume-4ef06dc5-9318-40e1-876b-e09ca4d61925 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jul 29 15:53:56.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4151" for this suite. 07/29/23 15:53:56.229
{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","completed":83,"skipped":1394,"failed":0}
------------------------------
â€¢ [4.188 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:53:52.055
    Jul 29 15:53:52.056: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename downward-api 07/29/23 15:53:52.059
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:53:52.092
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:53:52.096
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:52
    STEP: Creating a pod to test downward API volume plugin 07/29/23 15:53:52.101
    Jul 29 15:53:52.122: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4ef06dc5-9318-40e1-876b-e09ca4d61925" in namespace "downward-api-4151" to be "Succeeded or Failed"
    Jul 29 15:53:52.133: INFO: Pod "downwardapi-volume-4ef06dc5-9318-40e1-876b-e09ca4d61925": Phase="Pending", Reason="", readiness=false. Elapsed: 10.805569ms
    Jul 29 15:53:54.143: INFO: Pod "downwardapi-volume-4ef06dc5-9318-40e1-876b-e09ca4d61925": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020819508s
    Jul 29 15:53:56.144: INFO: Pod "downwardapi-volume-4ef06dc5-9318-40e1-876b-e09ca4d61925": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021345699s
    STEP: Saw pod success 07/29/23 15:53:56.144
    Jul 29 15:53:56.144: INFO: Pod "downwardapi-volume-4ef06dc5-9318-40e1-876b-e09ca4d61925" satisfied condition "Succeeded or Failed"
    Jul 29 15:53:56.151: INFO: Trying to get logs from node wa4quivohpee-3 pod downwardapi-volume-4ef06dc5-9318-40e1-876b-e09ca4d61925 container client-container: <nil>
    STEP: delete the pod 07/29/23 15:53:56.166
    Jul 29 15:53:56.207: INFO: Waiting for pod downwardapi-volume-4ef06dc5-9318-40e1-876b-e09ca4d61925 to disappear
    Jul 29 15:53:56.215: INFO: Pod downwardapi-volume-4ef06dc5-9318-40e1-876b-e09ca4d61925 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jul 29 15:53:56.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-4151" for this suite. 07/29/23 15:53:56.229
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Containers
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:53:56.255
Jul 29 15:53:56.255: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename containers 07/29/23 15:53:56.258
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:53:56.295
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:53:56.31
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38
Jul 29 15:53:56.351: INFO: Waiting up to 5m0s for pod "client-containers-0773d8b2-cc37-4cb2-a0fb-e8c9f8f2b476" in namespace "containers-6449" to be "running"
Jul 29 15:53:56.357: INFO: Pod "client-containers-0773d8b2-cc37-4cb2-a0fb-e8c9f8f2b476": Phase="Pending", Reason="", readiness=false. Elapsed: 6.552112ms
Jul 29 15:53:58.367: INFO: Pod "client-containers-0773d8b2-cc37-4cb2-a0fb-e8c9f8f2b476": Phase="Running", Reason="", readiness=true. Elapsed: 2.015781055s
Jul 29 15:53:58.367: INFO: Pod "client-containers-0773d8b2-cc37-4cb2-a0fb-e8c9f8f2b476" satisfied condition "running"
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Jul 29 15:53:58.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-6449" for this suite. 07/29/23 15:53:58.389
{"msg":"PASSED [sig-node] Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","completed":84,"skipped":1402,"failed":0}
------------------------------
â€¢ [2.147 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:53:56.255
    Jul 29 15:53:56.255: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename containers 07/29/23 15:53:56.258
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:53:56.295
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:53:56.31
    [It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:38
    Jul 29 15:53:56.351: INFO: Waiting up to 5m0s for pod "client-containers-0773d8b2-cc37-4cb2-a0fb-e8c9f8f2b476" in namespace "containers-6449" to be "running"
    Jul 29 15:53:56.357: INFO: Pod "client-containers-0773d8b2-cc37-4cb2-a0fb-e8c9f8f2b476": Phase="Pending", Reason="", readiness=false. Elapsed: 6.552112ms
    Jul 29 15:53:58.367: INFO: Pod "client-containers-0773d8b2-cc37-4cb2-a0fb-e8c9f8f2b476": Phase="Running", Reason="", readiness=true. Elapsed: 2.015781055s
    Jul 29 15:53:58.367: INFO: Pod "client-containers-0773d8b2-cc37-4cb2-a0fb-e8c9f8f2b476" satisfied condition "running"
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Jul 29 15:53:58.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-6449" for this suite. 07/29/23 15:53:58.389
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:846
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:53:58.407
Jul 29 15:53:58.407: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename statefulset 07/29/23 15:53:58.41
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:53:58.449
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:53:58.456
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-7494 07/29/23 15:53:58.46
[It] should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:846
STEP: Creating statefulset ss in namespace statefulset-7494 07/29/23 15:53:58.47
Jul 29 15:53:58.494: INFO: Found 0 stateful pods, waiting for 1
Jul 29 15:54:08.519: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource 07/29/23 15:54:08.537
STEP: updating a scale subresource 07/29/23 15:54:08.543
STEP: verifying the statefulset Spec.Replicas was modified 07/29/23 15:54:08.577
STEP: Patch a scale subresource 07/29/23 15:54:08.586
STEP: verifying the statefulset Spec.Replicas was modified 07/29/23 15:54:08.598
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jul 29 15:54:08.607: INFO: Deleting all statefulset in ns statefulset-7494
Jul 29 15:54:08.629: INFO: Scaling statefulset ss to 0
Jul 29 15:54:18.688: INFO: Waiting for statefulset status.replicas updated to 0
Jul 29 15:54:18.694: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jul 29 15:54:18.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7494" for this suite. 07/29/23 15:54:18.726
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","completed":85,"skipped":1404,"failed":0}
------------------------------
â€¢ [SLOW TEST] [20.333 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should have a working scale subresource [Conformance]
    test/e2e/apps/statefulset.go:846

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:53:58.407
    Jul 29 15:53:58.407: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename statefulset 07/29/23 15:53:58.41
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:53:58.449
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:53:58.456
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-7494 07/29/23 15:53:58.46
    [It] should have a working scale subresource [Conformance]
      test/e2e/apps/statefulset.go:846
    STEP: Creating statefulset ss in namespace statefulset-7494 07/29/23 15:53:58.47
    Jul 29 15:53:58.494: INFO: Found 0 stateful pods, waiting for 1
    Jul 29 15:54:08.519: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: getting scale subresource 07/29/23 15:54:08.537
    STEP: updating a scale subresource 07/29/23 15:54:08.543
    STEP: verifying the statefulset Spec.Replicas was modified 07/29/23 15:54:08.577
    STEP: Patch a scale subresource 07/29/23 15:54:08.586
    STEP: verifying the statefulset Spec.Replicas was modified 07/29/23 15:54:08.598
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jul 29 15:54:08.607: INFO: Deleting all statefulset in ns statefulset-7494
    Jul 29 15:54:08.629: INFO: Scaling statefulset ss to 0
    Jul 29 15:54:18.688: INFO: Waiting for statefulset status.replicas updated to 0
    Jul 29 15:54:18.694: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jul 29 15:54:18.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-7494" for this suite. 07/29/23 15:54:18.726
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:54:18.741
Jul 29 15:54:18.741: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename downward-api 07/29/23 15:54:18.746
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:54:18.78
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:54:18.784
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248
STEP: Creating a pod to test downward API volume plugin 07/29/23 15:54:18.788
Jul 29 15:54:18.802: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d13a4781-36db-4c94-940a-61d9e59bad28" in namespace "downward-api-1371" to be "Succeeded or Failed"
Jul 29 15:54:18.808: INFO: Pod "downwardapi-volume-d13a4781-36db-4c94-940a-61d9e59bad28": Phase="Pending", Reason="", readiness=false. Elapsed: 6.376998ms
Jul 29 15:54:20.816: INFO: Pod "downwardapi-volume-d13a4781-36db-4c94-940a-61d9e59bad28": Phase="Running", Reason="", readiness=true. Elapsed: 2.013866926s
Jul 29 15:54:22.815: INFO: Pod "downwardapi-volume-d13a4781-36db-4c94-940a-61d9e59bad28": Phase="Running", Reason="", readiness=false. Elapsed: 4.013730278s
Jul 29 15:54:24.816: INFO: Pod "downwardapi-volume-d13a4781-36db-4c94-940a-61d9e59bad28": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.014302668s
STEP: Saw pod success 07/29/23 15:54:24.816
Jul 29 15:54:24.817: INFO: Pod "downwardapi-volume-d13a4781-36db-4c94-940a-61d9e59bad28" satisfied condition "Succeeded or Failed"
Jul 29 15:54:24.824: INFO: Trying to get logs from node wa4quivohpee-3 pod downwardapi-volume-d13a4781-36db-4c94-940a-61d9e59bad28 container client-container: <nil>
STEP: delete the pod 07/29/23 15:54:24.839
Jul 29 15:54:24.864: INFO: Waiting for pod downwardapi-volume-d13a4781-36db-4c94-940a-61d9e59bad28 to disappear
Jul 29 15:54:24.870: INFO: Pod downwardapi-volume-d13a4781-36db-4c94-940a-61d9e59bad28 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jul 29 15:54:24.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1371" for this suite. 07/29/23 15:54:24.884
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","completed":86,"skipped":1413,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.154 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:54:18.741
    Jul 29 15:54:18.741: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename downward-api 07/29/23 15:54:18.746
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:54:18.78
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:54:18.784
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:248
    STEP: Creating a pod to test downward API volume plugin 07/29/23 15:54:18.788
    Jul 29 15:54:18.802: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d13a4781-36db-4c94-940a-61d9e59bad28" in namespace "downward-api-1371" to be "Succeeded or Failed"
    Jul 29 15:54:18.808: INFO: Pod "downwardapi-volume-d13a4781-36db-4c94-940a-61d9e59bad28": Phase="Pending", Reason="", readiness=false. Elapsed: 6.376998ms
    Jul 29 15:54:20.816: INFO: Pod "downwardapi-volume-d13a4781-36db-4c94-940a-61d9e59bad28": Phase="Running", Reason="", readiness=true. Elapsed: 2.013866926s
    Jul 29 15:54:22.815: INFO: Pod "downwardapi-volume-d13a4781-36db-4c94-940a-61d9e59bad28": Phase="Running", Reason="", readiness=false. Elapsed: 4.013730278s
    Jul 29 15:54:24.816: INFO: Pod "downwardapi-volume-d13a4781-36db-4c94-940a-61d9e59bad28": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.014302668s
    STEP: Saw pod success 07/29/23 15:54:24.816
    Jul 29 15:54:24.817: INFO: Pod "downwardapi-volume-d13a4781-36db-4c94-940a-61d9e59bad28" satisfied condition "Succeeded or Failed"
    Jul 29 15:54:24.824: INFO: Trying to get logs from node wa4quivohpee-3 pod downwardapi-volume-d13a4781-36db-4c94-940a-61d9e59bad28 container client-container: <nil>
    STEP: delete the pod 07/29/23 15:54:24.839
    Jul 29 15:54:24.864: INFO: Waiting for pod downwardapi-volume-d13a4781-36db-4c94-940a-61d9e59bad28 to disappear
    Jul 29 15:54:24.870: INFO: Pod downwardapi-volume-d13a4781-36db-4c94-940a-61d9e59bad28 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jul 29 15:54:24.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-1371" for this suite. 07/29/23 15:54:24.884
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:54:24.9
Jul 29 15:54:24.900: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename dns 07/29/23 15:54:24.903
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:54:24.936
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:54:24.941
[It] should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
STEP: Creating a test headless service 07/29/23 15:54:24.946
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8148.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8148.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8148.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8148.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8148.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-8148.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8148.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-8148.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8148.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-8148.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8148.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-8148.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 104.20.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.20.104_udp@PTR;check="$$(dig +tcp +noall +answer +search 104.20.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.20.104_tcp@PTR;sleep 1; done
 07/29/23 15:54:24.971
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8148.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8148.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8148.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8148.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8148.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-8148.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8148.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-8148.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8148.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-8148.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8148.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-8148.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 104.20.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.20.104_udp@PTR;check="$$(dig +tcp +noall +answer +search 104.20.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.20.104_tcp@PTR;sleep 1; done
 07/29/23 15:54:24.972
STEP: creating a pod to probe DNS 07/29/23 15:54:24.972
STEP: submitting the pod to kubernetes 07/29/23 15:54:24.973
Jul 29 15:54:25.003: INFO: Waiting up to 15m0s for pod "dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff" in namespace "dns-8148" to be "running"
Jul 29 15:54:25.018: INFO: Pod "dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff": Phase="Pending", Reason="", readiness=false. Elapsed: 15.142044ms
Jul 29 15:54:27.028: INFO: Pod "dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff": Phase="Running", Reason="", readiness=true. Elapsed: 2.025047741s
Jul 29 15:54:27.029: INFO: Pod "dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff" satisfied condition "running"
STEP: retrieving the pod 07/29/23 15:54:27.029
STEP: looking for the results for each expected name from probers 07/29/23 15:54:27.04
Jul 29 15:54:27.059: INFO: Unable to read wheezy_udp@dns-test-service.dns-8148.svc.cluster.local from pod dns-8148/dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff: the server could not find the requested resource (get pods dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff)
Jul 29 15:54:27.074: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8148.svc.cluster.local from pod dns-8148/dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff: the server could not find the requested resource (get pods dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff)
Jul 29 15:54:27.082: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8148.svc.cluster.local from pod dns-8148/dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff: the server could not find the requested resource (get pods dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff)
Jul 29 15:54:27.087: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8148.svc.cluster.local from pod dns-8148/dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff: the server could not find the requested resource (get pods dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff)
Jul 29 15:54:27.119: INFO: Unable to read jessie_udp@dns-test-service.dns-8148.svc.cluster.local from pod dns-8148/dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff: the server could not find the requested resource (get pods dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff)
Jul 29 15:54:27.125: INFO: Unable to read jessie_tcp@dns-test-service.dns-8148.svc.cluster.local from pod dns-8148/dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff: the server could not find the requested resource (get pods dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff)
Jul 29 15:54:27.135: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8148.svc.cluster.local from pod dns-8148/dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff: the server could not find the requested resource (get pods dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff)
Jul 29 15:54:27.142: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8148.svc.cluster.local from pod dns-8148/dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff: the server could not find the requested resource (get pods dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff)
Jul 29 15:54:27.165: INFO: Lookups using dns-8148/dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff failed for: [wheezy_udp@dns-test-service.dns-8148.svc.cluster.local wheezy_tcp@dns-test-service.dns-8148.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8148.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8148.svc.cluster.local jessie_udp@dns-test-service.dns-8148.svc.cluster.local jessie_tcp@dns-test-service.dns-8148.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8148.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8148.svc.cluster.local]

Jul 29 15:54:32.180: INFO: Unable to read wheezy_udp@dns-test-service.dns-8148.svc.cluster.local from pod dns-8148/dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff: the server could not find the requested resource (get pods dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff)
Jul 29 15:54:32.187: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8148.svc.cluster.local from pod dns-8148/dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff: the server could not find the requested resource (get pods dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff)
Jul 29 15:54:32.194: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8148.svc.cluster.local from pod dns-8148/dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff: the server could not find the requested resource (get pods dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff)
Jul 29 15:54:32.200: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8148.svc.cluster.local from pod dns-8148/dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff: the server could not find the requested resource (get pods dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff)
Jul 29 15:54:32.232: INFO: Unable to read jessie_udp@dns-test-service.dns-8148.svc.cluster.local from pod dns-8148/dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff: the server could not find the requested resource (get pods dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff)
Jul 29 15:54:32.239: INFO: Unable to read jessie_tcp@dns-test-service.dns-8148.svc.cluster.local from pod dns-8148/dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff: the server could not find the requested resource (get pods dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff)
Jul 29 15:54:32.247: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8148.svc.cluster.local from pod dns-8148/dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff: the server could not find the requested resource (get pods dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff)
Jul 29 15:54:32.253: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8148.svc.cluster.local from pod dns-8148/dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff: the server could not find the requested resource (get pods dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff)
Jul 29 15:54:32.280: INFO: Lookups using dns-8148/dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff failed for: [wheezy_udp@dns-test-service.dns-8148.svc.cluster.local wheezy_tcp@dns-test-service.dns-8148.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8148.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8148.svc.cluster.local jessie_udp@dns-test-service.dns-8148.svc.cluster.local jessie_tcp@dns-test-service.dns-8148.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8148.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8148.svc.cluster.local]

Jul 29 15:54:37.175: INFO: Unable to read wheezy_udp@dns-test-service.dns-8148.svc.cluster.local from pod dns-8148/dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff: the server could not find the requested resource (get pods dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff)
Jul 29 15:54:37.182: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8148.svc.cluster.local from pod dns-8148/dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff: the server could not find the requested resource (get pods dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff)
Jul 29 15:54:37.188: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8148.svc.cluster.local from pod dns-8148/dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff: the server could not find the requested resource (get pods dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff)
Jul 29 15:54:37.194: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8148.svc.cluster.local from pod dns-8148/dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff: the server could not find the requested resource (get pods dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff)
Jul 29 15:54:37.229: INFO: Unable to read jessie_udp@dns-test-service.dns-8148.svc.cluster.local from pod dns-8148/dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff: the server could not find the requested resource (get pods dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff)
Jul 29 15:54:37.236: INFO: Unable to read jessie_tcp@dns-test-service.dns-8148.svc.cluster.local from pod dns-8148/dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff: the server could not find the requested resource (get pods dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff)
Jul 29 15:54:37.245: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8148.svc.cluster.local from pod dns-8148/dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff: the server could not find the requested resource (get pods dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff)
Jul 29 15:54:37.252: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8148.svc.cluster.local from pod dns-8148/dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff: the server could not find the requested resource (get pods dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff)
Jul 29 15:54:37.302: INFO: Lookups using dns-8148/dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff failed for: [wheezy_udp@dns-test-service.dns-8148.svc.cluster.local wheezy_tcp@dns-test-service.dns-8148.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8148.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8148.svc.cluster.local jessie_udp@dns-test-service.dns-8148.svc.cluster.local jessie_tcp@dns-test-service.dns-8148.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8148.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8148.svc.cluster.local]

Jul 29 15:54:42.173: INFO: Unable to read wheezy_udp@dns-test-service.dns-8148.svc.cluster.local from pod dns-8148/dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff: the server could not find the requested resource (get pods dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff)
Jul 29 15:54:42.178: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8148.svc.cluster.local from pod dns-8148/dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff: the server could not find the requested resource (get pods dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff)
Jul 29 15:54:42.185: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8148.svc.cluster.local from pod dns-8148/dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff: the server could not find the requested resource (get pods dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff)
Jul 29 15:54:42.193: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8148.svc.cluster.local from pod dns-8148/dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff: the server could not find the requested resource (get pods dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff)
Jul 29 15:54:42.229: INFO: Unable to read jessie_udp@dns-test-service.dns-8148.svc.cluster.local from pod dns-8148/dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff: the server could not find the requested resource (get pods dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff)
Jul 29 15:54:42.236: INFO: Unable to read jessie_tcp@dns-test-service.dns-8148.svc.cluster.local from pod dns-8148/dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff: the server could not find the requested resource (get pods dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff)
Jul 29 15:54:42.242: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8148.svc.cluster.local from pod dns-8148/dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff: the server could not find the requested resource (get pods dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff)
Jul 29 15:54:42.250: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8148.svc.cluster.local from pod dns-8148/dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff: the server could not find the requested resource (get pods dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff)
Jul 29 15:54:42.275: INFO: Lookups using dns-8148/dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff failed for: [wheezy_udp@dns-test-service.dns-8148.svc.cluster.local wheezy_tcp@dns-test-service.dns-8148.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8148.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8148.svc.cluster.local jessie_udp@dns-test-service.dns-8148.svc.cluster.local jessie_tcp@dns-test-service.dns-8148.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8148.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8148.svc.cluster.local]

Jul 29 15:54:47.173: INFO: Unable to read wheezy_udp@dns-test-service.dns-8148.svc.cluster.local from pod dns-8148/dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff: the server could not find the requested resource (get pods dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff)
Jul 29 15:54:47.183: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8148.svc.cluster.local from pod dns-8148/dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff: the server could not find the requested resource (get pods dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff)
Jul 29 15:54:47.190: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8148.svc.cluster.local from pod dns-8148/dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff: the server could not find the requested resource (get pods dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff)
Jul 29 15:54:47.197: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8148.svc.cluster.local from pod dns-8148/dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff: the server could not find the requested resource (get pods dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff)
Jul 29 15:54:47.232: INFO: Unable to read jessie_udp@dns-test-service.dns-8148.svc.cluster.local from pod dns-8148/dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff: the server could not find the requested resource (get pods dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff)
Jul 29 15:54:47.239: INFO: Unable to read jessie_tcp@dns-test-service.dns-8148.svc.cluster.local from pod dns-8148/dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff: the server could not find the requested resource (get pods dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff)
Jul 29 15:54:47.247: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8148.svc.cluster.local from pod dns-8148/dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff: the server could not find the requested resource (get pods dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff)
Jul 29 15:54:47.253: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8148.svc.cluster.local from pod dns-8148/dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff: the server could not find the requested resource (get pods dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff)
Jul 29 15:54:47.276: INFO: Lookups using dns-8148/dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff failed for: [wheezy_udp@dns-test-service.dns-8148.svc.cluster.local wheezy_tcp@dns-test-service.dns-8148.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8148.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8148.svc.cluster.local jessie_udp@dns-test-service.dns-8148.svc.cluster.local jessie_tcp@dns-test-service.dns-8148.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8148.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8148.svc.cluster.local]

Jul 29 15:54:52.178: INFO: Unable to read wheezy_udp@dns-test-service.dns-8148.svc.cluster.local from pod dns-8148/dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff: the server could not find the requested resource (get pods dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff)
Jul 29 15:54:52.185: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8148.svc.cluster.local from pod dns-8148/dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff: the server could not find the requested resource (get pods dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff)
Jul 29 15:54:52.197: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8148.svc.cluster.local from pod dns-8148/dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff: the server could not find the requested resource (get pods dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff)
Jul 29 15:54:52.205: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8148.svc.cluster.local from pod dns-8148/dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff: the server could not find the requested resource (get pods dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff)
Jul 29 15:54:52.263: INFO: Unable to read jessie_udp@dns-test-service.dns-8148.svc.cluster.local from pod dns-8148/dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff: the server could not find the requested resource (get pods dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff)
Jul 29 15:54:52.275: INFO: Unable to read jessie_tcp@dns-test-service.dns-8148.svc.cluster.local from pod dns-8148/dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff: the server could not find the requested resource (get pods dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff)
Jul 29 15:54:52.283: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8148.svc.cluster.local from pod dns-8148/dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff: the server could not find the requested resource (get pods dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff)
Jul 29 15:54:52.298: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8148.svc.cluster.local from pod dns-8148/dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff: the server could not find the requested resource (get pods dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff)
Jul 29 15:54:52.327: INFO: Lookups using dns-8148/dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff failed for: [wheezy_udp@dns-test-service.dns-8148.svc.cluster.local wheezy_tcp@dns-test-service.dns-8148.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8148.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8148.svc.cluster.local jessie_udp@dns-test-service.dns-8148.svc.cluster.local jessie_tcp@dns-test-service.dns-8148.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8148.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8148.svc.cluster.local]

Jul 29 15:54:57.285: INFO: DNS probes using dns-8148/dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff succeeded

STEP: deleting the pod 07/29/23 15:54:57.285
STEP: deleting the test service 07/29/23 15:54:57.316
STEP: deleting the test headless service 07/29/23 15:54:57.42
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jul 29 15:54:57.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8148" for this suite. 07/29/23 15:54:57.466
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","completed":87,"skipped":1446,"failed":0}
------------------------------
â€¢ [SLOW TEST] [32.582 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:54:24.9
    Jul 29 15:54:24.900: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename dns 07/29/23 15:54:24.903
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:54:24.936
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:54:24.941
    [It] should provide DNS for services  [Conformance]
      test/e2e/network/dns.go:137
    STEP: Creating a test headless service 07/29/23 15:54:24.946
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8148.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8148.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8148.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8148.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8148.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-8148.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8148.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-8148.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8148.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-8148.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8148.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-8148.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 104.20.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.20.104_udp@PTR;check="$$(dig +tcp +noall +answer +search 104.20.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.20.104_tcp@PTR;sleep 1; done
     07/29/23 15:54:24.971
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8148.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8148.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8148.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8148.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8148.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-8148.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8148.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-8148.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8148.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-8148.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8148.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-8148.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 104.20.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.20.104_udp@PTR;check="$$(dig +tcp +noall +answer +search 104.20.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.20.104_tcp@PTR;sleep 1; done
     07/29/23 15:54:24.972
    STEP: creating a pod to probe DNS 07/29/23 15:54:24.972
    STEP: submitting the pod to kubernetes 07/29/23 15:54:24.973
    Jul 29 15:54:25.003: INFO: Waiting up to 15m0s for pod "dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff" in namespace "dns-8148" to be "running"
    Jul 29 15:54:25.018: INFO: Pod "dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff": Phase="Pending", Reason="", readiness=false. Elapsed: 15.142044ms
    Jul 29 15:54:27.028: INFO: Pod "dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff": Phase="Running", Reason="", readiness=true. Elapsed: 2.025047741s
    Jul 29 15:54:27.029: INFO: Pod "dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff" satisfied condition "running"
    STEP: retrieving the pod 07/29/23 15:54:27.029
    STEP: looking for the results for each expected name from probers 07/29/23 15:54:27.04
    Jul 29 15:54:27.059: INFO: Unable to read wheezy_udp@dns-test-service.dns-8148.svc.cluster.local from pod dns-8148/dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff: the server could not find the requested resource (get pods dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff)
    Jul 29 15:54:27.074: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8148.svc.cluster.local from pod dns-8148/dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff: the server could not find the requested resource (get pods dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff)
    Jul 29 15:54:27.082: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8148.svc.cluster.local from pod dns-8148/dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff: the server could not find the requested resource (get pods dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff)
    Jul 29 15:54:27.087: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8148.svc.cluster.local from pod dns-8148/dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff: the server could not find the requested resource (get pods dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff)
    Jul 29 15:54:27.119: INFO: Unable to read jessie_udp@dns-test-service.dns-8148.svc.cluster.local from pod dns-8148/dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff: the server could not find the requested resource (get pods dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff)
    Jul 29 15:54:27.125: INFO: Unable to read jessie_tcp@dns-test-service.dns-8148.svc.cluster.local from pod dns-8148/dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff: the server could not find the requested resource (get pods dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff)
    Jul 29 15:54:27.135: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8148.svc.cluster.local from pod dns-8148/dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff: the server could not find the requested resource (get pods dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff)
    Jul 29 15:54:27.142: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8148.svc.cluster.local from pod dns-8148/dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff: the server could not find the requested resource (get pods dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff)
    Jul 29 15:54:27.165: INFO: Lookups using dns-8148/dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff failed for: [wheezy_udp@dns-test-service.dns-8148.svc.cluster.local wheezy_tcp@dns-test-service.dns-8148.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8148.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8148.svc.cluster.local jessie_udp@dns-test-service.dns-8148.svc.cluster.local jessie_tcp@dns-test-service.dns-8148.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8148.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8148.svc.cluster.local]

    Jul 29 15:54:32.180: INFO: Unable to read wheezy_udp@dns-test-service.dns-8148.svc.cluster.local from pod dns-8148/dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff: the server could not find the requested resource (get pods dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff)
    Jul 29 15:54:32.187: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8148.svc.cluster.local from pod dns-8148/dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff: the server could not find the requested resource (get pods dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff)
    Jul 29 15:54:32.194: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8148.svc.cluster.local from pod dns-8148/dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff: the server could not find the requested resource (get pods dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff)
    Jul 29 15:54:32.200: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8148.svc.cluster.local from pod dns-8148/dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff: the server could not find the requested resource (get pods dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff)
    Jul 29 15:54:32.232: INFO: Unable to read jessie_udp@dns-test-service.dns-8148.svc.cluster.local from pod dns-8148/dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff: the server could not find the requested resource (get pods dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff)
    Jul 29 15:54:32.239: INFO: Unable to read jessie_tcp@dns-test-service.dns-8148.svc.cluster.local from pod dns-8148/dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff: the server could not find the requested resource (get pods dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff)
    Jul 29 15:54:32.247: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8148.svc.cluster.local from pod dns-8148/dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff: the server could not find the requested resource (get pods dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff)
    Jul 29 15:54:32.253: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8148.svc.cluster.local from pod dns-8148/dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff: the server could not find the requested resource (get pods dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff)
    Jul 29 15:54:32.280: INFO: Lookups using dns-8148/dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff failed for: [wheezy_udp@dns-test-service.dns-8148.svc.cluster.local wheezy_tcp@dns-test-service.dns-8148.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8148.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8148.svc.cluster.local jessie_udp@dns-test-service.dns-8148.svc.cluster.local jessie_tcp@dns-test-service.dns-8148.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8148.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8148.svc.cluster.local]

    Jul 29 15:54:37.175: INFO: Unable to read wheezy_udp@dns-test-service.dns-8148.svc.cluster.local from pod dns-8148/dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff: the server could not find the requested resource (get pods dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff)
    Jul 29 15:54:37.182: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8148.svc.cluster.local from pod dns-8148/dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff: the server could not find the requested resource (get pods dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff)
    Jul 29 15:54:37.188: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8148.svc.cluster.local from pod dns-8148/dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff: the server could not find the requested resource (get pods dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff)
    Jul 29 15:54:37.194: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8148.svc.cluster.local from pod dns-8148/dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff: the server could not find the requested resource (get pods dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff)
    Jul 29 15:54:37.229: INFO: Unable to read jessie_udp@dns-test-service.dns-8148.svc.cluster.local from pod dns-8148/dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff: the server could not find the requested resource (get pods dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff)
    Jul 29 15:54:37.236: INFO: Unable to read jessie_tcp@dns-test-service.dns-8148.svc.cluster.local from pod dns-8148/dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff: the server could not find the requested resource (get pods dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff)
    Jul 29 15:54:37.245: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8148.svc.cluster.local from pod dns-8148/dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff: the server could not find the requested resource (get pods dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff)
    Jul 29 15:54:37.252: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8148.svc.cluster.local from pod dns-8148/dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff: the server could not find the requested resource (get pods dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff)
    Jul 29 15:54:37.302: INFO: Lookups using dns-8148/dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff failed for: [wheezy_udp@dns-test-service.dns-8148.svc.cluster.local wheezy_tcp@dns-test-service.dns-8148.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8148.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8148.svc.cluster.local jessie_udp@dns-test-service.dns-8148.svc.cluster.local jessie_tcp@dns-test-service.dns-8148.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8148.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8148.svc.cluster.local]

    Jul 29 15:54:42.173: INFO: Unable to read wheezy_udp@dns-test-service.dns-8148.svc.cluster.local from pod dns-8148/dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff: the server could not find the requested resource (get pods dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff)
    Jul 29 15:54:42.178: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8148.svc.cluster.local from pod dns-8148/dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff: the server could not find the requested resource (get pods dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff)
    Jul 29 15:54:42.185: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8148.svc.cluster.local from pod dns-8148/dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff: the server could not find the requested resource (get pods dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff)
    Jul 29 15:54:42.193: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8148.svc.cluster.local from pod dns-8148/dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff: the server could not find the requested resource (get pods dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff)
    Jul 29 15:54:42.229: INFO: Unable to read jessie_udp@dns-test-service.dns-8148.svc.cluster.local from pod dns-8148/dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff: the server could not find the requested resource (get pods dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff)
    Jul 29 15:54:42.236: INFO: Unable to read jessie_tcp@dns-test-service.dns-8148.svc.cluster.local from pod dns-8148/dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff: the server could not find the requested resource (get pods dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff)
    Jul 29 15:54:42.242: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8148.svc.cluster.local from pod dns-8148/dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff: the server could not find the requested resource (get pods dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff)
    Jul 29 15:54:42.250: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8148.svc.cluster.local from pod dns-8148/dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff: the server could not find the requested resource (get pods dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff)
    Jul 29 15:54:42.275: INFO: Lookups using dns-8148/dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff failed for: [wheezy_udp@dns-test-service.dns-8148.svc.cluster.local wheezy_tcp@dns-test-service.dns-8148.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8148.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8148.svc.cluster.local jessie_udp@dns-test-service.dns-8148.svc.cluster.local jessie_tcp@dns-test-service.dns-8148.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8148.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8148.svc.cluster.local]

    Jul 29 15:54:47.173: INFO: Unable to read wheezy_udp@dns-test-service.dns-8148.svc.cluster.local from pod dns-8148/dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff: the server could not find the requested resource (get pods dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff)
    Jul 29 15:54:47.183: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8148.svc.cluster.local from pod dns-8148/dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff: the server could not find the requested resource (get pods dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff)
    Jul 29 15:54:47.190: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8148.svc.cluster.local from pod dns-8148/dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff: the server could not find the requested resource (get pods dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff)
    Jul 29 15:54:47.197: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8148.svc.cluster.local from pod dns-8148/dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff: the server could not find the requested resource (get pods dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff)
    Jul 29 15:54:47.232: INFO: Unable to read jessie_udp@dns-test-service.dns-8148.svc.cluster.local from pod dns-8148/dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff: the server could not find the requested resource (get pods dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff)
    Jul 29 15:54:47.239: INFO: Unable to read jessie_tcp@dns-test-service.dns-8148.svc.cluster.local from pod dns-8148/dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff: the server could not find the requested resource (get pods dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff)
    Jul 29 15:54:47.247: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8148.svc.cluster.local from pod dns-8148/dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff: the server could not find the requested resource (get pods dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff)
    Jul 29 15:54:47.253: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8148.svc.cluster.local from pod dns-8148/dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff: the server could not find the requested resource (get pods dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff)
    Jul 29 15:54:47.276: INFO: Lookups using dns-8148/dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff failed for: [wheezy_udp@dns-test-service.dns-8148.svc.cluster.local wheezy_tcp@dns-test-service.dns-8148.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8148.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8148.svc.cluster.local jessie_udp@dns-test-service.dns-8148.svc.cluster.local jessie_tcp@dns-test-service.dns-8148.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8148.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8148.svc.cluster.local]

    Jul 29 15:54:52.178: INFO: Unable to read wheezy_udp@dns-test-service.dns-8148.svc.cluster.local from pod dns-8148/dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff: the server could not find the requested resource (get pods dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff)
    Jul 29 15:54:52.185: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8148.svc.cluster.local from pod dns-8148/dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff: the server could not find the requested resource (get pods dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff)
    Jul 29 15:54:52.197: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8148.svc.cluster.local from pod dns-8148/dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff: the server could not find the requested resource (get pods dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff)
    Jul 29 15:54:52.205: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8148.svc.cluster.local from pod dns-8148/dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff: the server could not find the requested resource (get pods dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff)
    Jul 29 15:54:52.263: INFO: Unable to read jessie_udp@dns-test-service.dns-8148.svc.cluster.local from pod dns-8148/dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff: the server could not find the requested resource (get pods dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff)
    Jul 29 15:54:52.275: INFO: Unable to read jessie_tcp@dns-test-service.dns-8148.svc.cluster.local from pod dns-8148/dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff: the server could not find the requested resource (get pods dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff)
    Jul 29 15:54:52.283: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8148.svc.cluster.local from pod dns-8148/dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff: the server could not find the requested resource (get pods dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff)
    Jul 29 15:54:52.298: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8148.svc.cluster.local from pod dns-8148/dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff: the server could not find the requested resource (get pods dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff)
    Jul 29 15:54:52.327: INFO: Lookups using dns-8148/dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff failed for: [wheezy_udp@dns-test-service.dns-8148.svc.cluster.local wheezy_tcp@dns-test-service.dns-8148.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8148.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8148.svc.cluster.local jessie_udp@dns-test-service.dns-8148.svc.cluster.local jessie_tcp@dns-test-service.dns-8148.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8148.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8148.svc.cluster.local]

    Jul 29 15:54:57.285: INFO: DNS probes using dns-8148/dns-test-3fe267cd-58ec-4a3f-a495-aacb2ca9c9ff succeeded

    STEP: deleting the pod 07/29/23 15:54:57.285
    STEP: deleting the test service 07/29/23 15:54:57.316
    STEP: deleting the test headless service 07/29/23 15:54:57.42
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jul 29 15:54:57.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-8148" for this suite. 07/29/23 15:54:57.466
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath
  runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:543
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:54:57.511
Jul 29 15:54:57.511: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename sched-preemption 07/29/23 15:54:57.514
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:54:57.547
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:54:57.554
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Jul 29 15:54:57.595: INFO: Waiting up to 1m0s for all nodes to be ready
Jul 29 15:55:57.663: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:55:57.67
Jul 29 15:55:57.671: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename sched-preemption-path 07/29/23 15:55:57.676
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:55:57.718
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:55:57.724
[BeforeEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:496
STEP: Finding an available node 07/29/23 15:55:57.73
STEP: Trying to launch a pod without a label to get a node which can launch it. 07/29/23 15:55:57.73
Jul 29 15:55:57.745: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-7761" to be "running"
Jul 29 15:55:57.752: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 6.518636ms
Jul 29 15:55:59.760: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.01470801s
Jul 29 15:55:59.760: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 07/29/23 15:55:59.766
Jul 29 15:55:59.786: INFO: found a healthy node: wa4quivohpee-3
[It] runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:543
Jul 29 15:56:07.931: INFO: pods created so far: [1 1 1]
Jul 29 15:56:07.932: INFO: length of pods created so far: 3
Jul 29 15:56:09.952: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  test/e2e/framework/framework.go:187
Jul 29 15:56:16.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-7761" for this suite. 07/29/23 15:56:16.966
[AfterEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:470
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Jul 29 15:56:17.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-6035" for this suite. 07/29/23 15:56:17.064
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","completed":88,"skipped":1481,"failed":0}
------------------------------
â€¢ [SLOW TEST] [79.676 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:458
    runs ReplicaSets to verify preemption running path [Conformance]
    test/e2e/scheduling/preemption.go:543

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:54:57.511
    Jul 29 15:54:57.511: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename sched-preemption 07/29/23 15:54:57.514
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:54:57.547
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:54:57.554
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Jul 29 15:54:57.595: INFO: Waiting up to 1m0s for all nodes to be ready
    Jul 29 15:55:57.663: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PreemptionExecutionPath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:55:57.67
    Jul 29 15:55:57.671: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename sched-preemption-path 07/29/23 15:55:57.676
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:55:57.718
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:55:57.724
    [BeforeEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:496
    STEP: Finding an available node 07/29/23 15:55:57.73
    STEP: Trying to launch a pod without a label to get a node which can launch it. 07/29/23 15:55:57.73
    Jul 29 15:55:57.745: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-7761" to be "running"
    Jul 29 15:55:57.752: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 6.518636ms
    Jul 29 15:55:59.760: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.01470801s
    Jul 29 15:55:59.760: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 07/29/23 15:55:59.766
    Jul 29 15:55:59.786: INFO: found a healthy node: wa4quivohpee-3
    [It] runs ReplicaSets to verify preemption running path [Conformance]
      test/e2e/scheduling/preemption.go:543
    Jul 29 15:56:07.931: INFO: pods created so far: [1 1 1]
    Jul 29 15:56:07.932: INFO: length of pods created so far: 3
    Jul 29 15:56:09.952: INFO: pods created so far: [2 2 1]
    [AfterEach] PreemptionExecutionPath
      test/e2e/framework/framework.go:187
    Jul 29 15:56:16.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-path-7761" for this suite. 07/29/23 15:56:16.966
    [AfterEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:470
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Jul 29 15:56:17.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-6035" for this suite. 07/29/23 15:56:17.064
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:56:17.188
Jul 29 15:56:17.188: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename projected 07/29/23 15:56:17.19
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:56:17.247
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:56:17.256
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67
STEP: Creating a pod to test downward API volume plugin 07/29/23 15:56:17.269
Jul 29 15:56:17.292: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f43922de-9cdd-43fb-8d94-9d36e3d7764c" in namespace "projected-247" to be "Succeeded or Failed"
Jul 29 15:56:17.300: INFO: Pod "downwardapi-volume-f43922de-9cdd-43fb-8d94-9d36e3d7764c": Phase="Pending", Reason="", readiness=false. Elapsed: 7.487612ms
Jul 29 15:56:19.316: INFO: Pod "downwardapi-volume-f43922de-9cdd-43fb-8d94-9d36e3d7764c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024095609s
Jul 29 15:56:21.309: INFO: Pod "downwardapi-volume-f43922de-9cdd-43fb-8d94-9d36e3d7764c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017372994s
STEP: Saw pod success 07/29/23 15:56:21.31
Jul 29 15:56:21.310: INFO: Pod "downwardapi-volume-f43922de-9cdd-43fb-8d94-9d36e3d7764c" satisfied condition "Succeeded or Failed"
Jul 29 15:56:21.319: INFO: Trying to get logs from node wa4quivohpee-3 pod downwardapi-volume-f43922de-9cdd-43fb-8d94-9d36e3d7764c container client-container: <nil>
STEP: delete the pod 07/29/23 15:56:21.353
Jul 29 15:56:21.385: INFO: Waiting for pod downwardapi-volume-f43922de-9cdd-43fb-8d94-9d36e3d7764c to disappear
Jul 29 15:56:21.394: INFO: Pod downwardapi-volume-f43922de-9cdd-43fb-8d94-9d36e3d7764c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jul 29 15:56:21.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-247" for this suite. 07/29/23 15:56:21.411
{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","completed":89,"skipped":1483,"failed":0}
------------------------------
â€¢ [4.237 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:56:17.188
    Jul 29 15:56:17.188: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename projected 07/29/23 15:56:17.19
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:56:17.247
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:56:17.256
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:67
    STEP: Creating a pod to test downward API volume plugin 07/29/23 15:56:17.269
    Jul 29 15:56:17.292: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f43922de-9cdd-43fb-8d94-9d36e3d7764c" in namespace "projected-247" to be "Succeeded or Failed"
    Jul 29 15:56:17.300: INFO: Pod "downwardapi-volume-f43922de-9cdd-43fb-8d94-9d36e3d7764c": Phase="Pending", Reason="", readiness=false. Elapsed: 7.487612ms
    Jul 29 15:56:19.316: INFO: Pod "downwardapi-volume-f43922de-9cdd-43fb-8d94-9d36e3d7764c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024095609s
    Jul 29 15:56:21.309: INFO: Pod "downwardapi-volume-f43922de-9cdd-43fb-8d94-9d36e3d7764c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017372994s
    STEP: Saw pod success 07/29/23 15:56:21.31
    Jul 29 15:56:21.310: INFO: Pod "downwardapi-volume-f43922de-9cdd-43fb-8d94-9d36e3d7764c" satisfied condition "Succeeded or Failed"
    Jul 29 15:56:21.319: INFO: Trying to get logs from node wa4quivohpee-3 pod downwardapi-volume-f43922de-9cdd-43fb-8d94-9d36e3d7764c container client-container: <nil>
    STEP: delete the pod 07/29/23 15:56:21.353
    Jul 29 15:56:21.385: INFO: Waiting for pod downwardapi-volume-f43922de-9cdd-43fb-8d94-9d36e3d7764c to disappear
    Jul 29 15:56:21.394: INFO: Pod downwardapi-volume-f43922de-9cdd-43fb-8d94-9d36e3d7764c no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jul 29 15:56:21.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-247" for this suite. 07/29/23 15:56:21.411
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Variable Expansion
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:56:21.426
Jul 29 15:56:21.426: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename var-expansion 07/29/23 15:56:21.429
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:56:21.477
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:56:21.482
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296
STEP: creating the pod 07/29/23 15:56:21.486
STEP: waiting for pod running 07/29/23 15:56:21.509
Jul 29 15:56:21.509: INFO: Waiting up to 2m0s for pod "var-expansion-75255f39-b5c8-4e78-92fd-694c38defb3f" in namespace "var-expansion-7634" to be "running"
Jul 29 15:56:21.515: INFO: Pod "var-expansion-75255f39-b5c8-4e78-92fd-694c38defb3f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.785242ms
Jul 29 15:56:23.526: INFO: Pod "var-expansion-75255f39-b5c8-4e78-92fd-694c38defb3f": Phase="Running", Reason="", readiness=true. Elapsed: 2.017054206s
Jul 29 15:56:23.526: INFO: Pod "var-expansion-75255f39-b5c8-4e78-92fd-694c38defb3f" satisfied condition "running"
STEP: creating a file in subpath 07/29/23 15:56:23.526
Jul 29 15:56:23.532: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-7634 PodName:var-expansion-75255f39-b5c8-4e78-92fd-694c38defb3f ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 29 15:56:23.533: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
Jul 29 15:56:23.535: INFO: ExecWithOptions: Clientset creation
Jul 29 15:56:23.535: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/var-expansion-7634/pods/var-expansion-75255f39-b5c8-4e78-92fd-694c38defb3f/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: test for file in mounted path 07/29/23 15:56:23.645
Jul 29 15:56:23.652: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-7634 PodName:var-expansion-75255f39-b5c8-4e78-92fd-694c38defb3f ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 29 15:56:23.653: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
Jul 29 15:56:23.655: INFO: ExecWithOptions: Clientset creation
Jul 29 15:56:23.655: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/var-expansion-7634/pods/var-expansion-75255f39-b5c8-4e78-92fd-694c38defb3f/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: updating the annotation value 07/29/23 15:56:23.756
Jul 29 15:56:24.278: INFO: Successfully updated pod "var-expansion-75255f39-b5c8-4e78-92fd-694c38defb3f"
STEP: waiting for annotated pod running 07/29/23 15:56:24.278
Jul 29 15:56:24.278: INFO: Waiting up to 2m0s for pod "var-expansion-75255f39-b5c8-4e78-92fd-694c38defb3f" in namespace "var-expansion-7634" to be "running"
Jul 29 15:56:24.284: INFO: Pod "var-expansion-75255f39-b5c8-4e78-92fd-694c38defb3f": Phase="Running", Reason="", readiness=true. Elapsed: 5.732695ms
Jul 29 15:56:24.285: INFO: Pod "var-expansion-75255f39-b5c8-4e78-92fd-694c38defb3f" satisfied condition "running"
STEP: deleting the pod gracefully 07/29/23 15:56:24.285
Jul 29 15:56:24.285: INFO: Deleting pod "var-expansion-75255f39-b5c8-4e78-92fd-694c38defb3f" in namespace "var-expansion-7634"
Jul 29 15:56:24.304: INFO: Wait up to 5m0s for pod "var-expansion-75255f39-b5c8-4e78-92fd-694c38defb3f" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jul 29 15:56:58.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7634" for this suite. 07/29/23 15:56:58.329
{"msg":"PASSED [sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]","completed":90,"skipped":1483,"failed":0}
------------------------------
â€¢ [SLOW TEST] [36.914 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:56:21.426
    Jul 29 15:56:21.426: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename var-expansion 07/29/23 15:56:21.429
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:56:21.477
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:56:21.482
    [It] should succeed in writing subpaths in container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:296
    STEP: creating the pod 07/29/23 15:56:21.486
    STEP: waiting for pod running 07/29/23 15:56:21.509
    Jul 29 15:56:21.509: INFO: Waiting up to 2m0s for pod "var-expansion-75255f39-b5c8-4e78-92fd-694c38defb3f" in namespace "var-expansion-7634" to be "running"
    Jul 29 15:56:21.515: INFO: Pod "var-expansion-75255f39-b5c8-4e78-92fd-694c38defb3f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.785242ms
    Jul 29 15:56:23.526: INFO: Pod "var-expansion-75255f39-b5c8-4e78-92fd-694c38defb3f": Phase="Running", Reason="", readiness=true. Elapsed: 2.017054206s
    Jul 29 15:56:23.526: INFO: Pod "var-expansion-75255f39-b5c8-4e78-92fd-694c38defb3f" satisfied condition "running"
    STEP: creating a file in subpath 07/29/23 15:56:23.526
    Jul 29 15:56:23.532: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-7634 PodName:var-expansion-75255f39-b5c8-4e78-92fd-694c38defb3f ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jul 29 15:56:23.533: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    Jul 29 15:56:23.535: INFO: ExecWithOptions: Clientset creation
    Jul 29 15:56:23.535: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/var-expansion-7634/pods/var-expansion-75255f39-b5c8-4e78-92fd-694c38defb3f/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: test for file in mounted path 07/29/23 15:56:23.645
    Jul 29 15:56:23.652: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-7634 PodName:var-expansion-75255f39-b5c8-4e78-92fd-694c38defb3f ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jul 29 15:56:23.653: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    Jul 29 15:56:23.655: INFO: ExecWithOptions: Clientset creation
    Jul 29 15:56:23.655: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/var-expansion-7634/pods/var-expansion-75255f39-b5c8-4e78-92fd-694c38defb3f/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: updating the annotation value 07/29/23 15:56:23.756
    Jul 29 15:56:24.278: INFO: Successfully updated pod "var-expansion-75255f39-b5c8-4e78-92fd-694c38defb3f"
    STEP: waiting for annotated pod running 07/29/23 15:56:24.278
    Jul 29 15:56:24.278: INFO: Waiting up to 2m0s for pod "var-expansion-75255f39-b5c8-4e78-92fd-694c38defb3f" in namespace "var-expansion-7634" to be "running"
    Jul 29 15:56:24.284: INFO: Pod "var-expansion-75255f39-b5c8-4e78-92fd-694c38defb3f": Phase="Running", Reason="", readiness=true. Elapsed: 5.732695ms
    Jul 29 15:56:24.285: INFO: Pod "var-expansion-75255f39-b5c8-4e78-92fd-694c38defb3f" satisfied condition "running"
    STEP: deleting the pod gracefully 07/29/23 15:56:24.285
    Jul 29 15:56:24.285: INFO: Deleting pod "var-expansion-75255f39-b5c8-4e78-92fd-694c38defb3f" in namespace "var-expansion-7634"
    Jul 29 15:56:24.304: INFO: Wait up to 5m0s for pod "var-expansion-75255f39-b5c8-4e78-92fd-694c38defb3f" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jul 29 15:56:58.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-7634" for this suite. 07/29/23 15:56:58.329
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:56:58.343
Jul 29 15:56:58.343: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename pod-network-test 07/29/23 15:56:58.347
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:56:58.377
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:56:58.382
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
STEP: Performing setup for networking test in namespace pod-network-test-8672 07/29/23 15:56:58.386
STEP: creating a selector 07/29/23 15:56:58.387
STEP: Creating the service pods in kubernetes 07/29/23 15:56:58.387
Jul 29 15:56:58.387: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jul 29 15:56:58.470: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-8672" to be "running and ready"
Jul 29 15:56:58.484: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 14.533609ms
Jul 29 15:56:58.484: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jul 29 15:57:00.496: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.026741243s
Jul 29 15:57:00.497: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jul 29 15:57:02.494: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.023893559s
Jul 29 15:57:02.494: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jul 29 15:57:04.493: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.023714531s
Jul 29 15:57:04.493: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jul 29 15:57:06.493: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.023322355s
Jul 29 15:57:06.493: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jul 29 15:57:08.492: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.022753278s
Jul 29 15:57:08.493: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jul 29 15:57:10.492: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.022029346s
Jul 29 15:57:10.492: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jul 29 15:57:12.499: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.029088129s
Jul 29 15:57:12.499: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jul 29 15:57:14.496: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.026142676s
Jul 29 15:57:14.496: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jul 29 15:57:16.493: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.023341356s
Jul 29 15:57:16.493: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jul 29 15:57:18.493: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.023600461s
Jul 29 15:57:18.493: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jul 29 15:57:20.491: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.021237497s
Jul 29 15:57:20.491: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jul 29 15:57:20.491: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jul 29 15:57:20.501: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-8672" to be "running and ready"
Jul 29 15:57:20.508: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 6.538036ms
Jul 29 15:57:20.508: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jul 29 15:57:20.508: INFO: Pod "netserver-1" satisfied condition "running and ready"
Jul 29 15:57:20.514: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-8672" to be "running and ready"
Jul 29 15:57:20.519: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 4.12138ms
Jul 29 15:57:20.519: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Jul 29 15:57:20.519: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 07/29/23 15:57:20.526
Jul 29 15:57:20.544: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-8672" to be "running"
Jul 29 15:57:20.551: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.120437ms
Jul 29 15:57:22.579: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.034532963s
Jul 29 15:57:22.580: INFO: Pod "test-container-pod" satisfied condition "running"
Jul 29 15:57:22.587: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-8672" to be "running"
Jul 29 15:57:22.594: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 7.409707ms
Jul 29 15:57:22.594: INFO: Pod "host-test-container-pod" satisfied condition "running"
Jul 29 15:57:22.601: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Jul 29 15:57:22.601: INFO: Going to poll 10.233.64.113 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Jul 29 15:57:22.613: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.64.113:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8672 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 29 15:57:22.614: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
Jul 29 15:57:22.619: INFO: ExecWithOptions: Clientset creation
Jul 29 15:57:22.620: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-8672/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.64.113%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jul 29 15:57:22.812: INFO: Found all 1 expected endpoints: [netserver-0]
Jul 29 15:57:22.812: INFO: Going to poll 10.233.66.234 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Jul 29 15:57:22.821: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.66.234:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8672 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 29 15:57:22.821: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
Jul 29 15:57:22.822: INFO: ExecWithOptions: Clientset creation
Jul 29 15:57:22.822: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-8672/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.66.234%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jul 29 15:57:22.928: INFO: Found all 1 expected endpoints: [netserver-1]
Jul 29 15:57:22.928: INFO: Going to poll 10.233.65.89 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Jul 29 15:57:22.935: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.65.89:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8672 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 29 15:57:22.935: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
Jul 29 15:57:22.937: INFO: ExecWithOptions: Clientset creation
Jul 29 15:57:22.937: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-8672/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.65.89%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jul 29 15:57:23.049: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Jul 29 15:57:23.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8672" for this suite. 07/29/23 15:57:23.06
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","completed":91,"skipped":1491,"failed":0}
------------------------------
â€¢ [SLOW TEST] [24.731 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:56:58.343
    Jul 29 15:56:58.343: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename pod-network-test 07/29/23 15:56:58.347
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:56:58.377
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:56:58.382
    [It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:105
    STEP: Performing setup for networking test in namespace pod-network-test-8672 07/29/23 15:56:58.386
    STEP: creating a selector 07/29/23 15:56:58.387
    STEP: Creating the service pods in kubernetes 07/29/23 15:56:58.387
    Jul 29 15:56:58.387: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jul 29 15:56:58.470: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-8672" to be "running and ready"
    Jul 29 15:56:58.484: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 14.533609ms
    Jul 29 15:56:58.484: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jul 29 15:57:00.496: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.026741243s
    Jul 29 15:57:00.497: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jul 29 15:57:02.494: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.023893559s
    Jul 29 15:57:02.494: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jul 29 15:57:04.493: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.023714531s
    Jul 29 15:57:04.493: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jul 29 15:57:06.493: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.023322355s
    Jul 29 15:57:06.493: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jul 29 15:57:08.492: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.022753278s
    Jul 29 15:57:08.493: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jul 29 15:57:10.492: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.022029346s
    Jul 29 15:57:10.492: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jul 29 15:57:12.499: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.029088129s
    Jul 29 15:57:12.499: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jul 29 15:57:14.496: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.026142676s
    Jul 29 15:57:14.496: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jul 29 15:57:16.493: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.023341356s
    Jul 29 15:57:16.493: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jul 29 15:57:18.493: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.023600461s
    Jul 29 15:57:18.493: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jul 29 15:57:20.491: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.021237497s
    Jul 29 15:57:20.491: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jul 29 15:57:20.491: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jul 29 15:57:20.501: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-8672" to be "running and ready"
    Jul 29 15:57:20.508: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 6.538036ms
    Jul 29 15:57:20.508: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jul 29 15:57:20.508: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Jul 29 15:57:20.514: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-8672" to be "running and ready"
    Jul 29 15:57:20.519: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 4.12138ms
    Jul 29 15:57:20.519: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Jul 29 15:57:20.519: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 07/29/23 15:57:20.526
    Jul 29 15:57:20.544: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-8672" to be "running"
    Jul 29 15:57:20.551: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.120437ms
    Jul 29 15:57:22.579: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.034532963s
    Jul 29 15:57:22.580: INFO: Pod "test-container-pod" satisfied condition "running"
    Jul 29 15:57:22.587: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-8672" to be "running"
    Jul 29 15:57:22.594: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 7.409707ms
    Jul 29 15:57:22.594: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Jul 29 15:57:22.601: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Jul 29 15:57:22.601: INFO: Going to poll 10.233.64.113 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Jul 29 15:57:22.613: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.64.113:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8672 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jul 29 15:57:22.614: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    Jul 29 15:57:22.619: INFO: ExecWithOptions: Clientset creation
    Jul 29 15:57:22.620: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-8672/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.64.113%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jul 29 15:57:22.812: INFO: Found all 1 expected endpoints: [netserver-0]
    Jul 29 15:57:22.812: INFO: Going to poll 10.233.66.234 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Jul 29 15:57:22.821: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.66.234:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8672 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jul 29 15:57:22.821: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    Jul 29 15:57:22.822: INFO: ExecWithOptions: Clientset creation
    Jul 29 15:57:22.822: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-8672/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.66.234%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jul 29 15:57:22.928: INFO: Found all 1 expected endpoints: [netserver-1]
    Jul 29 15:57:22.928: INFO: Going to poll 10.233.65.89 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Jul 29 15:57:22.935: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.65.89:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8672 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jul 29 15:57:22.935: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    Jul 29 15:57:22.937: INFO: ExecWithOptions: Clientset creation
    Jul 29 15:57:22.937: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-8672/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.65.89%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jul 29 15:57:23.049: INFO: Found all 1 expected endpoints: [netserver-2]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Jul 29 15:57:23.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-8672" for this suite. 07/29/23 15:57:23.06
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:57:23.076
Jul 29 15:57:23.076: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename configmap 07/29/23 15:57:23.078
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:57:23.112
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:57:23.118
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174
STEP: Creating configMap with name configmap-test-upd-76b43ec1-e7b2-4c70-99e4-92641b91eb7e 07/29/23 15:57:23.134
STEP: Creating the pod 07/29/23 15:57:23.143
Jul 29 15:57:23.161: INFO: Waiting up to 5m0s for pod "pod-configmaps-75c876d8-bd88-4318-bfc3-eb541a442c06" in namespace "configmap-7433" to be "running"
Jul 29 15:57:23.168: INFO: Pod "pod-configmaps-75c876d8-bd88-4318-bfc3-eb541a442c06": Phase="Pending", Reason="", readiness=false. Elapsed: 6.052929ms
Jul 29 15:57:25.178: INFO: Pod "pod-configmaps-75c876d8-bd88-4318-bfc3-eb541a442c06": Phase="Running", Reason="", readiness=false. Elapsed: 2.016406214s
Jul 29 15:57:25.178: INFO: Pod "pod-configmaps-75c876d8-bd88-4318-bfc3-eb541a442c06" satisfied condition "running"
STEP: Waiting for pod with text data 07/29/23 15:57:25.178
STEP: Waiting for pod with binary data 07/29/23 15:57:25.192
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jul 29 15:57:25.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7433" for this suite. 07/29/23 15:57:25.214
{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","completed":92,"skipped":1502,"failed":0}
------------------------------
â€¢ [2.149 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:57:23.076
    Jul 29 15:57:23.076: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename configmap 07/29/23 15:57:23.078
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:57:23.112
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:57:23.118
    [It] binary data should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:174
    STEP: Creating configMap with name configmap-test-upd-76b43ec1-e7b2-4c70-99e4-92641b91eb7e 07/29/23 15:57:23.134
    STEP: Creating the pod 07/29/23 15:57:23.143
    Jul 29 15:57:23.161: INFO: Waiting up to 5m0s for pod "pod-configmaps-75c876d8-bd88-4318-bfc3-eb541a442c06" in namespace "configmap-7433" to be "running"
    Jul 29 15:57:23.168: INFO: Pod "pod-configmaps-75c876d8-bd88-4318-bfc3-eb541a442c06": Phase="Pending", Reason="", readiness=false. Elapsed: 6.052929ms
    Jul 29 15:57:25.178: INFO: Pod "pod-configmaps-75c876d8-bd88-4318-bfc3-eb541a442c06": Phase="Running", Reason="", readiness=false. Elapsed: 2.016406214s
    Jul 29 15:57:25.178: INFO: Pod "pod-configmaps-75c876d8-bd88-4318-bfc3-eb541a442c06" satisfied condition "running"
    STEP: Waiting for pod with text data 07/29/23 15:57:25.178
    STEP: Waiting for pod with binary data 07/29/23 15:57:25.192
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jul 29 15:57:25.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-7433" for this suite. 07/29/23 15:57:25.214
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:57:25.235
Jul 29 15:57:25.235: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename projected 07/29/23 15:57:25.237
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:57:25.274
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:57:25.281
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220
STEP: Creating a pod to test downward API volume plugin 07/29/23 15:57:25.287
Jul 29 15:57:25.307: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4e16b903-9e13-4bdd-99af-fb52500149f0" in namespace "projected-1512" to be "Succeeded or Failed"
Jul 29 15:57:25.323: INFO: Pod "downwardapi-volume-4e16b903-9e13-4bdd-99af-fb52500149f0": Phase="Pending", Reason="", readiness=false. Elapsed: 14.893121ms
Jul 29 15:57:27.330: INFO: Pod "downwardapi-volume-4e16b903-9e13-4bdd-99af-fb52500149f0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022212116s
Jul 29 15:57:29.348: INFO: Pod "downwardapi-volume-4e16b903-9e13-4bdd-99af-fb52500149f0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039763243s
STEP: Saw pod success 07/29/23 15:57:29.348
Jul 29 15:57:29.348: INFO: Pod "downwardapi-volume-4e16b903-9e13-4bdd-99af-fb52500149f0" satisfied condition "Succeeded or Failed"
Jul 29 15:57:29.364: INFO: Trying to get logs from node wa4quivohpee-3 pod downwardapi-volume-4e16b903-9e13-4bdd-99af-fb52500149f0 container client-container: <nil>
STEP: delete the pod 07/29/23 15:57:29.385
Jul 29 15:57:29.415: INFO: Waiting for pod downwardapi-volume-4e16b903-9e13-4bdd-99af-fb52500149f0 to disappear
Jul 29 15:57:29.420: INFO: Pod downwardapi-volume-4e16b903-9e13-4bdd-99af-fb52500149f0 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jul 29 15:57:29.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1512" for this suite. 07/29/23 15:57:29.432
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","completed":93,"skipped":1530,"failed":0}
------------------------------
â€¢ [4.210 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:57:25.235
    Jul 29 15:57:25.235: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename projected 07/29/23 15:57:25.237
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:57:25.274
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:57:25.281
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:220
    STEP: Creating a pod to test downward API volume plugin 07/29/23 15:57:25.287
    Jul 29 15:57:25.307: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4e16b903-9e13-4bdd-99af-fb52500149f0" in namespace "projected-1512" to be "Succeeded or Failed"
    Jul 29 15:57:25.323: INFO: Pod "downwardapi-volume-4e16b903-9e13-4bdd-99af-fb52500149f0": Phase="Pending", Reason="", readiness=false. Elapsed: 14.893121ms
    Jul 29 15:57:27.330: INFO: Pod "downwardapi-volume-4e16b903-9e13-4bdd-99af-fb52500149f0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022212116s
    Jul 29 15:57:29.348: INFO: Pod "downwardapi-volume-4e16b903-9e13-4bdd-99af-fb52500149f0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039763243s
    STEP: Saw pod success 07/29/23 15:57:29.348
    Jul 29 15:57:29.348: INFO: Pod "downwardapi-volume-4e16b903-9e13-4bdd-99af-fb52500149f0" satisfied condition "Succeeded or Failed"
    Jul 29 15:57:29.364: INFO: Trying to get logs from node wa4quivohpee-3 pod downwardapi-volume-4e16b903-9e13-4bdd-99af-fb52500149f0 container client-container: <nil>
    STEP: delete the pod 07/29/23 15:57:29.385
    Jul 29 15:57:29.415: INFO: Waiting for pod downwardapi-volume-4e16b903-9e13-4bdd-99af-fb52500149f0 to disappear
    Jul 29 15:57:29.420: INFO: Pod downwardapi-volume-4e16b903-9e13-4bdd-99af-fb52500149f0 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jul 29 15:57:29.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1512" for this suite. 07/29/23 15:57:29.432
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:57:29.458
Jul 29 15:57:29.458: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename podtemplate 07/29/23 15:57:29.46
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:57:29.492
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:57:29.496
[It] should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
Jul 29 15:57:29.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-2718" for this suite. 07/29/23 15:57:29.609
{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","completed":94,"skipped":1576,"failed":0}
------------------------------
â€¢ [0.176 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:57:29.458
    Jul 29 15:57:29.458: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename podtemplate 07/29/23 15:57:29.46
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:57:29.492
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:57:29.496
    [It] should run the lifecycle of PodTemplates [Conformance]
      test/e2e/common/node/podtemplates.go:53
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    Jul 29 15:57:29.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-2718" for this suite. 07/29/23 15:57:29.609
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:57:29.635
Jul 29 15:57:29.636: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename sysctl 07/29/23 15:57:29.637
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:57:29.664
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:57:29.668
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
STEP: Creating a pod with one valid and two invalid sysctls 07/29/23 15:57:29.671
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:187
Jul 29 15:57:29.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-9308" for this suite. 07/29/23 15:57:29.69
{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]","completed":95,"skipped":1576,"failed":0}
------------------------------
â€¢ [0.064 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:57:29.635
    Jul 29 15:57:29.636: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename sysctl 07/29/23 15:57:29.637
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:57:29.664
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:57:29.668
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:123
    STEP: Creating a pod with one valid and two invalid sysctls 07/29/23 15:57:29.671
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:187
    Jul 29 15:57:29.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sysctl-9308" for this suite. 07/29/23 15:57:29.69
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Containers
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:57:29.702
Jul 29 15:57:29.702: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename containers 07/29/23 15:57:29.705
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:57:29.731
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:57:29.736
[It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58
STEP: Creating a pod to test override arguments 07/29/23 15:57:29.74
Jul 29 15:57:29.758: INFO: Waiting up to 5m0s for pod "client-containers-e1532cde-58b3-410a-84ca-6da3f6e9cc63" in namespace "containers-911" to be "Succeeded or Failed"
Jul 29 15:57:29.767: INFO: Pod "client-containers-e1532cde-58b3-410a-84ca-6da3f6e9cc63": Phase="Pending", Reason="", readiness=false. Elapsed: 9.128633ms
Jul 29 15:57:31.773: INFO: Pod "client-containers-e1532cde-58b3-410a-84ca-6da3f6e9cc63": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014688043s
Jul 29 15:57:33.776: INFO: Pod "client-containers-e1532cde-58b3-410a-84ca-6da3f6e9cc63": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017375728s
Jul 29 15:57:35.778: INFO: Pod "client-containers-e1532cde-58b3-410a-84ca-6da3f6e9cc63": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.019954198s
STEP: Saw pod success 07/29/23 15:57:35.779
Jul 29 15:57:35.779: INFO: Pod "client-containers-e1532cde-58b3-410a-84ca-6da3f6e9cc63" satisfied condition "Succeeded or Failed"
Jul 29 15:57:35.788: INFO: Trying to get logs from node wa4quivohpee-3 pod client-containers-e1532cde-58b3-410a-84ca-6da3f6e9cc63 container agnhost-container: <nil>
STEP: delete the pod 07/29/23 15:57:35.812
Jul 29 15:57:35.847: INFO: Waiting for pod client-containers-e1532cde-58b3-410a-84ca-6da3f6e9cc63 to disappear
Jul 29 15:57:35.853: INFO: Pod client-containers-e1532cde-58b3-410a-84ca-6da3f6e9cc63 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Jul 29 15:57:35.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-911" for this suite. 07/29/23 15:57:35.862
{"msg":"PASSED [sig-node] Containers should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]","completed":96,"skipped":1577,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.174 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:57:29.702
    Jul 29 15:57:29.702: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename containers 07/29/23 15:57:29.705
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:57:29.731
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:57:29.736
    [It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:58
    STEP: Creating a pod to test override arguments 07/29/23 15:57:29.74
    Jul 29 15:57:29.758: INFO: Waiting up to 5m0s for pod "client-containers-e1532cde-58b3-410a-84ca-6da3f6e9cc63" in namespace "containers-911" to be "Succeeded or Failed"
    Jul 29 15:57:29.767: INFO: Pod "client-containers-e1532cde-58b3-410a-84ca-6da3f6e9cc63": Phase="Pending", Reason="", readiness=false. Elapsed: 9.128633ms
    Jul 29 15:57:31.773: INFO: Pod "client-containers-e1532cde-58b3-410a-84ca-6da3f6e9cc63": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014688043s
    Jul 29 15:57:33.776: INFO: Pod "client-containers-e1532cde-58b3-410a-84ca-6da3f6e9cc63": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017375728s
    Jul 29 15:57:35.778: INFO: Pod "client-containers-e1532cde-58b3-410a-84ca-6da3f6e9cc63": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.019954198s
    STEP: Saw pod success 07/29/23 15:57:35.779
    Jul 29 15:57:35.779: INFO: Pod "client-containers-e1532cde-58b3-410a-84ca-6da3f6e9cc63" satisfied condition "Succeeded or Failed"
    Jul 29 15:57:35.788: INFO: Trying to get logs from node wa4quivohpee-3 pod client-containers-e1532cde-58b3-410a-84ca-6da3f6e9cc63 container agnhost-container: <nil>
    STEP: delete the pod 07/29/23 15:57:35.812
    Jul 29 15:57:35.847: INFO: Waiting for pod client-containers-e1532cde-58b3-410a-84ca-6da3f6e9cc63 to disappear
    Jul 29 15:57:35.853: INFO: Pod client-containers-e1532cde-58b3-410a-84ca-6da3f6e9cc63 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Jul 29 15:57:35.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-911" for this suite. 07/29/23 15:57:35.862
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:57:35.88
Jul 29 15:57:35.880: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename proxy 07/29/23 15:57:35.883
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:57:35.916
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:57:35.921
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
Jul 29 15:57:35.925: INFO: Creating pod...
Jul 29 15:57:35.939: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-2616" to be "running"
Jul 29 15:57:35.947: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 7.516866ms
Jul 29 15:57:37.954: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.014550392s
Jul 29 15:57:37.954: INFO: Pod "agnhost" satisfied condition "running"
Jul 29 15:57:37.954: INFO: Creating service...
Jul 29 15:57:37.972: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2616/pods/agnhost/proxy/some/path/with/DELETE
Jul 29 15:57:37.995: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jul 29 15:57:37.995: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2616/pods/agnhost/proxy/some/path/with/GET
Jul 29 15:57:38.007: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Jul 29 15:57:38.007: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2616/pods/agnhost/proxy/some/path/with/HEAD
Jul 29 15:57:38.014: INFO: http.Client request:HEAD | StatusCode:200
Jul 29 15:57:38.014: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2616/pods/agnhost/proxy/some/path/with/OPTIONS
Jul 29 15:57:38.025: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jul 29 15:57:38.025: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2616/pods/agnhost/proxy/some/path/with/PATCH
Jul 29 15:57:38.036: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jul 29 15:57:38.036: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2616/pods/agnhost/proxy/some/path/with/POST
Jul 29 15:57:38.042: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jul 29 15:57:38.042: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2616/pods/agnhost/proxy/some/path/with/PUT
Jul 29 15:57:38.048: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jul 29 15:57:38.048: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2616/services/test-service/proxy/some/path/with/DELETE
Jul 29 15:57:38.059: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jul 29 15:57:38.059: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2616/services/test-service/proxy/some/path/with/GET
Jul 29 15:57:38.069: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Jul 29 15:57:38.069: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2616/services/test-service/proxy/some/path/with/HEAD
Jul 29 15:57:38.078: INFO: http.Client request:HEAD | StatusCode:200
Jul 29 15:57:38.078: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2616/services/test-service/proxy/some/path/with/OPTIONS
Jul 29 15:57:38.089: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jul 29 15:57:38.089: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2616/services/test-service/proxy/some/path/with/PATCH
Jul 29 15:57:38.099: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jul 29 15:57:38.100: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2616/services/test-service/proxy/some/path/with/POST
Jul 29 15:57:38.110: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jul 29 15:57:38.111: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2616/services/test-service/proxy/some/path/with/PUT
Jul 29 15:57:38.120: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  test/e2e/framework/framework.go:187
Jul 29 15:57:38.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-2616" for this suite. 07/29/23 15:57:38.13
{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]","completed":97,"skipped":1579,"failed":0}
------------------------------
â€¢ [2.265 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
    test/e2e/network/proxy.go:286

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:57:35.88
    Jul 29 15:57:35.880: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename proxy 07/29/23 15:57:35.883
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:57:35.916
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:57:35.921
    [It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
      test/e2e/network/proxy.go:286
    Jul 29 15:57:35.925: INFO: Creating pod...
    Jul 29 15:57:35.939: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-2616" to be "running"
    Jul 29 15:57:35.947: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 7.516866ms
    Jul 29 15:57:37.954: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.014550392s
    Jul 29 15:57:37.954: INFO: Pod "agnhost" satisfied condition "running"
    Jul 29 15:57:37.954: INFO: Creating service...
    Jul 29 15:57:37.972: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2616/pods/agnhost/proxy/some/path/with/DELETE
    Jul 29 15:57:37.995: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jul 29 15:57:37.995: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2616/pods/agnhost/proxy/some/path/with/GET
    Jul 29 15:57:38.007: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Jul 29 15:57:38.007: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2616/pods/agnhost/proxy/some/path/with/HEAD
    Jul 29 15:57:38.014: INFO: http.Client request:HEAD | StatusCode:200
    Jul 29 15:57:38.014: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2616/pods/agnhost/proxy/some/path/with/OPTIONS
    Jul 29 15:57:38.025: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jul 29 15:57:38.025: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2616/pods/agnhost/proxy/some/path/with/PATCH
    Jul 29 15:57:38.036: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jul 29 15:57:38.036: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2616/pods/agnhost/proxy/some/path/with/POST
    Jul 29 15:57:38.042: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jul 29 15:57:38.042: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2616/pods/agnhost/proxy/some/path/with/PUT
    Jul 29 15:57:38.048: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Jul 29 15:57:38.048: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2616/services/test-service/proxy/some/path/with/DELETE
    Jul 29 15:57:38.059: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jul 29 15:57:38.059: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2616/services/test-service/proxy/some/path/with/GET
    Jul 29 15:57:38.069: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Jul 29 15:57:38.069: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2616/services/test-service/proxy/some/path/with/HEAD
    Jul 29 15:57:38.078: INFO: http.Client request:HEAD | StatusCode:200
    Jul 29 15:57:38.078: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2616/services/test-service/proxy/some/path/with/OPTIONS
    Jul 29 15:57:38.089: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jul 29 15:57:38.089: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2616/services/test-service/proxy/some/path/with/PATCH
    Jul 29 15:57:38.099: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jul 29 15:57:38.100: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2616/services/test-service/proxy/some/path/with/POST
    Jul 29 15:57:38.110: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jul 29 15:57:38.111: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2616/services/test-service/proxy/some/path/with/PUT
    Jul 29 15:57:38.120: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    Jul 29 15:57:38.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-2616" for this suite. 07/29/23 15:57:38.13
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:975
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:57:38.147
Jul 29 15:57:38.147: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename statefulset 07/29/23 15:57:38.149
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:57:38.187
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:57:38.194
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-8510 07/29/23 15:57:38.204
[It] should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:975
STEP: Creating statefulset ss in namespace statefulset-8510 07/29/23 15:57:38.229
Jul 29 15:57:38.249: INFO: Found 0 stateful pods, waiting for 1
Jul 29 15:57:48.257: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label 07/29/23 15:57:48.27
STEP: Getting /status 07/29/23 15:57:48.288
Jul 29 15:57:48.298: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status 07/29/23 15:57:48.298
Jul 29 15:57:48.317: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated 07/29/23 15:57:48.317
Jul 29 15:57:48.321: INFO: Observed &StatefulSet event: ADDED
Jul 29 15:57:48.321: INFO: Found Statefulset ss in namespace statefulset-8510 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jul 29 15:57:48.321: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status 07/29/23 15:57:48.321
Jul 29 15:57:48.321: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jul 29 15:57:48.333: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched 07/29/23 15:57:48.333
Jul 29 15:57:48.337: INFO: Observed &StatefulSet event: ADDED
Jul 29 15:57:48.337: INFO: Observed Statefulset ss in namespace statefulset-8510 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jul 29 15:57:48.338: INFO: Observed &StatefulSet event: MODIFIED
Jul 29 15:57:48.338: INFO: Found Statefulset ss in namespace statefulset-8510 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jul 29 15:57:48.338: INFO: Deleting all statefulset in ns statefulset-8510
Jul 29 15:57:48.343: INFO: Scaling statefulset ss to 0
Jul 29 15:57:58.386: INFO: Waiting for statefulset status.replicas updated to 0
Jul 29 15:57:58.391: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jul 29 15:57:58.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8510" for this suite. 07/29/23 15:57:58.429
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]","completed":98,"skipped":1588,"failed":0}
------------------------------
â€¢ [SLOW TEST] [20.291 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should validate Statefulset Status endpoints [Conformance]
    test/e2e/apps/statefulset.go:975

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:57:38.147
    Jul 29 15:57:38.147: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename statefulset 07/29/23 15:57:38.149
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:57:38.187
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:57:38.194
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-8510 07/29/23 15:57:38.204
    [It] should validate Statefulset Status endpoints [Conformance]
      test/e2e/apps/statefulset.go:975
    STEP: Creating statefulset ss in namespace statefulset-8510 07/29/23 15:57:38.229
    Jul 29 15:57:38.249: INFO: Found 0 stateful pods, waiting for 1
    Jul 29 15:57:48.257: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Patch Statefulset to include a label 07/29/23 15:57:48.27
    STEP: Getting /status 07/29/23 15:57:48.288
    Jul 29 15:57:48.298: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
    STEP: updating the StatefulSet Status 07/29/23 15:57:48.298
    Jul 29 15:57:48.317: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the statefulset status to be updated 07/29/23 15:57:48.317
    Jul 29 15:57:48.321: INFO: Observed &StatefulSet event: ADDED
    Jul 29 15:57:48.321: INFO: Found Statefulset ss in namespace statefulset-8510 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jul 29 15:57:48.321: INFO: Statefulset ss has an updated status
    STEP: patching the Statefulset Status 07/29/23 15:57:48.321
    Jul 29 15:57:48.321: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Jul 29 15:57:48.333: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Statefulset status to be patched 07/29/23 15:57:48.333
    Jul 29 15:57:48.337: INFO: Observed &StatefulSet event: ADDED
    Jul 29 15:57:48.337: INFO: Observed Statefulset ss in namespace statefulset-8510 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jul 29 15:57:48.338: INFO: Observed &StatefulSet event: MODIFIED
    Jul 29 15:57:48.338: INFO: Found Statefulset ss in namespace statefulset-8510 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jul 29 15:57:48.338: INFO: Deleting all statefulset in ns statefulset-8510
    Jul 29 15:57:48.343: INFO: Scaling statefulset ss to 0
    Jul 29 15:57:58.386: INFO: Waiting for statefulset status.replicas updated to 0
    Jul 29 15:57:58.391: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jul 29 15:57:58.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-8510" for this suite. 07/29/23 15:57:58.429
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:346
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:57:58.447
Jul 29 15:57:58.448: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename security-context-test 07/29/23 15:57:58.45
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:57:58.483
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:57:58.488
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:346
Jul 29 15:57:58.507: INFO: Waiting up to 5m0s for pod "busybox-user-65534-5adcf66c-d1e7-413a-bc65-75fa3d0624e2" in namespace "security-context-test-2424" to be "Succeeded or Failed"
Jul 29 15:57:58.514: INFO: Pod "busybox-user-65534-5adcf66c-d1e7-413a-bc65-75fa3d0624e2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.634269ms
Jul 29 15:58:00.521: INFO: Pod "busybox-user-65534-5adcf66c-d1e7-413a-bc65-75fa3d0624e2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013631557s
Jul 29 15:58:02.524: INFO: Pod "busybox-user-65534-5adcf66c-d1e7-413a-bc65-75fa3d0624e2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016626728s
Jul 29 15:58:02.524: INFO: Pod "busybox-user-65534-5adcf66c-d1e7-413a-bc65-75fa3d0624e2" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Jul 29 15:58:02.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-2424" for this suite. 07/29/23 15:58:02.536
{"msg":"PASSED [sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","completed":99,"skipped":1612,"failed":0}
------------------------------
â€¢ [4.101 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a container with runAsUser
  test/e2e/common/node/security_context.go:308
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:346

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:57:58.447
    Jul 29 15:57:58.448: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename security-context-test 07/29/23 15:57:58.45
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:57:58.483
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:57:58.488
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:346
    Jul 29 15:57:58.507: INFO: Waiting up to 5m0s for pod "busybox-user-65534-5adcf66c-d1e7-413a-bc65-75fa3d0624e2" in namespace "security-context-test-2424" to be "Succeeded or Failed"
    Jul 29 15:57:58.514: INFO: Pod "busybox-user-65534-5adcf66c-d1e7-413a-bc65-75fa3d0624e2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.634269ms
    Jul 29 15:58:00.521: INFO: Pod "busybox-user-65534-5adcf66c-d1e7-413a-bc65-75fa3d0624e2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013631557s
    Jul 29 15:58:02.524: INFO: Pod "busybox-user-65534-5adcf66c-d1e7-413a-bc65-75fa3d0624e2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016626728s
    Jul 29 15:58:02.524: INFO: Pod "busybox-user-65534-5adcf66c-d1e7-413a-bc65-75fa3d0624e2" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Jul 29 15:58:02.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-2424" for this suite. 07/29/23 15:58:02.536
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] Proxy version v1
  should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:58:02.552
Jul 29 15:58:02.553: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename proxy 07/29/23 15:58:02.556
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:58:02.589
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:58:02.593
[It] should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
STEP: starting an echo server on multiple ports 07/29/23 15:58:02.619
STEP: creating replication controller proxy-service-gw6m5 in namespace proxy-2078 07/29/23 15:58:02.619
I0729 15:58:02.648222      13 runners.go:193] Created replication controller with name: proxy-service-gw6m5, namespace: proxy-2078, replica count: 1
I0729 15:58:03.702110      13 runners.go:193] proxy-service-gw6m5 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0729 15:58:04.702578      13 runners.go:193] proxy-service-gw6m5 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 29 15:58:04.708: INFO: setup took 2.110716832s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts 07/29/23 15:58:04.708
Jul 29 15:58:04.732: INFO: (0) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:160/proxy/: foo (200; 24.079425ms)
Jul 29 15:58:04.734: INFO: (0) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:160/proxy/: foo (200; 25.085161ms)
Jul 29 15:58:04.734: INFO: (0) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:1080/proxy/rewriteme">test<... (200; 25.50173ms)
Jul 29 15:58:04.734: INFO: (0) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:162/proxy/: bar (200; 25.404068ms)
Jul 29 15:58:04.745: INFO: (0) /api/v1/namespaces/proxy-2078/services/https:proxy-service-gw6m5:tlsportname2/proxy/: tls qux (200; 36.229121ms)
Jul 29 15:58:04.747: INFO: (0) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:462/proxy/: tls qux (200; 38.322462ms)
Jul 29 15:58:04.747: INFO: (0) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:443/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:443/proxy/tlsrewritem... (200; 39.063946ms)
Jul 29 15:58:04.747: INFO: (0) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv/proxy/rewriteme">test</a> (200; 38.692772ms)
Jul 29 15:58:04.747: INFO: (0) /api/v1/namespaces/proxy-2078/services/http:proxy-service-gw6m5:portname2/proxy/: bar (200; 38.439846ms)
Jul 29 15:58:04.753: INFO: (0) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:162/proxy/: bar (200; 44.202598ms)
Jul 29 15:58:04.753: INFO: (0) /api/v1/namespaces/proxy-2078/services/proxy-service-gw6m5:portname1/proxy/: foo (200; 44.085999ms)
Jul 29 15:58:04.753: INFO: (0) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:1080/proxy/rewriteme">... (200; 44.075381ms)
Jul 29 15:58:04.754: INFO: (0) /api/v1/namespaces/proxy-2078/services/proxy-service-gw6m5:portname2/proxy/: bar (200; 44.580522ms)
Jul 29 15:58:04.754: INFO: (0) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:460/proxy/: tls baz (200; 44.416529ms)
Jul 29 15:58:04.754: INFO: (0) /api/v1/namespaces/proxy-2078/services/https:proxy-service-gw6m5:tlsportname1/proxy/: tls baz (200; 45.500955ms)
Jul 29 15:58:04.754: INFO: (0) /api/v1/namespaces/proxy-2078/services/http:proxy-service-gw6m5:portname1/proxy/: foo (200; 44.970713ms)
Jul 29 15:58:04.771: INFO: (1) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:160/proxy/: foo (200; 16.986949ms)
Jul 29 15:58:04.770: INFO: (1) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:1080/proxy/rewriteme">... (200; 15.572862ms)
Jul 29 15:58:04.775: INFO: (1) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:443/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:443/proxy/tlsrewritem... (200; 20.653954ms)
Jul 29 15:58:04.776: INFO: (1) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:1080/proxy/rewriteme">test<... (200; 20.646564ms)
Jul 29 15:58:04.776: INFO: (1) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:162/proxy/: bar (200; 20.789081ms)
Jul 29 15:58:04.778: INFO: (1) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv/proxy/rewriteme">test</a> (200; 23.102951ms)
Jul 29 15:58:04.779: INFO: (1) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:162/proxy/: bar (200; 23.387332ms)
Jul 29 15:58:04.779: INFO: (1) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:160/proxy/: foo (200; 23.673669ms)
Jul 29 15:58:04.779: INFO: (1) /api/v1/namespaces/proxy-2078/services/proxy-service-gw6m5:portname1/proxy/: foo (200; 24.084074ms)
Jul 29 15:58:04.779: INFO: (1) /api/v1/namespaces/proxy-2078/services/http:proxy-service-gw6m5:portname1/proxy/: foo (200; 23.513098ms)
Jul 29 15:58:04.779: INFO: (1) /api/v1/namespaces/proxy-2078/services/http:proxy-service-gw6m5:portname2/proxy/: bar (200; 23.29095ms)
Jul 29 15:58:04.779: INFO: (1) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:462/proxy/: tls qux (200; 23.649749ms)
Jul 29 15:58:04.780: INFO: (1) /api/v1/namespaces/proxy-2078/services/proxy-service-gw6m5:portname2/proxy/: bar (200; 24.20574ms)
Jul 29 15:58:04.781: INFO: (1) /api/v1/namespaces/proxy-2078/services/https:proxy-service-gw6m5:tlsportname2/proxy/: tls qux (200; 26.324544ms)
Jul 29 15:58:04.783: INFO: (1) /api/v1/namespaces/proxy-2078/services/https:proxy-service-gw6m5:tlsportname1/proxy/: tls baz (200; 27.674305ms)
Jul 29 15:58:04.784: INFO: (1) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:460/proxy/: tls baz (200; 28.717402ms)
Jul 29 15:58:04.797: INFO: (2) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:1080/proxy/rewriteme">test<... (200; 12.646711ms)
Jul 29 15:58:04.797: INFO: (2) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:162/proxy/: bar (200; 13.05896ms)
Jul 29 15:58:04.802: INFO: (2) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv/proxy/rewriteme">test</a> (200; 18.253314ms)
Jul 29 15:58:04.802: INFO: (2) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:160/proxy/: foo (200; 17.125555ms)
Jul 29 15:58:04.803: INFO: (2) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:162/proxy/: bar (200; 18.472069ms)
Jul 29 15:58:04.805: INFO: (2) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:443/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:443/proxy/tlsrewritem... (200; 20.9655ms)
Jul 29 15:58:04.811: INFO: (2) /api/v1/namespaces/proxy-2078/services/https:proxy-service-gw6m5:tlsportname2/proxy/: tls qux (200; 27.009226ms)
Jul 29 15:58:04.811: INFO: (2) /api/v1/namespaces/proxy-2078/services/https:proxy-service-gw6m5:tlsportname1/proxy/: tls baz (200; 25.758279ms)
Jul 29 15:58:04.813: INFO: (2) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:1080/proxy/rewriteme">... (200; 28.017614ms)
Jul 29 15:58:04.814: INFO: (2) /api/v1/namespaces/proxy-2078/services/http:proxy-service-gw6m5:portname2/proxy/: bar (200; 29.712757ms)
Jul 29 15:58:04.814: INFO: (2) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:160/proxy/: foo (200; 28.821283ms)
Jul 29 15:58:04.814: INFO: (2) /api/v1/namespaces/proxy-2078/services/http:proxy-service-gw6m5:portname1/proxy/: foo (200; 29.363801ms)
Jul 29 15:58:04.815: INFO: (2) /api/v1/namespaces/proxy-2078/services/proxy-service-gw6m5:portname2/proxy/: bar (200; 30.647712ms)
Jul 29 15:58:04.816: INFO: (2) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:460/proxy/: tls baz (200; 31.090265ms)
Jul 29 15:58:04.816: INFO: (2) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:462/proxy/: tls qux (200; 31.175159ms)
Jul 29 15:58:04.819: INFO: (2) /api/v1/namespaces/proxy-2078/services/proxy-service-gw6m5:portname1/proxy/: foo (200; 34.21689ms)
Jul 29 15:58:04.840: INFO: (3) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:162/proxy/: bar (200; 20.766493ms)
Jul 29 15:58:04.840: INFO: (3) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:160/proxy/: foo (200; 21.13073ms)
Jul 29 15:58:04.843: INFO: (3) /api/v1/namespaces/proxy-2078/services/http:proxy-service-gw6m5:portname1/proxy/: foo (200; 23.663293ms)
Jul 29 15:58:04.843: INFO: (3) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:1080/proxy/rewriteme">test<... (200; 23.18723ms)
Jul 29 15:58:04.843: INFO: (3) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv/proxy/rewriteme">test</a> (200; 23.090704ms)
Jul 29 15:58:04.843: INFO: (3) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:462/proxy/: tls qux (200; 23.278027ms)
Jul 29 15:58:04.843: INFO: (3) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:460/proxy/: tls baz (200; 23.583151ms)
Jul 29 15:58:04.844: INFO: (3) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:443/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:443/proxy/tlsrewritem... (200; 23.160526ms)
Jul 29 15:58:04.846: INFO: (3) /api/v1/namespaces/proxy-2078/services/http:proxy-service-gw6m5:portname2/proxy/: bar (200; 25.898934ms)
Jul 29 15:58:04.846: INFO: (3) /api/v1/namespaces/proxy-2078/services/proxy-service-gw6m5:portname2/proxy/: bar (200; 26.458155ms)
Jul 29 15:58:04.848: INFO: (3) /api/v1/namespaces/proxy-2078/services/https:proxy-service-gw6m5:tlsportname1/proxy/: tls baz (200; 28.461018ms)
Jul 29 15:58:04.849: INFO: (3) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:162/proxy/: bar (200; 29.083301ms)
Jul 29 15:58:04.850: INFO: (3) /api/v1/namespaces/proxy-2078/services/https:proxy-service-gw6m5:tlsportname2/proxy/: tls qux (200; 29.409446ms)
Jul 29 15:58:04.850: INFO: (3) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:160/proxy/: foo (200; 29.285984ms)
Jul 29 15:58:04.853: INFO: (3) /api/v1/namespaces/proxy-2078/services/proxy-service-gw6m5:portname1/proxy/: foo (200; 32.876703ms)
Jul 29 15:58:04.855: INFO: (3) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:1080/proxy/rewriteme">... (200; 34.401924ms)
Jul 29 15:58:04.871: INFO: (4) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:1080/proxy/rewriteme">... (200; 14.071358ms)
Jul 29 15:58:04.871: INFO: (4) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:162/proxy/: bar (200; 14.397265ms)
Jul 29 15:58:04.872: INFO: (4) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:160/proxy/: foo (200; 15.327519ms)
Jul 29 15:58:04.872: INFO: (4) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:460/proxy/: tls baz (200; 15.179205ms)
Jul 29 15:58:04.872: INFO: (4) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:443/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:443/proxy/tlsrewritem... (200; 16.403323ms)
Jul 29 15:58:04.874: INFO: (4) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:160/proxy/: foo (200; 16.875593ms)
Jul 29 15:58:04.876: INFO: (4) /api/v1/namespaces/proxy-2078/services/http:proxy-service-gw6m5:portname2/proxy/: bar (200; 20.442691ms)
Jul 29 15:58:04.876: INFO: (4) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:462/proxy/: tls qux (200; 19.181794ms)
Jul 29 15:58:04.877: INFO: (4) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv/proxy/rewriteme">test</a> (200; 21.77372ms)
Jul 29 15:58:04.877: INFO: (4) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:1080/proxy/rewriteme">test<... (200; 19.899768ms)
Jul 29 15:58:04.881: INFO: (4) /api/v1/namespaces/proxy-2078/services/http:proxy-service-gw6m5:portname1/proxy/: foo (200; 24.798055ms)
Jul 29 15:58:04.881: INFO: (4) /api/v1/namespaces/proxy-2078/services/https:proxy-service-gw6m5:tlsportname1/proxy/: tls baz (200; 23.931289ms)
Jul 29 15:58:04.881: INFO: (4) /api/v1/namespaces/proxy-2078/services/https:proxy-service-gw6m5:tlsportname2/proxy/: tls qux (200; 23.957175ms)
Jul 29 15:58:04.881: INFO: (4) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:162/proxy/: bar (200; 24.301398ms)
Jul 29 15:58:04.886: INFO: (4) /api/v1/namespaces/proxy-2078/services/proxy-service-gw6m5:portname2/proxy/: bar (200; 29.956814ms)
Jul 29 15:58:04.886: INFO: (4) /api/v1/namespaces/proxy-2078/services/proxy-service-gw6m5:portname1/proxy/: foo (200; 31.290244ms)
Jul 29 15:58:04.901: INFO: (5) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:162/proxy/: bar (200; 13.76348ms)
Jul 29 15:58:04.902: INFO: (5) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:160/proxy/: foo (200; 14.990786ms)
Jul 29 15:58:04.902: INFO: (5) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:443/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:443/proxy/tlsrewritem... (200; 14.919705ms)
Jul 29 15:58:04.902: INFO: (5) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:160/proxy/: foo (200; 14.785823ms)
Jul 29 15:58:04.903: INFO: (5) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:1080/proxy/rewriteme">test<... (200; 15.918327ms)
Jul 29 15:58:04.903: INFO: (5) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:460/proxy/: tls baz (200; 15.630552ms)
Jul 29 15:58:04.904: INFO: (5) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv/proxy/rewriteme">test</a> (200; 16.277146ms)
Jul 29 15:58:04.905: INFO: (5) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:1080/proxy/rewriteme">... (200; 17.552005ms)
Jul 29 15:58:04.905: INFO: (5) /api/v1/namespaces/proxy-2078/services/http:proxy-service-gw6m5:portname2/proxy/: bar (200; 18.914879ms)
Jul 29 15:58:04.909: INFO: (5) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:162/proxy/: bar (200; 21.626819ms)
Jul 29 15:58:04.909: INFO: (5) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:462/proxy/: tls qux (200; 21.535613ms)
Jul 29 15:58:04.910: INFO: (5) /api/v1/namespaces/proxy-2078/services/http:proxy-service-gw6m5:portname1/proxy/: foo (200; 21.997156ms)
Jul 29 15:58:04.910: INFO: (5) /api/v1/namespaces/proxy-2078/services/https:proxy-service-gw6m5:tlsportname2/proxy/: tls qux (200; 22.258919ms)
Jul 29 15:58:04.910: INFO: (5) /api/v1/namespaces/proxy-2078/services/proxy-service-gw6m5:portname2/proxy/: bar (200; 22.757479ms)
Jul 29 15:58:04.910: INFO: (5) /api/v1/namespaces/proxy-2078/services/https:proxy-service-gw6m5:tlsportname1/proxy/: tls baz (200; 22.456292ms)
Jul 29 15:58:04.912: INFO: (5) /api/v1/namespaces/proxy-2078/services/proxy-service-gw6m5:portname1/proxy/: foo (200; 25.147007ms)
Jul 29 15:58:04.921: INFO: (6) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv/proxy/rewriteme">test</a> (200; 9.056037ms)
Jul 29 15:58:04.926: INFO: (6) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:162/proxy/: bar (200; 12.818104ms)
Jul 29 15:58:04.926: INFO: (6) /api/v1/namespaces/proxy-2078/services/proxy-service-gw6m5:portname1/proxy/: foo (200; 13.757142ms)
Jul 29 15:58:04.929: INFO: (6) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:160/proxy/: foo (200; 15.78379ms)
Jul 29 15:58:04.931: INFO: (6) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:160/proxy/: foo (200; 17.526904ms)
Jul 29 15:58:04.931: INFO: (6) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:162/proxy/: bar (200; 18.010614ms)
Jul 29 15:58:04.932: INFO: (6) /api/v1/namespaces/proxy-2078/services/http:proxy-service-gw6m5:portname2/proxy/: bar (200; 18.979035ms)
Jul 29 15:58:04.932: INFO: (6) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:1080/proxy/rewriteme">... (200; 18.894722ms)
Jul 29 15:58:04.932: INFO: (6) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:462/proxy/: tls qux (200; 18.806001ms)
Jul 29 15:58:04.932: INFO: (6) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:460/proxy/: tls baz (200; 18.300286ms)
Jul 29 15:58:04.932: INFO: (6) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:1080/proxy/rewriteme">test<... (200; 18.820681ms)
Jul 29 15:58:04.937: INFO: (6) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:443/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:443/proxy/tlsrewritem... (200; 24.210027ms)
Jul 29 15:58:04.938: INFO: (6) /api/v1/namespaces/proxy-2078/services/proxy-service-gw6m5:portname2/proxy/: bar (200; 24.700439ms)
Jul 29 15:58:04.940: INFO: (6) /api/v1/namespaces/proxy-2078/services/http:proxy-service-gw6m5:portname1/proxy/: foo (200; 26.376995ms)
Jul 29 15:58:04.941: INFO: (6) /api/v1/namespaces/proxy-2078/services/https:proxy-service-gw6m5:tlsportname2/proxy/: tls qux (200; 27.916892ms)
Jul 29 15:58:04.941: INFO: (6) /api/v1/namespaces/proxy-2078/services/https:proxy-service-gw6m5:tlsportname1/proxy/: tls baz (200; 27.558238ms)
Jul 29 15:58:04.955: INFO: (7) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:160/proxy/: foo (200; 13.000741ms)
Jul 29 15:58:04.956: INFO: (7) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:1080/proxy/rewriteme">test<... (200; 14.509089ms)
Jul 29 15:58:04.957: INFO: (7) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:1080/proxy/rewriteme">... (200; 15.025525ms)
Jul 29 15:58:04.957: INFO: (7) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:462/proxy/: tls qux (200; 15.464205ms)
Jul 29 15:58:04.959: INFO: (7) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv/proxy/rewriteme">test</a> (200; 16.761302ms)
Jul 29 15:58:04.960: INFO: (7) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:162/proxy/: bar (200; 18.524831ms)
Jul 29 15:58:04.960: INFO: (7) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:443/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:443/proxy/tlsrewritem... (200; 17.167578ms)
Jul 29 15:58:04.960: INFO: (7) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:460/proxy/: tls baz (200; 17.6487ms)
Jul 29 15:58:04.961: INFO: (7) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:162/proxy/: bar (200; 18.7453ms)
Jul 29 15:58:04.962: INFO: (7) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:160/proxy/: foo (200; 19.334105ms)
Jul 29 15:58:04.962: INFO: (7) /api/v1/namespaces/proxy-2078/services/https:proxy-service-gw6m5:tlsportname1/proxy/: tls baz (200; 20.5303ms)
Jul 29 15:58:04.964: INFO: (7) /api/v1/namespaces/proxy-2078/services/proxy-service-gw6m5:portname2/proxy/: bar (200; 21.350354ms)
Jul 29 15:58:04.965: INFO: (7) /api/v1/namespaces/proxy-2078/services/https:proxy-service-gw6m5:tlsportname2/proxy/: tls qux (200; 22.175027ms)
Jul 29 15:58:04.965: INFO: (7) /api/v1/namespaces/proxy-2078/services/proxy-service-gw6m5:portname1/proxy/: foo (200; 22.255825ms)
Jul 29 15:58:04.965: INFO: (7) /api/v1/namespaces/proxy-2078/services/http:proxy-service-gw6m5:portname1/proxy/: foo (200; 23.738499ms)
Jul 29 15:58:04.966: INFO: (7) /api/v1/namespaces/proxy-2078/services/http:proxy-service-gw6m5:portname2/proxy/: bar (200; 22.7611ms)
Jul 29 15:58:04.997: INFO: (8) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:160/proxy/: foo (200; 30.630324ms)
Jul 29 15:58:05.015: INFO: (8) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:162/proxy/: bar (200; 48.215404ms)
Jul 29 15:58:05.022: INFO: (8) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:1080/proxy/rewriteme">test<... (200; 55.4835ms)
Jul 29 15:58:05.022: INFO: (8) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:460/proxy/: tls baz (200; 55.501659ms)
Jul 29 15:58:05.022: INFO: (8) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:1080/proxy/rewriteme">... (200; 55.080261ms)
Jul 29 15:58:05.023: INFO: (8) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:443/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:443/proxy/tlsrewritem... (200; 56.335743ms)
Jul 29 15:58:05.023: INFO: (8) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:462/proxy/: tls qux (200; 56.22062ms)
Jul 29 15:58:05.027: INFO: (8) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:162/proxy/: bar (200; 60.360524ms)
Jul 29 15:58:05.027: INFO: (8) /api/v1/namespaces/proxy-2078/services/proxy-service-gw6m5:portname2/proxy/: bar (200; 61.104314ms)
Jul 29 15:58:05.027: INFO: (8) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:160/proxy/: foo (200; 60.989314ms)
Jul 29 15:58:05.038: INFO: (8) /api/v1/namespaces/proxy-2078/services/https:proxy-service-gw6m5:tlsportname2/proxy/: tls qux (200; 71.329481ms)
Jul 29 15:58:05.038: INFO: (8) /api/v1/namespaces/proxy-2078/services/https:proxy-service-gw6m5:tlsportname1/proxy/: tls baz (200; 72.108965ms)
Jul 29 15:58:05.038: INFO: (8) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv/proxy/rewriteme">test</a> (200; 71.422492ms)
Jul 29 15:58:05.038: INFO: (8) /api/v1/namespaces/proxy-2078/services/proxy-service-gw6m5:portname1/proxy/: foo (200; 72.003899ms)
Jul 29 15:58:05.039: INFO: (8) /api/v1/namespaces/proxy-2078/services/http:proxy-service-gw6m5:portname1/proxy/: foo (200; 71.958332ms)
Jul 29 15:58:05.039: INFO: (8) /api/v1/namespaces/proxy-2078/services/http:proxy-service-gw6m5:portname2/proxy/: bar (200; 72.183422ms)
Jul 29 15:58:05.061: INFO: (9) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:1080/proxy/rewriteme">... (200; 22.095103ms)
Jul 29 15:58:05.082: INFO: (9) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:460/proxy/: tls baz (200; 42.53584ms)
Jul 29 15:58:05.082: INFO: (9) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:160/proxy/: foo (200; 42.791774ms)
Jul 29 15:58:05.082: INFO: (9) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv/proxy/rewriteme">test</a> (200; 42.428418ms)
Jul 29 15:58:05.091: INFO: (9) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:160/proxy/: foo (200; 51.467724ms)
Jul 29 15:58:05.097: INFO: (9) /api/v1/namespaces/proxy-2078/services/https:proxy-service-gw6m5:tlsportname2/proxy/: tls qux (200; 56.354255ms)
Jul 29 15:58:05.098: INFO: (9) /api/v1/namespaces/proxy-2078/services/proxy-service-gw6m5:portname2/proxy/: bar (200; 58.023588ms)
Jul 29 15:58:05.098: INFO: (9) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:1080/proxy/rewriteme">test<... (200; 58.610936ms)
Jul 29 15:58:05.099: INFO: (9) /api/v1/namespaces/proxy-2078/services/http:proxy-service-gw6m5:portname1/proxy/: foo (200; 59.308558ms)
Jul 29 15:58:05.099: INFO: (9) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:162/proxy/: bar (200; 58.383891ms)
Jul 29 15:58:05.099: INFO: (9) /api/v1/namespaces/proxy-2078/services/proxy-service-gw6m5:portname1/proxy/: foo (200; 59.569279ms)
Jul 29 15:58:05.099: INFO: (9) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:162/proxy/: bar (200; 58.818078ms)
Jul 29 15:58:05.101: INFO: (9) /api/v1/namespaces/proxy-2078/services/http:proxy-service-gw6m5:portname2/proxy/: bar (200; 60.170494ms)
Jul 29 15:58:05.102: INFO: (9) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:443/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:443/proxy/tlsrewritem... (200; 60.869577ms)
Jul 29 15:58:05.109: INFO: (9) /api/v1/namespaces/proxy-2078/services/https:proxy-service-gw6m5:tlsportname1/proxy/: tls baz (200; 69.416193ms)
Jul 29 15:58:05.110: INFO: (9) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:462/proxy/: tls qux (200; 69.636528ms)
Jul 29 15:58:05.158: INFO: (10) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:460/proxy/: tls baz (200; 48.526028ms)
Jul 29 15:58:05.186: INFO: (10) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:1080/proxy/rewriteme">... (200; 73.7426ms)
Jul 29 15:58:05.206: INFO: (10) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:160/proxy/: foo (200; 93.232489ms)
Jul 29 15:58:05.206: INFO: (10) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:160/proxy/: foo (200; 93.890575ms)
Jul 29 15:58:05.207: INFO: (10) /api/v1/namespaces/proxy-2078/services/proxy-service-gw6m5:portname2/proxy/: bar (200; 95.281384ms)
Jul 29 15:58:05.207: INFO: (10) /api/v1/namespaces/proxy-2078/services/https:proxy-service-gw6m5:tlsportname1/proxy/: tls baz (200; 96.783597ms)
Jul 29 15:58:05.207: INFO: (10) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:443/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:443/proxy/tlsrewritem... (200; 95.229541ms)
Jul 29 15:58:05.207: INFO: (10) /api/v1/namespaces/proxy-2078/services/http:proxy-service-gw6m5:portname1/proxy/: foo (200; 95.775937ms)
Jul 29 15:58:05.207: INFO: (10) /api/v1/namespaces/proxy-2078/services/http:proxy-service-gw6m5:portname2/proxy/: bar (200; 95.600288ms)
Jul 29 15:58:05.207: INFO: (10) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:162/proxy/: bar (200; 96.813391ms)
Jul 29 15:58:05.207: INFO: (10) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:1080/proxy/rewriteme">test<... (200; 97.154523ms)
Jul 29 15:58:05.207: INFO: (10) /api/v1/namespaces/proxy-2078/services/https:proxy-service-gw6m5:tlsportname2/proxy/: tls qux (200; 96.668168ms)
Jul 29 15:58:05.208: INFO: (10) /api/v1/namespaces/proxy-2078/services/proxy-service-gw6m5:portname1/proxy/: foo (200; 95.699709ms)
Jul 29 15:58:05.208: INFO: (10) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv/proxy/rewriteme">test</a> (200; 96.782545ms)
Jul 29 15:58:05.208: INFO: (10) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:462/proxy/: tls qux (200; 97.248586ms)
Jul 29 15:58:05.208: INFO: (10) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:162/proxy/: bar (200; 96.501664ms)
Jul 29 15:58:05.230: INFO: (11) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:1080/proxy/rewriteme">... (200; 20.348773ms)
Jul 29 15:58:05.230: INFO: (11) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:162/proxy/: bar (200; 20.900454ms)
Jul 29 15:58:05.231: INFO: (11) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv/proxy/rewriteme">test</a> (200; 21.652859ms)
Jul 29 15:58:05.231: INFO: (11) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:160/proxy/: foo (200; 22.046937ms)
Jul 29 15:58:05.231: INFO: (11) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:1080/proxy/rewriteme">test<... (200; 22.697689ms)
Jul 29 15:58:05.231: INFO: (11) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:162/proxy/: bar (200; 22.5008ms)
Jul 29 15:58:05.231: INFO: (11) /api/v1/namespaces/proxy-2078/services/http:proxy-service-gw6m5:portname2/proxy/: bar (200; 22.895472ms)
Jul 29 15:58:05.235: INFO: (11) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:443/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:443/proxy/tlsrewritem... (200; 26.234914ms)
Jul 29 15:58:05.235: INFO: (11) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:460/proxy/: tls baz (200; 26.075388ms)
Jul 29 15:58:05.235: INFO: (11) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:462/proxy/: tls qux (200; 26.045708ms)
Jul 29 15:58:05.238: INFO: (11) /api/v1/namespaces/proxy-2078/services/http:proxy-service-gw6m5:portname1/proxy/: foo (200; 28.855851ms)
Jul 29 15:58:05.239: INFO: (11) /api/v1/namespaces/proxy-2078/services/proxy-service-gw6m5:portname1/proxy/: foo (200; 29.314193ms)
Jul 29 15:58:05.239: INFO: (11) /api/v1/namespaces/proxy-2078/services/proxy-service-gw6m5:portname2/proxy/: bar (200; 29.520478ms)
Jul 29 15:58:05.239: INFO: (11) /api/v1/namespaces/proxy-2078/services/https:proxy-service-gw6m5:tlsportname2/proxy/: tls qux (200; 29.878603ms)
Jul 29 15:58:05.239: INFO: (11) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:160/proxy/: foo (200; 29.434663ms)
Jul 29 15:58:05.246: INFO: (11) /api/v1/namespaces/proxy-2078/services/https:proxy-service-gw6m5:tlsportname1/proxy/: tls baz (200; 37.013617ms)
Jul 29 15:58:05.258: INFO: (12) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:162/proxy/: bar (200; 11.549556ms)
Jul 29 15:58:05.260: INFO: (12) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:160/proxy/: foo (200; 12.953066ms)
Jul 29 15:58:05.262: INFO: (12) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv/proxy/rewriteme">test</a> (200; 14.535588ms)
Jul 29 15:58:05.268: INFO: (12) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:1080/proxy/rewriteme">... (200; 20.324102ms)
Jul 29 15:58:05.268: INFO: (12) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:160/proxy/: foo (200; 20.356966ms)
Jul 29 15:58:05.268: INFO: (12) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:1080/proxy/rewriteme">test<... (200; 21.049061ms)
Jul 29 15:58:05.270: INFO: (12) /api/v1/namespaces/proxy-2078/services/proxy-service-gw6m5:portname2/proxy/: bar (200; 22.426016ms)
Jul 29 15:58:05.270: INFO: (12) /api/v1/namespaces/proxy-2078/services/https:proxy-service-gw6m5:tlsportname2/proxy/: tls qux (200; 22.421589ms)
Jul 29 15:58:05.270: INFO: (12) /api/v1/namespaces/proxy-2078/services/http:proxy-service-gw6m5:portname1/proxy/: foo (200; 23.048119ms)
Jul 29 15:58:05.270: INFO: (12) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:162/proxy/: bar (200; 22.7618ms)
Jul 29 15:58:05.270: INFO: (12) /api/v1/namespaces/proxy-2078/services/https:proxy-service-gw6m5:tlsportname1/proxy/: tls baz (200; 23.182947ms)
Jul 29 15:58:05.270: INFO: (12) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:460/proxy/: tls baz (200; 22.790557ms)
Jul 29 15:58:05.270: INFO: (12) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:462/proxy/: tls qux (200; 23.024507ms)
Jul 29 15:58:05.271: INFO: (12) /api/v1/namespaces/proxy-2078/services/http:proxy-service-gw6m5:portname2/proxy/: bar (200; 23.540437ms)
Jul 29 15:58:05.271: INFO: (12) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:443/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:443/proxy/tlsrewritem... (200; 23.559761ms)
Jul 29 15:58:05.271: INFO: (12) /api/v1/namespaces/proxy-2078/services/proxy-service-gw6m5:portname1/proxy/: foo (200; 23.666357ms)
Jul 29 15:58:05.282: INFO: (13) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:462/proxy/: tls qux (200; 10.685838ms)
Jul 29 15:58:05.284: INFO: (13) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:1080/proxy/rewriteme">... (200; 11.901279ms)
Jul 29 15:58:05.288: INFO: (13) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:160/proxy/: foo (200; 15.755631ms)
Jul 29 15:58:05.288: INFO: (13) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv/proxy/rewriteme">test</a> (200; 16.029205ms)
Jul 29 15:58:05.293: INFO: (13) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:1080/proxy/rewriteme">test<... (200; 20.594057ms)
Jul 29 15:58:05.294: INFO: (13) /api/v1/namespaces/proxy-2078/services/proxy-service-gw6m5:portname1/proxy/: foo (200; 21.625529ms)
Jul 29 15:58:05.295: INFO: (13) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:443/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:443/proxy/tlsrewritem... (200; 22.231737ms)
Jul 29 15:58:05.295: INFO: (13) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:162/proxy/: bar (200; 22.720503ms)
Jul 29 15:58:05.295: INFO: (13) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:162/proxy/: bar (200; 22.122479ms)
Jul 29 15:58:05.295: INFO: (13) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:460/proxy/: tls baz (200; 22.204764ms)
Jul 29 15:58:05.295: INFO: (13) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:160/proxy/: foo (200; 22.281944ms)
Jul 29 15:58:05.295: INFO: (13) /api/v1/namespaces/proxy-2078/services/https:proxy-service-gw6m5:tlsportname2/proxy/: tls qux (200; 23.162948ms)
Jul 29 15:58:05.297: INFO: (13) /api/v1/namespaces/proxy-2078/services/http:proxy-service-gw6m5:portname1/proxy/: foo (200; 25.356983ms)
Jul 29 15:58:05.298: INFO: (13) /api/v1/namespaces/proxy-2078/services/proxy-service-gw6m5:portname2/proxy/: bar (200; 25.581933ms)
Jul 29 15:58:05.298: INFO: (13) /api/v1/namespaces/proxy-2078/services/http:proxy-service-gw6m5:portname2/proxy/: bar (200; 25.587115ms)
Jul 29 15:58:05.298: INFO: (13) /api/v1/namespaces/proxy-2078/services/https:proxy-service-gw6m5:tlsportname1/proxy/: tls baz (200; 25.7133ms)
Jul 29 15:58:05.317: INFO: (14) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:1080/proxy/rewriteme">test<... (200; 17.801839ms)
Jul 29 15:58:05.317: INFO: (14) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:1080/proxy/rewriteme">... (200; 18.773813ms)
Jul 29 15:58:05.322: INFO: (14) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:160/proxy/: foo (200; 20.080654ms)
Jul 29 15:58:05.325: INFO: (14) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:162/proxy/: bar (200; 25.253978ms)
Jul 29 15:58:05.325: INFO: (14) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv/proxy/rewriteme">test</a> (200; 24.374075ms)
Jul 29 15:58:05.325: INFO: (14) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:162/proxy/: bar (200; 26.319849ms)
Jul 29 15:58:05.325: INFO: (14) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:443/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:443/proxy/tlsrewritem... (200; 23.620531ms)
Jul 29 15:58:05.325: INFO: (14) /api/v1/namespaces/proxy-2078/services/https:proxy-service-gw6m5:tlsportname2/proxy/: tls qux (200; 25.474706ms)
Jul 29 15:58:05.327: INFO: (14) /api/v1/namespaces/proxy-2078/services/http:proxy-service-gw6m5:portname1/proxy/: foo (200; 25.815599ms)
Jul 29 15:58:05.327: INFO: (14) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:160/proxy/: foo (200; 25.603877ms)
Jul 29 15:58:05.327: INFO: (14) /api/v1/namespaces/proxy-2078/services/http:proxy-service-gw6m5:portname2/proxy/: bar (200; 26.235375ms)
Jul 29 15:58:05.327: INFO: (14) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:460/proxy/: tls baz (200; 25.640146ms)
Jul 29 15:58:05.327: INFO: (14) /api/v1/namespaces/proxy-2078/services/https:proxy-service-gw6m5:tlsportname1/proxy/: tls baz (200; 28.475488ms)
Jul 29 15:58:05.328: INFO: (14) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:462/proxy/: tls qux (200; 28.741485ms)
Jul 29 15:58:05.328: INFO: (14) /api/v1/namespaces/proxy-2078/services/proxy-service-gw6m5:portname2/proxy/: bar (200; 27.011676ms)
Jul 29 15:58:05.331: INFO: (14) /api/v1/namespaces/proxy-2078/services/proxy-service-gw6m5:portname1/proxy/: foo (200; 28.606953ms)
Jul 29 15:58:05.355: INFO: (15) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:1080/proxy/rewriteme">test<... (200; 24.314678ms)
Jul 29 15:58:05.356: INFO: (15) /api/v1/namespaces/proxy-2078/services/http:proxy-service-gw6m5:portname2/proxy/: bar (200; 23.826392ms)
Jul 29 15:58:05.358: INFO: (15) /api/v1/namespaces/proxy-2078/services/https:proxy-service-gw6m5:tlsportname1/proxy/: tls baz (200; 25.822565ms)
Jul 29 15:58:05.358: INFO: (15) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:160/proxy/: foo (200; 26.725716ms)
Jul 29 15:58:05.358: INFO: (15) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:462/proxy/: tls qux (200; 26.303432ms)
Jul 29 15:58:05.358: INFO: (15) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv/proxy/rewriteme">test</a> (200; 25.908756ms)
Jul 29 15:58:05.358: INFO: (15) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:162/proxy/: bar (200; 26.539263ms)
Jul 29 15:58:05.358: INFO: (15) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:1080/proxy/rewriteme">... (200; 26.898279ms)
Jul 29 15:58:05.359: INFO: (15) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:460/proxy/: tls baz (200; 27.237549ms)
Jul 29 15:58:05.359: INFO: (15) /api/v1/namespaces/proxy-2078/services/proxy-service-gw6m5:portname2/proxy/: bar (200; 26.570582ms)
Jul 29 15:58:05.362: INFO: (15) /api/v1/namespaces/proxy-2078/services/http:proxy-service-gw6m5:portname1/proxy/: foo (200; 30.163453ms)
Jul 29 15:58:05.363: INFO: (15) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:160/proxy/: foo (200; 31.698448ms)
Jul 29 15:58:05.363: INFO: (15) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:162/proxy/: bar (200; 31.439659ms)
Jul 29 15:58:05.363: INFO: (15) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:443/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:443/proxy/tlsrewritem... (200; 30.950569ms)
Jul 29 15:58:05.364: INFO: (15) /api/v1/namespaces/proxy-2078/services/https:proxy-service-gw6m5:tlsportname2/proxy/: tls qux (200; 32.05307ms)
Jul 29 15:58:05.367: INFO: (15) /api/v1/namespaces/proxy-2078/services/proxy-service-gw6m5:portname1/proxy/: foo (200; 35.913893ms)
Jul 29 15:58:05.387: INFO: (16) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:162/proxy/: bar (200; 18.864407ms)
Jul 29 15:58:05.390: INFO: (16) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:443/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:443/proxy/tlsrewritem... (200; 22.438018ms)
Jul 29 15:58:05.390: INFO: (16) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:462/proxy/: tls qux (200; 22.364914ms)
Jul 29 15:58:05.390: INFO: (16) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:160/proxy/: foo (200; 21.636661ms)
Jul 29 15:58:05.393: INFO: (16) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:1080/proxy/rewriteme">... (200; 23.679063ms)
Jul 29 15:58:05.393: INFO: (16) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:1080/proxy/rewriteme">test<... (200; 23.956791ms)
Jul 29 15:58:05.393: INFO: (16) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:162/proxy/: bar (200; 24.942859ms)
Jul 29 15:58:05.393: INFO: (16) /api/v1/namespaces/proxy-2078/services/proxy-service-gw6m5:portname2/proxy/: bar (200; 24.904954ms)
Jul 29 15:58:05.394: INFO: (16) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:160/proxy/: foo (200; 24.37487ms)
Jul 29 15:58:05.394: INFO: (16) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv/proxy/rewriteme">test</a> (200; 25.354731ms)
Jul 29 15:58:05.394: INFO: (16) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:460/proxy/: tls baz (200; 24.324425ms)
Jul 29 15:58:05.395: INFO: (16) /api/v1/namespaces/proxy-2078/services/http:proxy-service-gw6m5:portname1/proxy/: foo (200; 26.83308ms)
Jul 29 15:58:05.395: INFO: (16) /api/v1/namespaces/proxy-2078/services/http:proxy-service-gw6m5:portname2/proxy/: bar (200; 26.707645ms)
Jul 29 15:58:05.398: INFO: (16) /api/v1/namespaces/proxy-2078/services/https:proxy-service-gw6m5:tlsportname2/proxy/: tls qux (200; 29.859652ms)
Jul 29 15:58:05.399: INFO: (16) /api/v1/namespaces/proxy-2078/services/proxy-service-gw6m5:portname1/proxy/: foo (200; 29.790232ms)
Jul 29 15:58:05.399: INFO: (16) /api/v1/namespaces/proxy-2078/services/https:proxy-service-gw6m5:tlsportname1/proxy/: tls baz (200; 29.829125ms)
Jul 29 15:58:05.415: INFO: (17) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:160/proxy/: foo (200; 14.604107ms)
Jul 29 15:58:05.415: INFO: (17) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:160/proxy/: foo (200; 14.660062ms)
Jul 29 15:58:05.415: INFO: (17) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:162/proxy/: bar (200; 15.894087ms)
Jul 29 15:58:05.415: INFO: (17) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:1080/proxy/rewriteme">... (200; 15.592819ms)
Jul 29 15:58:05.416: INFO: (17) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:162/proxy/: bar (200; 12.172462ms)
Jul 29 15:58:05.418: INFO: (17) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:443/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:443/proxy/tlsrewritem... (200; 18.400411ms)
Jul 29 15:58:05.418: INFO: (17) /api/v1/namespaces/proxy-2078/services/proxy-service-gw6m5:portname1/proxy/: foo (200; 18.37526ms)
Jul 29 15:58:05.418: INFO: (17) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:462/proxy/: tls qux (200; 14.545359ms)
Jul 29 15:58:05.423: INFO: (17) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv/proxy/rewriteme">test</a> (200; 18.705984ms)
Jul 29 15:58:05.424: INFO: (17) /api/v1/namespaces/proxy-2078/services/https:proxy-service-gw6m5:tlsportname2/proxy/: tls qux (200; 20.026425ms)
Jul 29 15:58:05.426: INFO: (17) /api/v1/namespaces/proxy-2078/services/proxy-service-gw6m5:portname2/proxy/: bar (200; 21.288037ms)
Jul 29 15:58:05.426: INFO: (17) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:1080/proxy/rewriteme">test<... (200; 22.197356ms)
Jul 29 15:58:05.428: INFO: (17) /api/v1/namespaces/proxy-2078/services/http:proxy-service-gw6m5:portname1/proxy/: foo (200; 23.362087ms)
Jul 29 15:58:05.428: INFO: (17) /api/v1/namespaces/proxy-2078/services/https:proxy-service-gw6m5:tlsportname1/proxy/: tls baz (200; 24.934574ms)
Jul 29 15:58:05.431: INFO: (17) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:460/proxy/: tls baz (200; 28.113191ms)
Jul 29 15:58:05.436: INFO: (17) /api/v1/namespaces/proxy-2078/services/http:proxy-service-gw6m5:portname2/proxy/: bar (200; 31.783412ms)
Jul 29 15:58:05.465: INFO: (18) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:460/proxy/: tls baz (200; 26.914816ms)
Jul 29 15:58:05.469: INFO: (18) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:160/proxy/: foo (200; 32.63968ms)
Jul 29 15:58:05.469: INFO: (18) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:160/proxy/: foo (200; 31.209345ms)
Jul 29 15:58:05.469: INFO: (18) /api/v1/namespaces/proxy-2078/services/proxy-service-gw6m5:portname2/proxy/: bar (200; 32.21179ms)
Jul 29 15:58:05.470: INFO: (18) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:1080/proxy/rewriteme">test<... (200; 31.702486ms)
Jul 29 15:58:05.472: INFO: (18) /api/v1/namespaces/proxy-2078/services/https:proxy-service-gw6m5:tlsportname1/proxy/: tls baz (200; 34.456011ms)
Jul 29 15:58:05.472: INFO: (18) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:162/proxy/: bar (200; 34.326393ms)
Jul 29 15:58:05.472: INFO: (18) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:162/proxy/: bar (200; 35.938387ms)
Jul 29 15:58:05.473: INFO: (18) /api/v1/namespaces/proxy-2078/services/http:proxy-service-gw6m5:portname2/proxy/: bar (200; 36.025785ms)
Jul 29 15:58:05.475: INFO: (18) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv/proxy/rewriteme">test</a> (200; 36.461454ms)
Jul 29 15:58:05.475: INFO: (18) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:1080/proxy/rewriteme">... (200; 37.041034ms)
Jul 29 15:58:05.475: INFO: (18) /api/v1/namespaces/proxy-2078/services/http:proxy-service-gw6m5:portname1/proxy/: foo (200; 38.247791ms)
Jul 29 15:58:05.476: INFO: (18) /api/v1/namespaces/proxy-2078/services/https:proxy-service-gw6m5:tlsportname2/proxy/: tls qux (200; 37.468815ms)
Jul 29 15:58:05.476: INFO: (18) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:462/proxy/: tls qux (200; 37.636366ms)
Jul 29 15:58:05.476: INFO: (18) /api/v1/namespaces/proxy-2078/services/proxy-service-gw6m5:portname1/proxy/: foo (200; 37.810309ms)
Jul 29 15:58:05.476: INFO: (18) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:443/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:443/proxy/tlsrewritem... (200; 37.989137ms)
Jul 29 15:58:05.491: INFO: (19) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:160/proxy/: foo (200; 15.597956ms)
Jul 29 15:58:05.497: INFO: (19) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:443/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:443/proxy/tlsrewritem... (200; 20.063691ms)
Jul 29 15:58:05.499: INFO: (19) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:162/proxy/: bar (200; 22.439266ms)
Jul 29 15:58:05.499: INFO: (19) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv/proxy/rewriteme">test</a> (200; 22.002012ms)
Jul 29 15:58:05.499: INFO: (19) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:1080/proxy/rewriteme">test<... (200; 22.629838ms)
Jul 29 15:58:05.503: INFO: (19) /api/v1/namespaces/proxy-2078/services/proxy-service-gw6m5:portname2/proxy/: bar (200; 26.049193ms)
Jul 29 15:58:05.504: INFO: (19) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:1080/proxy/rewriteme">... (200; 25.638969ms)
Jul 29 15:58:05.504: INFO: (19) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:462/proxy/: tls qux (200; 27.728505ms)
Jul 29 15:58:05.504: INFO: (19) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:162/proxy/: bar (200; 27.394467ms)
Jul 29 15:58:05.505: INFO: (19) /api/v1/namespaces/proxy-2078/services/proxy-service-gw6m5:portname1/proxy/: foo (200; 27.585549ms)
Jul 29 15:58:05.505: INFO: (19) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:160/proxy/: foo (200; 27.230081ms)
Jul 29 15:58:05.505: INFO: (19) /api/v1/namespaces/proxy-2078/services/https:proxy-service-gw6m5:tlsportname1/proxy/: tls baz (200; 29.474547ms)
Jul 29 15:58:05.506: INFO: (19) /api/v1/namespaces/proxy-2078/services/https:proxy-service-gw6m5:tlsportname2/proxy/: tls qux (200; 29.860279ms)
Jul 29 15:58:05.507: INFO: (19) /api/v1/namespaces/proxy-2078/services/http:proxy-service-gw6m5:portname1/proxy/: foo (200; 29.622079ms)
Jul 29 15:58:05.507: INFO: (19) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:460/proxy/: tls baz (200; 29.344485ms)
Jul 29 15:58:05.508: INFO: (19) /api/v1/namespaces/proxy-2078/services/http:proxy-service-gw6m5:portname2/proxy/: bar (200; 30.49504ms)
STEP: deleting ReplicationController proxy-service-gw6m5 in namespace proxy-2078, will wait for the garbage collector to delete the pods 07/29/23 15:58:05.508
Jul 29 15:58:05.578: INFO: Deleting ReplicationController proxy-service-gw6m5 took: 10.061354ms
Jul 29 15:58:05.679: INFO: Terminating ReplicationController proxy-service-gw6m5 pods took: 100.667796ms
[AfterEach] version v1
  test/e2e/framework/framework.go:187
Jul 29 15:58:08.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-2078" for this suite. 07/29/23 15:58:08.189
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","completed":100,"skipped":1616,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.649 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    test/e2e/network/proxy.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:58:02.552
    Jul 29 15:58:02.553: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename proxy 07/29/23 15:58:02.556
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:58:02.589
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:58:02.593
    [It] should proxy through a service and a pod  [Conformance]
      test/e2e/network/proxy.go:101
    STEP: starting an echo server on multiple ports 07/29/23 15:58:02.619
    STEP: creating replication controller proxy-service-gw6m5 in namespace proxy-2078 07/29/23 15:58:02.619
    I0729 15:58:02.648222      13 runners.go:193] Created replication controller with name: proxy-service-gw6m5, namespace: proxy-2078, replica count: 1
    I0729 15:58:03.702110      13 runners.go:193] proxy-service-gw6m5 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0729 15:58:04.702578      13 runners.go:193] proxy-service-gw6m5 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jul 29 15:58:04.708: INFO: setup took 2.110716832s, starting test cases
    STEP: running 16 cases, 20 attempts per case, 320 total attempts 07/29/23 15:58:04.708
    Jul 29 15:58:04.732: INFO: (0) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:160/proxy/: foo (200; 24.079425ms)
    Jul 29 15:58:04.734: INFO: (0) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:160/proxy/: foo (200; 25.085161ms)
    Jul 29 15:58:04.734: INFO: (0) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:1080/proxy/rewriteme">test<... (200; 25.50173ms)
    Jul 29 15:58:04.734: INFO: (0) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:162/proxy/: bar (200; 25.404068ms)
    Jul 29 15:58:04.745: INFO: (0) /api/v1/namespaces/proxy-2078/services/https:proxy-service-gw6m5:tlsportname2/proxy/: tls qux (200; 36.229121ms)
    Jul 29 15:58:04.747: INFO: (0) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:462/proxy/: tls qux (200; 38.322462ms)
    Jul 29 15:58:04.747: INFO: (0) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:443/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:443/proxy/tlsrewritem... (200; 39.063946ms)
    Jul 29 15:58:04.747: INFO: (0) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv/proxy/rewriteme">test</a> (200; 38.692772ms)
    Jul 29 15:58:04.747: INFO: (0) /api/v1/namespaces/proxy-2078/services/http:proxy-service-gw6m5:portname2/proxy/: bar (200; 38.439846ms)
    Jul 29 15:58:04.753: INFO: (0) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:162/proxy/: bar (200; 44.202598ms)
    Jul 29 15:58:04.753: INFO: (0) /api/v1/namespaces/proxy-2078/services/proxy-service-gw6m5:portname1/proxy/: foo (200; 44.085999ms)
    Jul 29 15:58:04.753: INFO: (0) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:1080/proxy/rewriteme">... (200; 44.075381ms)
    Jul 29 15:58:04.754: INFO: (0) /api/v1/namespaces/proxy-2078/services/proxy-service-gw6m5:portname2/proxy/: bar (200; 44.580522ms)
    Jul 29 15:58:04.754: INFO: (0) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:460/proxy/: tls baz (200; 44.416529ms)
    Jul 29 15:58:04.754: INFO: (0) /api/v1/namespaces/proxy-2078/services/https:proxy-service-gw6m5:tlsportname1/proxy/: tls baz (200; 45.500955ms)
    Jul 29 15:58:04.754: INFO: (0) /api/v1/namespaces/proxy-2078/services/http:proxy-service-gw6m5:portname1/proxy/: foo (200; 44.970713ms)
    Jul 29 15:58:04.771: INFO: (1) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:160/proxy/: foo (200; 16.986949ms)
    Jul 29 15:58:04.770: INFO: (1) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:1080/proxy/rewriteme">... (200; 15.572862ms)
    Jul 29 15:58:04.775: INFO: (1) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:443/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:443/proxy/tlsrewritem... (200; 20.653954ms)
    Jul 29 15:58:04.776: INFO: (1) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:1080/proxy/rewriteme">test<... (200; 20.646564ms)
    Jul 29 15:58:04.776: INFO: (1) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:162/proxy/: bar (200; 20.789081ms)
    Jul 29 15:58:04.778: INFO: (1) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv/proxy/rewriteme">test</a> (200; 23.102951ms)
    Jul 29 15:58:04.779: INFO: (1) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:162/proxy/: bar (200; 23.387332ms)
    Jul 29 15:58:04.779: INFO: (1) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:160/proxy/: foo (200; 23.673669ms)
    Jul 29 15:58:04.779: INFO: (1) /api/v1/namespaces/proxy-2078/services/proxy-service-gw6m5:portname1/proxy/: foo (200; 24.084074ms)
    Jul 29 15:58:04.779: INFO: (1) /api/v1/namespaces/proxy-2078/services/http:proxy-service-gw6m5:portname1/proxy/: foo (200; 23.513098ms)
    Jul 29 15:58:04.779: INFO: (1) /api/v1/namespaces/proxy-2078/services/http:proxy-service-gw6m5:portname2/proxy/: bar (200; 23.29095ms)
    Jul 29 15:58:04.779: INFO: (1) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:462/proxy/: tls qux (200; 23.649749ms)
    Jul 29 15:58:04.780: INFO: (1) /api/v1/namespaces/proxy-2078/services/proxy-service-gw6m5:portname2/proxy/: bar (200; 24.20574ms)
    Jul 29 15:58:04.781: INFO: (1) /api/v1/namespaces/proxy-2078/services/https:proxy-service-gw6m5:tlsportname2/proxy/: tls qux (200; 26.324544ms)
    Jul 29 15:58:04.783: INFO: (1) /api/v1/namespaces/proxy-2078/services/https:proxy-service-gw6m5:tlsportname1/proxy/: tls baz (200; 27.674305ms)
    Jul 29 15:58:04.784: INFO: (1) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:460/proxy/: tls baz (200; 28.717402ms)
    Jul 29 15:58:04.797: INFO: (2) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:1080/proxy/rewriteme">test<... (200; 12.646711ms)
    Jul 29 15:58:04.797: INFO: (2) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:162/proxy/: bar (200; 13.05896ms)
    Jul 29 15:58:04.802: INFO: (2) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv/proxy/rewriteme">test</a> (200; 18.253314ms)
    Jul 29 15:58:04.802: INFO: (2) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:160/proxy/: foo (200; 17.125555ms)
    Jul 29 15:58:04.803: INFO: (2) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:162/proxy/: bar (200; 18.472069ms)
    Jul 29 15:58:04.805: INFO: (2) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:443/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:443/proxy/tlsrewritem... (200; 20.9655ms)
    Jul 29 15:58:04.811: INFO: (2) /api/v1/namespaces/proxy-2078/services/https:proxy-service-gw6m5:tlsportname2/proxy/: tls qux (200; 27.009226ms)
    Jul 29 15:58:04.811: INFO: (2) /api/v1/namespaces/proxy-2078/services/https:proxy-service-gw6m5:tlsportname1/proxy/: tls baz (200; 25.758279ms)
    Jul 29 15:58:04.813: INFO: (2) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:1080/proxy/rewriteme">... (200; 28.017614ms)
    Jul 29 15:58:04.814: INFO: (2) /api/v1/namespaces/proxy-2078/services/http:proxy-service-gw6m5:portname2/proxy/: bar (200; 29.712757ms)
    Jul 29 15:58:04.814: INFO: (2) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:160/proxy/: foo (200; 28.821283ms)
    Jul 29 15:58:04.814: INFO: (2) /api/v1/namespaces/proxy-2078/services/http:proxy-service-gw6m5:portname1/proxy/: foo (200; 29.363801ms)
    Jul 29 15:58:04.815: INFO: (2) /api/v1/namespaces/proxy-2078/services/proxy-service-gw6m5:portname2/proxy/: bar (200; 30.647712ms)
    Jul 29 15:58:04.816: INFO: (2) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:460/proxy/: tls baz (200; 31.090265ms)
    Jul 29 15:58:04.816: INFO: (2) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:462/proxy/: tls qux (200; 31.175159ms)
    Jul 29 15:58:04.819: INFO: (2) /api/v1/namespaces/proxy-2078/services/proxy-service-gw6m5:portname1/proxy/: foo (200; 34.21689ms)
    Jul 29 15:58:04.840: INFO: (3) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:162/proxy/: bar (200; 20.766493ms)
    Jul 29 15:58:04.840: INFO: (3) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:160/proxy/: foo (200; 21.13073ms)
    Jul 29 15:58:04.843: INFO: (3) /api/v1/namespaces/proxy-2078/services/http:proxy-service-gw6m5:portname1/proxy/: foo (200; 23.663293ms)
    Jul 29 15:58:04.843: INFO: (3) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:1080/proxy/rewriteme">test<... (200; 23.18723ms)
    Jul 29 15:58:04.843: INFO: (3) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv/proxy/rewriteme">test</a> (200; 23.090704ms)
    Jul 29 15:58:04.843: INFO: (3) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:462/proxy/: tls qux (200; 23.278027ms)
    Jul 29 15:58:04.843: INFO: (3) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:460/proxy/: tls baz (200; 23.583151ms)
    Jul 29 15:58:04.844: INFO: (3) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:443/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:443/proxy/tlsrewritem... (200; 23.160526ms)
    Jul 29 15:58:04.846: INFO: (3) /api/v1/namespaces/proxy-2078/services/http:proxy-service-gw6m5:portname2/proxy/: bar (200; 25.898934ms)
    Jul 29 15:58:04.846: INFO: (3) /api/v1/namespaces/proxy-2078/services/proxy-service-gw6m5:portname2/proxy/: bar (200; 26.458155ms)
    Jul 29 15:58:04.848: INFO: (3) /api/v1/namespaces/proxy-2078/services/https:proxy-service-gw6m5:tlsportname1/proxy/: tls baz (200; 28.461018ms)
    Jul 29 15:58:04.849: INFO: (3) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:162/proxy/: bar (200; 29.083301ms)
    Jul 29 15:58:04.850: INFO: (3) /api/v1/namespaces/proxy-2078/services/https:proxy-service-gw6m5:tlsportname2/proxy/: tls qux (200; 29.409446ms)
    Jul 29 15:58:04.850: INFO: (3) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:160/proxy/: foo (200; 29.285984ms)
    Jul 29 15:58:04.853: INFO: (3) /api/v1/namespaces/proxy-2078/services/proxy-service-gw6m5:portname1/proxy/: foo (200; 32.876703ms)
    Jul 29 15:58:04.855: INFO: (3) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:1080/proxy/rewriteme">... (200; 34.401924ms)
    Jul 29 15:58:04.871: INFO: (4) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:1080/proxy/rewriteme">... (200; 14.071358ms)
    Jul 29 15:58:04.871: INFO: (4) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:162/proxy/: bar (200; 14.397265ms)
    Jul 29 15:58:04.872: INFO: (4) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:160/proxy/: foo (200; 15.327519ms)
    Jul 29 15:58:04.872: INFO: (4) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:460/proxy/: tls baz (200; 15.179205ms)
    Jul 29 15:58:04.872: INFO: (4) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:443/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:443/proxy/tlsrewritem... (200; 16.403323ms)
    Jul 29 15:58:04.874: INFO: (4) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:160/proxy/: foo (200; 16.875593ms)
    Jul 29 15:58:04.876: INFO: (4) /api/v1/namespaces/proxy-2078/services/http:proxy-service-gw6m5:portname2/proxy/: bar (200; 20.442691ms)
    Jul 29 15:58:04.876: INFO: (4) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:462/proxy/: tls qux (200; 19.181794ms)
    Jul 29 15:58:04.877: INFO: (4) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv/proxy/rewriteme">test</a> (200; 21.77372ms)
    Jul 29 15:58:04.877: INFO: (4) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:1080/proxy/rewriteme">test<... (200; 19.899768ms)
    Jul 29 15:58:04.881: INFO: (4) /api/v1/namespaces/proxy-2078/services/http:proxy-service-gw6m5:portname1/proxy/: foo (200; 24.798055ms)
    Jul 29 15:58:04.881: INFO: (4) /api/v1/namespaces/proxy-2078/services/https:proxy-service-gw6m5:tlsportname1/proxy/: tls baz (200; 23.931289ms)
    Jul 29 15:58:04.881: INFO: (4) /api/v1/namespaces/proxy-2078/services/https:proxy-service-gw6m5:tlsportname2/proxy/: tls qux (200; 23.957175ms)
    Jul 29 15:58:04.881: INFO: (4) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:162/proxy/: bar (200; 24.301398ms)
    Jul 29 15:58:04.886: INFO: (4) /api/v1/namespaces/proxy-2078/services/proxy-service-gw6m5:portname2/proxy/: bar (200; 29.956814ms)
    Jul 29 15:58:04.886: INFO: (4) /api/v1/namespaces/proxy-2078/services/proxy-service-gw6m5:portname1/proxy/: foo (200; 31.290244ms)
    Jul 29 15:58:04.901: INFO: (5) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:162/proxy/: bar (200; 13.76348ms)
    Jul 29 15:58:04.902: INFO: (5) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:160/proxy/: foo (200; 14.990786ms)
    Jul 29 15:58:04.902: INFO: (5) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:443/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:443/proxy/tlsrewritem... (200; 14.919705ms)
    Jul 29 15:58:04.902: INFO: (5) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:160/proxy/: foo (200; 14.785823ms)
    Jul 29 15:58:04.903: INFO: (5) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:1080/proxy/rewriteme">test<... (200; 15.918327ms)
    Jul 29 15:58:04.903: INFO: (5) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:460/proxy/: tls baz (200; 15.630552ms)
    Jul 29 15:58:04.904: INFO: (5) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv/proxy/rewriteme">test</a> (200; 16.277146ms)
    Jul 29 15:58:04.905: INFO: (5) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:1080/proxy/rewriteme">... (200; 17.552005ms)
    Jul 29 15:58:04.905: INFO: (5) /api/v1/namespaces/proxy-2078/services/http:proxy-service-gw6m5:portname2/proxy/: bar (200; 18.914879ms)
    Jul 29 15:58:04.909: INFO: (5) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:162/proxy/: bar (200; 21.626819ms)
    Jul 29 15:58:04.909: INFO: (5) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:462/proxy/: tls qux (200; 21.535613ms)
    Jul 29 15:58:04.910: INFO: (5) /api/v1/namespaces/proxy-2078/services/http:proxy-service-gw6m5:portname1/proxy/: foo (200; 21.997156ms)
    Jul 29 15:58:04.910: INFO: (5) /api/v1/namespaces/proxy-2078/services/https:proxy-service-gw6m5:tlsportname2/proxy/: tls qux (200; 22.258919ms)
    Jul 29 15:58:04.910: INFO: (5) /api/v1/namespaces/proxy-2078/services/proxy-service-gw6m5:portname2/proxy/: bar (200; 22.757479ms)
    Jul 29 15:58:04.910: INFO: (5) /api/v1/namespaces/proxy-2078/services/https:proxy-service-gw6m5:tlsportname1/proxy/: tls baz (200; 22.456292ms)
    Jul 29 15:58:04.912: INFO: (5) /api/v1/namespaces/proxy-2078/services/proxy-service-gw6m5:portname1/proxy/: foo (200; 25.147007ms)
    Jul 29 15:58:04.921: INFO: (6) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv/proxy/rewriteme">test</a> (200; 9.056037ms)
    Jul 29 15:58:04.926: INFO: (6) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:162/proxy/: bar (200; 12.818104ms)
    Jul 29 15:58:04.926: INFO: (6) /api/v1/namespaces/proxy-2078/services/proxy-service-gw6m5:portname1/proxy/: foo (200; 13.757142ms)
    Jul 29 15:58:04.929: INFO: (6) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:160/proxy/: foo (200; 15.78379ms)
    Jul 29 15:58:04.931: INFO: (6) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:160/proxy/: foo (200; 17.526904ms)
    Jul 29 15:58:04.931: INFO: (6) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:162/proxy/: bar (200; 18.010614ms)
    Jul 29 15:58:04.932: INFO: (6) /api/v1/namespaces/proxy-2078/services/http:proxy-service-gw6m5:portname2/proxy/: bar (200; 18.979035ms)
    Jul 29 15:58:04.932: INFO: (6) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:1080/proxy/rewriteme">... (200; 18.894722ms)
    Jul 29 15:58:04.932: INFO: (6) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:462/proxy/: tls qux (200; 18.806001ms)
    Jul 29 15:58:04.932: INFO: (6) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:460/proxy/: tls baz (200; 18.300286ms)
    Jul 29 15:58:04.932: INFO: (6) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:1080/proxy/rewriteme">test<... (200; 18.820681ms)
    Jul 29 15:58:04.937: INFO: (6) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:443/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:443/proxy/tlsrewritem... (200; 24.210027ms)
    Jul 29 15:58:04.938: INFO: (6) /api/v1/namespaces/proxy-2078/services/proxy-service-gw6m5:portname2/proxy/: bar (200; 24.700439ms)
    Jul 29 15:58:04.940: INFO: (6) /api/v1/namespaces/proxy-2078/services/http:proxy-service-gw6m5:portname1/proxy/: foo (200; 26.376995ms)
    Jul 29 15:58:04.941: INFO: (6) /api/v1/namespaces/proxy-2078/services/https:proxy-service-gw6m5:tlsportname2/proxy/: tls qux (200; 27.916892ms)
    Jul 29 15:58:04.941: INFO: (6) /api/v1/namespaces/proxy-2078/services/https:proxy-service-gw6m5:tlsportname1/proxy/: tls baz (200; 27.558238ms)
    Jul 29 15:58:04.955: INFO: (7) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:160/proxy/: foo (200; 13.000741ms)
    Jul 29 15:58:04.956: INFO: (7) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:1080/proxy/rewriteme">test<... (200; 14.509089ms)
    Jul 29 15:58:04.957: INFO: (7) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:1080/proxy/rewriteme">... (200; 15.025525ms)
    Jul 29 15:58:04.957: INFO: (7) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:462/proxy/: tls qux (200; 15.464205ms)
    Jul 29 15:58:04.959: INFO: (7) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv/proxy/rewriteme">test</a> (200; 16.761302ms)
    Jul 29 15:58:04.960: INFO: (7) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:162/proxy/: bar (200; 18.524831ms)
    Jul 29 15:58:04.960: INFO: (7) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:443/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:443/proxy/tlsrewritem... (200; 17.167578ms)
    Jul 29 15:58:04.960: INFO: (7) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:460/proxy/: tls baz (200; 17.6487ms)
    Jul 29 15:58:04.961: INFO: (7) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:162/proxy/: bar (200; 18.7453ms)
    Jul 29 15:58:04.962: INFO: (7) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:160/proxy/: foo (200; 19.334105ms)
    Jul 29 15:58:04.962: INFO: (7) /api/v1/namespaces/proxy-2078/services/https:proxy-service-gw6m5:tlsportname1/proxy/: tls baz (200; 20.5303ms)
    Jul 29 15:58:04.964: INFO: (7) /api/v1/namespaces/proxy-2078/services/proxy-service-gw6m5:portname2/proxy/: bar (200; 21.350354ms)
    Jul 29 15:58:04.965: INFO: (7) /api/v1/namespaces/proxy-2078/services/https:proxy-service-gw6m5:tlsportname2/proxy/: tls qux (200; 22.175027ms)
    Jul 29 15:58:04.965: INFO: (7) /api/v1/namespaces/proxy-2078/services/proxy-service-gw6m5:portname1/proxy/: foo (200; 22.255825ms)
    Jul 29 15:58:04.965: INFO: (7) /api/v1/namespaces/proxy-2078/services/http:proxy-service-gw6m5:portname1/proxy/: foo (200; 23.738499ms)
    Jul 29 15:58:04.966: INFO: (7) /api/v1/namespaces/proxy-2078/services/http:proxy-service-gw6m5:portname2/proxy/: bar (200; 22.7611ms)
    Jul 29 15:58:04.997: INFO: (8) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:160/proxy/: foo (200; 30.630324ms)
    Jul 29 15:58:05.015: INFO: (8) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:162/proxy/: bar (200; 48.215404ms)
    Jul 29 15:58:05.022: INFO: (8) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:1080/proxy/rewriteme">test<... (200; 55.4835ms)
    Jul 29 15:58:05.022: INFO: (8) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:460/proxy/: tls baz (200; 55.501659ms)
    Jul 29 15:58:05.022: INFO: (8) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:1080/proxy/rewriteme">... (200; 55.080261ms)
    Jul 29 15:58:05.023: INFO: (8) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:443/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:443/proxy/tlsrewritem... (200; 56.335743ms)
    Jul 29 15:58:05.023: INFO: (8) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:462/proxy/: tls qux (200; 56.22062ms)
    Jul 29 15:58:05.027: INFO: (8) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:162/proxy/: bar (200; 60.360524ms)
    Jul 29 15:58:05.027: INFO: (8) /api/v1/namespaces/proxy-2078/services/proxy-service-gw6m5:portname2/proxy/: bar (200; 61.104314ms)
    Jul 29 15:58:05.027: INFO: (8) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:160/proxy/: foo (200; 60.989314ms)
    Jul 29 15:58:05.038: INFO: (8) /api/v1/namespaces/proxy-2078/services/https:proxy-service-gw6m5:tlsportname2/proxy/: tls qux (200; 71.329481ms)
    Jul 29 15:58:05.038: INFO: (8) /api/v1/namespaces/proxy-2078/services/https:proxy-service-gw6m5:tlsportname1/proxy/: tls baz (200; 72.108965ms)
    Jul 29 15:58:05.038: INFO: (8) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv/proxy/rewriteme">test</a> (200; 71.422492ms)
    Jul 29 15:58:05.038: INFO: (8) /api/v1/namespaces/proxy-2078/services/proxy-service-gw6m5:portname1/proxy/: foo (200; 72.003899ms)
    Jul 29 15:58:05.039: INFO: (8) /api/v1/namespaces/proxy-2078/services/http:proxy-service-gw6m5:portname1/proxy/: foo (200; 71.958332ms)
    Jul 29 15:58:05.039: INFO: (8) /api/v1/namespaces/proxy-2078/services/http:proxy-service-gw6m5:portname2/proxy/: bar (200; 72.183422ms)
    Jul 29 15:58:05.061: INFO: (9) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:1080/proxy/rewriteme">... (200; 22.095103ms)
    Jul 29 15:58:05.082: INFO: (9) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:460/proxy/: tls baz (200; 42.53584ms)
    Jul 29 15:58:05.082: INFO: (9) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:160/proxy/: foo (200; 42.791774ms)
    Jul 29 15:58:05.082: INFO: (9) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv/proxy/rewriteme">test</a> (200; 42.428418ms)
    Jul 29 15:58:05.091: INFO: (9) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:160/proxy/: foo (200; 51.467724ms)
    Jul 29 15:58:05.097: INFO: (9) /api/v1/namespaces/proxy-2078/services/https:proxy-service-gw6m5:tlsportname2/proxy/: tls qux (200; 56.354255ms)
    Jul 29 15:58:05.098: INFO: (9) /api/v1/namespaces/proxy-2078/services/proxy-service-gw6m5:portname2/proxy/: bar (200; 58.023588ms)
    Jul 29 15:58:05.098: INFO: (9) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:1080/proxy/rewriteme">test<... (200; 58.610936ms)
    Jul 29 15:58:05.099: INFO: (9) /api/v1/namespaces/proxy-2078/services/http:proxy-service-gw6m5:portname1/proxy/: foo (200; 59.308558ms)
    Jul 29 15:58:05.099: INFO: (9) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:162/proxy/: bar (200; 58.383891ms)
    Jul 29 15:58:05.099: INFO: (9) /api/v1/namespaces/proxy-2078/services/proxy-service-gw6m5:portname1/proxy/: foo (200; 59.569279ms)
    Jul 29 15:58:05.099: INFO: (9) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:162/proxy/: bar (200; 58.818078ms)
    Jul 29 15:58:05.101: INFO: (9) /api/v1/namespaces/proxy-2078/services/http:proxy-service-gw6m5:portname2/proxy/: bar (200; 60.170494ms)
    Jul 29 15:58:05.102: INFO: (9) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:443/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:443/proxy/tlsrewritem... (200; 60.869577ms)
    Jul 29 15:58:05.109: INFO: (9) /api/v1/namespaces/proxy-2078/services/https:proxy-service-gw6m5:tlsportname1/proxy/: tls baz (200; 69.416193ms)
    Jul 29 15:58:05.110: INFO: (9) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:462/proxy/: tls qux (200; 69.636528ms)
    Jul 29 15:58:05.158: INFO: (10) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:460/proxy/: tls baz (200; 48.526028ms)
    Jul 29 15:58:05.186: INFO: (10) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:1080/proxy/rewriteme">... (200; 73.7426ms)
    Jul 29 15:58:05.206: INFO: (10) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:160/proxy/: foo (200; 93.232489ms)
    Jul 29 15:58:05.206: INFO: (10) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:160/proxy/: foo (200; 93.890575ms)
    Jul 29 15:58:05.207: INFO: (10) /api/v1/namespaces/proxy-2078/services/proxy-service-gw6m5:portname2/proxy/: bar (200; 95.281384ms)
    Jul 29 15:58:05.207: INFO: (10) /api/v1/namespaces/proxy-2078/services/https:proxy-service-gw6m5:tlsportname1/proxy/: tls baz (200; 96.783597ms)
    Jul 29 15:58:05.207: INFO: (10) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:443/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:443/proxy/tlsrewritem... (200; 95.229541ms)
    Jul 29 15:58:05.207: INFO: (10) /api/v1/namespaces/proxy-2078/services/http:proxy-service-gw6m5:portname1/proxy/: foo (200; 95.775937ms)
    Jul 29 15:58:05.207: INFO: (10) /api/v1/namespaces/proxy-2078/services/http:proxy-service-gw6m5:portname2/proxy/: bar (200; 95.600288ms)
    Jul 29 15:58:05.207: INFO: (10) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:162/proxy/: bar (200; 96.813391ms)
    Jul 29 15:58:05.207: INFO: (10) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:1080/proxy/rewriteme">test<... (200; 97.154523ms)
    Jul 29 15:58:05.207: INFO: (10) /api/v1/namespaces/proxy-2078/services/https:proxy-service-gw6m5:tlsportname2/proxy/: tls qux (200; 96.668168ms)
    Jul 29 15:58:05.208: INFO: (10) /api/v1/namespaces/proxy-2078/services/proxy-service-gw6m5:portname1/proxy/: foo (200; 95.699709ms)
    Jul 29 15:58:05.208: INFO: (10) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv/proxy/rewriteme">test</a> (200; 96.782545ms)
    Jul 29 15:58:05.208: INFO: (10) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:462/proxy/: tls qux (200; 97.248586ms)
    Jul 29 15:58:05.208: INFO: (10) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:162/proxy/: bar (200; 96.501664ms)
    Jul 29 15:58:05.230: INFO: (11) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:1080/proxy/rewriteme">... (200; 20.348773ms)
    Jul 29 15:58:05.230: INFO: (11) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:162/proxy/: bar (200; 20.900454ms)
    Jul 29 15:58:05.231: INFO: (11) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv/proxy/rewriteme">test</a> (200; 21.652859ms)
    Jul 29 15:58:05.231: INFO: (11) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:160/proxy/: foo (200; 22.046937ms)
    Jul 29 15:58:05.231: INFO: (11) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:1080/proxy/rewriteme">test<... (200; 22.697689ms)
    Jul 29 15:58:05.231: INFO: (11) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:162/proxy/: bar (200; 22.5008ms)
    Jul 29 15:58:05.231: INFO: (11) /api/v1/namespaces/proxy-2078/services/http:proxy-service-gw6m5:portname2/proxy/: bar (200; 22.895472ms)
    Jul 29 15:58:05.235: INFO: (11) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:443/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:443/proxy/tlsrewritem... (200; 26.234914ms)
    Jul 29 15:58:05.235: INFO: (11) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:460/proxy/: tls baz (200; 26.075388ms)
    Jul 29 15:58:05.235: INFO: (11) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:462/proxy/: tls qux (200; 26.045708ms)
    Jul 29 15:58:05.238: INFO: (11) /api/v1/namespaces/proxy-2078/services/http:proxy-service-gw6m5:portname1/proxy/: foo (200; 28.855851ms)
    Jul 29 15:58:05.239: INFO: (11) /api/v1/namespaces/proxy-2078/services/proxy-service-gw6m5:portname1/proxy/: foo (200; 29.314193ms)
    Jul 29 15:58:05.239: INFO: (11) /api/v1/namespaces/proxy-2078/services/proxy-service-gw6m5:portname2/proxy/: bar (200; 29.520478ms)
    Jul 29 15:58:05.239: INFO: (11) /api/v1/namespaces/proxy-2078/services/https:proxy-service-gw6m5:tlsportname2/proxy/: tls qux (200; 29.878603ms)
    Jul 29 15:58:05.239: INFO: (11) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:160/proxy/: foo (200; 29.434663ms)
    Jul 29 15:58:05.246: INFO: (11) /api/v1/namespaces/proxy-2078/services/https:proxy-service-gw6m5:tlsportname1/proxy/: tls baz (200; 37.013617ms)
    Jul 29 15:58:05.258: INFO: (12) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:162/proxy/: bar (200; 11.549556ms)
    Jul 29 15:58:05.260: INFO: (12) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:160/proxy/: foo (200; 12.953066ms)
    Jul 29 15:58:05.262: INFO: (12) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv/proxy/rewriteme">test</a> (200; 14.535588ms)
    Jul 29 15:58:05.268: INFO: (12) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:1080/proxy/rewriteme">... (200; 20.324102ms)
    Jul 29 15:58:05.268: INFO: (12) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:160/proxy/: foo (200; 20.356966ms)
    Jul 29 15:58:05.268: INFO: (12) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:1080/proxy/rewriteme">test<... (200; 21.049061ms)
    Jul 29 15:58:05.270: INFO: (12) /api/v1/namespaces/proxy-2078/services/proxy-service-gw6m5:portname2/proxy/: bar (200; 22.426016ms)
    Jul 29 15:58:05.270: INFO: (12) /api/v1/namespaces/proxy-2078/services/https:proxy-service-gw6m5:tlsportname2/proxy/: tls qux (200; 22.421589ms)
    Jul 29 15:58:05.270: INFO: (12) /api/v1/namespaces/proxy-2078/services/http:proxy-service-gw6m5:portname1/proxy/: foo (200; 23.048119ms)
    Jul 29 15:58:05.270: INFO: (12) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:162/proxy/: bar (200; 22.7618ms)
    Jul 29 15:58:05.270: INFO: (12) /api/v1/namespaces/proxy-2078/services/https:proxy-service-gw6m5:tlsportname1/proxy/: tls baz (200; 23.182947ms)
    Jul 29 15:58:05.270: INFO: (12) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:460/proxy/: tls baz (200; 22.790557ms)
    Jul 29 15:58:05.270: INFO: (12) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:462/proxy/: tls qux (200; 23.024507ms)
    Jul 29 15:58:05.271: INFO: (12) /api/v1/namespaces/proxy-2078/services/http:proxy-service-gw6m5:portname2/proxy/: bar (200; 23.540437ms)
    Jul 29 15:58:05.271: INFO: (12) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:443/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:443/proxy/tlsrewritem... (200; 23.559761ms)
    Jul 29 15:58:05.271: INFO: (12) /api/v1/namespaces/proxy-2078/services/proxy-service-gw6m5:portname1/proxy/: foo (200; 23.666357ms)
    Jul 29 15:58:05.282: INFO: (13) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:462/proxy/: tls qux (200; 10.685838ms)
    Jul 29 15:58:05.284: INFO: (13) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:1080/proxy/rewriteme">... (200; 11.901279ms)
    Jul 29 15:58:05.288: INFO: (13) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:160/proxy/: foo (200; 15.755631ms)
    Jul 29 15:58:05.288: INFO: (13) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv/proxy/rewriteme">test</a> (200; 16.029205ms)
    Jul 29 15:58:05.293: INFO: (13) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:1080/proxy/rewriteme">test<... (200; 20.594057ms)
    Jul 29 15:58:05.294: INFO: (13) /api/v1/namespaces/proxy-2078/services/proxy-service-gw6m5:portname1/proxy/: foo (200; 21.625529ms)
    Jul 29 15:58:05.295: INFO: (13) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:443/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:443/proxy/tlsrewritem... (200; 22.231737ms)
    Jul 29 15:58:05.295: INFO: (13) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:162/proxy/: bar (200; 22.720503ms)
    Jul 29 15:58:05.295: INFO: (13) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:162/proxy/: bar (200; 22.122479ms)
    Jul 29 15:58:05.295: INFO: (13) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:460/proxy/: tls baz (200; 22.204764ms)
    Jul 29 15:58:05.295: INFO: (13) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:160/proxy/: foo (200; 22.281944ms)
    Jul 29 15:58:05.295: INFO: (13) /api/v1/namespaces/proxy-2078/services/https:proxy-service-gw6m5:tlsportname2/proxy/: tls qux (200; 23.162948ms)
    Jul 29 15:58:05.297: INFO: (13) /api/v1/namespaces/proxy-2078/services/http:proxy-service-gw6m5:portname1/proxy/: foo (200; 25.356983ms)
    Jul 29 15:58:05.298: INFO: (13) /api/v1/namespaces/proxy-2078/services/proxy-service-gw6m5:portname2/proxy/: bar (200; 25.581933ms)
    Jul 29 15:58:05.298: INFO: (13) /api/v1/namespaces/proxy-2078/services/http:proxy-service-gw6m5:portname2/proxy/: bar (200; 25.587115ms)
    Jul 29 15:58:05.298: INFO: (13) /api/v1/namespaces/proxy-2078/services/https:proxy-service-gw6m5:tlsportname1/proxy/: tls baz (200; 25.7133ms)
    Jul 29 15:58:05.317: INFO: (14) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:1080/proxy/rewriteme">test<... (200; 17.801839ms)
    Jul 29 15:58:05.317: INFO: (14) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:1080/proxy/rewriteme">... (200; 18.773813ms)
    Jul 29 15:58:05.322: INFO: (14) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:160/proxy/: foo (200; 20.080654ms)
    Jul 29 15:58:05.325: INFO: (14) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:162/proxy/: bar (200; 25.253978ms)
    Jul 29 15:58:05.325: INFO: (14) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv/proxy/rewriteme">test</a> (200; 24.374075ms)
    Jul 29 15:58:05.325: INFO: (14) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:162/proxy/: bar (200; 26.319849ms)
    Jul 29 15:58:05.325: INFO: (14) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:443/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:443/proxy/tlsrewritem... (200; 23.620531ms)
    Jul 29 15:58:05.325: INFO: (14) /api/v1/namespaces/proxy-2078/services/https:proxy-service-gw6m5:tlsportname2/proxy/: tls qux (200; 25.474706ms)
    Jul 29 15:58:05.327: INFO: (14) /api/v1/namespaces/proxy-2078/services/http:proxy-service-gw6m5:portname1/proxy/: foo (200; 25.815599ms)
    Jul 29 15:58:05.327: INFO: (14) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:160/proxy/: foo (200; 25.603877ms)
    Jul 29 15:58:05.327: INFO: (14) /api/v1/namespaces/proxy-2078/services/http:proxy-service-gw6m5:portname2/proxy/: bar (200; 26.235375ms)
    Jul 29 15:58:05.327: INFO: (14) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:460/proxy/: tls baz (200; 25.640146ms)
    Jul 29 15:58:05.327: INFO: (14) /api/v1/namespaces/proxy-2078/services/https:proxy-service-gw6m5:tlsportname1/proxy/: tls baz (200; 28.475488ms)
    Jul 29 15:58:05.328: INFO: (14) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:462/proxy/: tls qux (200; 28.741485ms)
    Jul 29 15:58:05.328: INFO: (14) /api/v1/namespaces/proxy-2078/services/proxy-service-gw6m5:portname2/proxy/: bar (200; 27.011676ms)
    Jul 29 15:58:05.331: INFO: (14) /api/v1/namespaces/proxy-2078/services/proxy-service-gw6m5:portname1/proxy/: foo (200; 28.606953ms)
    Jul 29 15:58:05.355: INFO: (15) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:1080/proxy/rewriteme">test<... (200; 24.314678ms)
    Jul 29 15:58:05.356: INFO: (15) /api/v1/namespaces/proxy-2078/services/http:proxy-service-gw6m5:portname2/proxy/: bar (200; 23.826392ms)
    Jul 29 15:58:05.358: INFO: (15) /api/v1/namespaces/proxy-2078/services/https:proxy-service-gw6m5:tlsportname1/proxy/: tls baz (200; 25.822565ms)
    Jul 29 15:58:05.358: INFO: (15) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:160/proxy/: foo (200; 26.725716ms)
    Jul 29 15:58:05.358: INFO: (15) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:462/proxy/: tls qux (200; 26.303432ms)
    Jul 29 15:58:05.358: INFO: (15) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv/proxy/rewriteme">test</a> (200; 25.908756ms)
    Jul 29 15:58:05.358: INFO: (15) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:162/proxy/: bar (200; 26.539263ms)
    Jul 29 15:58:05.358: INFO: (15) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:1080/proxy/rewriteme">... (200; 26.898279ms)
    Jul 29 15:58:05.359: INFO: (15) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:460/proxy/: tls baz (200; 27.237549ms)
    Jul 29 15:58:05.359: INFO: (15) /api/v1/namespaces/proxy-2078/services/proxy-service-gw6m5:portname2/proxy/: bar (200; 26.570582ms)
    Jul 29 15:58:05.362: INFO: (15) /api/v1/namespaces/proxy-2078/services/http:proxy-service-gw6m5:portname1/proxy/: foo (200; 30.163453ms)
    Jul 29 15:58:05.363: INFO: (15) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:160/proxy/: foo (200; 31.698448ms)
    Jul 29 15:58:05.363: INFO: (15) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:162/proxy/: bar (200; 31.439659ms)
    Jul 29 15:58:05.363: INFO: (15) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:443/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:443/proxy/tlsrewritem... (200; 30.950569ms)
    Jul 29 15:58:05.364: INFO: (15) /api/v1/namespaces/proxy-2078/services/https:proxy-service-gw6m5:tlsportname2/proxy/: tls qux (200; 32.05307ms)
    Jul 29 15:58:05.367: INFO: (15) /api/v1/namespaces/proxy-2078/services/proxy-service-gw6m5:portname1/proxy/: foo (200; 35.913893ms)
    Jul 29 15:58:05.387: INFO: (16) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:162/proxy/: bar (200; 18.864407ms)
    Jul 29 15:58:05.390: INFO: (16) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:443/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:443/proxy/tlsrewritem... (200; 22.438018ms)
    Jul 29 15:58:05.390: INFO: (16) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:462/proxy/: tls qux (200; 22.364914ms)
    Jul 29 15:58:05.390: INFO: (16) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:160/proxy/: foo (200; 21.636661ms)
    Jul 29 15:58:05.393: INFO: (16) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:1080/proxy/rewriteme">... (200; 23.679063ms)
    Jul 29 15:58:05.393: INFO: (16) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:1080/proxy/rewriteme">test<... (200; 23.956791ms)
    Jul 29 15:58:05.393: INFO: (16) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:162/proxy/: bar (200; 24.942859ms)
    Jul 29 15:58:05.393: INFO: (16) /api/v1/namespaces/proxy-2078/services/proxy-service-gw6m5:portname2/proxy/: bar (200; 24.904954ms)
    Jul 29 15:58:05.394: INFO: (16) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:160/proxy/: foo (200; 24.37487ms)
    Jul 29 15:58:05.394: INFO: (16) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv/proxy/rewriteme">test</a> (200; 25.354731ms)
    Jul 29 15:58:05.394: INFO: (16) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:460/proxy/: tls baz (200; 24.324425ms)
    Jul 29 15:58:05.395: INFO: (16) /api/v1/namespaces/proxy-2078/services/http:proxy-service-gw6m5:portname1/proxy/: foo (200; 26.83308ms)
    Jul 29 15:58:05.395: INFO: (16) /api/v1/namespaces/proxy-2078/services/http:proxy-service-gw6m5:portname2/proxy/: bar (200; 26.707645ms)
    Jul 29 15:58:05.398: INFO: (16) /api/v1/namespaces/proxy-2078/services/https:proxy-service-gw6m5:tlsportname2/proxy/: tls qux (200; 29.859652ms)
    Jul 29 15:58:05.399: INFO: (16) /api/v1/namespaces/proxy-2078/services/proxy-service-gw6m5:portname1/proxy/: foo (200; 29.790232ms)
    Jul 29 15:58:05.399: INFO: (16) /api/v1/namespaces/proxy-2078/services/https:proxy-service-gw6m5:tlsportname1/proxy/: tls baz (200; 29.829125ms)
    Jul 29 15:58:05.415: INFO: (17) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:160/proxy/: foo (200; 14.604107ms)
    Jul 29 15:58:05.415: INFO: (17) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:160/proxy/: foo (200; 14.660062ms)
    Jul 29 15:58:05.415: INFO: (17) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:162/proxy/: bar (200; 15.894087ms)
    Jul 29 15:58:05.415: INFO: (17) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:1080/proxy/rewriteme">... (200; 15.592819ms)
    Jul 29 15:58:05.416: INFO: (17) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:162/proxy/: bar (200; 12.172462ms)
    Jul 29 15:58:05.418: INFO: (17) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:443/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:443/proxy/tlsrewritem... (200; 18.400411ms)
    Jul 29 15:58:05.418: INFO: (17) /api/v1/namespaces/proxy-2078/services/proxy-service-gw6m5:portname1/proxy/: foo (200; 18.37526ms)
    Jul 29 15:58:05.418: INFO: (17) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:462/proxy/: tls qux (200; 14.545359ms)
    Jul 29 15:58:05.423: INFO: (17) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv/proxy/rewriteme">test</a> (200; 18.705984ms)
    Jul 29 15:58:05.424: INFO: (17) /api/v1/namespaces/proxy-2078/services/https:proxy-service-gw6m5:tlsportname2/proxy/: tls qux (200; 20.026425ms)
    Jul 29 15:58:05.426: INFO: (17) /api/v1/namespaces/proxy-2078/services/proxy-service-gw6m5:portname2/proxy/: bar (200; 21.288037ms)
    Jul 29 15:58:05.426: INFO: (17) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:1080/proxy/rewriteme">test<... (200; 22.197356ms)
    Jul 29 15:58:05.428: INFO: (17) /api/v1/namespaces/proxy-2078/services/http:proxy-service-gw6m5:portname1/proxy/: foo (200; 23.362087ms)
    Jul 29 15:58:05.428: INFO: (17) /api/v1/namespaces/proxy-2078/services/https:proxy-service-gw6m5:tlsportname1/proxy/: tls baz (200; 24.934574ms)
    Jul 29 15:58:05.431: INFO: (17) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:460/proxy/: tls baz (200; 28.113191ms)
    Jul 29 15:58:05.436: INFO: (17) /api/v1/namespaces/proxy-2078/services/http:proxy-service-gw6m5:portname2/proxy/: bar (200; 31.783412ms)
    Jul 29 15:58:05.465: INFO: (18) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:460/proxy/: tls baz (200; 26.914816ms)
    Jul 29 15:58:05.469: INFO: (18) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:160/proxy/: foo (200; 32.63968ms)
    Jul 29 15:58:05.469: INFO: (18) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:160/proxy/: foo (200; 31.209345ms)
    Jul 29 15:58:05.469: INFO: (18) /api/v1/namespaces/proxy-2078/services/proxy-service-gw6m5:portname2/proxy/: bar (200; 32.21179ms)
    Jul 29 15:58:05.470: INFO: (18) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:1080/proxy/rewriteme">test<... (200; 31.702486ms)
    Jul 29 15:58:05.472: INFO: (18) /api/v1/namespaces/proxy-2078/services/https:proxy-service-gw6m5:tlsportname1/proxy/: tls baz (200; 34.456011ms)
    Jul 29 15:58:05.472: INFO: (18) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:162/proxy/: bar (200; 34.326393ms)
    Jul 29 15:58:05.472: INFO: (18) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:162/proxy/: bar (200; 35.938387ms)
    Jul 29 15:58:05.473: INFO: (18) /api/v1/namespaces/proxy-2078/services/http:proxy-service-gw6m5:portname2/proxy/: bar (200; 36.025785ms)
    Jul 29 15:58:05.475: INFO: (18) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv/proxy/rewriteme">test</a> (200; 36.461454ms)
    Jul 29 15:58:05.475: INFO: (18) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:1080/proxy/rewriteme">... (200; 37.041034ms)
    Jul 29 15:58:05.475: INFO: (18) /api/v1/namespaces/proxy-2078/services/http:proxy-service-gw6m5:portname1/proxy/: foo (200; 38.247791ms)
    Jul 29 15:58:05.476: INFO: (18) /api/v1/namespaces/proxy-2078/services/https:proxy-service-gw6m5:tlsportname2/proxy/: tls qux (200; 37.468815ms)
    Jul 29 15:58:05.476: INFO: (18) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:462/proxy/: tls qux (200; 37.636366ms)
    Jul 29 15:58:05.476: INFO: (18) /api/v1/namespaces/proxy-2078/services/proxy-service-gw6m5:portname1/proxy/: foo (200; 37.810309ms)
    Jul 29 15:58:05.476: INFO: (18) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:443/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:443/proxy/tlsrewritem... (200; 37.989137ms)
    Jul 29 15:58:05.491: INFO: (19) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:160/proxy/: foo (200; 15.597956ms)
    Jul 29 15:58:05.497: INFO: (19) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:443/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:443/proxy/tlsrewritem... (200; 20.063691ms)
    Jul 29 15:58:05.499: INFO: (19) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:162/proxy/: bar (200; 22.439266ms)
    Jul 29 15:58:05.499: INFO: (19) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv/proxy/rewriteme">test</a> (200; 22.002012ms)
    Jul 29 15:58:05.499: INFO: (19) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:1080/proxy/rewriteme">test<... (200; 22.629838ms)
    Jul 29 15:58:05.503: INFO: (19) /api/v1/namespaces/proxy-2078/services/proxy-service-gw6m5:portname2/proxy/: bar (200; 26.049193ms)
    Jul 29 15:58:05.504: INFO: (19) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:1080/proxy/: <a href="/api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:1080/proxy/rewriteme">... (200; 25.638969ms)
    Jul 29 15:58:05.504: INFO: (19) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:462/proxy/: tls qux (200; 27.728505ms)
    Jul 29 15:58:05.504: INFO: (19) /api/v1/namespaces/proxy-2078/pods/http:proxy-service-gw6m5-cvzxv:162/proxy/: bar (200; 27.394467ms)
    Jul 29 15:58:05.505: INFO: (19) /api/v1/namespaces/proxy-2078/services/proxy-service-gw6m5:portname1/proxy/: foo (200; 27.585549ms)
    Jul 29 15:58:05.505: INFO: (19) /api/v1/namespaces/proxy-2078/pods/proxy-service-gw6m5-cvzxv:160/proxy/: foo (200; 27.230081ms)
    Jul 29 15:58:05.505: INFO: (19) /api/v1/namespaces/proxy-2078/services/https:proxy-service-gw6m5:tlsportname1/proxy/: tls baz (200; 29.474547ms)
    Jul 29 15:58:05.506: INFO: (19) /api/v1/namespaces/proxy-2078/services/https:proxy-service-gw6m5:tlsportname2/proxy/: tls qux (200; 29.860279ms)
    Jul 29 15:58:05.507: INFO: (19) /api/v1/namespaces/proxy-2078/services/http:proxy-service-gw6m5:portname1/proxy/: foo (200; 29.622079ms)
    Jul 29 15:58:05.507: INFO: (19) /api/v1/namespaces/proxy-2078/pods/https:proxy-service-gw6m5-cvzxv:460/proxy/: tls baz (200; 29.344485ms)
    Jul 29 15:58:05.508: INFO: (19) /api/v1/namespaces/proxy-2078/services/http:proxy-service-gw6m5:portname2/proxy/: bar (200; 30.49504ms)
    STEP: deleting ReplicationController proxy-service-gw6m5 in namespace proxy-2078, will wait for the garbage collector to delete the pods 07/29/23 15:58:05.508
    Jul 29 15:58:05.578: INFO: Deleting ReplicationController proxy-service-gw6m5 took: 10.061354ms
    Jul 29 15:58:05.679: INFO: Terminating ReplicationController proxy-service-gw6m5 pods took: 100.667796ms
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    Jul 29 15:58:08.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-2078" for this suite. 07/29/23 15:58:08.189
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:58:08.206
Jul 29 15:58:08.206: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename emptydir 07/29/23 15:58:08.21
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:58:08.235
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:58:08.239
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196
STEP: Creating a pod to test emptydir 0644 on node default medium 07/29/23 15:58:08.244
Jul 29 15:58:08.261: INFO: Waiting up to 5m0s for pod "pod-928429c9-bfb4-4bda-8d7d-20f2a9a3920a" in namespace "emptydir-5375" to be "Succeeded or Failed"
Jul 29 15:58:08.266: INFO: Pod "pod-928429c9-bfb4-4bda-8d7d-20f2a9a3920a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.184171ms
Jul 29 15:58:10.277: INFO: Pod "pod-928429c9-bfb4-4bda-8d7d-20f2a9a3920a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016880517s
Jul 29 15:58:12.274: INFO: Pod "pod-928429c9-bfb4-4bda-8d7d-20f2a9a3920a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013724652s
STEP: Saw pod success 07/29/23 15:58:12.274
Jul 29 15:58:12.275: INFO: Pod "pod-928429c9-bfb4-4bda-8d7d-20f2a9a3920a" satisfied condition "Succeeded or Failed"
Jul 29 15:58:12.281: INFO: Trying to get logs from node wa4quivohpee-3 pod pod-928429c9-bfb4-4bda-8d7d-20f2a9a3920a container test-container: <nil>
STEP: delete the pod 07/29/23 15:58:12.295
Jul 29 15:58:12.320: INFO: Waiting for pod pod-928429c9-bfb4-4bda-8d7d-20f2a9a3920a to disappear
Jul 29 15:58:12.328: INFO: Pod pod-928429c9-bfb4-4bda-8d7d-20f2a9a3920a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jul 29 15:58:12.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5375" for this suite. 07/29/23 15:58:12.338
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":101,"skipped":1628,"failed":0}
------------------------------
â€¢ [4.146 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:58:08.206
    Jul 29 15:58:08.206: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename emptydir 07/29/23 15:58:08.21
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:58:08.235
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:58:08.239
    [It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:196
    STEP: Creating a pod to test emptydir 0644 on node default medium 07/29/23 15:58:08.244
    Jul 29 15:58:08.261: INFO: Waiting up to 5m0s for pod "pod-928429c9-bfb4-4bda-8d7d-20f2a9a3920a" in namespace "emptydir-5375" to be "Succeeded or Failed"
    Jul 29 15:58:08.266: INFO: Pod "pod-928429c9-bfb4-4bda-8d7d-20f2a9a3920a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.184171ms
    Jul 29 15:58:10.277: INFO: Pod "pod-928429c9-bfb4-4bda-8d7d-20f2a9a3920a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016880517s
    Jul 29 15:58:12.274: INFO: Pod "pod-928429c9-bfb4-4bda-8d7d-20f2a9a3920a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013724652s
    STEP: Saw pod success 07/29/23 15:58:12.274
    Jul 29 15:58:12.275: INFO: Pod "pod-928429c9-bfb4-4bda-8d7d-20f2a9a3920a" satisfied condition "Succeeded or Failed"
    Jul 29 15:58:12.281: INFO: Trying to get logs from node wa4quivohpee-3 pod pod-928429c9-bfb4-4bda-8d7d-20f2a9a3920a container test-container: <nil>
    STEP: delete the pod 07/29/23 15:58:12.295
    Jul 29 15:58:12.320: INFO: Waiting for pod pod-928429c9-bfb4-4bda-8d7d-20f2a9a3920a to disappear
    Jul 29 15:58:12.328: INFO: Pod pod-928429c9-bfb4-4bda-8d7d-20f2a9a3920a no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jul 29 15:58:12.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-5375" for this suite. 07/29/23 15:58:12.338
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:58:12.356
Jul 29 15:58:12.356: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename pod-network-test 07/29/23 15:58:12.36
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:58:12.399
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:58:12.405
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
STEP: Performing setup for networking test in namespace pod-network-test-5115 07/29/23 15:58:12.41
STEP: creating a selector 07/29/23 15:58:12.41
STEP: Creating the service pods in kubernetes 07/29/23 15:58:12.41
Jul 29 15:58:12.411: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jul 29 15:58:12.576: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-5115" to be "running and ready"
Jul 29 15:58:12.586: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 8.926568ms
Jul 29 15:58:12.586: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jul 29 15:58:14.595: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.018068199s
Jul 29 15:58:14.595: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jul 29 15:58:16.593: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.016793681s
Jul 29 15:58:16.594: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jul 29 15:58:18.592: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.015804163s
Jul 29 15:58:18.593: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jul 29 15:58:20.595: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.018179352s
Jul 29 15:58:20.595: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jul 29 15:58:22.595: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.017999045s
Jul 29 15:58:22.595: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jul 29 15:58:24.594: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.017490627s
Jul 29 15:58:24.594: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jul 29 15:58:26.595: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.018716678s
Jul 29 15:58:26.595: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jul 29 15:58:28.596: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.019360324s
Jul 29 15:58:28.596: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jul 29 15:58:30.600: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.022939877s
Jul 29 15:58:30.600: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jul 29 15:58:32.594: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.017818001s
Jul 29 15:58:32.594: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jul 29 15:58:34.597: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.020892983s
Jul 29 15:58:34.598: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jul 29 15:58:34.598: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jul 29 15:58:34.606: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-5115" to be "running and ready"
Jul 29 15:58:34.612: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 6.148088ms
Jul 29 15:58:34.612: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jul 29 15:58:34.612: INFO: Pod "netserver-1" satisfied condition "running and ready"
Jul 29 15:58:34.628: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-5115" to be "running and ready"
Jul 29 15:58:34.633: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 4.224912ms
Jul 29 15:58:34.633: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Jul 29 15:58:34.633: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 07/29/23 15:58:34.639
Jul 29 15:58:34.658: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-5115" to be "running"
Jul 29 15:58:34.680: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 21.85267ms
Jul 29 15:58:36.687: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.028903487s
Jul 29 15:58:36.687: INFO: Pod "test-container-pod" satisfied condition "running"
Jul 29 15:58:36.693: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-5115" to be "running"
Jul 29 15:58:36.699: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 5.368582ms
Jul 29 15:58:36.699: INFO: Pod "host-test-container-pod" satisfied condition "running"
Jul 29 15:58:36.703: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Jul 29 15:58:36.703: INFO: Going to poll 10.233.64.138 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Jul 29 15:58:36.707: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.64.138 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5115 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 29 15:58:36.708: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
Jul 29 15:58:36.710: INFO: ExecWithOptions: Clientset creation
Jul 29 15:58:36.710: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-5115/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.64.138+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jul 29 15:58:37.837: INFO: Found all 1 expected endpoints: [netserver-0]
Jul 29 15:58:37.837: INFO: Going to poll 10.233.66.215 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Jul 29 15:58:37.844: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.66.215 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5115 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 29 15:58:37.844: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
Jul 29 15:58:37.845: INFO: ExecWithOptions: Clientset creation
Jul 29 15:58:37.846: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-5115/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.66.215+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jul 29 15:58:38.949: INFO: Found all 1 expected endpoints: [netserver-1]
Jul 29 15:58:38.949: INFO: Going to poll 10.233.65.244 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Jul 29 15:58:38.955: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.65.244 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5115 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 29 15:58:38.955: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
Jul 29 15:58:38.956: INFO: ExecWithOptions: Clientset creation
Jul 29 15:58:38.956: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-5115/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.65.244+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jul 29 15:58:40.054: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Jul 29 15:58:40.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5115" for this suite. 07/29/23 15:58:40.063
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","completed":102,"skipped":1632,"failed":0}
------------------------------
â€¢ [SLOW TEST] [27.717 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:58:12.356
    Jul 29 15:58:12.356: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename pod-network-test 07/29/23 15:58:12.36
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:58:12.399
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:58:12.405
    [It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:122
    STEP: Performing setup for networking test in namespace pod-network-test-5115 07/29/23 15:58:12.41
    STEP: creating a selector 07/29/23 15:58:12.41
    STEP: Creating the service pods in kubernetes 07/29/23 15:58:12.41
    Jul 29 15:58:12.411: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jul 29 15:58:12.576: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-5115" to be "running and ready"
    Jul 29 15:58:12.586: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 8.926568ms
    Jul 29 15:58:12.586: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jul 29 15:58:14.595: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.018068199s
    Jul 29 15:58:14.595: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jul 29 15:58:16.593: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.016793681s
    Jul 29 15:58:16.594: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jul 29 15:58:18.592: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.015804163s
    Jul 29 15:58:18.593: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jul 29 15:58:20.595: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.018179352s
    Jul 29 15:58:20.595: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jul 29 15:58:22.595: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.017999045s
    Jul 29 15:58:22.595: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jul 29 15:58:24.594: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.017490627s
    Jul 29 15:58:24.594: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jul 29 15:58:26.595: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.018716678s
    Jul 29 15:58:26.595: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jul 29 15:58:28.596: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.019360324s
    Jul 29 15:58:28.596: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jul 29 15:58:30.600: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.022939877s
    Jul 29 15:58:30.600: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jul 29 15:58:32.594: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.017818001s
    Jul 29 15:58:32.594: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jul 29 15:58:34.597: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.020892983s
    Jul 29 15:58:34.598: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jul 29 15:58:34.598: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jul 29 15:58:34.606: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-5115" to be "running and ready"
    Jul 29 15:58:34.612: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 6.148088ms
    Jul 29 15:58:34.612: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jul 29 15:58:34.612: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Jul 29 15:58:34.628: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-5115" to be "running and ready"
    Jul 29 15:58:34.633: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 4.224912ms
    Jul 29 15:58:34.633: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Jul 29 15:58:34.633: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 07/29/23 15:58:34.639
    Jul 29 15:58:34.658: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-5115" to be "running"
    Jul 29 15:58:34.680: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 21.85267ms
    Jul 29 15:58:36.687: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.028903487s
    Jul 29 15:58:36.687: INFO: Pod "test-container-pod" satisfied condition "running"
    Jul 29 15:58:36.693: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-5115" to be "running"
    Jul 29 15:58:36.699: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 5.368582ms
    Jul 29 15:58:36.699: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Jul 29 15:58:36.703: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Jul 29 15:58:36.703: INFO: Going to poll 10.233.64.138 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Jul 29 15:58:36.707: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.64.138 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5115 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jul 29 15:58:36.708: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    Jul 29 15:58:36.710: INFO: ExecWithOptions: Clientset creation
    Jul 29 15:58:36.710: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-5115/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.64.138+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jul 29 15:58:37.837: INFO: Found all 1 expected endpoints: [netserver-0]
    Jul 29 15:58:37.837: INFO: Going to poll 10.233.66.215 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Jul 29 15:58:37.844: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.66.215 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5115 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jul 29 15:58:37.844: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    Jul 29 15:58:37.845: INFO: ExecWithOptions: Clientset creation
    Jul 29 15:58:37.846: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-5115/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.66.215+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jul 29 15:58:38.949: INFO: Found all 1 expected endpoints: [netserver-1]
    Jul 29 15:58:38.949: INFO: Going to poll 10.233.65.244 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Jul 29 15:58:38.955: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.65.244 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5115 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jul 29 15:58:38.955: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    Jul 29 15:58:38.956: INFO: ExecWithOptions: Clientset creation
    Jul 29 15:58:38.956: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-5115/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.65.244+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jul 29 15:58:40.054: INFO: Found all 1 expected endpoints: [netserver-2]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Jul 29 15:58:40.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-5115" for this suite. 07/29/23 15:58:40.063
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:58:40.093
Jul 29 15:58:40.093: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename endpointslice 07/29/23 15:58:40.096
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:58:40.127
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:58:40.131
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Jul 29 15:58:44.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-5701" for this suite. 07/29/23 15:58:44.235
{"msg":"PASSED [sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]","completed":103,"skipped":1684,"failed":0}
------------------------------
â€¢ [4.154 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:58:40.093
    Jul 29 15:58:40.093: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename endpointslice 07/29/23 15:58:40.096
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:58:40.127
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:58:40.131
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
      test/e2e/network/endpointslice.go:101
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Jul 29 15:58:44.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-5701" for this suite. 07/29/23 15:58:44.235
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:58:44.26
Jul 29 15:58:44.261: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename dns 07/29/23 15:58:44.263
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:58:44.296
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:58:44.3
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
STEP: Creating a test headless service 07/29/23 15:58:44.303
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5521 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5521;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5521 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5521;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5521.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5521.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5521.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5521.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5521.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-5521.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5521.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-5521.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5521.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-5521.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5521.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-5521.svc;check="$$(dig +notcp +noall +answer +search 200.20.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.20.200_udp@PTR;check="$$(dig +tcp +noall +answer +search 200.20.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.20.200_tcp@PTR;sleep 1; done
 07/29/23 15:58:44.332
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5521 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5521;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5521 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5521;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5521.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5521.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5521.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5521.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5521.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-5521.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5521.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-5521.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5521.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-5521.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5521.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-5521.svc;check="$$(dig +notcp +noall +answer +search 200.20.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.20.200_udp@PTR;check="$$(dig +tcp +noall +answer +search 200.20.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.20.200_tcp@PTR;sleep 1; done
 07/29/23 15:58:44.333
STEP: creating a pod to probe DNS 07/29/23 15:58:44.333
STEP: submitting the pod to kubernetes 07/29/23 15:58:44.337
Jul 29 15:58:44.357: INFO: Waiting up to 15m0s for pod "dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f" in namespace "dns-5521" to be "running"
Jul 29 15:58:44.366: INFO: Pod "dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.65843ms
Jul 29 15:58:46.378: INFO: Pod "dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f": Phase="Running", Reason="", readiness=true. Elapsed: 2.02132599s
Jul 29 15:58:46.379: INFO: Pod "dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f" satisfied condition "running"
STEP: retrieving the pod 07/29/23 15:58:46.379
STEP: looking for the results for each expected name from probers 07/29/23 15:58:46.389
Jul 29 15:58:46.400: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
Jul 29 15:58:46.407: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
Jul 29 15:58:46.418: INFO: Unable to read wheezy_udp@dns-test-service.dns-5521 from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
Jul 29 15:58:46.427: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5521 from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
Jul 29 15:58:46.435: INFO: Unable to read wheezy_udp@dns-test-service.dns-5521.svc from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
Jul 29 15:58:46.443: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5521.svc from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
Jul 29 15:58:46.450: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5521.svc from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
Jul 29 15:58:46.456: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5521.svc from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
Jul 29 15:58:46.491: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
Jul 29 15:58:46.499: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
Jul 29 15:58:46.512: INFO: Unable to read jessie_udp@dns-test-service.dns-5521 from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
Jul 29 15:58:46.521: INFO: Unable to read jessie_tcp@dns-test-service.dns-5521 from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
Jul 29 15:58:46.536: INFO: Unable to read jessie_udp@dns-test-service.dns-5521.svc from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
Jul 29 15:58:46.557: INFO: Unable to read jessie_tcp@dns-test-service.dns-5521.svc from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
Jul 29 15:58:46.582: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5521.svc from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
Jul 29 15:58:46.611: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5521.svc from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
Jul 29 15:58:46.705: INFO: Lookups using dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5521 wheezy_tcp@dns-test-service.dns-5521 wheezy_udp@dns-test-service.dns-5521.svc wheezy_tcp@dns-test-service.dns-5521.svc wheezy_udp@_http._tcp.dns-test-service.dns-5521.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5521.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5521 jessie_tcp@dns-test-service.dns-5521 jessie_udp@dns-test-service.dns-5521.svc jessie_tcp@dns-test-service.dns-5521.svc jessie_udp@_http._tcp.dns-test-service.dns-5521.svc jessie_tcp@_http._tcp.dns-test-service.dns-5521.svc]

Jul 29 15:58:51.719: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
Jul 29 15:58:51.725: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
Jul 29 15:58:51.730: INFO: Unable to read wheezy_udp@dns-test-service.dns-5521 from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
Jul 29 15:58:51.736: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5521 from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
Jul 29 15:58:51.741: INFO: Unable to read wheezy_udp@dns-test-service.dns-5521.svc from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
Jul 29 15:58:51.747: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5521.svc from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
Jul 29 15:58:51.754: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5521.svc from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
Jul 29 15:58:51.760: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5521.svc from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
Jul 29 15:58:51.797: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
Jul 29 15:58:51.802: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
Jul 29 15:58:51.810: INFO: Unable to read jessie_udp@dns-test-service.dns-5521 from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
Jul 29 15:58:51.816: INFO: Unable to read jessie_tcp@dns-test-service.dns-5521 from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
Jul 29 15:58:51.823: INFO: Unable to read jessie_udp@dns-test-service.dns-5521.svc from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
Jul 29 15:58:51.830: INFO: Unable to read jessie_tcp@dns-test-service.dns-5521.svc from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
Jul 29 15:58:51.836: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5521.svc from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
Jul 29 15:58:51.843: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5521.svc from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
Jul 29 15:58:51.870: INFO: Lookups using dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5521 wheezy_tcp@dns-test-service.dns-5521 wheezy_udp@dns-test-service.dns-5521.svc wheezy_tcp@dns-test-service.dns-5521.svc wheezy_udp@_http._tcp.dns-test-service.dns-5521.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5521.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5521 jessie_tcp@dns-test-service.dns-5521 jessie_udp@dns-test-service.dns-5521.svc jessie_tcp@dns-test-service.dns-5521.svc jessie_udp@_http._tcp.dns-test-service.dns-5521.svc jessie_tcp@_http._tcp.dns-test-service.dns-5521.svc]

Jul 29 15:58:56.716: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
Jul 29 15:58:56.723: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
Jul 29 15:58:56.732: INFO: Unable to read wheezy_udp@dns-test-service.dns-5521 from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
Jul 29 15:58:56.738: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5521 from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
Jul 29 15:58:56.744: INFO: Unable to read wheezy_udp@dns-test-service.dns-5521.svc from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
Jul 29 15:58:56.753: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5521.svc from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
Jul 29 15:58:56.759: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5521.svc from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
Jul 29 15:58:56.766: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5521.svc from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
Jul 29 15:58:56.798: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
Jul 29 15:58:56.806: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
Jul 29 15:58:56.813: INFO: Unable to read jessie_udp@dns-test-service.dns-5521 from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
Jul 29 15:58:56.821: INFO: Unable to read jessie_tcp@dns-test-service.dns-5521 from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
Jul 29 15:58:56.827: INFO: Unable to read jessie_udp@dns-test-service.dns-5521.svc from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
Jul 29 15:58:56.833: INFO: Unable to read jessie_tcp@dns-test-service.dns-5521.svc from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
Jul 29 15:58:56.837: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5521.svc from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
Jul 29 15:58:56.845: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5521.svc from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
Jul 29 15:58:56.868: INFO: Lookups using dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5521 wheezy_tcp@dns-test-service.dns-5521 wheezy_udp@dns-test-service.dns-5521.svc wheezy_tcp@dns-test-service.dns-5521.svc wheezy_udp@_http._tcp.dns-test-service.dns-5521.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5521.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5521 jessie_tcp@dns-test-service.dns-5521 jessie_udp@dns-test-service.dns-5521.svc jessie_tcp@dns-test-service.dns-5521.svc jessie_udp@_http._tcp.dns-test-service.dns-5521.svc jessie_tcp@_http._tcp.dns-test-service.dns-5521.svc]

Jul 29 15:59:01.719: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
Jul 29 15:59:01.730: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
Jul 29 15:59:01.737: INFO: Unable to read wheezy_udp@dns-test-service.dns-5521 from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
Jul 29 15:59:01.744: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5521 from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
Jul 29 15:59:01.750: INFO: Unable to read wheezy_udp@dns-test-service.dns-5521.svc from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
Jul 29 15:59:01.756: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5521.svc from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
Jul 29 15:59:01.762: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5521.svc from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
Jul 29 15:59:01.768: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5521.svc from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
Jul 29 15:59:01.800: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
Jul 29 15:59:01.806: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
Jul 29 15:59:01.813: INFO: Unable to read jessie_udp@dns-test-service.dns-5521 from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
Jul 29 15:59:01.820: INFO: Unable to read jessie_tcp@dns-test-service.dns-5521 from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
Jul 29 15:59:01.828: INFO: Unable to read jessie_udp@dns-test-service.dns-5521.svc from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
Jul 29 15:59:01.836: INFO: Unable to read jessie_tcp@dns-test-service.dns-5521.svc from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
Jul 29 15:59:01.845: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5521.svc from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
Jul 29 15:59:01.852: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5521.svc from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
Jul 29 15:59:01.876: INFO: Lookups using dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5521 wheezy_tcp@dns-test-service.dns-5521 wheezy_udp@dns-test-service.dns-5521.svc wheezy_tcp@dns-test-service.dns-5521.svc wheezy_udp@_http._tcp.dns-test-service.dns-5521.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5521.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5521 jessie_tcp@dns-test-service.dns-5521 jessie_udp@dns-test-service.dns-5521.svc jessie_tcp@dns-test-service.dns-5521.svc jessie_udp@_http._tcp.dns-test-service.dns-5521.svc jessie_tcp@_http._tcp.dns-test-service.dns-5521.svc]

Jul 29 15:59:06.724: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
Jul 29 15:59:06.733: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
Jul 29 15:59:06.743: INFO: Unable to read wheezy_udp@dns-test-service.dns-5521 from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
Jul 29 15:59:06.760: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5521 from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
Jul 29 15:59:06.778: INFO: Unable to read wheezy_udp@dns-test-service.dns-5521.svc from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
Jul 29 15:59:06.787: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5521.svc from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
Jul 29 15:59:06.804: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5521.svc from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
Jul 29 15:59:06.818: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5521.svc from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
Jul 29 15:59:06.891: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
Jul 29 15:59:06.903: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
Jul 29 15:59:06.915: INFO: Unable to read jessie_udp@dns-test-service.dns-5521 from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
Jul 29 15:59:06.931: INFO: Unable to read jessie_tcp@dns-test-service.dns-5521 from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
Jul 29 15:59:06.941: INFO: Unable to read jessie_udp@dns-test-service.dns-5521.svc from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
Jul 29 15:59:06.951: INFO: Unable to read jessie_tcp@dns-test-service.dns-5521.svc from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
Jul 29 15:59:06.960: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5521.svc from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
Jul 29 15:59:06.972: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5521.svc from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
Jul 29 15:59:07.021: INFO: Lookups using dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5521 wheezy_tcp@dns-test-service.dns-5521 wheezy_udp@dns-test-service.dns-5521.svc wheezy_tcp@dns-test-service.dns-5521.svc wheezy_udp@_http._tcp.dns-test-service.dns-5521.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5521.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5521 jessie_tcp@dns-test-service.dns-5521 jessie_udp@dns-test-service.dns-5521.svc jessie_tcp@dns-test-service.dns-5521.svc jessie_udp@_http._tcp.dns-test-service.dns-5521.svc jessie_tcp@_http._tcp.dns-test-service.dns-5521.svc]

Jul 29 15:59:11.716: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
Jul 29 15:59:11.727: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
Jul 29 15:59:11.736: INFO: Unable to read wheezy_udp@dns-test-service.dns-5521 from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
Jul 29 15:59:11.743: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5521 from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
Jul 29 15:59:11.751: INFO: Unable to read wheezy_udp@dns-test-service.dns-5521.svc from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
Jul 29 15:59:11.757: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5521.svc from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
Jul 29 15:59:11.764: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5521.svc from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
Jul 29 15:59:11.770: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5521.svc from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
Jul 29 15:59:11.801: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
Jul 29 15:59:11.807: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
Jul 29 15:59:11.813: INFO: Unable to read jessie_udp@dns-test-service.dns-5521 from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
Jul 29 15:59:11.821: INFO: Unable to read jessie_tcp@dns-test-service.dns-5521 from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
Jul 29 15:59:11.827: INFO: Unable to read jessie_udp@dns-test-service.dns-5521.svc from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
Jul 29 15:59:11.833: INFO: Unable to read jessie_tcp@dns-test-service.dns-5521.svc from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
Jul 29 15:59:11.852: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5521.svc from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
Jul 29 15:59:11.859: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5521.svc from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
Jul 29 15:59:11.891: INFO: Lookups using dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5521 wheezy_tcp@dns-test-service.dns-5521 wheezy_udp@dns-test-service.dns-5521.svc wheezy_tcp@dns-test-service.dns-5521.svc wheezy_udp@_http._tcp.dns-test-service.dns-5521.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5521.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5521 jessie_tcp@dns-test-service.dns-5521 jessie_udp@dns-test-service.dns-5521.svc jessie_tcp@dns-test-service.dns-5521.svc jessie_udp@_http._tcp.dns-test-service.dns-5521.svc jessie_tcp@_http._tcp.dns-test-service.dns-5521.svc]

Jul 29 15:59:16.745: INFO: Unable to read wheezy_udp@dns-test-service.dns-5521 from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
Jul 29 15:59:16.755: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5521 from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
Jul 29 15:59:16.763: INFO: Unable to read wheezy_udp@dns-test-service.dns-5521.svc from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
Jul 29 15:59:16.774: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5521.svc from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
Jul 29 15:59:16.889: INFO: Lookups using dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f failed for: [wheezy_udp@dns-test-service.dns-5521 wheezy_tcp@dns-test-service.dns-5521 wheezy_udp@dns-test-service.dns-5521.svc wheezy_udp@_http._tcp.dns-test-service.dns-5521.svc]

Jul 29 15:59:21.906: INFO: DNS probes using dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f succeeded

STEP: deleting the pod 07/29/23 15:59:21.906
STEP: deleting the test service 07/29/23 15:59:21.991
STEP: deleting the test headless service 07/29/23 15:59:22.044
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jul 29 15:59:22.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5521" for this suite. 07/29/23 15:59:22.084
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","completed":104,"skipped":1724,"failed":0}
------------------------------
â€¢ [SLOW TEST] [37.841 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:58:44.26
    Jul 29 15:58:44.261: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename dns 07/29/23 15:58:44.263
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:58:44.296
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:58:44.3
    [It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
      test/e2e/network/dns.go:193
    STEP: Creating a test headless service 07/29/23 15:58:44.303
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5521 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5521;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5521 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5521;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5521.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5521.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5521.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5521.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5521.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-5521.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5521.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-5521.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5521.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-5521.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5521.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-5521.svc;check="$$(dig +notcp +noall +answer +search 200.20.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.20.200_udp@PTR;check="$$(dig +tcp +noall +answer +search 200.20.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.20.200_tcp@PTR;sleep 1; done
     07/29/23 15:58:44.332
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5521 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5521;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5521 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5521;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5521.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5521.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5521.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5521.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5521.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-5521.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5521.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-5521.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5521.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-5521.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5521.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-5521.svc;check="$$(dig +notcp +noall +answer +search 200.20.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.20.200_udp@PTR;check="$$(dig +tcp +noall +answer +search 200.20.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.20.200_tcp@PTR;sleep 1; done
     07/29/23 15:58:44.333
    STEP: creating a pod to probe DNS 07/29/23 15:58:44.333
    STEP: submitting the pod to kubernetes 07/29/23 15:58:44.337
    Jul 29 15:58:44.357: INFO: Waiting up to 15m0s for pod "dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f" in namespace "dns-5521" to be "running"
    Jul 29 15:58:44.366: INFO: Pod "dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.65843ms
    Jul 29 15:58:46.378: INFO: Pod "dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f": Phase="Running", Reason="", readiness=true. Elapsed: 2.02132599s
    Jul 29 15:58:46.379: INFO: Pod "dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f" satisfied condition "running"
    STEP: retrieving the pod 07/29/23 15:58:46.379
    STEP: looking for the results for each expected name from probers 07/29/23 15:58:46.389
    Jul 29 15:58:46.400: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
    Jul 29 15:58:46.407: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
    Jul 29 15:58:46.418: INFO: Unable to read wheezy_udp@dns-test-service.dns-5521 from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
    Jul 29 15:58:46.427: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5521 from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
    Jul 29 15:58:46.435: INFO: Unable to read wheezy_udp@dns-test-service.dns-5521.svc from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
    Jul 29 15:58:46.443: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5521.svc from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
    Jul 29 15:58:46.450: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5521.svc from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
    Jul 29 15:58:46.456: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5521.svc from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
    Jul 29 15:58:46.491: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
    Jul 29 15:58:46.499: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
    Jul 29 15:58:46.512: INFO: Unable to read jessie_udp@dns-test-service.dns-5521 from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
    Jul 29 15:58:46.521: INFO: Unable to read jessie_tcp@dns-test-service.dns-5521 from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
    Jul 29 15:58:46.536: INFO: Unable to read jessie_udp@dns-test-service.dns-5521.svc from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
    Jul 29 15:58:46.557: INFO: Unable to read jessie_tcp@dns-test-service.dns-5521.svc from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
    Jul 29 15:58:46.582: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5521.svc from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
    Jul 29 15:58:46.611: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5521.svc from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
    Jul 29 15:58:46.705: INFO: Lookups using dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5521 wheezy_tcp@dns-test-service.dns-5521 wheezy_udp@dns-test-service.dns-5521.svc wheezy_tcp@dns-test-service.dns-5521.svc wheezy_udp@_http._tcp.dns-test-service.dns-5521.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5521.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5521 jessie_tcp@dns-test-service.dns-5521 jessie_udp@dns-test-service.dns-5521.svc jessie_tcp@dns-test-service.dns-5521.svc jessie_udp@_http._tcp.dns-test-service.dns-5521.svc jessie_tcp@_http._tcp.dns-test-service.dns-5521.svc]

    Jul 29 15:58:51.719: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
    Jul 29 15:58:51.725: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
    Jul 29 15:58:51.730: INFO: Unable to read wheezy_udp@dns-test-service.dns-5521 from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
    Jul 29 15:58:51.736: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5521 from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
    Jul 29 15:58:51.741: INFO: Unable to read wheezy_udp@dns-test-service.dns-5521.svc from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
    Jul 29 15:58:51.747: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5521.svc from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
    Jul 29 15:58:51.754: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5521.svc from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
    Jul 29 15:58:51.760: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5521.svc from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
    Jul 29 15:58:51.797: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
    Jul 29 15:58:51.802: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
    Jul 29 15:58:51.810: INFO: Unable to read jessie_udp@dns-test-service.dns-5521 from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
    Jul 29 15:58:51.816: INFO: Unable to read jessie_tcp@dns-test-service.dns-5521 from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
    Jul 29 15:58:51.823: INFO: Unable to read jessie_udp@dns-test-service.dns-5521.svc from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
    Jul 29 15:58:51.830: INFO: Unable to read jessie_tcp@dns-test-service.dns-5521.svc from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
    Jul 29 15:58:51.836: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5521.svc from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
    Jul 29 15:58:51.843: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5521.svc from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
    Jul 29 15:58:51.870: INFO: Lookups using dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5521 wheezy_tcp@dns-test-service.dns-5521 wheezy_udp@dns-test-service.dns-5521.svc wheezy_tcp@dns-test-service.dns-5521.svc wheezy_udp@_http._tcp.dns-test-service.dns-5521.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5521.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5521 jessie_tcp@dns-test-service.dns-5521 jessie_udp@dns-test-service.dns-5521.svc jessie_tcp@dns-test-service.dns-5521.svc jessie_udp@_http._tcp.dns-test-service.dns-5521.svc jessie_tcp@_http._tcp.dns-test-service.dns-5521.svc]

    Jul 29 15:58:56.716: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
    Jul 29 15:58:56.723: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
    Jul 29 15:58:56.732: INFO: Unable to read wheezy_udp@dns-test-service.dns-5521 from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
    Jul 29 15:58:56.738: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5521 from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
    Jul 29 15:58:56.744: INFO: Unable to read wheezy_udp@dns-test-service.dns-5521.svc from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
    Jul 29 15:58:56.753: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5521.svc from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
    Jul 29 15:58:56.759: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5521.svc from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
    Jul 29 15:58:56.766: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5521.svc from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
    Jul 29 15:58:56.798: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
    Jul 29 15:58:56.806: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
    Jul 29 15:58:56.813: INFO: Unable to read jessie_udp@dns-test-service.dns-5521 from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
    Jul 29 15:58:56.821: INFO: Unable to read jessie_tcp@dns-test-service.dns-5521 from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
    Jul 29 15:58:56.827: INFO: Unable to read jessie_udp@dns-test-service.dns-5521.svc from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
    Jul 29 15:58:56.833: INFO: Unable to read jessie_tcp@dns-test-service.dns-5521.svc from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
    Jul 29 15:58:56.837: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5521.svc from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
    Jul 29 15:58:56.845: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5521.svc from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
    Jul 29 15:58:56.868: INFO: Lookups using dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5521 wheezy_tcp@dns-test-service.dns-5521 wheezy_udp@dns-test-service.dns-5521.svc wheezy_tcp@dns-test-service.dns-5521.svc wheezy_udp@_http._tcp.dns-test-service.dns-5521.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5521.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5521 jessie_tcp@dns-test-service.dns-5521 jessie_udp@dns-test-service.dns-5521.svc jessie_tcp@dns-test-service.dns-5521.svc jessie_udp@_http._tcp.dns-test-service.dns-5521.svc jessie_tcp@_http._tcp.dns-test-service.dns-5521.svc]

    Jul 29 15:59:01.719: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
    Jul 29 15:59:01.730: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
    Jul 29 15:59:01.737: INFO: Unable to read wheezy_udp@dns-test-service.dns-5521 from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
    Jul 29 15:59:01.744: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5521 from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
    Jul 29 15:59:01.750: INFO: Unable to read wheezy_udp@dns-test-service.dns-5521.svc from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
    Jul 29 15:59:01.756: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5521.svc from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
    Jul 29 15:59:01.762: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5521.svc from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
    Jul 29 15:59:01.768: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5521.svc from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
    Jul 29 15:59:01.800: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
    Jul 29 15:59:01.806: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
    Jul 29 15:59:01.813: INFO: Unable to read jessie_udp@dns-test-service.dns-5521 from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
    Jul 29 15:59:01.820: INFO: Unable to read jessie_tcp@dns-test-service.dns-5521 from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
    Jul 29 15:59:01.828: INFO: Unable to read jessie_udp@dns-test-service.dns-5521.svc from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
    Jul 29 15:59:01.836: INFO: Unable to read jessie_tcp@dns-test-service.dns-5521.svc from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
    Jul 29 15:59:01.845: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5521.svc from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
    Jul 29 15:59:01.852: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5521.svc from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
    Jul 29 15:59:01.876: INFO: Lookups using dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5521 wheezy_tcp@dns-test-service.dns-5521 wheezy_udp@dns-test-service.dns-5521.svc wheezy_tcp@dns-test-service.dns-5521.svc wheezy_udp@_http._tcp.dns-test-service.dns-5521.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5521.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5521 jessie_tcp@dns-test-service.dns-5521 jessie_udp@dns-test-service.dns-5521.svc jessie_tcp@dns-test-service.dns-5521.svc jessie_udp@_http._tcp.dns-test-service.dns-5521.svc jessie_tcp@_http._tcp.dns-test-service.dns-5521.svc]

    Jul 29 15:59:06.724: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
    Jul 29 15:59:06.733: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
    Jul 29 15:59:06.743: INFO: Unable to read wheezy_udp@dns-test-service.dns-5521 from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
    Jul 29 15:59:06.760: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5521 from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
    Jul 29 15:59:06.778: INFO: Unable to read wheezy_udp@dns-test-service.dns-5521.svc from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
    Jul 29 15:59:06.787: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5521.svc from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
    Jul 29 15:59:06.804: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5521.svc from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
    Jul 29 15:59:06.818: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5521.svc from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
    Jul 29 15:59:06.891: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
    Jul 29 15:59:06.903: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
    Jul 29 15:59:06.915: INFO: Unable to read jessie_udp@dns-test-service.dns-5521 from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
    Jul 29 15:59:06.931: INFO: Unable to read jessie_tcp@dns-test-service.dns-5521 from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
    Jul 29 15:59:06.941: INFO: Unable to read jessie_udp@dns-test-service.dns-5521.svc from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
    Jul 29 15:59:06.951: INFO: Unable to read jessie_tcp@dns-test-service.dns-5521.svc from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
    Jul 29 15:59:06.960: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5521.svc from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
    Jul 29 15:59:06.972: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5521.svc from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
    Jul 29 15:59:07.021: INFO: Lookups using dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5521 wheezy_tcp@dns-test-service.dns-5521 wheezy_udp@dns-test-service.dns-5521.svc wheezy_tcp@dns-test-service.dns-5521.svc wheezy_udp@_http._tcp.dns-test-service.dns-5521.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5521.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5521 jessie_tcp@dns-test-service.dns-5521 jessie_udp@dns-test-service.dns-5521.svc jessie_tcp@dns-test-service.dns-5521.svc jessie_udp@_http._tcp.dns-test-service.dns-5521.svc jessie_tcp@_http._tcp.dns-test-service.dns-5521.svc]

    Jul 29 15:59:11.716: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
    Jul 29 15:59:11.727: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
    Jul 29 15:59:11.736: INFO: Unable to read wheezy_udp@dns-test-service.dns-5521 from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
    Jul 29 15:59:11.743: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5521 from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
    Jul 29 15:59:11.751: INFO: Unable to read wheezy_udp@dns-test-service.dns-5521.svc from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
    Jul 29 15:59:11.757: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5521.svc from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
    Jul 29 15:59:11.764: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5521.svc from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
    Jul 29 15:59:11.770: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5521.svc from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
    Jul 29 15:59:11.801: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
    Jul 29 15:59:11.807: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
    Jul 29 15:59:11.813: INFO: Unable to read jessie_udp@dns-test-service.dns-5521 from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
    Jul 29 15:59:11.821: INFO: Unable to read jessie_tcp@dns-test-service.dns-5521 from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
    Jul 29 15:59:11.827: INFO: Unable to read jessie_udp@dns-test-service.dns-5521.svc from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
    Jul 29 15:59:11.833: INFO: Unable to read jessie_tcp@dns-test-service.dns-5521.svc from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
    Jul 29 15:59:11.852: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5521.svc from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
    Jul 29 15:59:11.859: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5521.svc from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
    Jul 29 15:59:11.891: INFO: Lookups using dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5521 wheezy_tcp@dns-test-service.dns-5521 wheezy_udp@dns-test-service.dns-5521.svc wheezy_tcp@dns-test-service.dns-5521.svc wheezy_udp@_http._tcp.dns-test-service.dns-5521.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5521.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5521 jessie_tcp@dns-test-service.dns-5521 jessie_udp@dns-test-service.dns-5521.svc jessie_tcp@dns-test-service.dns-5521.svc jessie_udp@_http._tcp.dns-test-service.dns-5521.svc jessie_tcp@_http._tcp.dns-test-service.dns-5521.svc]

    Jul 29 15:59:16.745: INFO: Unable to read wheezy_udp@dns-test-service.dns-5521 from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
    Jul 29 15:59:16.755: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5521 from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
    Jul 29 15:59:16.763: INFO: Unable to read wheezy_udp@dns-test-service.dns-5521.svc from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
    Jul 29 15:59:16.774: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5521.svc from pod dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f: the server could not find the requested resource (get pods dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f)
    Jul 29 15:59:16.889: INFO: Lookups using dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f failed for: [wheezy_udp@dns-test-service.dns-5521 wheezy_tcp@dns-test-service.dns-5521 wheezy_udp@dns-test-service.dns-5521.svc wheezy_udp@_http._tcp.dns-test-service.dns-5521.svc]

    Jul 29 15:59:21.906: INFO: DNS probes using dns-5521/dns-test-080aeba9-637b-43b4-ae44-fccbce8e055f succeeded

    STEP: deleting the pod 07/29/23 15:59:21.906
    STEP: deleting the test service 07/29/23 15:59:21.991
    STEP: deleting the test headless service 07/29/23 15:59:22.044
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jul 29 15:59:22.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-5521" for this suite. 07/29/23 15:59:22.084
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:59:22.114
Jul 29 15:59:22.114: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename disruption 07/29/23 15:59:22.121
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:59:22.158
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:59:22.165
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140
STEP: Waiting for the pdb to be processed 07/29/23 15:59:22.183
STEP: Waiting for all pods to be running 07/29/23 15:59:24.247
Jul 29 15:59:24.266: INFO: running pods: 0 < 3
Jul 29 15:59:26.275: INFO: running pods: 0 < 3
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Jul 29 15:59:28.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-6605" for this suite. 07/29/23 15:59:28.293
{"msg":"PASSED [sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]","completed":105,"skipped":1737,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.191 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:59:22.114
    Jul 29 15:59:22.114: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename disruption 07/29/23 15:59:22.121
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:59:22.158
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:59:22.165
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should observe PodDisruptionBudget status updated [Conformance]
      test/e2e/apps/disruption.go:140
    STEP: Waiting for the pdb to be processed 07/29/23 15:59:22.183
    STEP: Waiting for all pods to be running 07/29/23 15:59:24.247
    Jul 29 15:59:24.266: INFO: running pods: 0 < 3
    Jul 29 15:59:26.275: INFO: running pods: 0 < 3
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Jul 29 15:59:28.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-6605" for this suite. 07/29/23 15:59:28.293
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:59:28.308
Jul 29 15:59:28.308: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename containers 07/29/23 15:59:28.31
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:59:28.337
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:59:28.342
[It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72
STEP: Creating a pod to test override command 07/29/23 15:59:28.347
Jul 29 15:59:28.368: INFO: Waiting up to 5m0s for pod "client-containers-dc4fc22a-f58b-4059-acca-69f43be2322d" in namespace "containers-8808" to be "Succeeded or Failed"
Jul 29 15:59:28.373: INFO: Pod "client-containers-dc4fc22a-f58b-4059-acca-69f43be2322d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.657685ms
Jul 29 15:59:30.387: INFO: Pod "client-containers-dc4fc22a-f58b-4059-acca-69f43be2322d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017956165s
Jul 29 15:59:32.382: INFO: Pod "client-containers-dc4fc22a-f58b-4059-acca-69f43be2322d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013043808s
STEP: Saw pod success 07/29/23 15:59:32.382
Jul 29 15:59:32.382: INFO: Pod "client-containers-dc4fc22a-f58b-4059-acca-69f43be2322d" satisfied condition "Succeeded or Failed"
Jul 29 15:59:32.388: INFO: Trying to get logs from node wa4quivohpee-3 pod client-containers-dc4fc22a-f58b-4059-acca-69f43be2322d container agnhost-container: <nil>
STEP: delete the pod 07/29/23 15:59:32.411
Jul 29 15:59:32.437: INFO: Waiting for pod client-containers-dc4fc22a-f58b-4059-acca-69f43be2322d to disappear
Jul 29 15:59:32.447: INFO: Pod client-containers-dc4fc22a-f58b-4059-acca-69f43be2322d no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Jul 29 15:59:32.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-8808" for this suite. 07/29/23 15:59:32.469
{"msg":"PASSED [sig-node] Containers should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]","completed":106,"skipped":1746,"failed":0}
------------------------------
â€¢ [4.198 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:59:28.308
    Jul 29 15:59:28.308: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename containers 07/29/23 15:59:28.31
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:59:28.337
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:59:28.342
    [It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:72
    STEP: Creating a pod to test override command 07/29/23 15:59:28.347
    Jul 29 15:59:28.368: INFO: Waiting up to 5m0s for pod "client-containers-dc4fc22a-f58b-4059-acca-69f43be2322d" in namespace "containers-8808" to be "Succeeded or Failed"
    Jul 29 15:59:28.373: INFO: Pod "client-containers-dc4fc22a-f58b-4059-acca-69f43be2322d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.657685ms
    Jul 29 15:59:30.387: INFO: Pod "client-containers-dc4fc22a-f58b-4059-acca-69f43be2322d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017956165s
    Jul 29 15:59:32.382: INFO: Pod "client-containers-dc4fc22a-f58b-4059-acca-69f43be2322d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013043808s
    STEP: Saw pod success 07/29/23 15:59:32.382
    Jul 29 15:59:32.382: INFO: Pod "client-containers-dc4fc22a-f58b-4059-acca-69f43be2322d" satisfied condition "Succeeded or Failed"
    Jul 29 15:59:32.388: INFO: Trying to get logs from node wa4quivohpee-3 pod client-containers-dc4fc22a-f58b-4059-acca-69f43be2322d container agnhost-container: <nil>
    STEP: delete the pod 07/29/23 15:59:32.411
    Jul 29 15:59:32.437: INFO: Waiting for pod client-containers-dc4fc22a-f58b-4059-acca-69f43be2322d to disappear
    Jul 29 15:59:32.447: INFO: Pod client-containers-dc4fc22a-f58b-4059-acca-69f43be2322d no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Jul 29 15:59:32.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-8808" for this suite. 07/29/23 15:59:32.469
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:59:32.511
Jul 29 15:59:32.511: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename emptydir 07/29/23 15:59:32.518
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:59:32.57
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:59:32.574
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186
STEP: Creating a pod to test emptydir 0777 on node default medium 07/29/23 15:59:32.58
Jul 29 15:59:32.597: INFO: Waiting up to 5m0s for pod "pod-06f6d116-b966-4172-a7cc-2e47bacdbad4" in namespace "emptydir-5243" to be "Succeeded or Failed"
Jul 29 15:59:32.607: INFO: Pod "pod-06f6d116-b966-4172-a7cc-2e47bacdbad4": Phase="Pending", Reason="", readiness=false. Elapsed: 9.994802ms
Jul 29 15:59:34.619: INFO: Pod "pod-06f6d116-b966-4172-a7cc-2e47bacdbad4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022245076s
Jul 29 15:59:36.618: INFO: Pod "pod-06f6d116-b966-4172-a7cc-2e47bacdbad4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.02122554s
Jul 29 15:59:38.622: INFO: Pod "pod-06f6d116-b966-4172-a7cc-2e47bacdbad4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.025065107s
STEP: Saw pod success 07/29/23 15:59:38.622
Jul 29 15:59:38.623: INFO: Pod "pod-06f6d116-b966-4172-a7cc-2e47bacdbad4" satisfied condition "Succeeded or Failed"
Jul 29 15:59:38.628: INFO: Trying to get logs from node wa4quivohpee-3 pod pod-06f6d116-b966-4172-a7cc-2e47bacdbad4 container test-container: <nil>
STEP: delete the pod 07/29/23 15:59:38.639
Jul 29 15:59:38.686: INFO: Waiting for pod pod-06f6d116-b966-4172-a7cc-2e47bacdbad4 to disappear
Jul 29 15:59:38.692: INFO: Pod pod-06f6d116-b966-4172-a7cc-2e47bacdbad4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jul 29 15:59:38.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5243" for this suite. 07/29/23 15:59:38.701
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":107,"skipped":1755,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.213 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:59:32.511
    Jul 29 15:59:32.511: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename emptydir 07/29/23 15:59:32.518
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:59:32.57
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:59:32.574
    [It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:186
    STEP: Creating a pod to test emptydir 0777 on node default medium 07/29/23 15:59:32.58
    Jul 29 15:59:32.597: INFO: Waiting up to 5m0s for pod "pod-06f6d116-b966-4172-a7cc-2e47bacdbad4" in namespace "emptydir-5243" to be "Succeeded or Failed"
    Jul 29 15:59:32.607: INFO: Pod "pod-06f6d116-b966-4172-a7cc-2e47bacdbad4": Phase="Pending", Reason="", readiness=false. Elapsed: 9.994802ms
    Jul 29 15:59:34.619: INFO: Pod "pod-06f6d116-b966-4172-a7cc-2e47bacdbad4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022245076s
    Jul 29 15:59:36.618: INFO: Pod "pod-06f6d116-b966-4172-a7cc-2e47bacdbad4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.02122554s
    Jul 29 15:59:38.622: INFO: Pod "pod-06f6d116-b966-4172-a7cc-2e47bacdbad4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.025065107s
    STEP: Saw pod success 07/29/23 15:59:38.622
    Jul 29 15:59:38.623: INFO: Pod "pod-06f6d116-b966-4172-a7cc-2e47bacdbad4" satisfied condition "Succeeded or Failed"
    Jul 29 15:59:38.628: INFO: Trying to get logs from node wa4quivohpee-3 pod pod-06f6d116-b966-4172-a7cc-2e47bacdbad4 container test-container: <nil>
    STEP: delete the pod 07/29/23 15:59:38.639
    Jul 29 15:59:38.686: INFO: Waiting for pod pod-06f6d116-b966-4172-a7cc-2e47bacdbad4 to disappear
    Jul 29 15:59:38.692: INFO: Pod pod-06f6d116-b966-4172-a7cc-2e47bacdbad4 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jul 29 15:59:38.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-5243" for this suite. 07/29/23 15:59:38.701
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:59:38.733
Jul 29 15:59:38.733: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename webhook 07/29/23 15:59:38.735
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:59:38.765
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:59:38.771
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 07/29/23 15:59:38.8
STEP: Create role binding to let webhook read extension-apiserver-authentication 07/29/23 15:59:39.638
STEP: Deploying the webhook pod 07/29/23 15:59:39.653
STEP: Wait for the deployment to be ready 07/29/23 15:59:39.674
Jul 29 15:59:39.689: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 07/29/23 15:59:41.71
STEP: Verifying the service has paired with the endpoint 07/29/23 15:59:41.729
Jul 29 15:59:42.730: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507
STEP: Creating a mutating webhook configuration 07/29/23 15:59:42.737
STEP: Updating a mutating webhook configuration's rules to not include the create operation 07/29/23 15:59:42.772
STEP: Creating a configMap that should not be mutated 07/29/23 15:59:42.783
STEP: Patching a mutating webhook configuration's rules to include the create operation 07/29/23 15:59:42.799
STEP: Creating a configMap that should be mutated 07/29/23 15:59:42.812
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jul 29 15:59:42.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1460" for this suite. 07/29/23 15:59:42.867
STEP: Destroying namespace "webhook-1460-markers" for this suite. 07/29/23 15:59:42.878
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","completed":108,"skipped":1782,"failed":0}
------------------------------
â€¢ [4.259 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:59:38.733
    Jul 29 15:59:38.733: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename webhook 07/29/23 15:59:38.735
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:59:38.765
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:59:38.771
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 07/29/23 15:59:38.8
    STEP: Create role binding to let webhook read extension-apiserver-authentication 07/29/23 15:59:39.638
    STEP: Deploying the webhook pod 07/29/23 15:59:39.653
    STEP: Wait for the deployment to be ready 07/29/23 15:59:39.674
    Jul 29 15:59:39.689: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 07/29/23 15:59:41.71
    STEP: Verifying the service has paired with the endpoint 07/29/23 15:59:41.729
    Jul 29 15:59:42.730: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a mutating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:507
    STEP: Creating a mutating webhook configuration 07/29/23 15:59:42.737
    STEP: Updating a mutating webhook configuration's rules to not include the create operation 07/29/23 15:59:42.772
    STEP: Creating a configMap that should not be mutated 07/29/23 15:59:42.783
    STEP: Patching a mutating webhook configuration's rules to include the create operation 07/29/23 15:59:42.799
    STEP: Creating a configMap that should be mutated 07/29/23 15:59:42.812
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jul 29 15:59:42.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-1460" for this suite. 07/29/23 15:59:42.867
    STEP: Destroying namespace "webhook-1460-markers" for this suite. 07/29/23 15:59:42.878
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:59:43.006
Jul 29 15:59:43.006: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename downward-api 07/29/23 15:59:43.01
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:59:43.061
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:59:43.067
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216
STEP: Creating a pod to test downward api env vars 07/29/23 15:59:43.074
Jul 29 15:59:43.087: INFO: Waiting up to 5m0s for pod "downward-api-4595e167-9d09-4cdb-b642-f262bc557152" in namespace "downward-api-7808" to be "Succeeded or Failed"
Jul 29 15:59:43.093: INFO: Pod "downward-api-4595e167-9d09-4cdb-b642-f262bc557152": Phase="Pending", Reason="", readiness=false. Elapsed: 5.822752ms
Jul 29 15:59:45.103: INFO: Pod "downward-api-4595e167-9d09-4cdb-b642-f262bc557152": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015341016s
Jul 29 15:59:47.105: INFO: Pod "downward-api-4595e167-9d09-4cdb-b642-f262bc557152": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017213826s
STEP: Saw pod success 07/29/23 15:59:47.105
Jul 29 15:59:47.106: INFO: Pod "downward-api-4595e167-9d09-4cdb-b642-f262bc557152" satisfied condition "Succeeded or Failed"
Jul 29 15:59:47.114: INFO: Trying to get logs from node wa4quivohpee-3 pod downward-api-4595e167-9d09-4cdb-b642-f262bc557152 container dapi-container: <nil>
STEP: delete the pod 07/29/23 15:59:47.132
Jul 29 15:59:47.159: INFO: Waiting for pod downward-api-4595e167-9d09-4cdb-b642-f262bc557152 to disappear
Jul 29 15:59:47.164: INFO: Pod downward-api-4595e167-9d09-4cdb-b642-f262bc557152 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Jul 29 15:59:47.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7808" for this suite. 07/29/23 15:59:47.189
{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","completed":109,"skipped":1839,"failed":0}
------------------------------
â€¢ [4.201 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:59:43.006
    Jul 29 15:59:43.006: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename downward-api 07/29/23 15:59:43.01
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:59:43.061
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:59:43.067
    [It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:216
    STEP: Creating a pod to test downward api env vars 07/29/23 15:59:43.074
    Jul 29 15:59:43.087: INFO: Waiting up to 5m0s for pod "downward-api-4595e167-9d09-4cdb-b642-f262bc557152" in namespace "downward-api-7808" to be "Succeeded or Failed"
    Jul 29 15:59:43.093: INFO: Pod "downward-api-4595e167-9d09-4cdb-b642-f262bc557152": Phase="Pending", Reason="", readiness=false. Elapsed: 5.822752ms
    Jul 29 15:59:45.103: INFO: Pod "downward-api-4595e167-9d09-4cdb-b642-f262bc557152": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015341016s
    Jul 29 15:59:47.105: INFO: Pod "downward-api-4595e167-9d09-4cdb-b642-f262bc557152": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017213826s
    STEP: Saw pod success 07/29/23 15:59:47.105
    Jul 29 15:59:47.106: INFO: Pod "downward-api-4595e167-9d09-4cdb-b642-f262bc557152" satisfied condition "Succeeded or Failed"
    Jul 29 15:59:47.114: INFO: Trying to get logs from node wa4quivohpee-3 pod downward-api-4595e167-9d09-4cdb-b642-f262bc557152 container dapi-container: <nil>
    STEP: delete the pod 07/29/23 15:59:47.132
    Jul 29 15:59:47.159: INFO: Waiting for pod downward-api-4595e167-9d09-4cdb-b642-f262bc557152 to disappear
    Jul 29 15:59:47.164: INFO: Pod downward-api-4595e167-9d09-4cdb-b642-f262bc557152 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Jul 29 15:59:47.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-7808" for this suite. 07/29/23 15:59:47.189
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:59:47.219
Jul 29 15:59:47.220: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename discovery 07/29/23 15:59:47.221
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:59:47.254
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:59:47.258
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/apimachinery/discovery.go:43
STEP: Setting up server cert 07/29/23 15:59:47.266
[It] should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
Jul 29 15:59:48.125: INFO: Checking APIGroup: apiregistration.k8s.io
Jul 29 15:59:48.126: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Jul 29 15:59:48.126: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Jul 29 15:59:48.127: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Jul 29 15:59:48.127: INFO: Checking APIGroup: apps
Jul 29 15:59:48.128: INFO: PreferredVersion.GroupVersion: apps/v1
Jul 29 15:59:48.128: INFO: Versions found [{apps/v1 v1}]
Jul 29 15:59:48.128: INFO: apps/v1 matches apps/v1
Jul 29 15:59:48.128: INFO: Checking APIGroup: events.k8s.io
Jul 29 15:59:48.130: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Jul 29 15:59:48.131: INFO: Versions found [{events.k8s.io/v1 v1}]
Jul 29 15:59:48.131: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Jul 29 15:59:48.131: INFO: Checking APIGroup: authentication.k8s.io
Jul 29 15:59:48.132: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Jul 29 15:59:48.132: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Jul 29 15:59:48.132: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Jul 29 15:59:48.133: INFO: Checking APIGroup: authorization.k8s.io
Jul 29 15:59:48.134: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Jul 29 15:59:48.134: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Jul 29 15:59:48.134: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Jul 29 15:59:48.134: INFO: Checking APIGroup: autoscaling
Jul 29 15:59:48.136: INFO: PreferredVersion.GroupVersion: autoscaling/v2
Jul 29 15:59:48.136: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1} {autoscaling/v2beta2 v2beta2}]
Jul 29 15:59:48.136: INFO: autoscaling/v2 matches autoscaling/v2
Jul 29 15:59:48.136: INFO: Checking APIGroup: batch
Jul 29 15:59:48.138: INFO: PreferredVersion.GroupVersion: batch/v1
Jul 29 15:59:48.138: INFO: Versions found [{batch/v1 v1}]
Jul 29 15:59:48.138: INFO: batch/v1 matches batch/v1
Jul 29 15:59:48.138: INFO: Checking APIGroup: certificates.k8s.io
Jul 29 15:59:48.140: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Jul 29 15:59:48.140: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Jul 29 15:59:48.140: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Jul 29 15:59:48.141: INFO: Checking APIGroup: networking.k8s.io
Jul 29 15:59:48.143: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Jul 29 15:59:48.143: INFO: Versions found [{networking.k8s.io/v1 v1}]
Jul 29 15:59:48.143: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Jul 29 15:59:48.143: INFO: Checking APIGroup: policy
Jul 29 15:59:48.146: INFO: PreferredVersion.GroupVersion: policy/v1
Jul 29 15:59:48.147: INFO: Versions found [{policy/v1 v1}]
Jul 29 15:59:48.147: INFO: policy/v1 matches policy/v1
Jul 29 15:59:48.147: INFO: Checking APIGroup: rbac.authorization.k8s.io
Jul 29 15:59:48.149: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Jul 29 15:59:48.150: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Jul 29 15:59:48.150: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Jul 29 15:59:48.150: INFO: Checking APIGroup: storage.k8s.io
Jul 29 15:59:48.151: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Jul 29 15:59:48.151: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Jul 29 15:59:48.151: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Jul 29 15:59:48.151: INFO: Checking APIGroup: admissionregistration.k8s.io
Jul 29 15:59:48.153: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Jul 29 15:59:48.153: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Jul 29 15:59:48.153: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Jul 29 15:59:48.153: INFO: Checking APIGroup: apiextensions.k8s.io
Jul 29 15:59:48.155: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Jul 29 15:59:48.155: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Jul 29 15:59:48.155: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Jul 29 15:59:48.155: INFO: Checking APIGroup: scheduling.k8s.io
Jul 29 15:59:48.157: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Jul 29 15:59:48.157: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Jul 29 15:59:48.157: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Jul 29 15:59:48.157: INFO: Checking APIGroup: coordination.k8s.io
Jul 29 15:59:48.159: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Jul 29 15:59:48.159: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Jul 29 15:59:48.159: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Jul 29 15:59:48.159: INFO: Checking APIGroup: node.k8s.io
Jul 29 15:59:48.161: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Jul 29 15:59:48.161: INFO: Versions found [{node.k8s.io/v1 v1}]
Jul 29 15:59:48.161: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Jul 29 15:59:48.161: INFO: Checking APIGroup: discovery.k8s.io
Jul 29 15:59:48.163: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Jul 29 15:59:48.163: INFO: Versions found [{discovery.k8s.io/v1 v1}]
Jul 29 15:59:48.164: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Jul 29 15:59:48.164: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Jul 29 15:59:48.166: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta2
Jul 29 15:59:48.166: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta2 v1beta2} {flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
Jul 29 15:59:48.166: INFO: flowcontrol.apiserver.k8s.io/v1beta2 matches flowcontrol.apiserver.k8s.io/v1beta2
Jul 29 15:59:48.166: INFO: Checking APIGroup: cilium.io
Jul 29 15:59:48.169: INFO: PreferredVersion.GroupVersion: cilium.io/v2
Jul 29 15:59:48.169: INFO: Versions found [{cilium.io/v2 v2} {cilium.io/v2alpha1 v2alpha1}]
Jul 29 15:59:48.169: INFO: cilium.io/v2 matches cilium.io/v2
[AfterEach] [sig-api-machinery] Discovery
  test/e2e/framework/framework.go:187
Jul 29 15:59:48.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-3607" for this suite. 07/29/23 15:59:48.18
{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","completed":110,"skipped":1882,"failed":0}
------------------------------
â€¢ [0.977 seconds]
[sig-api-machinery] Discovery
test/e2e/apimachinery/framework.go:23
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:59:47.219
    Jul 29 15:59:47.220: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename discovery 07/29/23 15:59:47.221
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:59:47.254
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:59:47.258
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/apimachinery/discovery.go:43
    STEP: Setting up server cert 07/29/23 15:59:47.266
    [It] should validate PreferredVersion for each APIGroup [Conformance]
      test/e2e/apimachinery/discovery.go:122
    Jul 29 15:59:48.125: INFO: Checking APIGroup: apiregistration.k8s.io
    Jul 29 15:59:48.126: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
    Jul 29 15:59:48.126: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
    Jul 29 15:59:48.127: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
    Jul 29 15:59:48.127: INFO: Checking APIGroup: apps
    Jul 29 15:59:48.128: INFO: PreferredVersion.GroupVersion: apps/v1
    Jul 29 15:59:48.128: INFO: Versions found [{apps/v1 v1}]
    Jul 29 15:59:48.128: INFO: apps/v1 matches apps/v1
    Jul 29 15:59:48.128: INFO: Checking APIGroup: events.k8s.io
    Jul 29 15:59:48.130: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
    Jul 29 15:59:48.131: INFO: Versions found [{events.k8s.io/v1 v1}]
    Jul 29 15:59:48.131: INFO: events.k8s.io/v1 matches events.k8s.io/v1
    Jul 29 15:59:48.131: INFO: Checking APIGroup: authentication.k8s.io
    Jul 29 15:59:48.132: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
    Jul 29 15:59:48.132: INFO: Versions found [{authentication.k8s.io/v1 v1}]
    Jul 29 15:59:48.132: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
    Jul 29 15:59:48.133: INFO: Checking APIGroup: authorization.k8s.io
    Jul 29 15:59:48.134: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
    Jul 29 15:59:48.134: INFO: Versions found [{authorization.k8s.io/v1 v1}]
    Jul 29 15:59:48.134: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
    Jul 29 15:59:48.134: INFO: Checking APIGroup: autoscaling
    Jul 29 15:59:48.136: INFO: PreferredVersion.GroupVersion: autoscaling/v2
    Jul 29 15:59:48.136: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1} {autoscaling/v2beta2 v2beta2}]
    Jul 29 15:59:48.136: INFO: autoscaling/v2 matches autoscaling/v2
    Jul 29 15:59:48.136: INFO: Checking APIGroup: batch
    Jul 29 15:59:48.138: INFO: PreferredVersion.GroupVersion: batch/v1
    Jul 29 15:59:48.138: INFO: Versions found [{batch/v1 v1}]
    Jul 29 15:59:48.138: INFO: batch/v1 matches batch/v1
    Jul 29 15:59:48.138: INFO: Checking APIGroup: certificates.k8s.io
    Jul 29 15:59:48.140: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
    Jul 29 15:59:48.140: INFO: Versions found [{certificates.k8s.io/v1 v1}]
    Jul 29 15:59:48.140: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
    Jul 29 15:59:48.141: INFO: Checking APIGroup: networking.k8s.io
    Jul 29 15:59:48.143: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
    Jul 29 15:59:48.143: INFO: Versions found [{networking.k8s.io/v1 v1}]
    Jul 29 15:59:48.143: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
    Jul 29 15:59:48.143: INFO: Checking APIGroup: policy
    Jul 29 15:59:48.146: INFO: PreferredVersion.GroupVersion: policy/v1
    Jul 29 15:59:48.147: INFO: Versions found [{policy/v1 v1}]
    Jul 29 15:59:48.147: INFO: policy/v1 matches policy/v1
    Jul 29 15:59:48.147: INFO: Checking APIGroup: rbac.authorization.k8s.io
    Jul 29 15:59:48.149: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
    Jul 29 15:59:48.150: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
    Jul 29 15:59:48.150: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
    Jul 29 15:59:48.150: INFO: Checking APIGroup: storage.k8s.io
    Jul 29 15:59:48.151: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
    Jul 29 15:59:48.151: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
    Jul 29 15:59:48.151: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
    Jul 29 15:59:48.151: INFO: Checking APIGroup: admissionregistration.k8s.io
    Jul 29 15:59:48.153: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
    Jul 29 15:59:48.153: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
    Jul 29 15:59:48.153: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
    Jul 29 15:59:48.153: INFO: Checking APIGroup: apiextensions.k8s.io
    Jul 29 15:59:48.155: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
    Jul 29 15:59:48.155: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
    Jul 29 15:59:48.155: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
    Jul 29 15:59:48.155: INFO: Checking APIGroup: scheduling.k8s.io
    Jul 29 15:59:48.157: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
    Jul 29 15:59:48.157: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
    Jul 29 15:59:48.157: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
    Jul 29 15:59:48.157: INFO: Checking APIGroup: coordination.k8s.io
    Jul 29 15:59:48.159: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
    Jul 29 15:59:48.159: INFO: Versions found [{coordination.k8s.io/v1 v1}]
    Jul 29 15:59:48.159: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
    Jul 29 15:59:48.159: INFO: Checking APIGroup: node.k8s.io
    Jul 29 15:59:48.161: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
    Jul 29 15:59:48.161: INFO: Versions found [{node.k8s.io/v1 v1}]
    Jul 29 15:59:48.161: INFO: node.k8s.io/v1 matches node.k8s.io/v1
    Jul 29 15:59:48.161: INFO: Checking APIGroup: discovery.k8s.io
    Jul 29 15:59:48.163: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
    Jul 29 15:59:48.163: INFO: Versions found [{discovery.k8s.io/v1 v1}]
    Jul 29 15:59:48.164: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
    Jul 29 15:59:48.164: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
    Jul 29 15:59:48.166: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta2
    Jul 29 15:59:48.166: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta2 v1beta2} {flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
    Jul 29 15:59:48.166: INFO: flowcontrol.apiserver.k8s.io/v1beta2 matches flowcontrol.apiserver.k8s.io/v1beta2
    Jul 29 15:59:48.166: INFO: Checking APIGroup: cilium.io
    Jul 29 15:59:48.169: INFO: PreferredVersion.GroupVersion: cilium.io/v2
    Jul 29 15:59:48.169: INFO: Versions found [{cilium.io/v2 v2} {cilium.io/v2alpha1 v2alpha1}]
    Jul 29 15:59:48.169: INFO: cilium.io/v2 matches cilium.io/v2
    [AfterEach] [sig-api-machinery] Discovery
      test/e2e/framework/framework.go:187
    Jul 29 15:59:48.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "discovery-3607" for this suite. 07/29/23 15:59:48.18
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:152
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:59:48.2
Jul 29 15:59:48.200: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename container-lifecycle-hook 07/29/23 15:59:48.203
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:59:48.266
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:59:48.271
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 07/29/23 15:59:48.287
Jul 29 15:59:48.317: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-6731" to be "running and ready"
Jul 29 15:59:48.329: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 11.145784ms
Jul 29 15:59:48.329: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jul 29 15:59:50.340: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.022310838s
Jul 29 15:59:50.340: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jul 29 15:59:50.340: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:152
STEP: create the pod with lifecycle hook 07/29/23 15:59:50.346
Jul 29 15:59:50.361: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-6731" to be "running and ready"
Jul 29 15:59:50.370: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 9.090862ms
Jul 29 15:59:50.371: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jul 29 15:59:52.379: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.017503146s
Jul 29 15:59:52.379: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
Jul 29 15:59:52.379: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 07/29/23 15:59:52.386
Jul 29 15:59:52.401: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 29 15:59:52.408: INFO: Pod pod-with-prestop-http-hook still exists
Jul 29 15:59:54.409: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 29 15:59:54.415: INFO: Pod pod-with-prestop-http-hook still exists
Jul 29 15:59:56.408: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 29 15:59:56.416: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook 07/29/23 15:59:56.416
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Jul 29 15:59:56.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6731" for this suite. 07/29/23 15:59:56.458
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","completed":111,"skipped":1889,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.272 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:59:48.2
    Jul 29 15:59:48.200: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename container-lifecycle-hook 07/29/23 15:59:48.203
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:59:48.266
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:59:48.271
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 07/29/23 15:59:48.287
    Jul 29 15:59:48.317: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-6731" to be "running and ready"
    Jul 29 15:59:48.329: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 11.145784ms
    Jul 29 15:59:48.329: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jul 29 15:59:50.340: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.022310838s
    Jul 29 15:59:50.340: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jul 29 15:59:50.340: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:152
    STEP: create the pod with lifecycle hook 07/29/23 15:59:50.346
    Jul 29 15:59:50.361: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-6731" to be "running and ready"
    Jul 29 15:59:50.370: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 9.090862ms
    Jul 29 15:59:50.371: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Jul 29 15:59:52.379: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.017503146s
    Jul 29 15:59:52.379: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
    Jul 29 15:59:52.379: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 07/29/23 15:59:52.386
    Jul 29 15:59:52.401: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Jul 29 15:59:52.408: INFO: Pod pod-with-prestop-http-hook still exists
    Jul 29 15:59:54.409: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Jul 29 15:59:54.415: INFO: Pod pod-with-prestop-http-hook still exists
    Jul 29 15:59:56.408: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Jul 29 15:59:56.416: INFO: Pod pod-with-prestop-http-hook no longer exists
    STEP: check prestop hook 07/29/23 15:59:56.416
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Jul 29 15:59:56.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-6731" for this suite. 07/29/23 15:59:56.458
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1785
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:59:56.476
Jul 29 15:59:56.477: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename kubectl 07/29/23 15:59:56.483
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:59:56.54
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:59:56.544
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1785
STEP: starting the proxy server 07/29/23 15:59:56.55
Jul 29 15:59:56.551: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-1829 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output 07/29/23 15:59:56.675
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jul 29 15:59:56.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1829" for this suite. 07/29/23 15:59:56.7
{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","completed":112,"skipped":1917,"failed":0}
------------------------------
â€¢ [0.238 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1778
    should support proxy with --port 0  [Conformance]
    test/e2e/kubectl/kubectl.go:1785

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:59:56.476
    Jul 29 15:59:56.477: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename kubectl 07/29/23 15:59:56.483
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:59:56.54
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:59:56.544
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should support proxy with --port 0  [Conformance]
      test/e2e/kubectl/kubectl.go:1785
    STEP: starting the proxy server 07/29/23 15:59:56.55
    Jul 29 15:59:56.551: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-1829 proxy -p 0 --disable-filter'
    STEP: curling proxy /api/ output 07/29/23 15:59:56.675
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jul 29 15:59:56.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-1829" for this suite. 07/29/23 15:59:56.7
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 15:59:56.717
Jul 29 15:59:56.717: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename downward-api 07/29/23 15:59:56.719
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:59:56.751
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:59:56.755
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83
STEP: Creating a pod to test downward API volume plugin 07/29/23 15:59:56.761
Jul 29 15:59:56.776: INFO: Waiting up to 5m0s for pod "downwardapi-volume-67e93334-e0b5-4636-8ca6-fa6409112b15" in namespace "downward-api-6320" to be "Succeeded or Failed"
Jul 29 15:59:56.784: INFO: Pod "downwardapi-volume-67e93334-e0b5-4636-8ca6-fa6409112b15": Phase="Pending", Reason="", readiness=false. Elapsed: 7.823756ms
Jul 29 15:59:58.793: INFO: Pod "downwardapi-volume-67e93334-e0b5-4636-8ca6-fa6409112b15": Phase="Running", Reason="", readiness=false. Elapsed: 2.017533296s
Jul 29 16:00:00.793: INFO: Pod "downwardapi-volume-67e93334-e0b5-4636-8ca6-fa6409112b15": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017057616s
STEP: Saw pod success 07/29/23 16:00:00.793
Jul 29 16:00:00.794: INFO: Pod "downwardapi-volume-67e93334-e0b5-4636-8ca6-fa6409112b15" satisfied condition "Succeeded or Failed"
Jul 29 16:00:00.798: INFO: Trying to get logs from node wa4quivohpee-3 pod downwardapi-volume-67e93334-e0b5-4636-8ca6-fa6409112b15 container client-container: <nil>
STEP: delete the pod 07/29/23 16:00:00.811
Jul 29 16:00:00.831: INFO: Waiting for pod downwardapi-volume-67e93334-e0b5-4636-8ca6-fa6409112b15 to disappear
Jul 29 16:00:00.838: INFO: Pod downwardapi-volume-67e93334-e0b5-4636-8ca6-fa6409112b15 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jul 29 16:00:00.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6320" for this suite. 07/29/23 16:00:00.844
{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","completed":113,"skipped":1951,"failed":0}
------------------------------
â€¢ [4.145 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 15:59:56.717
    Jul 29 15:59:56.717: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename downward-api 07/29/23 15:59:56.719
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 15:59:56.751
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 15:59:56.755
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:83
    STEP: Creating a pod to test downward API volume plugin 07/29/23 15:59:56.761
    Jul 29 15:59:56.776: INFO: Waiting up to 5m0s for pod "downwardapi-volume-67e93334-e0b5-4636-8ca6-fa6409112b15" in namespace "downward-api-6320" to be "Succeeded or Failed"
    Jul 29 15:59:56.784: INFO: Pod "downwardapi-volume-67e93334-e0b5-4636-8ca6-fa6409112b15": Phase="Pending", Reason="", readiness=false. Elapsed: 7.823756ms
    Jul 29 15:59:58.793: INFO: Pod "downwardapi-volume-67e93334-e0b5-4636-8ca6-fa6409112b15": Phase="Running", Reason="", readiness=false. Elapsed: 2.017533296s
    Jul 29 16:00:00.793: INFO: Pod "downwardapi-volume-67e93334-e0b5-4636-8ca6-fa6409112b15": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017057616s
    STEP: Saw pod success 07/29/23 16:00:00.793
    Jul 29 16:00:00.794: INFO: Pod "downwardapi-volume-67e93334-e0b5-4636-8ca6-fa6409112b15" satisfied condition "Succeeded or Failed"
    Jul 29 16:00:00.798: INFO: Trying to get logs from node wa4quivohpee-3 pod downwardapi-volume-67e93334-e0b5-4636-8ca6-fa6409112b15 container client-container: <nil>
    STEP: delete the pod 07/29/23 16:00:00.811
    Jul 29 16:00:00.831: INFO: Waiting for pod downwardapi-volume-67e93334-e0b5-4636-8ca6-fa6409112b15 to disappear
    Jul 29 16:00:00.838: INFO: Pod downwardapi-volume-67e93334-e0b5-4636-8ca6-fa6409112b15 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jul 29 16:00:00.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-6320" for this suite. 07/29/23 16:00:00.844
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Downward API
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:00:00.868
Jul 29 16:00:00.868: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename downward-api 07/29/23 16:00:00.872
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:00:00.9
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:00:00.904
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43
STEP: Creating a pod to test downward api env vars 07/29/23 16:00:00.909
Jul 29 16:00:00.921: INFO: Waiting up to 5m0s for pod "downward-api-bdcdea90-740d-4ee2-911b-6f893fd07eff" in namespace "downward-api-1732" to be "Succeeded or Failed"
Jul 29 16:00:00.927: INFO: Pod "downward-api-bdcdea90-740d-4ee2-911b-6f893fd07eff": Phase="Pending", Reason="", readiness=false. Elapsed: 5.708021ms
Jul 29 16:00:02.934: INFO: Pod "downward-api-bdcdea90-740d-4ee2-911b-6f893fd07eff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012220327s
Jul 29 16:00:04.936: INFO: Pod "downward-api-bdcdea90-740d-4ee2-911b-6f893fd07eff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014763779s
STEP: Saw pod success 07/29/23 16:00:04.936
Jul 29 16:00:04.937: INFO: Pod "downward-api-bdcdea90-740d-4ee2-911b-6f893fd07eff" satisfied condition "Succeeded or Failed"
Jul 29 16:00:04.942: INFO: Trying to get logs from node wa4quivohpee-3 pod downward-api-bdcdea90-740d-4ee2-911b-6f893fd07eff container dapi-container: <nil>
STEP: delete the pod 07/29/23 16:00:04.961
Jul 29 16:00:04.989: INFO: Waiting for pod downward-api-bdcdea90-740d-4ee2-911b-6f893fd07eff to disappear
Jul 29 16:00:04.995: INFO: Pod downward-api-bdcdea90-740d-4ee2-911b-6f893fd07eff no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Jul 29 16:00:04.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1732" for this suite. 07/29/23 16:00:05.003
{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","completed":114,"skipped":1957,"failed":0}
------------------------------
â€¢ [4.147 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:00:00.868
    Jul 29 16:00:00.868: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename downward-api 07/29/23 16:00:00.872
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:00:00.9
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:00:00.904
    [It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:43
    STEP: Creating a pod to test downward api env vars 07/29/23 16:00:00.909
    Jul 29 16:00:00.921: INFO: Waiting up to 5m0s for pod "downward-api-bdcdea90-740d-4ee2-911b-6f893fd07eff" in namespace "downward-api-1732" to be "Succeeded or Failed"
    Jul 29 16:00:00.927: INFO: Pod "downward-api-bdcdea90-740d-4ee2-911b-6f893fd07eff": Phase="Pending", Reason="", readiness=false. Elapsed: 5.708021ms
    Jul 29 16:00:02.934: INFO: Pod "downward-api-bdcdea90-740d-4ee2-911b-6f893fd07eff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012220327s
    Jul 29 16:00:04.936: INFO: Pod "downward-api-bdcdea90-740d-4ee2-911b-6f893fd07eff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014763779s
    STEP: Saw pod success 07/29/23 16:00:04.936
    Jul 29 16:00:04.937: INFO: Pod "downward-api-bdcdea90-740d-4ee2-911b-6f893fd07eff" satisfied condition "Succeeded or Failed"
    Jul 29 16:00:04.942: INFO: Trying to get logs from node wa4quivohpee-3 pod downward-api-bdcdea90-740d-4ee2-911b-6f893fd07eff container dapi-container: <nil>
    STEP: delete the pod 07/29/23 16:00:04.961
    Jul 29 16:00:04.989: INFO: Waiting for pod downward-api-bdcdea90-740d-4ee2-911b-6f893fd07eff to disappear
    Jul 29 16:00:04.995: INFO: Pod downward-api-bdcdea90-740d-4ee2-911b-6f893fd07eff no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Jul 29 16:00:04.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-1732" for this suite. 07/29/23 16:00:05.003
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:00:05.025
Jul 29 16:00:05.025: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename svcaccounts 07/29/23 16:00:05.027
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:00:05.06
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:00:05.066
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528
Jul 29 16:00:05.102: INFO: created pod
Jul 29 16:00:05.102: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-5478" to be "Succeeded or Failed"
Jul 29 16:00:05.116: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 13.150555ms
Jul 29 16:00:07.124: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021684312s
Jul 29 16:00:09.151: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.04855672s
STEP: Saw pod success 07/29/23 16:00:09.151
Jul 29 16:00:09.151: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Jul 29 16:00:39.152: INFO: polling logs
Jul 29 16:00:39.171: INFO: Pod logs: 
I0729 16:00:06.117652       1 log.go:195] OK: Got token
I0729 16:00:06.118299       1 log.go:195] validating with in-cluster discovery
I0729 16:00:06.119495       1 log.go:195] OK: got issuer https://kubernetes.default.svc.cluster.local
I0729 16:00:06.119618       1 log.go:195] Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-5478:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1690647005, NotBefore:1690646405, IssuedAt:1690646405, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-5478", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"3bdfedd1-87ad-422b-b7ad-3292eefdfa74"}}}
I0729 16:00:06.148661       1 log.go:195] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
I0729 16:00:06.161466       1 log.go:195] OK: Validated signature on JWT
I0729 16:00:06.161631       1 log.go:195] OK: Got valid claims from token!
I0729 16:00:06.161698       1 log.go:195] Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-5478:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1690647005, NotBefore:1690646405, IssuedAt:1690646405, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-5478", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"3bdfedd1-87ad-422b-b7ad-3292eefdfa74"}}}

Jul 29 16:00:39.171: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Jul 29 16:00:39.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-5478" for this suite. 07/29/23 16:00:39.198
{"msg":"PASSED [sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]","completed":115,"skipped":1981,"failed":0}
------------------------------
â€¢ [SLOW TEST] [34.185 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:00:05.025
    Jul 29 16:00:05.025: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename svcaccounts 07/29/23 16:00:05.027
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:00:05.06
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:00:05.066
    [It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
      test/e2e/auth/service_accounts.go:528
    Jul 29 16:00:05.102: INFO: created pod
    Jul 29 16:00:05.102: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-5478" to be "Succeeded or Failed"
    Jul 29 16:00:05.116: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 13.150555ms
    Jul 29 16:00:07.124: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021684312s
    Jul 29 16:00:09.151: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.04855672s
    STEP: Saw pod success 07/29/23 16:00:09.151
    Jul 29 16:00:09.151: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
    Jul 29 16:00:39.152: INFO: polling logs
    Jul 29 16:00:39.171: INFO: Pod logs: 
    I0729 16:00:06.117652       1 log.go:195] OK: Got token
    I0729 16:00:06.118299       1 log.go:195] validating with in-cluster discovery
    I0729 16:00:06.119495       1 log.go:195] OK: got issuer https://kubernetes.default.svc.cluster.local
    I0729 16:00:06.119618       1 log.go:195] Full, not-validated claims: 
    openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-5478:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1690647005, NotBefore:1690646405, IssuedAt:1690646405, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-5478", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"3bdfedd1-87ad-422b-b7ad-3292eefdfa74"}}}
    I0729 16:00:06.148661       1 log.go:195] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
    I0729 16:00:06.161466       1 log.go:195] OK: Validated signature on JWT
    I0729 16:00:06.161631       1 log.go:195] OK: Got valid claims from token!
    I0729 16:00:06.161698       1 log.go:195] Full, validated claims: 
    &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-5478:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1690647005, NotBefore:1690646405, IssuedAt:1690646405, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-5478", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"3bdfedd1-87ad-422b-b7ad-3292eefdfa74"}}}

    Jul 29 16:00:39.171: INFO: completed pod
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Jul 29 16:00:39.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-5478" for this suite. 07/29/23 16:00:39.198
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Secrets
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:00:39.212
Jul 29 16:00:39.212: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename secrets 07/29/23 16:00:39.216
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:00:39.258
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:00:39.263
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124
STEP: Creating secret with name secret-test-3be82e24-ddf1-4f0d-9fae-89582d38c77e 07/29/23 16:00:39.268
STEP: Creating a pod to test consume secrets 07/29/23 16:00:39.283
Jul 29 16:00:39.307: INFO: Waiting up to 5m0s for pod "pod-secrets-c255d789-60f4-4e03-a077-2295d242b8a2" in namespace "secrets-394" to be "Succeeded or Failed"
Jul 29 16:00:39.317: INFO: Pod "pod-secrets-c255d789-60f4-4e03-a077-2295d242b8a2": Phase="Pending", Reason="", readiness=false. Elapsed: 10.588332ms
Jul 29 16:00:41.327: INFO: Pod "pod-secrets-c255d789-60f4-4e03-a077-2295d242b8a2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019849643s
Jul 29 16:00:43.327: INFO: Pod "pod-secrets-c255d789-60f4-4e03-a077-2295d242b8a2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020049087s
STEP: Saw pod success 07/29/23 16:00:43.327
Jul 29 16:00:43.327: INFO: Pod "pod-secrets-c255d789-60f4-4e03-a077-2295d242b8a2" satisfied condition "Succeeded or Failed"
Jul 29 16:00:43.334: INFO: Trying to get logs from node wa4quivohpee-3 pod pod-secrets-c255d789-60f4-4e03-a077-2295d242b8a2 container secret-volume-test: <nil>
STEP: delete the pod 07/29/23 16:00:43.356
Jul 29 16:00:43.383: INFO: Waiting for pod pod-secrets-c255d789-60f4-4e03-a077-2295d242b8a2 to disappear
Jul 29 16:00:43.401: INFO: Pod pod-secrets-c255d789-60f4-4e03-a077-2295d242b8a2 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jul 29 16:00:43.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-394" for this suite. 07/29/23 16:00:43.412
{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","completed":116,"skipped":1986,"failed":0}
------------------------------
â€¢ [4.224 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:00:39.212
    Jul 29 16:00:39.212: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename secrets 07/29/23 16:00:39.216
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:00:39.258
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:00:39.263
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:124
    STEP: Creating secret with name secret-test-3be82e24-ddf1-4f0d-9fae-89582d38c77e 07/29/23 16:00:39.268
    STEP: Creating a pod to test consume secrets 07/29/23 16:00:39.283
    Jul 29 16:00:39.307: INFO: Waiting up to 5m0s for pod "pod-secrets-c255d789-60f4-4e03-a077-2295d242b8a2" in namespace "secrets-394" to be "Succeeded or Failed"
    Jul 29 16:00:39.317: INFO: Pod "pod-secrets-c255d789-60f4-4e03-a077-2295d242b8a2": Phase="Pending", Reason="", readiness=false. Elapsed: 10.588332ms
    Jul 29 16:00:41.327: INFO: Pod "pod-secrets-c255d789-60f4-4e03-a077-2295d242b8a2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019849643s
    Jul 29 16:00:43.327: INFO: Pod "pod-secrets-c255d789-60f4-4e03-a077-2295d242b8a2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020049087s
    STEP: Saw pod success 07/29/23 16:00:43.327
    Jul 29 16:00:43.327: INFO: Pod "pod-secrets-c255d789-60f4-4e03-a077-2295d242b8a2" satisfied condition "Succeeded or Failed"
    Jul 29 16:00:43.334: INFO: Trying to get logs from node wa4quivohpee-3 pod pod-secrets-c255d789-60f4-4e03-a077-2295d242b8a2 container secret-volume-test: <nil>
    STEP: delete the pod 07/29/23 16:00:43.356
    Jul 29 16:00:43.383: INFO: Waiting for pod pod-secrets-c255d789-60f4-4e03-a077-2295d242b8a2 to disappear
    Jul 29 16:00:43.401: INFO: Pod pod-secrets-c255d789-60f4-4e03-a077-2295d242b8a2 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jul 29 16:00:43.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-394" for this suite. 07/29/23 16:00:43.412
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:00:43.449
Jul 29 16:00:43.450: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename pods 07/29/23 16:00:43.452
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:00:43.49
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:00:43.497
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343
STEP: creating the pod 07/29/23 16:00:43.501
STEP: submitting the pod to kubernetes 07/29/23 16:00:43.502
Jul 29 16:00:43.525: INFO: Waiting up to 5m0s for pod "pod-update-a67435f9-11e5-4803-9a54-4fc696208d3b" in namespace "pods-7117" to be "running and ready"
Jul 29 16:00:43.530: INFO: Pod "pod-update-a67435f9-11e5-4803-9a54-4fc696208d3b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.336829ms
Jul 29 16:00:43.530: INFO: The phase of Pod pod-update-a67435f9-11e5-4803-9a54-4fc696208d3b is Pending, waiting for it to be Running (with Ready = true)
Jul 29 16:00:45.542: INFO: Pod "pod-update-a67435f9-11e5-4803-9a54-4fc696208d3b": Phase="Running", Reason="", readiness=true. Elapsed: 2.017357438s
Jul 29 16:00:45.542: INFO: The phase of Pod pod-update-a67435f9-11e5-4803-9a54-4fc696208d3b is Running (Ready = true)
Jul 29 16:00:45.542: INFO: Pod "pod-update-a67435f9-11e5-4803-9a54-4fc696208d3b" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 07/29/23 16:00:45.548
STEP: updating the pod 07/29/23 16:00:45.554
Jul 29 16:00:46.079: INFO: Successfully updated pod "pod-update-a67435f9-11e5-4803-9a54-4fc696208d3b"
Jul 29 16:00:46.079: INFO: Waiting up to 5m0s for pod "pod-update-a67435f9-11e5-4803-9a54-4fc696208d3b" in namespace "pods-7117" to be "running"
Jul 29 16:00:46.093: INFO: Pod "pod-update-a67435f9-11e5-4803-9a54-4fc696208d3b": Phase="Running", Reason="", readiness=true. Elapsed: 14.028332ms
Jul 29 16:00:46.093: INFO: Pod "pod-update-a67435f9-11e5-4803-9a54-4fc696208d3b" satisfied condition "running"
STEP: verifying the updated pod is in kubernetes 07/29/23 16:00:46.094
Jul 29 16:00:46.100: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jul 29 16:00:46.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7117" for this suite. 07/29/23 16:00:46.109
{"msg":"PASSED [sig-node] Pods should be updated [NodeConformance] [Conformance]","completed":117,"skipped":2043,"failed":0}
------------------------------
â€¢ [2.673 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:00:43.449
    Jul 29 16:00:43.450: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename pods 07/29/23 16:00:43.452
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:00:43.49
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:00:43.497
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:343
    STEP: creating the pod 07/29/23 16:00:43.501
    STEP: submitting the pod to kubernetes 07/29/23 16:00:43.502
    Jul 29 16:00:43.525: INFO: Waiting up to 5m0s for pod "pod-update-a67435f9-11e5-4803-9a54-4fc696208d3b" in namespace "pods-7117" to be "running and ready"
    Jul 29 16:00:43.530: INFO: Pod "pod-update-a67435f9-11e5-4803-9a54-4fc696208d3b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.336829ms
    Jul 29 16:00:43.530: INFO: The phase of Pod pod-update-a67435f9-11e5-4803-9a54-4fc696208d3b is Pending, waiting for it to be Running (with Ready = true)
    Jul 29 16:00:45.542: INFO: Pod "pod-update-a67435f9-11e5-4803-9a54-4fc696208d3b": Phase="Running", Reason="", readiness=true. Elapsed: 2.017357438s
    Jul 29 16:00:45.542: INFO: The phase of Pod pod-update-a67435f9-11e5-4803-9a54-4fc696208d3b is Running (Ready = true)
    Jul 29 16:00:45.542: INFO: Pod "pod-update-a67435f9-11e5-4803-9a54-4fc696208d3b" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 07/29/23 16:00:45.548
    STEP: updating the pod 07/29/23 16:00:45.554
    Jul 29 16:00:46.079: INFO: Successfully updated pod "pod-update-a67435f9-11e5-4803-9a54-4fc696208d3b"
    Jul 29 16:00:46.079: INFO: Waiting up to 5m0s for pod "pod-update-a67435f9-11e5-4803-9a54-4fc696208d3b" in namespace "pods-7117" to be "running"
    Jul 29 16:00:46.093: INFO: Pod "pod-update-a67435f9-11e5-4803-9a54-4fc696208d3b": Phase="Running", Reason="", readiness=true. Elapsed: 14.028332ms
    Jul 29 16:00:46.093: INFO: Pod "pod-update-a67435f9-11e5-4803-9a54-4fc696208d3b" satisfied condition "running"
    STEP: verifying the updated pod is in kubernetes 07/29/23 16:00:46.094
    Jul 29 16:00:46.100: INFO: Pod update OK
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jul 29 16:00:46.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-7117" for this suite. 07/29/23 16:00:46.109
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Deployment
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:00:46.127
Jul 29 16:00:46.127: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename deployment 07/29/23 16:00:46.13
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:00:46.161
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:00:46.166
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
STEP: creating a Deployment 07/29/23 16:00:46.179
STEP: waiting for Deployment to be created 07/29/23 16:00:46.188
STEP: waiting for all Replicas to be Ready 07/29/23 16:00:46.191
Jul 29 16:00:46.193: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jul 29 16:00:46.193: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jul 29 16:00:46.219: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jul 29 16:00:46.220: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jul 29 16:00:46.330: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jul 29 16:00:46.330: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jul 29 16:00:46.374: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jul 29 16:00:46.374: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jul 29 16:00:46.424: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jul 29 16:00:46.424: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jul 29 16:00:47.999: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Jul 29 16:00:47.999: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Jul 29 16:00:48.275: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment 07/29/23 16:00:48.275
W0729 16:00:48.295363      13 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Jul 29 16:00:48.298: INFO: observed event type ADDED
STEP: waiting for Replicas to scale 07/29/23 16:00:48.298
Jul 29 16:00:48.303: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 0
Jul 29 16:00:48.304: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 0
Jul 29 16:00:48.304: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 0
Jul 29 16:00:48.304: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 0
Jul 29 16:00:48.304: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 0
Jul 29 16:00:48.304: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 0
Jul 29 16:00:48.304: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 0
Jul 29 16:00:48.304: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 0
Jul 29 16:00:48.304: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 0
Jul 29 16:00:48.305: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 0
Jul 29 16:00:48.305: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 1
Jul 29 16:00:48.305: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 1
Jul 29 16:00:48.305: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 2
Jul 29 16:00:48.305: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 2
Jul 29 16:00:48.305: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 2
Jul 29 16:00:48.305: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 2
Jul 29 16:00:48.318: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 2
Jul 29 16:00:48.318: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 2
Jul 29 16:00:48.357: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 2
Jul 29 16:00:48.357: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 2
Jul 29 16:00:48.408: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 1
Jul 29 16:00:48.408: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 1
Jul 29 16:00:48.427: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 1
Jul 29 16:00:48.427: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 1
Jul 29 16:00:50.039: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 2
Jul 29 16:00:50.039: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 2
Jul 29 16:00:50.105: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 1
STEP: listing Deployments 07/29/23 16:00:50.106
Jul 29 16:00:50.115: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment 07/29/23 16:00:50.116
Jul 29 16:00:50.153: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 1
STEP: fetching the DeploymentStatus 07/29/23 16:00:50.154
Jul 29 16:00:50.178: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jul 29 16:00:50.180: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jul 29 16:00:50.254: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jul 29 16:00:50.323: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jul 29 16:00:50.375: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jul 29 16:00:52.051: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jul 29 16:00:52.205: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jul 29 16:00:52.207: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jul 29 16:00:53.326: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus 07/29/23 16:00:53.424
STEP: fetching the DeploymentStatus 07/29/23 16:00:53.443
Jul 29 16:00:53.456: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 1
Jul 29 16:00:53.457: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 1
Jul 29 16:00:53.457: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 1
Jul 29 16:00:53.457: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 1
Jul 29 16:00:53.458: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 1
Jul 29 16:00:53.458: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 2
Jul 29 16:00:53.458: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 2
Jul 29 16:00:53.459: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 2
Jul 29 16:00:53.459: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 3
STEP: deleting the Deployment 07/29/23 16:00:53.459
Jul 29 16:00:53.483: INFO: observed event type MODIFIED
Jul 29 16:00:53.483: INFO: observed event type MODIFIED
Jul 29 16:00:53.483: INFO: observed event type MODIFIED
Jul 29 16:00:53.484: INFO: observed event type MODIFIED
Jul 29 16:00:53.484: INFO: observed event type MODIFIED
Jul 29 16:00:53.485: INFO: observed event type MODIFIED
Jul 29 16:00:53.485: INFO: observed event type MODIFIED
Jul 29 16:00:53.485: INFO: observed event type MODIFIED
Jul 29 16:00:53.486: INFO: observed event type MODIFIED
Jul 29 16:00:53.486: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jul 29 16:00:53.492: INFO: Log out all the ReplicaSets if there is no deployment created
Jul 29 16:00:53.500: INFO: ReplicaSet "test-deployment-54cc775c4b":
&ReplicaSet{ObjectMeta:{test-deployment-54cc775c4b  deployment-3453  9d74024f-4096-46d9-a094-ce12036aef8e 11304 4 2023-07-29 16:00:48 +0000 UTC <nil> <nil> map[pod-template-hash:54cc775c4b test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 34c5cf9c-fe88-4b52-afe1-1f3499e84215 0xc002c17707 0xc002c17708}] [] [{kube-controller-manager Update apps/v1 2023-07-29 16:00:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"34c5cf9c-fe88-4b52-afe1-1f3499e84215\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-29 16:00:53 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 54cc775c4b,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:54cc775c4b test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002c17790 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Jul 29 16:00:53.508: INFO: pod: "test-deployment-54cc775c4b-2rtl5":
&Pod{ObjectMeta:{test-deployment-54cc775c4b-2rtl5 test-deployment-54cc775c4b- deployment-3453  0161d24f-dbd4-4b20-9be9-e852e65a3c7e 11299 0 2023-07-29 16:00:48 +0000 UTC 2023-07-29 16:00:54 +0000 UTC 0xc00248efa8 map[pod-template-hash:54cc775c4b test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-54cc775c4b 9d74024f-4096-46d9-a094-ce12036aef8e 0xc00248efd7 0xc00248efd8}] [] [{kube-controller-manager Update v1 2023-07-29 16:00:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9d74024f-4096-46d9-a094-ce12036aef8e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 16:00:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.65.231\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nkqcg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nkqcg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wa4quivohpee-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:00:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:00:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:00:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:00:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.234,PodIP:10.233.65.231,StartTime:2023-07-29 16:00:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-29 16:00:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.8,ImageID:registry.k8s.io/pause@sha256:9001185023633d17a2f98ff69b6ff2615b8ea02a825adffa40422f51dfdcde9d,ContainerID:cri-o://8e3efbf9f57d133ab5cce9234ed2127db166c519f714abaa7a71ad5874794df4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.65.231,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jul 29 16:00:53.509: INFO: pod: "test-deployment-54cc775c4b-55krm":
&Pod{ObjectMeta:{test-deployment-54cc775c4b-55krm test-deployment-54cc775c4b- deployment-3453  c7418eac-dc91-4f3d-9e5c-bbe6f7130472 11278 0 2023-07-29 16:00:50 +0000 UTC 2023-07-29 16:00:53 +0000 UTC 0xc00248f1a0 map[pod-template-hash:54cc775c4b test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-54cc775c4b 9d74024f-4096-46d9-a094-ce12036aef8e 0xc00248f1d7 0xc00248f1d8}] [] [{kube-controller-manager Update v1 2023-07-29 16:00:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9d74024f-4096-46d9-a094-ce12036aef8e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 16:00:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.64.39\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cxm7p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cxm7p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wa4quivohpee-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:00:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:00:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:00:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:00:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.28,PodIP:10.233.64.39,StartTime:2023-07-29 16:00:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-29 16:00:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.8,ImageID:registry.k8s.io/pause@sha256:9001185023633d17a2f98ff69b6ff2615b8ea02a825adffa40422f51dfdcde9d,ContainerID:cri-o://68e6412f0dd3e286b36dc0b47bf8ce697738b90b099ef1a958de9983d950903a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.64.39,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jul 29 16:00:53.510: INFO: ReplicaSet "test-deployment-7c7d8d58c8":
&ReplicaSet{ObjectMeta:{test-deployment-7c7d8d58c8  deployment-3453  b9118701-ac3a-4706-8ee4-dba91059f6c8 11296 2 2023-07-29 16:00:50 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 34c5cf9c-fe88-4b52-afe1-1f3499e84215 0xc002c177f7 0xc002c177f8}] [] [{kube-controller-manager Update apps/v1 2023-07-29 16:00:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"34c5cf9c-fe88-4b52-afe1-1f3499e84215\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-29 16:00:53 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7c7d8d58c8,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002c17890 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

Jul 29 16:00:53.541: INFO: pod: "test-deployment-7c7d8d58c8-92h2b":
&Pod{ObjectMeta:{test-deployment-7c7d8d58c8-92h2b test-deployment-7c7d8d58c8- deployment-3453  28211085-667b-4efa-9081-c09eccb19766 11265 0 2023-07-29 16:00:50 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7c7d8d58c8 b9118701-ac3a-4706-8ee4-dba91059f6c8 0xc002c17c47 0xc002c17c48}] [] [{kube-controller-manager Update v1 2023-07-29 16:00:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b9118701-ac3a-4706-8ee4-dba91059f6c8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 16:00:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.65.196\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lfh8h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lfh8h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wa4quivohpee-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:00:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:00:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:00:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:00:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.234,PodIP:10.233.65.196,StartTime:2023-07-29 16:00:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-29 16:00:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://1d0dd33695015342b8a4770cdb1abf19a185410b913862c73d2f7797191abac6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.65.196,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jul 29 16:00:53.544: INFO: pod: "test-deployment-7c7d8d58c8-pmw4k":
&Pod{ObjectMeta:{test-deployment-7c7d8d58c8-pmw4k test-deployment-7c7d8d58c8- deployment-3453  627050b6-822e-433b-ad2c-089215ddc1a3 11294 0 2023-07-29 16:00:52 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7c7d8d58c8 b9118701-ac3a-4706-8ee4-dba91059f6c8 0xc002c17e57 0xc002c17e58}] [] [{kube-controller-manager Update v1 2023-07-29 16:00:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b9118701-ac3a-4706-8ee4-dba91059f6c8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 16:00:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.253\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-q5ct8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-q5ct8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wa4quivohpee-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:00:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:00:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:00:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:00:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.206,PodIP:10.233.66.253,StartTime:2023-07-29 16:00:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-29 16:00:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://8400ebe5eb131572c8dfa106aac461757fafad6bbfe7b67a57249b7851e71406,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.253,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jul 29 16:00:53.544: INFO: ReplicaSet "test-deployment-8594bb6fdd":
&ReplicaSet{ObjectMeta:{test-deployment-8594bb6fdd  deployment-3453  2838e3be-47ea-4af0-bc4e-a4ba0fe36cf7 11209 3 2023-07-29 16:00:46 +0000 UTC <nil> <nil> map[pod-template-hash:8594bb6fdd test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 34c5cf9c-fe88-4b52-afe1-1f3499e84215 0xc002c178f7 0xc002c178f8}] [] [{kube-controller-manager Update apps/v1 2023-07-29 16:00:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"34c5cf9c-fe88-4b52-afe1-1f3499e84215\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-29 16:00:50 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 8594bb6fdd,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:8594bb6fdd test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002c17990 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jul 29 16:00:53.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3453" for this suite. 07/29/23 16:00:53.57
{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","completed":118,"skipped":2051,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.466 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:00:46.127
    Jul 29 16:00:46.127: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename deployment 07/29/23 16:00:46.13
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:00:46.161
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:00:46.166
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should run the lifecycle of a Deployment [Conformance]
      test/e2e/apps/deployment.go:185
    STEP: creating a Deployment 07/29/23 16:00:46.179
    STEP: waiting for Deployment to be created 07/29/23 16:00:46.188
    STEP: waiting for all Replicas to be Ready 07/29/23 16:00:46.191
    Jul 29 16:00:46.193: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jul 29 16:00:46.193: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jul 29 16:00:46.219: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jul 29 16:00:46.220: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jul 29 16:00:46.330: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jul 29 16:00:46.330: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jul 29 16:00:46.374: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jul 29 16:00:46.374: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jul 29 16:00:46.424: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jul 29 16:00:46.424: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jul 29 16:00:47.999: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Jul 29 16:00:47.999: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Jul 29 16:00:48.275: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 2 and labels map[test-deployment-static:true]
    STEP: patching the Deployment 07/29/23 16:00:48.275
    W0729 16:00:48.295363      13 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Jul 29 16:00:48.298: INFO: observed event type ADDED
    STEP: waiting for Replicas to scale 07/29/23 16:00:48.298
    Jul 29 16:00:48.303: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 0
    Jul 29 16:00:48.304: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 0
    Jul 29 16:00:48.304: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 0
    Jul 29 16:00:48.304: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 0
    Jul 29 16:00:48.304: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 0
    Jul 29 16:00:48.304: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 0
    Jul 29 16:00:48.304: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 0
    Jul 29 16:00:48.304: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 0
    Jul 29 16:00:48.304: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 0
    Jul 29 16:00:48.305: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 0
    Jul 29 16:00:48.305: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 1
    Jul 29 16:00:48.305: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 1
    Jul 29 16:00:48.305: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 2
    Jul 29 16:00:48.305: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 2
    Jul 29 16:00:48.305: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 2
    Jul 29 16:00:48.305: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 2
    Jul 29 16:00:48.318: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 2
    Jul 29 16:00:48.318: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 2
    Jul 29 16:00:48.357: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 2
    Jul 29 16:00:48.357: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 2
    Jul 29 16:00:48.408: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 1
    Jul 29 16:00:48.408: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 1
    Jul 29 16:00:48.427: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 1
    Jul 29 16:00:48.427: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 1
    Jul 29 16:00:50.039: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 2
    Jul 29 16:00:50.039: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 2
    Jul 29 16:00:50.105: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 1
    STEP: listing Deployments 07/29/23 16:00:50.106
    Jul 29 16:00:50.115: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
    STEP: updating the Deployment 07/29/23 16:00:50.116
    Jul 29 16:00:50.153: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 1
    STEP: fetching the DeploymentStatus 07/29/23 16:00:50.154
    Jul 29 16:00:50.178: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jul 29 16:00:50.180: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jul 29 16:00:50.254: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jul 29 16:00:50.323: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jul 29 16:00:50.375: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jul 29 16:00:52.051: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Jul 29 16:00:52.205: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Jul 29 16:00:52.207: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Jul 29 16:00:53.326: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    STEP: patching the DeploymentStatus 07/29/23 16:00:53.424
    STEP: fetching the DeploymentStatus 07/29/23 16:00:53.443
    Jul 29 16:00:53.456: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 1
    Jul 29 16:00:53.457: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 1
    Jul 29 16:00:53.457: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 1
    Jul 29 16:00:53.457: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 1
    Jul 29 16:00:53.458: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 1
    Jul 29 16:00:53.458: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 2
    Jul 29 16:00:53.458: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 2
    Jul 29 16:00:53.459: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 2
    Jul 29 16:00:53.459: INFO: observed Deployment test-deployment in namespace deployment-3453 with ReadyReplicas 3
    STEP: deleting the Deployment 07/29/23 16:00:53.459
    Jul 29 16:00:53.483: INFO: observed event type MODIFIED
    Jul 29 16:00:53.483: INFO: observed event type MODIFIED
    Jul 29 16:00:53.483: INFO: observed event type MODIFIED
    Jul 29 16:00:53.484: INFO: observed event type MODIFIED
    Jul 29 16:00:53.484: INFO: observed event type MODIFIED
    Jul 29 16:00:53.485: INFO: observed event type MODIFIED
    Jul 29 16:00:53.485: INFO: observed event type MODIFIED
    Jul 29 16:00:53.485: INFO: observed event type MODIFIED
    Jul 29 16:00:53.486: INFO: observed event type MODIFIED
    Jul 29 16:00:53.486: INFO: observed event type MODIFIED
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jul 29 16:00:53.492: INFO: Log out all the ReplicaSets if there is no deployment created
    Jul 29 16:00:53.500: INFO: ReplicaSet "test-deployment-54cc775c4b":
    &ReplicaSet{ObjectMeta:{test-deployment-54cc775c4b  deployment-3453  9d74024f-4096-46d9-a094-ce12036aef8e 11304 4 2023-07-29 16:00:48 +0000 UTC <nil> <nil> map[pod-template-hash:54cc775c4b test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 34c5cf9c-fe88-4b52-afe1-1f3499e84215 0xc002c17707 0xc002c17708}] [] [{kube-controller-manager Update apps/v1 2023-07-29 16:00:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"34c5cf9c-fe88-4b52-afe1-1f3499e84215\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-29 16:00:53 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 54cc775c4b,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:54cc775c4b test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002c17790 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    Jul 29 16:00:53.508: INFO: pod: "test-deployment-54cc775c4b-2rtl5":
    &Pod{ObjectMeta:{test-deployment-54cc775c4b-2rtl5 test-deployment-54cc775c4b- deployment-3453  0161d24f-dbd4-4b20-9be9-e852e65a3c7e 11299 0 2023-07-29 16:00:48 +0000 UTC 2023-07-29 16:00:54 +0000 UTC 0xc00248efa8 map[pod-template-hash:54cc775c4b test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-54cc775c4b 9d74024f-4096-46d9-a094-ce12036aef8e 0xc00248efd7 0xc00248efd8}] [] [{kube-controller-manager Update v1 2023-07-29 16:00:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9d74024f-4096-46d9-a094-ce12036aef8e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 16:00:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.65.231\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nkqcg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nkqcg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wa4quivohpee-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:00:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:00:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:00:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:00:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.234,PodIP:10.233.65.231,StartTime:2023-07-29 16:00:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-29 16:00:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.8,ImageID:registry.k8s.io/pause@sha256:9001185023633d17a2f98ff69b6ff2615b8ea02a825adffa40422f51dfdcde9d,ContainerID:cri-o://8e3efbf9f57d133ab5cce9234ed2127db166c519f714abaa7a71ad5874794df4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.65.231,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Jul 29 16:00:53.509: INFO: pod: "test-deployment-54cc775c4b-55krm":
    &Pod{ObjectMeta:{test-deployment-54cc775c4b-55krm test-deployment-54cc775c4b- deployment-3453  c7418eac-dc91-4f3d-9e5c-bbe6f7130472 11278 0 2023-07-29 16:00:50 +0000 UTC 2023-07-29 16:00:53 +0000 UTC 0xc00248f1a0 map[pod-template-hash:54cc775c4b test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-54cc775c4b 9d74024f-4096-46d9-a094-ce12036aef8e 0xc00248f1d7 0xc00248f1d8}] [] [{kube-controller-manager Update v1 2023-07-29 16:00:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9d74024f-4096-46d9-a094-ce12036aef8e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 16:00:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.64.39\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cxm7p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cxm7p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wa4quivohpee-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:00:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:00:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:00:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:00:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.28,PodIP:10.233.64.39,StartTime:2023-07-29 16:00:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-29 16:00:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.8,ImageID:registry.k8s.io/pause@sha256:9001185023633d17a2f98ff69b6ff2615b8ea02a825adffa40422f51dfdcde9d,ContainerID:cri-o://68e6412f0dd3e286b36dc0b47bf8ce697738b90b099ef1a958de9983d950903a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.64.39,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Jul 29 16:00:53.510: INFO: ReplicaSet "test-deployment-7c7d8d58c8":
    &ReplicaSet{ObjectMeta:{test-deployment-7c7d8d58c8  deployment-3453  b9118701-ac3a-4706-8ee4-dba91059f6c8 11296 2 2023-07-29 16:00:50 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 34c5cf9c-fe88-4b52-afe1-1f3499e84215 0xc002c177f7 0xc002c177f8}] [] [{kube-controller-manager Update apps/v1 2023-07-29 16:00:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"34c5cf9c-fe88-4b52-afe1-1f3499e84215\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-29 16:00:53 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7c7d8d58c8,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002c17890 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

    Jul 29 16:00:53.541: INFO: pod: "test-deployment-7c7d8d58c8-92h2b":
    &Pod{ObjectMeta:{test-deployment-7c7d8d58c8-92h2b test-deployment-7c7d8d58c8- deployment-3453  28211085-667b-4efa-9081-c09eccb19766 11265 0 2023-07-29 16:00:50 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7c7d8d58c8 b9118701-ac3a-4706-8ee4-dba91059f6c8 0xc002c17c47 0xc002c17c48}] [] [{kube-controller-manager Update v1 2023-07-29 16:00:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b9118701-ac3a-4706-8ee4-dba91059f6c8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 16:00:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.65.196\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lfh8h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lfh8h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wa4quivohpee-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:00:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:00:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:00:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:00:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.234,PodIP:10.233.65.196,StartTime:2023-07-29 16:00:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-29 16:00:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://1d0dd33695015342b8a4770cdb1abf19a185410b913862c73d2f7797191abac6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.65.196,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Jul 29 16:00:53.544: INFO: pod: "test-deployment-7c7d8d58c8-pmw4k":
    &Pod{ObjectMeta:{test-deployment-7c7d8d58c8-pmw4k test-deployment-7c7d8d58c8- deployment-3453  627050b6-822e-433b-ad2c-089215ddc1a3 11294 0 2023-07-29 16:00:52 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7c7d8d58c8 b9118701-ac3a-4706-8ee4-dba91059f6c8 0xc002c17e57 0xc002c17e58}] [] [{kube-controller-manager Update v1 2023-07-29 16:00:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b9118701-ac3a-4706-8ee4-dba91059f6c8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 16:00:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.253\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-q5ct8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-q5ct8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wa4quivohpee-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:00:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:00:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:00:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:00:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.206,PodIP:10.233.66.253,StartTime:2023-07-29 16:00:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-29 16:00:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://8400ebe5eb131572c8dfa106aac461757fafad6bbfe7b67a57249b7851e71406,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.253,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Jul 29 16:00:53.544: INFO: ReplicaSet "test-deployment-8594bb6fdd":
    &ReplicaSet{ObjectMeta:{test-deployment-8594bb6fdd  deployment-3453  2838e3be-47ea-4af0-bc4e-a4ba0fe36cf7 11209 3 2023-07-29 16:00:46 +0000 UTC <nil> <nil> map[pod-template-hash:8594bb6fdd test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 34c5cf9c-fe88-4b52-afe1-1f3499e84215 0xc002c178f7 0xc002c178f8}] [] [{kube-controller-manager Update apps/v1 2023-07-29 16:00:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"34c5cf9c-fe88-4b52-afe1-1f3499e84215\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-29 16:00:50 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 8594bb6fdd,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:8594bb6fdd test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002c17990 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jul 29 16:00:53.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-3453" for this suite. 07/29/23 16:00:53.57
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:114
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:00:53.601
Jul 29 16:00:53.602: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename container-lifecycle-hook 07/29/23 16:00:53.605
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:00:53.703
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:00:53.708
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 07/29/23 16:00:53.726
Jul 29 16:00:53.742: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-8639" to be "running and ready"
Jul 29 16:00:53.748: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 5.967944ms
Jul 29 16:00:53.748: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jul 29 16:00:55.759: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.016323303s
Jul 29 16:00:55.759: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jul 29 16:00:55.759: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:114
STEP: create the pod with lifecycle hook 07/29/23 16:00:55.765
Jul 29 16:00:55.777: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-8639" to be "running and ready"
Jul 29 16:00:55.788: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 11.456236ms
Jul 29 16:00:55.788: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jul 29 16:00:57.796: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.018939588s
Jul 29 16:00:57.796: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
Jul 29 16:00:57.796: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 07/29/23 16:00:57.802
Jul 29 16:00:57.816: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 29 16:00:57.831: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 29 16:00:59.832: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 29 16:00:59.840: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 29 16:01:01.833: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 29 16:01:01.838: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook 07/29/23 16:01:01.838
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Jul 29 16:01:01.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8639" for this suite. 07/29/23 16:01:01.881
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","completed":119,"skipped":2060,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.295 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:114

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:00:53.601
    Jul 29 16:00:53.602: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename container-lifecycle-hook 07/29/23 16:00:53.605
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:00:53.703
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:00:53.708
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 07/29/23 16:00:53.726
    Jul 29 16:00:53.742: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-8639" to be "running and ready"
    Jul 29 16:00:53.748: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 5.967944ms
    Jul 29 16:00:53.748: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jul 29 16:00:55.759: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.016323303s
    Jul 29 16:00:55.759: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jul 29 16:00:55.759: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:114
    STEP: create the pod with lifecycle hook 07/29/23 16:00:55.765
    Jul 29 16:00:55.777: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-8639" to be "running and ready"
    Jul 29 16:00:55.788: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 11.456236ms
    Jul 29 16:00:55.788: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Jul 29 16:00:57.796: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.018939588s
    Jul 29 16:00:57.796: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
    Jul 29 16:00:57.796: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 07/29/23 16:00:57.802
    Jul 29 16:00:57.816: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Jul 29 16:00:57.831: INFO: Pod pod-with-prestop-exec-hook still exists
    Jul 29 16:00:59.832: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Jul 29 16:00:59.840: INFO: Pod pod-with-prestop-exec-hook still exists
    Jul 29 16:01:01.833: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Jul 29 16:01:01.838: INFO: Pod pod-with-prestop-exec-hook no longer exists
    STEP: check prestop hook 07/29/23 16:01:01.838
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Jul 29 16:01:01.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-8639" for this suite. 07/29/23 16:01:01.881
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:01:01.9
Jul 29 16:01:01.900: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename init-container 07/29/23 16:01:01.905
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:01:01.933
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:01:01.939
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176
STEP: creating the pod 07/29/23 16:01:01.944
Jul 29 16:01:01.945: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Jul 29 16:01:07.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-413" for this suite. 07/29/23 16:01:07.138
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","completed":120,"skipped":2086,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.249 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:01:01.9
    Jul 29 16:01:01.900: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename init-container 07/29/23 16:01:01.905
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:01:01.933
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:01:01.939
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should invoke init containers on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:176
    STEP: creating the pod 07/29/23 16:01:01.944
    Jul 29 16:01:01.945: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Jul 29 16:01:07.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-413" for this suite. 07/29/23 16:01:07.138
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs
  should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1590
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:01:07.165
Jul 29 16:01:07.166: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename kubectl 07/29/23 16:01:07.169
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:01:07.209
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:01:07.215
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1570
STEP: creating an pod 07/29/23 16:01:07.22
Jul 29 16:01:07.221: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-8828 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.40 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Jul 29 16:01:07.449: INFO: stderr: ""
Jul 29 16:01:07.449: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1590
STEP: Waiting for log generator to start. 07/29/23 16:01:07.449
Jul 29 16:01:07.449: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Jul 29 16:01:07.450: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-8828" to be "running and ready, or succeeded"
Jul 29 16:01:07.464: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 14.307545ms
Jul 29 16:01:07.464: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'wa4quivohpee-3' to be 'Running' but was 'Pending'
Jul 29 16:01:09.477: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.027828802s
Jul 29 16:01:09.478: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Jul 29 16:01:09.478: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings 07/29/23 16:01:09.478
Jul 29 16:01:09.479: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-8828 logs logs-generator logs-generator'
Jul 29 16:01:09.646: INFO: stderr: ""
Jul 29 16:01:09.646: INFO: stdout: "I0729 16:01:08.395093       1 logs_generator.go:76] 0 POST /api/v1/namespaces/default/pods/zrzq 435\nI0729 16:01:08.595567       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/default/pods/99b 357\nI0729 16:01:08.796091       1 logs_generator.go:76] 2 GET /api/v1/namespaces/default/pods/5mh 333\nI0729 16:01:08.995523       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/x9k 542\nI0729 16:01:09.195978       1 logs_generator.go:76] 4 GET /api/v1/namespaces/kube-system/pods/s5vz 533\nI0729 16:01:09.395309       1 logs_generator.go:76] 5 POST /api/v1/namespaces/ns/pods/t6k5 284\nI0729 16:01:09.597051       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/kube-system/pods/hdtd 383\n"
STEP: limiting log lines 07/29/23 16:01:09.647
Jul 29 16:01:09.647: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-8828 logs logs-generator logs-generator --tail=1'
Jul 29 16:01:09.819: INFO: stderr: ""
Jul 29 16:01:09.819: INFO: stdout: "I0729 16:01:09.795247       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/4mwx 518\n"
Jul 29 16:01:09.819: INFO: got output "I0729 16:01:09.795247       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/4mwx 518\n"
STEP: limiting log bytes 07/29/23 16:01:09.819
Jul 29 16:01:09.819: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-8828 logs logs-generator logs-generator --limit-bytes=1'
Jul 29 16:01:09.988: INFO: stderr: ""
Jul 29 16:01:09.988: INFO: stdout: "I"
Jul 29 16:01:09.988: INFO: got output "I"
STEP: exposing timestamps 07/29/23 16:01:09.988
Jul 29 16:01:09.990: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-8828 logs logs-generator logs-generator --tail=1 --timestamps'
Jul 29 16:01:10.135: INFO: stderr: ""
Jul 29 16:01:10.135: INFO: stdout: "2023-07-29T16:01:09.996277867Z I0729 16:01:09.996197       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/ns/pods/8kl8 216\n"
Jul 29 16:01:10.135: INFO: got output "2023-07-29T16:01:09.996277867Z I0729 16:01:09.996197       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/ns/pods/8kl8 216\n"
STEP: restricting to a time range 07/29/23 16:01:10.135
Jul 29 16:01:12.637: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-8828 logs logs-generator logs-generator --since=1s'
Jul 29 16:01:12.798: INFO: stderr: ""
Jul 29 16:01:12.798: INFO: stdout: "I0729 16:01:11.795727       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/ns/pods/djx 363\nI0729 16:01:11.996171       1 logs_generator.go:76] 18 POST /api/v1/namespaces/default/pods/ggz 222\nI0729 16:01:12.195820       1 logs_generator.go:76] 19 GET /api/v1/namespaces/default/pods/sc62 282\nI0729 16:01:12.395128       1 logs_generator.go:76] 20 POST /api/v1/namespaces/default/pods/q8g 441\nI0729 16:01:12.595506       1 logs_generator.go:76] 21 GET /api/v1/namespaces/default/pods/jjs 576\n"
Jul 29 16:01:12.799: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-8828 logs logs-generator logs-generator --since=24h'
Jul 29 16:01:12.967: INFO: stderr: ""
Jul 29 16:01:12.967: INFO: stdout: "I0729 16:01:08.395093       1 logs_generator.go:76] 0 POST /api/v1/namespaces/default/pods/zrzq 435\nI0729 16:01:08.595567       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/default/pods/99b 357\nI0729 16:01:08.796091       1 logs_generator.go:76] 2 GET /api/v1/namespaces/default/pods/5mh 333\nI0729 16:01:08.995523       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/x9k 542\nI0729 16:01:09.195978       1 logs_generator.go:76] 4 GET /api/v1/namespaces/kube-system/pods/s5vz 533\nI0729 16:01:09.395309       1 logs_generator.go:76] 5 POST /api/v1/namespaces/ns/pods/t6k5 284\nI0729 16:01:09.597051       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/kube-system/pods/hdtd 383\nI0729 16:01:09.795247       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/4mwx 518\nI0729 16:01:09.996197       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/ns/pods/8kl8 216\nI0729 16:01:10.195687       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/ns/pods/nxw 340\nI0729 16:01:10.396185       1 logs_generator.go:76] 10 GET /api/v1/namespaces/default/pods/x7f 457\nI0729 16:01:10.595430       1 logs_generator.go:76] 11 POST /api/v1/namespaces/default/pods/c7xw 497\nI0729 16:01:10.795839       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/kube-system/pods/kmmc 412\nI0729 16:01:10.995188       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/ns/pods/wczc 556\nI0729 16:01:11.195621       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/default/pods/gw2f 505\nI0729 16:01:11.395899       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/ns/pods/mxvj 331\nI0729 16:01:11.595231       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/kube-system/pods/6g57 275\nI0729 16:01:11.795727       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/ns/pods/djx 363\nI0729 16:01:11.996171       1 logs_generator.go:76] 18 POST /api/v1/namespaces/default/pods/ggz 222\nI0729 16:01:12.195820       1 logs_generator.go:76] 19 GET /api/v1/namespaces/default/pods/sc62 282\nI0729 16:01:12.395128       1 logs_generator.go:76] 20 POST /api/v1/namespaces/default/pods/q8g 441\nI0729 16:01:12.595506       1 logs_generator.go:76] 21 GET /api/v1/namespaces/default/pods/jjs 576\nI0729 16:01:12.796034       1 logs_generator.go:76] 22 PUT /api/v1/namespaces/ns/pods/2pn 418\n"
[AfterEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1575
Jul 29 16:01:12.968: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-8828 delete pod logs-generator'
Jul 29 16:01:14.184: INFO: stderr: ""
Jul 29 16:01:14.184: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jul 29 16:01:14.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8828" for this suite. 07/29/23 16:01:14.197
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","completed":121,"skipped":2129,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.046 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl logs
  test/e2e/kubectl/kubectl.go:1567
    should be able to retrieve and filter logs  [Conformance]
    test/e2e/kubectl/kubectl.go:1590

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:01:07.165
    Jul 29 16:01:07.166: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename kubectl 07/29/23 16:01:07.169
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:01:07.209
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:01:07.215
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1570
    STEP: creating an pod 07/29/23 16:01:07.22
    Jul 29 16:01:07.221: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-8828 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.40 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
    Jul 29 16:01:07.449: INFO: stderr: ""
    Jul 29 16:01:07.449: INFO: stdout: "pod/logs-generator created\n"
    [It] should be able to retrieve and filter logs  [Conformance]
      test/e2e/kubectl/kubectl.go:1590
    STEP: Waiting for log generator to start. 07/29/23 16:01:07.449
    Jul 29 16:01:07.449: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
    Jul 29 16:01:07.450: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-8828" to be "running and ready, or succeeded"
    Jul 29 16:01:07.464: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 14.307545ms
    Jul 29 16:01:07.464: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'wa4quivohpee-3' to be 'Running' but was 'Pending'
    Jul 29 16:01:09.477: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.027828802s
    Jul 29 16:01:09.478: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
    Jul 29 16:01:09.478: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
    STEP: checking for a matching strings 07/29/23 16:01:09.478
    Jul 29 16:01:09.479: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-8828 logs logs-generator logs-generator'
    Jul 29 16:01:09.646: INFO: stderr: ""
    Jul 29 16:01:09.646: INFO: stdout: "I0729 16:01:08.395093       1 logs_generator.go:76] 0 POST /api/v1/namespaces/default/pods/zrzq 435\nI0729 16:01:08.595567       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/default/pods/99b 357\nI0729 16:01:08.796091       1 logs_generator.go:76] 2 GET /api/v1/namespaces/default/pods/5mh 333\nI0729 16:01:08.995523       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/x9k 542\nI0729 16:01:09.195978       1 logs_generator.go:76] 4 GET /api/v1/namespaces/kube-system/pods/s5vz 533\nI0729 16:01:09.395309       1 logs_generator.go:76] 5 POST /api/v1/namespaces/ns/pods/t6k5 284\nI0729 16:01:09.597051       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/kube-system/pods/hdtd 383\n"
    STEP: limiting log lines 07/29/23 16:01:09.647
    Jul 29 16:01:09.647: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-8828 logs logs-generator logs-generator --tail=1'
    Jul 29 16:01:09.819: INFO: stderr: ""
    Jul 29 16:01:09.819: INFO: stdout: "I0729 16:01:09.795247       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/4mwx 518\n"
    Jul 29 16:01:09.819: INFO: got output "I0729 16:01:09.795247       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/4mwx 518\n"
    STEP: limiting log bytes 07/29/23 16:01:09.819
    Jul 29 16:01:09.819: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-8828 logs logs-generator logs-generator --limit-bytes=1'
    Jul 29 16:01:09.988: INFO: stderr: ""
    Jul 29 16:01:09.988: INFO: stdout: "I"
    Jul 29 16:01:09.988: INFO: got output "I"
    STEP: exposing timestamps 07/29/23 16:01:09.988
    Jul 29 16:01:09.990: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-8828 logs logs-generator logs-generator --tail=1 --timestamps'
    Jul 29 16:01:10.135: INFO: stderr: ""
    Jul 29 16:01:10.135: INFO: stdout: "2023-07-29T16:01:09.996277867Z I0729 16:01:09.996197       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/ns/pods/8kl8 216\n"
    Jul 29 16:01:10.135: INFO: got output "2023-07-29T16:01:09.996277867Z I0729 16:01:09.996197       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/ns/pods/8kl8 216\n"
    STEP: restricting to a time range 07/29/23 16:01:10.135
    Jul 29 16:01:12.637: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-8828 logs logs-generator logs-generator --since=1s'
    Jul 29 16:01:12.798: INFO: stderr: ""
    Jul 29 16:01:12.798: INFO: stdout: "I0729 16:01:11.795727       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/ns/pods/djx 363\nI0729 16:01:11.996171       1 logs_generator.go:76] 18 POST /api/v1/namespaces/default/pods/ggz 222\nI0729 16:01:12.195820       1 logs_generator.go:76] 19 GET /api/v1/namespaces/default/pods/sc62 282\nI0729 16:01:12.395128       1 logs_generator.go:76] 20 POST /api/v1/namespaces/default/pods/q8g 441\nI0729 16:01:12.595506       1 logs_generator.go:76] 21 GET /api/v1/namespaces/default/pods/jjs 576\n"
    Jul 29 16:01:12.799: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-8828 logs logs-generator logs-generator --since=24h'
    Jul 29 16:01:12.967: INFO: stderr: ""
    Jul 29 16:01:12.967: INFO: stdout: "I0729 16:01:08.395093       1 logs_generator.go:76] 0 POST /api/v1/namespaces/default/pods/zrzq 435\nI0729 16:01:08.595567       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/default/pods/99b 357\nI0729 16:01:08.796091       1 logs_generator.go:76] 2 GET /api/v1/namespaces/default/pods/5mh 333\nI0729 16:01:08.995523       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/x9k 542\nI0729 16:01:09.195978       1 logs_generator.go:76] 4 GET /api/v1/namespaces/kube-system/pods/s5vz 533\nI0729 16:01:09.395309       1 logs_generator.go:76] 5 POST /api/v1/namespaces/ns/pods/t6k5 284\nI0729 16:01:09.597051       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/kube-system/pods/hdtd 383\nI0729 16:01:09.795247       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/4mwx 518\nI0729 16:01:09.996197       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/ns/pods/8kl8 216\nI0729 16:01:10.195687       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/ns/pods/nxw 340\nI0729 16:01:10.396185       1 logs_generator.go:76] 10 GET /api/v1/namespaces/default/pods/x7f 457\nI0729 16:01:10.595430       1 logs_generator.go:76] 11 POST /api/v1/namespaces/default/pods/c7xw 497\nI0729 16:01:10.795839       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/kube-system/pods/kmmc 412\nI0729 16:01:10.995188       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/ns/pods/wczc 556\nI0729 16:01:11.195621       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/default/pods/gw2f 505\nI0729 16:01:11.395899       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/ns/pods/mxvj 331\nI0729 16:01:11.595231       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/kube-system/pods/6g57 275\nI0729 16:01:11.795727       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/ns/pods/djx 363\nI0729 16:01:11.996171       1 logs_generator.go:76] 18 POST /api/v1/namespaces/default/pods/ggz 222\nI0729 16:01:12.195820       1 logs_generator.go:76] 19 GET /api/v1/namespaces/default/pods/sc62 282\nI0729 16:01:12.395128       1 logs_generator.go:76] 20 POST /api/v1/namespaces/default/pods/q8g 441\nI0729 16:01:12.595506       1 logs_generator.go:76] 21 GET /api/v1/namespaces/default/pods/jjs 576\nI0729 16:01:12.796034       1 logs_generator.go:76] 22 PUT /api/v1/namespaces/ns/pods/2pn 418\n"
    [AfterEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1575
    Jul 29 16:01:12.968: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-8828 delete pod logs-generator'
    Jul 29 16:01:14.184: INFO: stderr: ""
    Jul 29 16:01:14.184: INFO: stdout: "pod \"logs-generator\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jul 29 16:01:14.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-8828" for this suite. 07/29/23 16:01:14.197
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Kubectl label
  should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1507
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:01:14.214
Jul 29 16:01:14.214: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename kubectl 07/29/23 16:01:14.217
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:01:14.247
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:01:14.252
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1492
STEP: creating the pod 07/29/23 16:01:14.257
Jul 29 16:01:14.258: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-9750 create -f -'
Jul 29 16:01:14.840: INFO: stderr: ""
Jul 29 16:01:14.840: INFO: stdout: "pod/pause created\n"
Jul 29 16:01:14.840: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Jul 29 16:01:14.840: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-9750" to be "running and ready"
Jul 29 16:01:14.851: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 11.209381ms
Jul 29 16:01:14.851: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'wa4quivohpee-3' to be 'Running' but was 'Pending'
Jul 29 16:01:16.860: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.019838869s
Jul 29 16:01:16.860: INFO: Pod "pause" satisfied condition "running and ready"
Jul 29 16:01:16.860: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1507
STEP: adding the label testing-label with value testing-label-value to a pod 07/29/23 16:01:16.86
Jul 29 16:01:16.860: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-9750 label pods pause testing-label=testing-label-value'
Jul 29 16:01:17.032: INFO: stderr: ""
Jul 29 16:01:17.032: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value 07/29/23 16:01:17.032
Jul 29 16:01:17.033: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-9750 get pod pause -L testing-label'
Jul 29 16:01:17.191: INFO: stderr: ""
Jul 29 16:01:17.191: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    testing-label-value\n"
STEP: removing the label testing-label of a pod 07/29/23 16:01:17.191
Jul 29 16:01:17.191: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-9750 label pods pause testing-label-'
Jul 29 16:01:17.361: INFO: stderr: ""
Jul 29 16:01:17.361: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label 07/29/23 16:01:17.361
Jul 29 16:01:17.362: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-9750 get pod pause -L testing-label'
Jul 29 16:01:17.503: INFO: stderr: ""
Jul 29 16:01:17.503: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
[AfterEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1498
STEP: using delete to clean up resources 07/29/23 16:01:17.503
Jul 29 16:01:17.504: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-9750 delete --grace-period=0 --force -f -'
Jul 29 16:01:17.651: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 29 16:01:17.651: INFO: stdout: "pod \"pause\" force deleted\n"
Jul 29 16:01:17.651: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-9750 get rc,svc -l name=pause --no-headers'
Jul 29 16:01:17.803: INFO: stderr: "No resources found in kubectl-9750 namespace.\n"
Jul 29 16:01:17.803: INFO: stdout: ""
Jul 29 16:01:17.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-9750 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul 29 16:01:17.953: INFO: stderr: ""
Jul 29 16:01:17.953: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jul 29 16:01:17.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9750" for this suite. 07/29/23 16:01:17.961
{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","completed":122,"skipped":2132,"failed":0}
------------------------------
â€¢ [3.759 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl label
  test/e2e/kubectl/kubectl.go:1490
    should update the label on a resource  [Conformance]
    test/e2e/kubectl/kubectl.go:1507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:01:14.214
    Jul 29 16:01:14.214: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename kubectl 07/29/23 16:01:14.217
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:01:14.247
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:01:14.252
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1492
    STEP: creating the pod 07/29/23 16:01:14.257
    Jul 29 16:01:14.258: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-9750 create -f -'
    Jul 29 16:01:14.840: INFO: stderr: ""
    Jul 29 16:01:14.840: INFO: stdout: "pod/pause created\n"
    Jul 29 16:01:14.840: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
    Jul 29 16:01:14.840: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-9750" to be "running and ready"
    Jul 29 16:01:14.851: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 11.209381ms
    Jul 29 16:01:14.851: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'wa4quivohpee-3' to be 'Running' but was 'Pending'
    Jul 29 16:01:16.860: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.019838869s
    Jul 29 16:01:16.860: INFO: Pod "pause" satisfied condition "running and ready"
    Jul 29 16:01:16.860: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
    [It] should update the label on a resource  [Conformance]
      test/e2e/kubectl/kubectl.go:1507
    STEP: adding the label testing-label with value testing-label-value to a pod 07/29/23 16:01:16.86
    Jul 29 16:01:16.860: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-9750 label pods pause testing-label=testing-label-value'
    Jul 29 16:01:17.032: INFO: stderr: ""
    Jul 29 16:01:17.032: INFO: stdout: "pod/pause labeled\n"
    STEP: verifying the pod has the label testing-label with the value testing-label-value 07/29/23 16:01:17.032
    Jul 29 16:01:17.033: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-9750 get pod pause -L testing-label'
    Jul 29 16:01:17.191: INFO: stderr: ""
    Jul 29 16:01:17.191: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    testing-label-value\n"
    STEP: removing the label testing-label of a pod 07/29/23 16:01:17.191
    Jul 29 16:01:17.191: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-9750 label pods pause testing-label-'
    Jul 29 16:01:17.361: INFO: stderr: ""
    Jul 29 16:01:17.361: INFO: stdout: "pod/pause unlabeled\n"
    STEP: verifying the pod doesn't have the label testing-label 07/29/23 16:01:17.361
    Jul 29 16:01:17.362: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-9750 get pod pause -L testing-label'
    Jul 29 16:01:17.503: INFO: stderr: ""
    Jul 29 16:01:17.503: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
    [AfterEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1498
    STEP: using delete to clean up resources 07/29/23 16:01:17.503
    Jul 29 16:01:17.504: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-9750 delete --grace-period=0 --force -f -'
    Jul 29 16:01:17.651: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jul 29 16:01:17.651: INFO: stdout: "pod \"pause\" force deleted\n"
    Jul 29 16:01:17.651: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-9750 get rc,svc -l name=pause --no-headers'
    Jul 29 16:01:17.803: INFO: stderr: "No resources found in kubectl-9750 namespace.\n"
    Jul 29 16:01:17.803: INFO: stdout: ""
    Jul 29 16:01:17.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-9750 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Jul 29 16:01:17.953: INFO: stderr: ""
    Jul 29 16:01:17.953: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jul 29 16:01:17.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-9750" for this suite. 07/29/23 16:01:17.961
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:01:17.976
Jul 29 16:01:17.976: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename job 07/29/23 16:01:17.979
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:01:18.009
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:01:18.013
[It] should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531
STEP: Creating a suspended job 07/29/23 16:01:18.02
STEP: Patching the Job 07/29/23 16:01:18.032
STEP: Watching for Job to be patched 07/29/23 16:01:18.059
Jul 29 16:01:18.062: INFO: Event ADDED observed for Job e2e-vw9lf in namespace job-6062 with labels: map[e2e-job-label:e2e-vw9lf] and annotations: map[batch.kubernetes.io/job-tracking:]
Jul 29 16:01:18.062: INFO: Event MODIFIED observed for Job e2e-vw9lf in namespace job-6062 with labels: map[e2e-job-label:e2e-vw9lf] and annotations: map[batch.kubernetes.io/job-tracking:]
Jul 29 16:01:18.062: INFO: Event MODIFIED found for Job e2e-vw9lf in namespace job-6062 with labels: map[e2e-job-label:e2e-vw9lf e2e-vw9lf:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
STEP: Updating the job 07/29/23 16:01:18.062
STEP: Watching for Job to be updated 07/29/23 16:01:18.075
Jul 29 16:01:18.077: INFO: Event MODIFIED found for Job e2e-vw9lf in namespace job-6062 with labels: map[e2e-job-label:e2e-vw9lf e2e-vw9lf:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jul 29 16:01:18.078: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
STEP: Listing all Jobs with LabelSelector 07/29/23 16:01:18.078
Jul 29 16:01:18.083: INFO: Job: e2e-vw9lf as labels: map[e2e-job-label:e2e-vw9lf e2e-vw9lf:patched]
STEP: Waiting for job to complete 07/29/23 16:01:18.083
STEP: Delete a job collection with a labelselector 07/29/23 16:01:28.092
STEP: Watching for Job to be deleted 07/29/23 16:01:28.111
Jul 29 16:01:28.118: INFO: Event MODIFIED observed for Job e2e-vw9lf in namespace job-6062 with labels: map[e2e-job-label:e2e-vw9lf e2e-vw9lf:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jul 29 16:01:28.119: INFO: Event MODIFIED observed for Job e2e-vw9lf in namespace job-6062 with labels: map[e2e-job-label:e2e-vw9lf e2e-vw9lf:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jul 29 16:01:28.119: INFO: Event MODIFIED observed for Job e2e-vw9lf in namespace job-6062 with labels: map[e2e-job-label:e2e-vw9lf e2e-vw9lf:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jul 29 16:01:28.120: INFO: Event MODIFIED observed for Job e2e-vw9lf in namespace job-6062 with labels: map[e2e-job-label:e2e-vw9lf e2e-vw9lf:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jul 29 16:01:28.120: INFO: Event MODIFIED observed for Job e2e-vw9lf in namespace job-6062 with labels: map[e2e-job-label:e2e-vw9lf e2e-vw9lf:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jul 29 16:01:28.120: INFO: Event DELETED found for Job e2e-vw9lf in namespace job-6062 with labels: map[e2e-job-label:e2e-vw9lf e2e-vw9lf:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
STEP: Relist jobs to confirm deletion 07/29/23 16:01:28.12
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Jul 29 16:01:28.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-6062" for this suite. 07/29/23 16:01:28.142
{"msg":"PASSED [sig-apps] Job should manage the lifecycle of a job [Conformance]","completed":123,"skipped":2151,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.183 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:01:17.976
    Jul 29 16:01:17.976: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename job 07/29/23 16:01:17.979
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:01:18.009
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:01:18.013
    [It] should manage the lifecycle of a job [Conformance]
      test/e2e/apps/job.go:531
    STEP: Creating a suspended job 07/29/23 16:01:18.02
    STEP: Patching the Job 07/29/23 16:01:18.032
    STEP: Watching for Job to be patched 07/29/23 16:01:18.059
    Jul 29 16:01:18.062: INFO: Event ADDED observed for Job e2e-vw9lf in namespace job-6062 with labels: map[e2e-job-label:e2e-vw9lf] and annotations: map[batch.kubernetes.io/job-tracking:]
    Jul 29 16:01:18.062: INFO: Event MODIFIED observed for Job e2e-vw9lf in namespace job-6062 with labels: map[e2e-job-label:e2e-vw9lf] and annotations: map[batch.kubernetes.io/job-tracking:]
    Jul 29 16:01:18.062: INFO: Event MODIFIED found for Job e2e-vw9lf in namespace job-6062 with labels: map[e2e-job-label:e2e-vw9lf e2e-vw9lf:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
    STEP: Updating the job 07/29/23 16:01:18.062
    STEP: Watching for Job to be updated 07/29/23 16:01:18.075
    Jul 29 16:01:18.077: INFO: Event MODIFIED found for Job e2e-vw9lf in namespace job-6062 with labels: map[e2e-job-label:e2e-vw9lf e2e-vw9lf:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jul 29 16:01:18.078: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
    STEP: Listing all Jobs with LabelSelector 07/29/23 16:01:18.078
    Jul 29 16:01:18.083: INFO: Job: e2e-vw9lf as labels: map[e2e-job-label:e2e-vw9lf e2e-vw9lf:patched]
    STEP: Waiting for job to complete 07/29/23 16:01:18.083
    STEP: Delete a job collection with a labelselector 07/29/23 16:01:28.092
    STEP: Watching for Job to be deleted 07/29/23 16:01:28.111
    Jul 29 16:01:28.118: INFO: Event MODIFIED observed for Job e2e-vw9lf in namespace job-6062 with labels: map[e2e-job-label:e2e-vw9lf e2e-vw9lf:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jul 29 16:01:28.119: INFO: Event MODIFIED observed for Job e2e-vw9lf in namespace job-6062 with labels: map[e2e-job-label:e2e-vw9lf e2e-vw9lf:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jul 29 16:01:28.119: INFO: Event MODIFIED observed for Job e2e-vw9lf in namespace job-6062 with labels: map[e2e-job-label:e2e-vw9lf e2e-vw9lf:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jul 29 16:01:28.120: INFO: Event MODIFIED observed for Job e2e-vw9lf in namespace job-6062 with labels: map[e2e-job-label:e2e-vw9lf e2e-vw9lf:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jul 29 16:01:28.120: INFO: Event MODIFIED observed for Job e2e-vw9lf in namespace job-6062 with labels: map[e2e-job-label:e2e-vw9lf e2e-vw9lf:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jul 29 16:01:28.120: INFO: Event DELETED found for Job e2e-vw9lf in namespace job-6062 with labels: map[e2e-job-label:e2e-vw9lf e2e-vw9lf:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    STEP: Relist jobs to confirm deletion 07/29/23 16:01:28.12
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Jul 29 16:01:28.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-6062" for this suite. 07/29/23 16:01:28.142
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:485
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:01:28.182
Jul 29 16:01:28.185: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename security-context-test 07/29/23 16:01:28.197
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:01:28.241
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:01:28.249
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:485
Jul 29 16:01:28.274: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-3f91ef93-57fc-4d88-b4f8-98e9c6ef447a" in namespace "security-context-test-3818" to be "Succeeded or Failed"
Jul 29 16:01:28.280: INFO: Pod "busybox-readonly-false-3f91ef93-57fc-4d88-b4f8-98e9c6ef447a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.928866ms
Jul 29 16:01:30.291: INFO: Pod "busybox-readonly-false-3f91ef93-57fc-4d88-b4f8-98e9c6ef447a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016383674s
Jul 29 16:01:32.289: INFO: Pod "busybox-readonly-false-3f91ef93-57fc-4d88-b4f8-98e9c6ef447a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014479605s
Jul 29 16:01:32.289: INFO: Pod "busybox-readonly-false-3f91ef93-57fc-4d88-b4f8-98e9c6ef447a" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Jul 29 16:01:32.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-3818" for this suite. 07/29/23 16:01:32.298
{"msg":"PASSED [sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","completed":124,"skipped":2168,"failed":0}
------------------------------
â€¢ [4.131 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with readOnlyRootFilesystem
  test/e2e/common/node/security_context.go:429
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:485

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:01:28.182
    Jul 29 16:01:28.185: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename security-context-test 07/29/23 16:01:28.197
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:01:28.241
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:01:28.249
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:485
    Jul 29 16:01:28.274: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-3f91ef93-57fc-4d88-b4f8-98e9c6ef447a" in namespace "security-context-test-3818" to be "Succeeded or Failed"
    Jul 29 16:01:28.280: INFO: Pod "busybox-readonly-false-3f91ef93-57fc-4d88-b4f8-98e9c6ef447a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.928866ms
    Jul 29 16:01:30.291: INFO: Pod "busybox-readonly-false-3f91ef93-57fc-4d88-b4f8-98e9c6ef447a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016383674s
    Jul 29 16:01:32.289: INFO: Pod "busybox-readonly-false-3f91ef93-57fc-4d88-b4f8-98e9c6ef447a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014479605s
    Jul 29 16:01:32.289: INFO: Pod "busybox-readonly-false-3f91ef93-57fc-4d88-b4f8-98e9c6ef447a" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Jul 29 16:01:32.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-3818" for this suite. 07/29/23 16:01:32.298
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:01:32.317
Jul 29 16:01:32.317: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename services 07/29/23 16:01:32.319
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:01:32.347
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:01:32.353
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523
STEP: creating a service nodeport-service with the type=NodePort in namespace services-5214 07/29/23 16:01:32.359
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 07/29/23 16:01:32.393
STEP: creating service externalsvc in namespace services-5214 07/29/23 16:01:32.393
STEP: creating replication controller externalsvc in namespace services-5214 07/29/23 16:01:32.427
I0729 16:01:32.447700      13 runners.go:193] Created replication controller with name: externalsvc, namespace: services-5214, replica count: 2
I0729 16:01:35.501766      13 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName 07/29/23 16:01:35.508
Jul 29 16:01:35.552: INFO: Creating new exec pod
Jul 29 16:01:35.572: INFO: Waiting up to 5m0s for pod "execpodjcwdk" in namespace "services-5214" to be "running"
Jul 29 16:01:35.583: INFO: Pod "execpodjcwdk": Phase="Pending", Reason="", readiness=false. Elapsed: 10.92708ms
Jul 29 16:01:37.591: INFO: Pod "execpodjcwdk": Phase="Running", Reason="", readiness=true. Elapsed: 2.019383103s
Jul 29 16:01:37.592: INFO: Pod "execpodjcwdk" satisfied condition "running"
Jul 29 16:01:37.592: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=services-5214 exec execpodjcwdk -- /bin/sh -x -c nslookup nodeport-service.services-5214.svc.cluster.local'
Jul 29 16:01:38.051: INFO: stderr: "+ nslookup nodeport-service.services-5214.svc.cluster.local\n"
Jul 29 16:01:38.051: INFO: stdout: "Server:\t\t10.233.0.10\nAddress:\t10.233.0.10#53\n\nnodeport-service.services-5214.svc.cluster.local\tcanonical name = externalsvc.services-5214.svc.cluster.local.\nName:\texternalsvc.services-5214.svc.cluster.local\nAddress: 10.233.1.169\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-5214, will wait for the garbage collector to delete the pods 07/29/23 16:01:38.051
Jul 29 16:01:38.122: INFO: Deleting ReplicationController externalsvc took: 10.873041ms
Jul 29 16:01:38.224: INFO: Terminating ReplicationController externalsvc pods took: 101.633543ms
Jul 29 16:01:40.482: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jul 29 16:01:40.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5214" for this suite. 07/29/23 16:01:40.53
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","completed":125,"skipped":2183,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.270 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:01:32.317
    Jul 29 16:01:32.317: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename services 07/29/23 16:01:32.319
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:01:32.347
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:01:32.353
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from NodePort to ExternalName [Conformance]
      test/e2e/network/service.go:1523
    STEP: creating a service nodeport-service with the type=NodePort in namespace services-5214 07/29/23 16:01:32.359
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 07/29/23 16:01:32.393
    STEP: creating service externalsvc in namespace services-5214 07/29/23 16:01:32.393
    STEP: creating replication controller externalsvc in namespace services-5214 07/29/23 16:01:32.427
    I0729 16:01:32.447700      13 runners.go:193] Created replication controller with name: externalsvc, namespace: services-5214, replica count: 2
    I0729 16:01:35.501766      13 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the NodePort service to type=ExternalName 07/29/23 16:01:35.508
    Jul 29 16:01:35.552: INFO: Creating new exec pod
    Jul 29 16:01:35.572: INFO: Waiting up to 5m0s for pod "execpodjcwdk" in namespace "services-5214" to be "running"
    Jul 29 16:01:35.583: INFO: Pod "execpodjcwdk": Phase="Pending", Reason="", readiness=false. Elapsed: 10.92708ms
    Jul 29 16:01:37.591: INFO: Pod "execpodjcwdk": Phase="Running", Reason="", readiness=true. Elapsed: 2.019383103s
    Jul 29 16:01:37.592: INFO: Pod "execpodjcwdk" satisfied condition "running"
    Jul 29 16:01:37.592: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=services-5214 exec execpodjcwdk -- /bin/sh -x -c nslookup nodeport-service.services-5214.svc.cluster.local'
    Jul 29 16:01:38.051: INFO: stderr: "+ nslookup nodeport-service.services-5214.svc.cluster.local\n"
    Jul 29 16:01:38.051: INFO: stdout: "Server:\t\t10.233.0.10\nAddress:\t10.233.0.10#53\n\nnodeport-service.services-5214.svc.cluster.local\tcanonical name = externalsvc.services-5214.svc.cluster.local.\nName:\texternalsvc.services-5214.svc.cluster.local\nAddress: 10.233.1.169\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-5214, will wait for the garbage collector to delete the pods 07/29/23 16:01:38.051
    Jul 29 16:01:38.122: INFO: Deleting ReplicationController externalsvc took: 10.873041ms
    Jul 29 16:01:38.224: INFO: Terminating ReplicationController externalsvc pods took: 101.633543ms
    Jul 29 16:01:40.482: INFO: Cleaning up the NodePort to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jul 29 16:01:40.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-5214" for this suite. 07/29/23 16:01:40.53
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] ReplicationController
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:01:40.593
Jul 29 16:01:40.594: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename replication-controller 07/29/23 16:01:40.602
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:01:40.646
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:01:40.651
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109
STEP: creating a ReplicationController 07/29/23 16:01:40.666
STEP: waiting for RC to be added 07/29/23 16:01:40.687
STEP: waiting for available Replicas 07/29/23 16:01:40.688
STEP: patching ReplicationController 07/29/23 16:01:44.351
STEP: waiting for RC to be modified 07/29/23 16:01:44.364
STEP: patching ReplicationController status 07/29/23 16:01:44.366
STEP: waiting for RC to be modified 07/29/23 16:01:44.377
STEP: waiting for available Replicas 07/29/23 16:01:44.377
STEP: fetching ReplicationController status 07/29/23 16:01:44.386
STEP: patching ReplicationController scale 07/29/23 16:01:44.393
STEP: waiting for RC to be modified 07/29/23 16:01:44.404
STEP: waiting for ReplicationController's scale to be the max amount 07/29/23 16:01:44.404
STEP: fetching ReplicationController; ensuring that it's patched 07/29/23 16:01:48.353
STEP: updating ReplicationController status 07/29/23 16:01:48.363
STEP: waiting for RC to be modified 07/29/23 16:01:48.375
STEP: listing all ReplicationControllers 07/29/23 16:01:48.375
STEP: checking that ReplicationController has expected values 07/29/23 16:01:48.383
STEP: deleting ReplicationControllers by collection 07/29/23 16:01:48.384
STEP: waiting for ReplicationController to have a DELETED watchEvent 07/29/23 16:01:48.4
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Jul 29 16:01:48.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9361" for this suite. 07/29/23 16:01:48.479
{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","completed":126,"skipped":2187,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.898 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:01:40.593
    Jul 29 16:01:40.594: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename replication-controller 07/29/23 16:01:40.602
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:01:40.646
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:01:40.651
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should test the lifecycle of a ReplicationController [Conformance]
      test/e2e/apps/rc.go:109
    STEP: creating a ReplicationController 07/29/23 16:01:40.666
    STEP: waiting for RC to be added 07/29/23 16:01:40.687
    STEP: waiting for available Replicas 07/29/23 16:01:40.688
    STEP: patching ReplicationController 07/29/23 16:01:44.351
    STEP: waiting for RC to be modified 07/29/23 16:01:44.364
    STEP: patching ReplicationController status 07/29/23 16:01:44.366
    STEP: waiting for RC to be modified 07/29/23 16:01:44.377
    STEP: waiting for available Replicas 07/29/23 16:01:44.377
    STEP: fetching ReplicationController status 07/29/23 16:01:44.386
    STEP: patching ReplicationController scale 07/29/23 16:01:44.393
    STEP: waiting for RC to be modified 07/29/23 16:01:44.404
    STEP: waiting for ReplicationController's scale to be the max amount 07/29/23 16:01:44.404
    STEP: fetching ReplicationController; ensuring that it's patched 07/29/23 16:01:48.353
    STEP: updating ReplicationController status 07/29/23 16:01:48.363
    STEP: waiting for RC to be modified 07/29/23 16:01:48.375
    STEP: listing all ReplicationControllers 07/29/23 16:01:48.375
    STEP: checking that ReplicationController has expected values 07/29/23 16:01:48.383
    STEP: deleting ReplicationControllers by collection 07/29/23 16:01:48.384
    STEP: waiting for ReplicationController to have a DELETED watchEvent 07/29/23 16:01:48.4
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Jul 29 16:01:48.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-9361" for this suite. 07/29/23 16:01:48.479
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:01:48.496
Jul 29 16:01:48.496: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename kubelet-test 07/29/23 16:01:48.5
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:01:48.582
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:01:48.588
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Jul 29 16:01:52.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9377" for this suite. 07/29/23 16:01:52.636
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","completed":127,"skipped":2200,"failed":0}
------------------------------
â€¢ [4.151 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should have an terminated reason [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:01:48.496
    Jul 29 16:01:48.496: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename kubelet-test 07/29/23 16:01:48.5
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:01:48.582
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:01:48.588
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should have an terminated reason [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:110
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Jul 29 16:01:52.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-9377" for this suite. 07/29/23 16:01:52.636
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:01:52.652
Jul 29 16:01:52.652: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename pods 07/29/23 16:01:52.655
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:01:52.686
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:01:52.692
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225
STEP: creating the pod 07/29/23 16:01:52.697
STEP: setting up watch 07/29/23 16:01:52.698
STEP: submitting the pod to kubernetes 07/29/23 16:01:52.804
STEP: verifying the pod is in kubernetes 07/29/23 16:01:52.826
STEP: verifying pod creation was observed 07/29/23 16:01:52.84
Jul 29 16:01:52.840: INFO: Waiting up to 5m0s for pod "pod-submit-remove-dfa0e626-f9c8-475c-a208-a6423112e4a6" in namespace "pods-2097" to be "running"
Jul 29 16:01:52.851: INFO: Pod "pod-submit-remove-dfa0e626-f9c8-475c-a208-a6423112e4a6": Phase="Pending", Reason="", readiness=false. Elapsed: 10.465187ms
Jul 29 16:01:54.857: INFO: Pod "pod-submit-remove-dfa0e626-f9c8-475c-a208-a6423112e4a6": Phase="Running", Reason="", readiness=true. Elapsed: 2.01673549s
Jul 29 16:01:54.857: INFO: Pod "pod-submit-remove-dfa0e626-f9c8-475c-a208-a6423112e4a6" satisfied condition "running"
STEP: deleting the pod gracefully 07/29/23 16:01:54.862
STEP: verifying pod deletion was observed 07/29/23 16:01:54.874
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jul 29 16:01:57.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2097" for this suite. 07/29/23 16:01:57.469
{"msg":"PASSED [sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]","completed":128,"skipped":2213,"failed":0}
------------------------------
â€¢ [4.830 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:01:52.652
    Jul 29 16:01:52.652: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename pods 07/29/23 16:01:52.655
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:01:52.686
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:01:52.692
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should be submitted and removed [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:225
    STEP: creating the pod 07/29/23 16:01:52.697
    STEP: setting up watch 07/29/23 16:01:52.698
    STEP: submitting the pod to kubernetes 07/29/23 16:01:52.804
    STEP: verifying the pod is in kubernetes 07/29/23 16:01:52.826
    STEP: verifying pod creation was observed 07/29/23 16:01:52.84
    Jul 29 16:01:52.840: INFO: Waiting up to 5m0s for pod "pod-submit-remove-dfa0e626-f9c8-475c-a208-a6423112e4a6" in namespace "pods-2097" to be "running"
    Jul 29 16:01:52.851: INFO: Pod "pod-submit-remove-dfa0e626-f9c8-475c-a208-a6423112e4a6": Phase="Pending", Reason="", readiness=false. Elapsed: 10.465187ms
    Jul 29 16:01:54.857: INFO: Pod "pod-submit-remove-dfa0e626-f9c8-475c-a208-a6423112e4a6": Phase="Running", Reason="", readiness=true. Elapsed: 2.01673549s
    Jul 29 16:01:54.857: INFO: Pod "pod-submit-remove-dfa0e626-f9c8-475c-a208-a6423112e4a6" satisfied condition "running"
    STEP: deleting the pod gracefully 07/29/23 16:01:54.862
    STEP: verifying pod deletion was observed 07/29/23 16:01:54.874
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jul 29 16:01:57.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-2097" for this suite. 07/29/23 16:01:57.469
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:01:57.513
Jul 29 16:01:57.514: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename replicaset 07/29/23 16:01:57.516
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:01:57.564
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:01:57.569
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
Jul 29 16:01:57.573: INFO: Creating ReplicaSet my-hostname-basic-6b7b1504-647b-4786-be84-855aa62b81a9
Jul 29 16:01:57.593: INFO: Pod name my-hostname-basic-6b7b1504-647b-4786-be84-855aa62b81a9: Found 0 pods out of 1
Jul 29 16:02:02.603: INFO: Pod name my-hostname-basic-6b7b1504-647b-4786-be84-855aa62b81a9: Found 1 pods out of 1
Jul 29 16:02:02.603: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-6b7b1504-647b-4786-be84-855aa62b81a9" is running
Jul 29 16:02:02.604: INFO: Waiting up to 5m0s for pod "my-hostname-basic-6b7b1504-647b-4786-be84-855aa62b81a9-w8wgb" in namespace "replicaset-7675" to be "running"
Jul 29 16:02:02.623: INFO: Pod "my-hostname-basic-6b7b1504-647b-4786-be84-855aa62b81a9-w8wgb": Phase="Running", Reason="", readiness=true. Elapsed: 19.267009ms
Jul 29 16:02:02.623: INFO: Pod "my-hostname-basic-6b7b1504-647b-4786-be84-855aa62b81a9-w8wgb" satisfied condition "running"
Jul 29 16:02:02.623: INFO: Pod "my-hostname-basic-6b7b1504-647b-4786-be84-855aa62b81a9-w8wgb" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-07-29 16:01:57 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-07-29 16:01:59 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-07-29 16:01:59 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-07-29 16:01:57 +0000 UTC Reason: Message:}])
Jul 29 16:02:02.623: INFO: Trying to dial the pod
Jul 29 16:02:07.655: INFO: Controller my-hostname-basic-6b7b1504-647b-4786-be84-855aa62b81a9: Got expected result from replica 1 [my-hostname-basic-6b7b1504-647b-4786-be84-855aa62b81a9-w8wgb]: "my-hostname-basic-6b7b1504-647b-4786-be84-855aa62b81a9-w8wgb", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Jul 29 16:02:07.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-7675" for this suite. 07/29/23 16:02:07.665
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","completed":129,"skipped":2316,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.159 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:01:57.513
    Jul 29 16:01:57.514: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename replicaset 07/29/23 16:01:57.516
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:01:57.564
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:01:57.569
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/replica_set.go:111
    Jul 29 16:01:57.573: INFO: Creating ReplicaSet my-hostname-basic-6b7b1504-647b-4786-be84-855aa62b81a9
    Jul 29 16:01:57.593: INFO: Pod name my-hostname-basic-6b7b1504-647b-4786-be84-855aa62b81a9: Found 0 pods out of 1
    Jul 29 16:02:02.603: INFO: Pod name my-hostname-basic-6b7b1504-647b-4786-be84-855aa62b81a9: Found 1 pods out of 1
    Jul 29 16:02:02.603: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-6b7b1504-647b-4786-be84-855aa62b81a9" is running
    Jul 29 16:02:02.604: INFO: Waiting up to 5m0s for pod "my-hostname-basic-6b7b1504-647b-4786-be84-855aa62b81a9-w8wgb" in namespace "replicaset-7675" to be "running"
    Jul 29 16:02:02.623: INFO: Pod "my-hostname-basic-6b7b1504-647b-4786-be84-855aa62b81a9-w8wgb": Phase="Running", Reason="", readiness=true. Elapsed: 19.267009ms
    Jul 29 16:02:02.623: INFO: Pod "my-hostname-basic-6b7b1504-647b-4786-be84-855aa62b81a9-w8wgb" satisfied condition "running"
    Jul 29 16:02:02.623: INFO: Pod "my-hostname-basic-6b7b1504-647b-4786-be84-855aa62b81a9-w8wgb" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-07-29 16:01:57 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-07-29 16:01:59 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-07-29 16:01:59 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-07-29 16:01:57 +0000 UTC Reason: Message:}])
    Jul 29 16:02:02.623: INFO: Trying to dial the pod
    Jul 29 16:02:07.655: INFO: Controller my-hostname-basic-6b7b1504-647b-4786-be84-855aa62b81a9: Got expected result from replica 1 [my-hostname-basic-6b7b1504-647b-4786-be84-855aa62b81a9-w8wgb]: "my-hostname-basic-6b7b1504-647b-4786-be84-855aa62b81a9-w8wgb", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Jul 29 16:02:07.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-7675" for this suite. 07/29/23 16:02:07.665
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] ReplicaSet
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:02:07.675
Jul 29 16:02:07.676: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename replicaset 07/29/23 16:02:07.678
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:02:07.712
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:02:07.72
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
STEP: Given a Pod with a 'name' label pod-adoption-release is created 07/29/23 16:02:07.725
Jul 29 16:02:07.741: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-5560" to be "running and ready"
Jul 29 16:02:07.748: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 7.05816ms
Jul 29 16:02:07.748: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Jul 29 16:02:09.755: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.014382111s
Jul 29 16:02:09.755: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
Jul 29 16:02:09.755: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
STEP: When a replicaset with a matching selector is created 07/29/23 16:02:09.763
STEP: Then the orphan pod is adopted 07/29/23 16:02:09.774
STEP: When the matched label of one of its pods change 07/29/23 16:02:10.798
Jul 29 16:02:10.805: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released 07/29/23 16:02:10.823
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Jul 29 16:02:11.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-5560" for this suite. 07/29/23 16:02:11.861
{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","completed":130,"skipped":2319,"failed":0}
------------------------------
â€¢ [4.198 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:02:07.675
    Jul 29 16:02:07.676: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename replicaset 07/29/23 16:02:07.678
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:02:07.712
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:02:07.72
    [It] should adopt matching pods on creation and release no longer matching pods [Conformance]
      test/e2e/apps/replica_set.go:131
    STEP: Given a Pod with a 'name' label pod-adoption-release is created 07/29/23 16:02:07.725
    Jul 29 16:02:07.741: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-5560" to be "running and ready"
    Jul 29 16:02:07.748: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 7.05816ms
    Jul 29 16:02:07.748: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Jul 29 16:02:09.755: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.014382111s
    Jul 29 16:02:09.755: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
    Jul 29 16:02:09.755: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
    STEP: When a replicaset with a matching selector is created 07/29/23 16:02:09.763
    STEP: Then the orphan pod is adopted 07/29/23 16:02:09.774
    STEP: When the matched label of one of its pods change 07/29/23 16:02:10.798
    Jul 29 16:02:10.805: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
    STEP: Then the pod is released 07/29/23 16:02:10.823
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Jul 29 16:02:11.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-5560" for this suite. 07/29/23 16:02:11.861
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:02:11.896
Jul 29 16:02:11.896: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename limitrange 07/29/23 16:02:11.898
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:02:11.934
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:02:11.939
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57
STEP: Creating a LimitRange 07/29/23 16:02:11.944
STEP: Setting up watch 07/29/23 16:02:11.944
STEP: Submitting a LimitRange 07/29/23 16:02:12.052
STEP: Verifying LimitRange creation was observed 07/29/23 16:02:12.066
STEP: Fetching the LimitRange to ensure it has proper values 07/29/23 16:02:12.069
Jul 29 16:02:12.075: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jul 29 16:02:12.075: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements 07/29/23 16:02:12.075
STEP: Ensuring Pod has resource requirements applied from LimitRange 07/29/23 16:02:12.088
Jul 29 16:02:12.097: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jul 29 16:02:12.097: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements 07/29/23 16:02:12.097
STEP: Ensuring Pod has merged resource requirements applied from LimitRange 07/29/23 16:02:12.11
Jul 29 16:02:12.121: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Jul 29 16:02:12.121: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources 07/29/23 16:02:12.122
STEP: Failing to create a Pod with more than max resources 07/29/23 16:02:12.128
STEP: Updating a LimitRange 07/29/23 16:02:12.132
STEP: Verifying LimitRange updating is effective 07/29/23 16:02:12.142
STEP: Creating a Pod with less than former min resources 07/29/23 16:02:14.151
STEP: Failing to create a Pod with more than max resources 07/29/23 16:02:14.16
STEP: Deleting a LimitRange 07/29/23 16:02:14.164
STEP: Verifying the LimitRange was deleted 07/29/23 16:02:14.178
Jul 29 16:02:19.186: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources 07/29/23 16:02:19.186
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/framework.go:187
Jul 29 16:02:19.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-1684" for this suite. 07/29/23 16:02:19.211
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","completed":131,"skipped":2402,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.331 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:02:11.896
    Jul 29 16:02:11.896: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename limitrange 07/29/23 16:02:11.898
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:02:11.934
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:02:11.939
    [It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
      test/e2e/scheduling/limit_range.go:57
    STEP: Creating a LimitRange 07/29/23 16:02:11.944
    STEP: Setting up watch 07/29/23 16:02:11.944
    STEP: Submitting a LimitRange 07/29/23 16:02:12.052
    STEP: Verifying LimitRange creation was observed 07/29/23 16:02:12.066
    STEP: Fetching the LimitRange to ensure it has proper values 07/29/23 16:02:12.069
    Jul 29 16:02:12.075: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Jul 29 16:02:12.075: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with no resource requirements 07/29/23 16:02:12.075
    STEP: Ensuring Pod has resource requirements applied from LimitRange 07/29/23 16:02:12.088
    Jul 29 16:02:12.097: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Jul 29 16:02:12.097: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with partial resource requirements 07/29/23 16:02:12.097
    STEP: Ensuring Pod has merged resource requirements applied from LimitRange 07/29/23 16:02:12.11
    Jul 29 16:02:12.121: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
    Jul 29 16:02:12.121: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Failing to create a Pod with less than min resources 07/29/23 16:02:12.122
    STEP: Failing to create a Pod with more than max resources 07/29/23 16:02:12.128
    STEP: Updating a LimitRange 07/29/23 16:02:12.132
    STEP: Verifying LimitRange updating is effective 07/29/23 16:02:12.142
    STEP: Creating a Pod with less than former min resources 07/29/23 16:02:14.151
    STEP: Failing to create a Pod with more than max resources 07/29/23 16:02:14.16
    STEP: Deleting a LimitRange 07/29/23 16:02:14.164
    STEP: Verifying the LimitRange was deleted 07/29/23 16:02:14.178
    Jul 29 16:02:19.186: INFO: limitRange is already deleted
    STEP: Creating a Pod with more than former max resources 07/29/23 16:02:19.186
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/framework.go:187
    Jul 29 16:02:19.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "limitrange-1684" for this suite. 07/29/23 16:02:19.211
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Downward API volume
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:02:19.23
Jul 29 16:02:19.230: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename downward-api 07/29/23 16:02:19.232
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:02:19.269
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:02:19.275
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234
STEP: Creating a pod to test downward API volume plugin 07/29/23 16:02:19.28
Jul 29 16:02:19.297: INFO: Waiting up to 5m0s for pod "downwardapi-volume-72a81863-147f-4b41-9fa0-051bde9983b1" in namespace "downward-api-7325" to be "Succeeded or Failed"
Jul 29 16:02:19.303: INFO: Pod "downwardapi-volume-72a81863-147f-4b41-9fa0-051bde9983b1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.129495ms
Jul 29 16:02:21.312: INFO: Pod "downwardapi-volume-72a81863-147f-4b41-9fa0-051bde9983b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014828379s
Jul 29 16:02:23.313: INFO: Pod "downwardapi-volume-72a81863-147f-4b41-9fa0-051bde9983b1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015856181s
STEP: Saw pod success 07/29/23 16:02:23.313
Jul 29 16:02:23.313: INFO: Pod "downwardapi-volume-72a81863-147f-4b41-9fa0-051bde9983b1" satisfied condition "Succeeded or Failed"
Jul 29 16:02:23.321: INFO: Trying to get logs from node wa4quivohpee-3 pod downwardapi-volume-72a81863-147f-4b41-9fa0-051bde9983b1 container client-container: <nil>
STEP: delete the pod 07/29/23 16:02:23.344
Jul 29 16:02:23.367: INFO: Waiting for pod downwardapi-volume-72a81863-147f-4b41-9fa0-051bde9983b1 to disappear
Jul 29 16:02:23.373: INFO: Pod downwardapi-volume-72a81863-147f-4b41-9fa0-051bde9983b1 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jul 29 16:02:23.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7325" for this suite. 07/29/23 16:02:23.38
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","completed":132,"skipped":2403,"failed":0}
------------------------------
â€¢ [4.162 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:02:19.23
    Jul 29 16:02:19.230: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename downward-api 07/29/23 16:02:19.232
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:02:19.269
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:02:19.275
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:234
    STEP: Creating a pod to test downward API volume plugin 07/29/23 16:02:19.28
    Jul 29 16:02:19.297: INFO: Waiting up to 5m0s for pod "downwardapi-volume-72a81863-147f-4b41-9fa0-051bde9983b1" in namespace "downward-api-7325" to be "Succeeded or Failed"
    Jul 29 16:02:19.303: INFO: Pod "downwardapi-volume-72a81863-147f-4b41-9fa0-051bde9983b1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.129495ms
    Jul 29 16:02:21.312: INFO: Pod "downwardapi-volume-72a81863-147f-4b41-9fa0-051bde9983b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014828379s
    Jul 29 16:02:23.313: INFO: Pod "downwardapi-volume-72a81863-147f-4b41-9fa0-051bde9983b1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015856181s
    STEP: Saw pod success 07/29/23 16:02:23.313
    Jul 29 16:02:23.313: INFO: Pod "downwardapi-volume-72a81863-147f-4b41-9fa0-051bde9983b1" satisfied condition "Succeeded or Failed"
    Jul 29 16:02:23.321: INFO: Trying to get logs from node wa4quivohpee-3 pod downwardapi-volume-72a81863-147f-4b41-9fa0-051bde9983b1 container client-container: <nil>
    STEP: delete the pod 07/29/23 16:02:23.344
    Jul 29 16:02:23.367: INFO: Waiting for pod downwardapi-volume-72a81863-147f-4b41-9fa0-051bde9983b1 to disappear
    Jul 29 16:02:23.373: INFO: Pod downwardapi-volume-72a81863-147f-4b41-9fa0-051bde9983b1 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jul 29 16:02:23.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-7325" for this suite. 07/29/23 16:02:23.38
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43
[BeforeEach] [sig-storage] Projected combined
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:02:23.399
Jul 29 16:02:23.400: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename projected 07/29/23 16:02:23.401
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:02:23.43
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:02:23.435
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43
STEP: Creating configMap with name configmap-projected-all-test-volume-bbb48fa3-ef0b-4960-82aa-45be23b28a97 07/29/23 16:02:23.439
STEP: Creating secret with name secret-projected-all-test-volume-fe05fb9c-6226-41be-9257-4364e606bb27 07/29/23 16:02:23.451
STEP: Creating a pod to test Check all projections for projected volume plugin 07/29/23 16:02:23.46
Jul 29 16:02:23.475: INFO: Waiting up to 5m0s for pod "projected-volume-ff226ec3-d51e-4e9f-9eed-238c7486668f" in namespace "projected-7815" to be "Succeeded or Failed"
Jul 29 16:02:23.482: INFO: Pod "projected-volume-ff226ec3-d51e-4e9f-9eed-238c7486668f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.457297ms
Jul 29 16:02:25.490: INFO: Pod "projected-volume-ff226ec3-d51e-4e9f-9eed-238c7486668f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014009049s
Jul 29 16:02:27.490: INFO: Pod "projected-volume-ff226ec3-d51e-4e9f-9eed-238c7486668f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014387861s
STEP: Saw pod success 07/29/23 16:02:27.49
Jul 29 16:02:27.490: INFO: Pod "projected-volume-ff226ec3-d51e-4e9f-9eed-238c7486668f" satisfied condition "Succeeded or Failed"
Jul 29 16:02:27.495: INFO: Trying to get logs from node wa4quivohpee-3 pod projected-volume-ff226ec3-d51e-4e9f-9eed-238c7486668f container projected-all-volume-test: <nil>
STEP: delete the pod 07/29/23 16:02:27.505
Jul 29 16:02:27.524: INFO: Waiting for pod projected-volume-ff226ec3-d51e-4e9f-9eed-238c7486668f to disappear
Jul 29 16:02:27.539: INFO: Pod projected-volume-ff226ec3-d51e-4e9f-9eed-238c7486668f no longer exists
[AfterEach] [sig-storage] Projected combined
  test/e2e/framework/framework.go:187
Jul 29 16:02:27.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7815" for this suite. 07/29/23 16:02:27.549
{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","completed":133,"skipped":2416,"failed":0}
------------------------------
â€¢ [4.162 seconds]
[sig-storage] Projected combined
test/e2e/common/storage/framework.go:23
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected combined
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:02:23.399
    Jul 29 16:02:23.400: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename projected 07/29/23 16:02:23.401
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:02:23.43
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:02:23.435
    [It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
      test/e2e/common/storage/projected_combined.go:43
    STEP: Creating configMap with name configmap-projected-all-test-volume-bbb48fa3-ef0b-4960-82aa-45be23b28a97 07/29/23 16:02:23.439
    STEP: Creating secret with name secret-projected-all-test-volume-fe05fb9c-6226-41be-9257-4364e606bb27 07/29/23 16:02:23.451
    STEP: Creating a pod to test Check all projections for projected volume plugin 07/29/23 16:02:23.46
    Jul 29 16:02:23.475: INFO: Waiting up to 5m0s for pod "projected-volume-ff226ec3-d51e-4e9f-9eed-238c7486668f" in namespace "projected-7815" to be "Succeeded or Failed"
    Jul 29 16:02:23.482: INFO: Pod "projected-volume-ff226ec3-d51e-4e9f-9eed-238c7486668f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.457297ms
    Jul 29 16:02:25.490: INFO: Pod "projected-volume-ff226ec3-d51e-4e9f-9eed-238c7486668f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014009049s
    Jul 29 16:02:27.490: INFO: Pod "projected-volume-ff226ec3-d51e-4e9f-9eed-238c7486668f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014387861s
    STEP: Saw pod success 07/29/23 16:02:27.49
    Jul 29 16:02:27.490: INFO: Pod "projected-volume-ff226ec3-d51e-4e9f-9eed-238c7486668f" satisfied condition "Succeeded or Failed"
    Jul 29 16:02:27.495: INFO: Trying to get logs from node wa4quivohpee-3 pod projected-volume-ff226ec3-d51e-4e9f-9eed-238c7486668f container projected-all-volume-test: <nil>
    STEP: delete the pod 07/29/23 16:02:27.505
    Jul 29 16:02:27.524: INFO: Waiting for pod projected-volume-ff226ec3-d51e-4e9f-9eed-238c7486668f to disappear
    Jul 29 16:02:27.539: INFO: Pod projected-volume-ff226ec3-d51e-4e9f-9eed-238c7486668f no longer exists
    [AfterEach] [sig-storage] Projected combined
      test/e2e/framework/framework.go:187
    Jul 29 16:02:27.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-7815" for this suite. 07/29/23 16:02:27.549
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:02:27.569
Jul 29 16:02:27.569: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename gc 07/29/23 16:02:27.571
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:02:27.599
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:02:27.605
[It] should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
STEP: create the rc 07/29/23 16:02:27.609
STEP: delete the rc 07/29/23 16:02:32.633
STEP: wait for all pods to be garbage collected 07/29/23 16:02:32.644
STEP: Gathering metrics 07/29/23 16:02:37.659
Jul 29 16:02:37.710: INFO: Waiting up to 5m0s for pod "kube-controller-manager-wa4quivohpee-2" in namespace "kube-system" to be "running and ready"
Jul 29 16:02:37.720: INFO: Pod "kube-controller-manager-wa4quivohpee-2": Phase="Running", Reason="", readiness=true. Elapsed: 10.897832ms
Jul 29 16:02:37.721: INFO: The phase of Pod kube-controller-manager-wa4quivohpee-2 is Running (Ready = true)
Jul 29 16:02:37.721: INFO: Pod "kube-controller-manager-wa4quivohpee-2" satisfied condition "running and ready"
Jul 29 16:02:37.832: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Jul 29 16:02:37.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1637" for this suite. 07/29/23 16:02:37.841
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","completed":134,"skipped":2423,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.285 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:02:27.569
    Jul 29 16:02:27.569: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename gc 07/29/23 16:02:27.571
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:02:27.599
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:02:27.605
    [It] should delete pods created by rc when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:312
    STEP: create the rc 07/29/23 16:02:27.609
    STEP: delete the rc 07/29/23 16:02:32.633
    STEP: wait for all pods to be garbage collected 07/29/23 16:02:32.644
    STEP: Gathering metrics 07/29/23 16:02:37.659
    Jul 29 16:02:37.710: INFO: Waiting up to 5m0s for pod "kube-controller-manager-wa4quivohpee-2" in namespace "kube-system" to be "running and ready"
    Jul 29 16:02:37.720: INFO: Pod "kube-controller-manager-wa4quivohpee-2": Phase="Running", Reason="", readiness=true. Elapsed: 10.897832ms
    Jul 29 16:02:37.721: INFO: The phase of Pod kube-controller-manager-wa4quivohpee-2 is Running (Ready = true)
    Jul 29 16:02:37.721: INFO: Pod "kube-controller-manager-wa4quivohpee-2" satisfied condition "running and ready"
    Jul 29 16:02:37.832: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Jul 29 16:02:37.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-1637" for this suite. 07/29/23 16:02:37.841
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:02:37.857
Jul 29 16:02:37.857: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename tables 07/29/23 16:02:37.859
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:02:37.91
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:02:37.913
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/apimachinery/table_conversion.go:49
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/framework.go:187
Jul 29 16:02:37.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-8537" for this suite. 07/29/23 16:02:37.926
{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","completed":135,"skipped":2437,"failed":0}
------------------------------
â€¢ [0.086 seconds]
[sig-api-machinery] Servers with support for Table transformation
test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:02:37.857
    Jul 29 16:02:37.857: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename tables 07/29/23 16:02:37.859
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:02:37.91
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:02:37.913
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/apimachinery/table_conversion.go:49
    [It] should return a 406 for a backend which does not implement metadata [Conformance]
      test/e2e/apimachinery/table_conversion.go:154
    [AfterEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/framework.go:187
    Jul 29 16:02:37.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "tables-8537" for this suite. 07/29/23 16:02:37.926
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:02:37.944
Jul 29 16:02:37.944: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename deployment 07/29/23 16:02:37.947
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:02:37.972
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:02:37.977
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
Jul 29 16:02:37.994: INFO: Pod name rollover-pod: Found 0 pods out of 1
Jul 29 16:02:43.001: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 07/29/23 16:02:43.001
Jul 29 16:02:43.001: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Jul 29 16:02:45.010: INFO: Creating deployment "test-rollover-deployment"
Jul 29 16:02:45.034: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Jul 29 16:02:47.054: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Jul 29 16:02:47.067: INFO: Ensure that both replica sets have 1 created replica
Jul 29 16:02:47.078: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Jul 29 16:02:47.093: INFO: Updating deployment test-rollover-deployment
Jul 29 16:02:47.093: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Jul 29 16:02:49.150: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Jul 29 16:02:49.170: INFO: Make sure deployment "test-rollover-deployment" is complete
Jul 29 16:02:49.182: INFO: all replica sets need to contain the pod-template-hash label
Jul 29 16:02:49.182: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 16, 2, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 16, 2, 45, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 16, 2, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 16, 2, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 29 16:02:51.194: INFO: all replica sets need to contain the pod-template-hash label
Jul 29 16:02:51.194: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 16, 2, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 16, 2, 45, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 16, 2, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 16, 2, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 29 16:02:53.203: INFO: all replica sets need to contain the pod-template-hash label
Jul 29 16:02:53.203: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 16, 2, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 16, 2, 45, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 16, 2, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 16, 2, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 29 16:02:55.196: INFO: all replica sets need to contain the pod-template-hash label
Jul 29 16:02:55.196: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 16, 2, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 16, 2, 45, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 16, 2, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 16, 2, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 29 16:02:57.197: INFO: all replica sets need to contain the pod-template-hash label
Jul 29 16:02:57.197: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 16, 2, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 16, 2, 45, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 16, 2, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 16, 2, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 29 16:02:59.197: INFO: 
Jul 29 16:02:59.197: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jul 29 16:02:59.211: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-533  fe40ede9-05fe-4a4b-aafe-380ccc744f27 12505 2 2023-07-29 16:02:45 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-07-29 16:02:47 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-29 16:02:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003717d08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-07-29 16:02:45 +0000 UTC,LastTransitionTime:2023-07-29 16:02:45 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6d45fd857b" has successfully progressed.,LastUpdateTime:2023-07-29 16:02:58 +0000 UTC,LastTransitionTime:2023-07-29 16:02:45 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jul 29 16:02:59.217: INFO: New ReplicaSet "test-rollover-deployment-6d45fd857b" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-6d45fd857b  deployment-533  ed6b7069-1f4c-4118-b95e-01b8cc3ed317 12494 2 2023-07-29 16:02:47 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment fe40ede9-05fe-4a4b-aafe-380ccc744f27 0xc0035842a7 0xc0035842a8}] [] [{kube-controller-manager Update apps/v1 2023-07-29 16:02:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fe40ede9-05fe-4a4b-aafe-380ccc744f27\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-29 16:02:58 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6d45fd857b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003584358 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jul 29 16:02:59.217: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Jul 29 16:02:59.217: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-533  d2f3a929-4632-49ca-8764-7e00ecc5aa1e 12503 2 2023-07-29 16:02:37 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment fe40ede9-05fe-4a4b-aafe-380ccc744f27 0xc003584057 0xc003584058}] [] [{e2e.test Update apps/v1 2023-07-29 16:02:37 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-29 16:02:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fe40ede9-05fe-4a4b-aafe-380ccc744f27\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-07-29 16:02:58 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003584118 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jul 29 16:02:59.217: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-59b9df946d  deployment-533  22bdd928-72e1-4b86-a76c-9c2fb4ad6e26 12461 2 2023-07-29 16:02:45 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment fe40ede9-05fe-4a4b-aafe-380ccc744f27 0xc003584187 0xc003584188}] [] [{kube-controller-manager Update apps/v1 2023-07-29 16:02:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fe40ede9-05fe-4a4b-aafe-380ccc744f27\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-29 16:02:47 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 59b9df946d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003584238 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jul 29 16:02:59.223: INFO: Pod "test-rollover-deployment-6d45fd857b-n6fjg" is available:
&Pod{ObjectMeta:{test-rollover-deployment-6d45fd857b-n6fjg test-rollover-deployment-6d45fd857b- deployment-533  790c9aa9-b222-49af-a91d-b10c06c6ad7b 12471 0 2023-07-29 16:02:47 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[] [{apps/v1 ReplicaSet test-rollover-deployment-6d45fd857b ed6b7069-1f4c-4118-b95e-01b8cc3ed317 0xc003660df7 0xc003660df8}] [] [{kube-controller-manager Update v1 2023-07-29 16:02:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ed6b7069-1f4c-4118-b95e-01b8cc3ed317\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 16:02:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.65.118\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sw4gx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sw4gx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wa4quivohpee-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:02:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:02:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:02:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:02:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.234,PodIP:10.233.65.118,StartTime:2023-07-29 16:02:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-29 16:02:48 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:37dd9ad84b34913b4bc57e00b0a28e8b00f18ca6a5ecec69461aa57402e5c787,ContainerID:cri-o://cbec745199bbd0f36db7dce846c25174ed6da795c021e2af11a159c3363572eb,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.65.118,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jul 29 16:02:59.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-533" for this suite. 07/29/23 16:02:59.231
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","completed":136,"skipped":2446,"failed":0}
------------------------------
â€¢ [SLOW TEST] [21.298 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:02:37.944
    Jul 29 16:02:37.944: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename deployment 07/29/23 16:02:37.947
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:02:37.972
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:02:37.977
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support rollover [Conformance]
      test/e2e/apps/deployment.go:132
    Jul 29 16:02:37.994: INFO: Pod name rollover-pod: Found 0 pods out of 1
    Jul 29 16:02:43.001: INFO: Pod name rollover-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 07/29/23 16:02:43.001
    Jul 29 16:02:43.001: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
    Jul 29 16:02:45.010: INFO: Creating deployment "test-rollover-deployment"
    Jul 29 16:02:45.034: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
    Jul 29 16:02:47.054: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
    Jul 29 16:02:47.067: INFO: Ensure that both replica sets have 1 created replica
    Jul 29 16:02:47.078: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
    Jul 29 16:02:47.093: INFO: Updating deployment test-rollover-deployment
    Jul 29 16:02:47.093: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
    Jul 29 16:02:49.150: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
    Jul 29 16:02:49.170: INFO: Make sure deployment "test-rollover-deployment" is complete
    Jul 29 16:02:49.182: INFO: all replica sets need to contain the pod-template-hash label
    Jul 29 16:02:49.182: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 16, 2, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 16, 2, 45, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 16, 2, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 16, 2, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jul 29 16:02:51.194: INFO: all replica sets need to contain the pod-template-hash label
    Jul 29 16:02:51.194: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 16, 2, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 16, 2, 45, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 16, 2, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 16, 2, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jul 29 16:02:53.203: INFO: all replica sets need to contain the pod-template-hash label
    Jul 29 16:02:53.203: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 16, 2, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 16, 2, 45, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 16, 2, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 16, 2, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jul 29 16:02:55.196: INFO: all replica sets need to contain the pod-template-hash label
    Jul 29 16:02:55.196: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 16, 2, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 16, 2, 45, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 16, 2, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 16, 2, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jul 29 16:02:57.197: INFO: all replica sets need to contain the pod-template-hash label
    Jul 29 16:02:57.197: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 16, 2, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 16, 2, 45, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 16, 2, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 16, 2, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jul 29 16:02:59.197: INFO: 
    Jul 29 16:02:59.197: INFO: Ensure that both old replica sets have no replicas
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jul 29 16:02:59.211: INFO: Deployment "test-rollover-deployment":
    &Deployment{ObjectMeta:{test-rollover-deployment  deployment-533  fe40ede9-05fe-4a4b-aafe-380ccc744f27 12505 2 2023-07-29 16:02:45 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-07-29 16:02:47 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-29 16:02:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003717d08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-07-29 16:02:45 +0000 UTC,LastTransitionTime:2023-07-29 16:02:45 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6d45fd857b" has successfully progressed.,LastUpdateTime:2023-07-29 16:02:58 +0000 UTC,LastTransitionTime:2023-07-29 16:02:45 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jul 29 16:02:59.217: INFO: New ReplicaSet "test-rollover-deployment-6d45fd857b" of Deployment "test-rollover-deployment":
    &ReplicaSet{ObjectMeta:{test-rollover-deployment-6d45fd857b  deployment-533  ed6b7069-1f4c-4118-b95e-01b8cc3ed317 12494 2 2023-07-29 16:02:47 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment fe40ede9-05fe-4a4b-aafe-380ccc744f27 0xc0035842a7 0xc0035842a8}] [] [{kube-controller-manager Update apps/v1 2023-07-29 16:02:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fe40ede9-05fe-4a4b-aafe-380ccc744f27\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-29 16:02:58 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6d45fd857b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003584358 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jul 29 16:02:59.217: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
    Jul 29 16:02:59.217: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-533  d2f3a929-4632-49ca-8764-7e00ecc5aa1e 12503 2 2023-07-29 16:02:37 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment fe40ede9-05fe-4a4b-aafe-380ccc744f27 0xc003584057 0xc003584058}] [] [{e2e.test Update apps/v1 2023-07-29 16:02:37 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-29 16:02:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fe40ede9-05fe-4a4b-aafe-380ccc744f27\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-07-29 16:02:58 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003584118 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jul 29 16:02:59.217: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-59b9df946d  deployment-533  22bdd928-72e1-4b86-a76c-9c2fb4ad6e26 12461 2 2023-07-29 16:02:45 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment fe40ede9-05fe-4a4b-aafe-380ccc744f27 0xc003584187 0xc003584188}] [] [{kube-controller-manager Update apps/v1 2023-07-29 16:02:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fe40ede9-05fe-4a4b-aafe-380ccc744f27\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-29 16:02:47 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 59b9df946d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003584238 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jul 29 16:02:59.223: INFO: Pod "test-rollover-deployment-6d45fd857b-n6fjg" is available:
    &Pod{ObjectMeta:{test-rollover-deployment-6d45fd857b-n6fjg test-rollover-deployment-6d45fd857b- deployment-533  790c9aa9-b222-49af-a91d-b10c06c6ad7b 12471 0 2023-07-29 16:02:47 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[] [{apps/v1 ReplicaSet test-rollover-deployment-6d45fd857b ed6b7069-1f4c-4118-b95e-01b8cc3ed317 0xc003660df7 0xc003660df8}] [] [{kube-controller-manager Update v1 2023-07-29 16:02:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ed6b7069-1f4c-4118-b95e-01b8cc3ed317\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 16:02:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.65.118\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sw4gx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sw4gx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wa4quivohpee-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:02:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:02:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:02:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:02:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.234,PodIP:10.233.65.118,StartTime:2023-07-29 16:02:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-29 16:02:48 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:37dd9ad84b34913b4bc57e00b0a28e8b00f18ca6a5ecec69461aa57402e5c787,ContainerID:cri-o://cbec745199bbd0f36db7dce846c25174ed6da795c021e2af11a159c3363572eb,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.65.118,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jul 29 16:02:59.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-533" for this suite. 07/29/23 16:02:59.231
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:02:59.256
Jul 29 16:02:59.256: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename daemonsets 07/29/23 16:02:59.259
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:02:59.293
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:02:59.296
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293
STEP: Creating a simple DaemonSet "daemon-set" 07/29/23 16:02:59.332
STEP: Check that daemon pods launch on every node of the cluster. 07/29/23 16:02:59.343
Jul 29 16:02:59.359: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jul 29 16:02:59.359: INFO: Node wa4quivohpee-1 is running 0 daemon pod, expected 1
Jul 29 16:03:00.377: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jul 29 16:03:00.377: INFO: Node wa4quivohpee-1 is running 0 daemon pod, expected 1
Jul 29 16:03:01.375: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jul 29 16:03:01.375: INFO: Node wa4quivohpee-1 is running 0 daemon pod, expected 1
Jul 29 16:03:02.380: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jul 29 16:03:02.380: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 07/29/23 16:03:02.386
Jul 29 16:03:02.444: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jul 29 16:03:02.444: INFO: Node wa4quivohpee-3 is running 0 daemon pod, expected 1
Jul 29 16:03:03.465: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jul 29 16:03:03.465: INFO: Node wa4quivohpee-3 is running 0 daemon pod, expected 1
Jul 29 16:03:04.477: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jul 29 16:03:04.477: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted. 07/29/23 16:03:04.477
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 07/29/23 16:03:04.493
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6914, will wait for the garbage collector to delete the pods 07/29/23 16:03:04.494
Jul 29 16:03:04.560: INFO: Deleting DaemonSet.extensions daemon-set took: 10.809255ms
Jul 29 16:03:04.661: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.293322ms
Jul 29 16:03:06.868: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jul 29 16:03:06.868: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jul 29 16:03:06.874: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"12638"},"items":null}

Jul 29 16:03:06.882: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"12638"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Jul 29 16:03:06.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6914" for this suite. 07/29/23 16:03:06.923
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","completed":137,"skipped":2526,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.681 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:02:59.256
    Jul 29 16:02:59.256: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename daemonsets 07/29/23 16:02:59.259
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:02:59.293
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:02:59.296
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should retry creating failed daemon pods [Conformance]
      test/e2e/apps/daemon_set.go:293
    STEP: Creating a simple DaemonSet "daemon-set" 07/29/23 16:02:59.332
    STEP: Check that daemon pods launch on every node of the cluster. 07/29/23 16:02:59.343
    Jul 29 16:02:59.359: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jul 29 16:02:59.359: INFO: Node wa4quivohpee-1 is running 0 daemon pod, expected 1
    Jul 29 16:03:00.377: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jul 29 16:03:00.377: INFO: Node wa4quivohpee-1 is running 0 daemon pod, expected 1
    Jul 29 16:03:01.375: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jul 29 16:03:01.375: INFO: Node wa4quivohpee-1 is running 0 daemon pod, expected 1
    Jul 29 16:03:02.380: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jul 29 16:03:02.380: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 07/29/23 16:03:02.386
    Jul 29 16:03:02.444: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jul 29 16:03:02.444: INFO: Node wa4quivohpee-3 is running 0 daemon pod, expected 1
    Jul 29 16:03:03.465: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jul 29 16:03:03.465: INFO: Node wa4quivohpee-3 is running 0 daemon pod, expected 1
    Jul 29 16:03:04.477: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jul 29 16:03:04.477: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Wait for the failed daemon pod to be completely deleted. 07/29/23 16:03:04.477
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 07/29/23 16:03:04.493
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6914, will wait for the garbage collector to delete the pods 07/29/23 16:03:04.494
    Jul 29 16:03:04.560: INFO: Deleting DaemonSet.extensions daemon-set took: 10.809255ms
    Jul 29 16:03:04.661: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.293322ms
    Jul 29 16:03:06.868: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jul 29 16:03:06.868: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jul 29 16:03:06.874: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"12638"},"items":null}

    Jul 29 16:03:06.882: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"12638"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Jul 29 16:03:06.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-6914" for this suite. 07/29/23 16:03:06.923
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:03:06.943
Jul 29 16:03:06.944: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename downward-api 07/29/23 16:03:06.946
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:03:06.982
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:03:06.987
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129
STEP: Creating the pod 07/29/23 16:03:06.991
Jul 29 16:03:07.013: INFO: Waiting up to 5m0s for pod "labelsupdate1c6964f6-8d3b-43ee-8ddc-2b1f4ada6365" in namespace "downward-api-6374" to be "running and ready"
Jul 29 16:03:07.020: INFO: Pod "labelsupdate1c6964f6-8d3b-43ee-8ddc-2b1f4ada6365": Phase="Pending", Reason="", readiness=false. Elapsed: 7.035393ms
Jul 29 16:03:07.020: INFO: The phase of Pod labelsupdate1c6964f6-8d3b-43ee-8ddc-2b1f4ada6365 is Pending, waiting for it to be Running (with Ready = true)
Jul 29 16:03:09.030: INFO: Pod "labelsupdate1c6964f6-8d3b-43ee-8ddc-2b1f4ada6365": Phase="Running", Reason="", readiness=true. Elapsed: 2.016614701s
Jul 29 16:03:09.030: INFO: The phase of Pod labelsupdate1c6964f6-8d3b-43ee-8ddc-2b1f4ada6365 is Running (Ready = true)
Jul 29 16:03:09.030: INFO: Pod "labelsupdate1c6964f6-8d3b-43ee-8ddc-2b1f4ada6365" satisfied condition "running and ready"
Jul 29 16:03:09.571: INFO: Successfully updated pod "labelsupdate1c6964f6-8d3b-43ee-8ddc-2b1f4ada6365"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jul 29 16:03:11.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6374" for this suite. 07/29/23 16:03:11.614
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","completed":138,"skipped":2545,"failed":0}
------------------------------
â€¢ [4.683 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:03:06.943
    Jul 29 16:03:06.944: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename downward-api 07/29/23 16:03:06.946
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:03:06.982
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:03:06.987
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:129
    STEP: Creating the pod 07/29/23 16:03:06.991
    Jul 29 16:03:07.013: INFO: Waiting up to 5m0s for pod "labelsupdate1c6964f6-8d3b-43ee-8ddc-2b1f4ada6365" in namespace "downward-api-6374" to be "running and ready"
    Jul 29 16:03:07.020: INFO: Pod "labelsupdate1c6964f6-8d3b-43ee-8ddc-2b1f4ada6365": Phase="Pending", Reason="", readiness=false. Elapsed: 7.035393ms
    Jul 29 16:03:07.020: INFO: The phase of Pod labelsupdate1c6964f6-8d3b-43ee-8ddc-2b1f4ada6365 is Pending, waiting for it to be Running (with Ready = true)
    Jul 29 16:03:09.030: INFO: Pod "labelsupdate1c6964f6-8d3b-43ee-8ddc-2b1f4ada6365": Phase="Running", Reason="", readiness=true. Elapsed: 2.016614701s
    Jul 29 16:03:09.030: INFO: The phase of Pod labelsupdate1c6964f6-8d3b-43ee-8ddc-2b1f4ada6365 is Running (Ready = true)
    Jul 29 16:03:09.030: INFO: Pod "labelsupdate1c6964f6-8d3b-43ee-8ddc-2b1f4ada6365" satisfied condition "running and ready"
    Jul 29 16:03:09.571: INFO: Successfully updated pod "labelsupdate1c6964f6-8d3b-43ee-8ddc-2b1f4ada6365"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jul 29 16:03:11.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-6374" for this suite. 07/29/23 16:03:11.614
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3210
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:03:11.629
Jul 29 16:03:11.629: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename services 07/29/23 16:03:11.633
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:03:11.664
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:03:11.669
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3210
STEP: creating an Endpoint 07/29/23 16:03:11.681
STEP: waiting for available Endpoint 07/29/23 16:03:11.69
STEP: listing all Endpoints 07/29/23 16:03:11.692
STEP: updating the Endpoint 07/29/23 16:03:11.697
STEP: fetching the Endpoint 07/29/23 16:03:11.707
STEP: patching the Endpoint 07/29/23 16:03:11.722
STEP: fetching the Endpoint 07/29/23 16:03:11.741
STEP: deleting the Endpoint by Collection 07/29/23 16:03:11.748
STEP: waiting for Endpoint deletion 07/29/23 16:03:11.762
STEP: fetching the Endpoint 07/29/23 16:03:11.765
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jul 29 16:03:11.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7550" for this suite. 07/29/23 16:03:11.785
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","completed":139,"skipped":2574,"failed":0}
------------------------------
â€¢ [0.171 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3210

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:03:11.629
    Jul 29 16:03:11.629: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename services 07/29/23 16:03:11.633
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:03:11.664
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:03:11.669
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should test the lifecycle of an Endpoint [Conformance]
      test/e2e/network/service.go:3210
    STEP: creating an Endpoint 07/29/23 16:03:11.681
    STEP: waiting for available Endpoint 07/29/23 16:03:11.69
    STEP: listing all Endpoints 07/29/23 16:03:11.692
    STEP: updating the Endpoint 07/29/23 16:03:11.697
    STEP: fetching the Endpoint 07/29/23 16:03:11.707
    STEP: patching the Endpoint 07/29/23 16:03:11.722
    STEP: fetching the Endpoint 07/29/23 16:03:11.741
    STEP: deleting the Endpoint by Collection 07/29/23 16:03:11.748
    STEP: waiting for Endpoint deletion 07/29/23 16:03:11.762
    STEP: fetching the Endpoint 07/29/23 16:03:11.765
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jul 29 16:03:11.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-7550" for this suite. 07/29/23 16:03:11.785
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:03:11.803
Jul 29 16:03:11.804: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename projected 07/29/23 16:03:11.806
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:03:11.849
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:03:11.853
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129
STEP: Creating the pod 07/29/23 16:03:11.861
Jul 29 16:03:11.881: INFO: Waiting up to 5m0s for pod "labelsupdatedd02bb3a-fe39-4bd1-b708-96b8be79f1ce" in namespace "projected-318" to be "running and ready"
Jul 29 16:03:11.889: INFO: Pod "labelsupdatedd02bb3a-fe39-4bd1-b708-96b8be79f1ce": Phase="Pending", Reason="", readiness=false. Elapsed: 7.064329ms
Jul 29 16:03:11.889: INFO: The phase of Pod labelsupdatedd02bb3a-fe39-4bd1-b708-96b8be79f1ce is Pending, waiting for it to be Running (with Ready = true)
Jul 29 16:03:13.896: INFO: Pod "labelsupdatedd02bb3a-fe39-4bd1-b708-96b8be79f1ce": Phase="Running", Reason="", readiness=true. Elapsed: 2.014583917s
Jul 29 16:03:13.896: INFO: The phase of Pod labelsupdatedd02bb3a-fe39-4bd1-b708-96b8be79f1ce is Running (Ready = true)
Jul 29 16:03:13.896: INFO: Pod "labelsupdatedd02bb3a-fe39-4bd1-b708-96b8be79f1ce" satisfied condition "running and ready"
Jul 29 16:03:14.439: INFO: Successfully updated pod "labelsupdatedd02bb3a-fe39-4bd1-b708-96b8be79f1ce"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jul 29 16:03:16.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-318" for this suite. 07/29/23 16:03:16.497
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","completed":140,"skipped":2583,"failed":0}
------------------------------
â€¢ [4.720 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:03:11.803
    Jul 29 16:03:11.804: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename projected 07/29/23 16:03:11.806
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:03:11.849
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:03:11.853
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:129
    STEP: Creating the pod 07/29/23 16:03:11.861
    Jul 29 16:03:11.881: INFO: Waiting up to 5m0s for pod "labelsupdatedd02bb3a-fe39-4bd1-b708-96b8be79f1ce" in namespace "projected-318" to be "running and ready"
    Jul 29 16:03:11.889: INFO: Pod "labelsupdatedd02bb3a-fe39-4bd1-b708-96b8be79f1ce": Phase="Pending", Reason="", readiness=false. Elapsed: 7.064329ms
    Jul 29 16:03:11.889: INFO: The phase of Pod labelsupdatedd02bb3a-fe39-4bd1-b708-96b8be79f1ce is Pending, waiting for it to be Running (with Ready = true)
    Jul 29 16:03:13.896: INFO: Pod "labelsupdatedd02bb3a-fe39-4bd1-b708-96b8be79f1ce": Phase="Running", Reason="", readiness=true. Elapsed: 2.014583917s
    Jul 29 16:03:13.896: INFO: The phase of Pod labelsupdatedd02bb3a-fe39-4bd1-b708-96b8be79f1ce is Running (Ready = true)
    Jul 29 16:03:13.896: INFO: Pod "labelsupdatedd02bb3a-fe39-4bd1-b708-96b8be79f1ce" satisfied condition "running and ready"
    Jul 29 16:03:14.439: INFO: Successfully updated pod "labelsupdatedd02bb3a-fe39-4bd1-b708-96b8be79f1ce"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jul 29 16:03:16.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-318" for this suite. 07/29/23 16:03:16.497
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:315
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:03:16.53
Jul 29 16:03:16.530: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename statefulset 07/29/23 16:03:16.532
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:03:16.576
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:03:16.582
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-7939 07/29/23 16:03:16.587
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:315
STEP: Creating a new StatefulSet 07/29/23 16:03:16.597
Jul 29 16:03:16.618: INFO: Found 0 stateful pods, waiting for 3
Jul 29 16:03:26.636: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 29 16:03:26.636: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 29 16:03:26.636: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 07/29/23 16:03:26.657
Jul 29 16:03:26.687: INFO: Updating stateful set ss2
STEP: Creating a new revision 07/29/23 16:03:26.688
STEP: Not applying an update when the partition is greater than the number of replicas 07/29/23 16:03:36.715
STEP: Performing a canary update 07/29/23 16:03:36.715
Jul 29 16:03:36.745: INFO: Updating stateful set ss2
Jul 29 16:03:36.769: INFO: Waiting for Pod statefulset-7939/ss2-2 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
STEP: Restoring Pods to the correct revision when they are deleted 07/29/23 16:03:46.79
Jul 29 16:03:46.877: INFO: Found 1 stateful pods, waiting for 3
Jul 29 16:03:56.887: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 29 16:03:56.887: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 29 16:03:56.887: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Jul 29 16:04:06.884: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 29 16:04:06.884: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 29 16:04:06.884: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update 07/29/23 16:04:06.896
Jul 29 16:04:06.923: INFO: Updating stateful set ss2
Jul 29 16:04:06.953: INFO: Waiting for Pod statefulset-7939/ss2-1 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
Jul 29 16:04:16.995: INFO: Updating stateful set ss2
Jul 29 16:04:17.016: INFO: Waiting for StatefulSet statefulset-7939/ss2 to complete update
Jul 29 16:04:17.016: INFO: Waiting for Pod statefulset-7939/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jul 29 16:04:27.035: INFO: Deleting all statefulset in ns statefulset-7939
Jul 29 16:04:27.042: INFO: Scaling statefulset ss2 to 0
Jul 29 16:04:37.109: INFO: Waiting for statefulset status.replicas updated to 0
Jul 29 16:04:37.118: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jul 29 16:04:37.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7939" for this suite. 07/29/23 16:04:37.208
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","completed":141,"skipped":2600,"failed":0}
------------------------------
â€¢ [SLOW TEST] [80.692 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    test/e2e/apps/statefulset.go:315

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:03:16.53
    Jul 29 16:03:16.530: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename statefulset 07/29/23 16:03:16.532
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:03:16.576
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:03:16.582
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-7939 07/29/23 16:03:16.587
    [It] should perform canary updates and phased rolling updates of template modifications [Conformance]
      test/e2e/apps/statefulset.go:315
    STEP: Creating a new StatefulSet 07/29/23 16:03:16.597
    Jul 29 16:03:16.618: INFO: Found 0 stateful pods, waiting for 3
    Jul 29 16:03:26.636: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Jul 29 16:03:26.636: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Jul 29 16:03:26.636: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 07/29/23 16:03:26.657
    Jul 29 16:03:26.687: INFO: Updating stateful set ss2
    STEP: Creating a new revision 07/29/23 16:03:26.688
    STEP: Not applying an update when the partition is greater than the number of replicas 07/29/23 16:03:36.715
    STEP: Performing a canary update 07/29/23 16:03:36.715
    Jul 29 16:03:36.745: INFO: Updating stateful set ss2
    Jul 29 16:03:36.769: INFO: Waiting for Pod statefulset-7939/ss2-2 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    STEP: Restoring Pods to the correct revision when they are deleted 07/29/23 16:03:46.79
    Jul 29 16:03:46.877: INFO: Found 1 stateful pods, waiting for 3
    Jul 29 16:03:56.887: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Jul 29 16:03:56.887: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Jul 29 16:03:56.887: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
    Jul 29 16:04:06.884: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Jul 29 16:04:06.884: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Jul 29 16:04:06.884: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Performing a phased rolling update 07/29/23 16:04:06.896
    Jul 29 16:04:06.923: INFO: Updating stateful set ss2
    Jul 29 16:04:06.953: INFO: Waiting for Pod statefulset-7939/ss2-1 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    Jul 29 16:04:16.995: INFO: Updating stateful set ss2
    Jul 29 16:04:17.016: INFO: Waiting for StatefulSet statefulset-7939/ss2 to complete update
    Jul 29 16:04:17.016: INFO: Waiting for Pod statefulset-7939/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jul 29 16:04:27.035: INFO: Deleting all statefulset in ns statefulset-7939
    Jul 29 16:04:27.042: INFO: Scaling statefulset ss2 to 0
    Jul 29 16:04:37.109: INFO: Waiting for statefulset status.replicas updated to 0
    Jul 29 16:04:37.118: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jul 29 16:04:37.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-7939" for this suite. 07/29/23 16:04:37.208
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3394
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:04:37.227
Jul 29 16:04:37.228: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename services 07/29/23 16:04:37.235
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:04:37.272
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:04:37.277
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3394
STEP: creating a Service 07/29/23 16:04:37.287
STEP: watching for the Service to be added 07/29/23 16:04:37.306
Jul 29 16:04:37.309: INFO: Found Service test-service-cxdw9 in namespace services-3479 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Jul 29 16:04:37.309: INFO: Service test-service-cxdw9 created
STEP: Getting /status 07/29/23 16:04:37.309
Jul 29 16:04:37.319: INFO: Service test-service-cxdw9 has LoadBalancer: {[]}
STEP: patching the ServiceStatus 07/29/23 16:04:37.32
STEP: watching for the Service to be patched 07/29/23 16:04:37.336
Jul 29 16:04:37.339: INFO: observed Service test-service-cxdw9 in namespace services-3479 with annotations: map[] & LoadBalancer: {[]}
Jul 29 16:04:37.340: INFO: Found Service test-service-cxdw9 in namespace services-3479 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Jul 29 16:04:37.340: INFO: Service test-service-cxdw9 has service status patched
STEP: updating the ServiceStatus 07/29/23 16:04:37.34
Jul 29 16:04:37.357: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated 07/29/23 16:04:37.358
Jul 29 16:04:37.363: INFO: Observed Service test-service-cxdw9 in namespace services-3479 with annotations: map[] & Conditions: {[]}
Jul 29 16:04:37.363: INFO: Observed event: &Service{ObjectMeta:{test-service-cxdw9  services-3479  f7e622d9-2de0-41cb-85c8-6d729486560a 13196 0 2023-07-29 16:04:37 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-07-29 16:04:37 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-07-29 16:04:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.233.29.154,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.233.29.154],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Jul 29 16:04:37.364: INFO: Found Service test-service-cxdw9 in namespace services-3479 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jul 29 16:04:37.364: INFO: Service test-service-cxdw9 has service status updated
STEP: patching the service 07/29/23 16:04:37.364
STEP: watching for the Service to be patched 07/29/23 16:04:37.387
Jul 29 16:04:37.391: INFO: observed Service test-service-cxdw9 in namespace services-3479 with labels: map[test-service-static:true]
Jul 29 16:04:37.391: INFO: observed Service test-service-cxdw9 in namespace services-3479 with labels: map[test-service-static:true]
Jul 29 16:04:37.391: INFO: observed Service test-service-cxdw9 in namespace services-3479 with labels: map[test-service-static:true]
Jul 29 16:04:37.391: INFO: Found Service test-service-cxdw9 in namespace services-3479 with labels: map[test-service:patched test-service-static:true]
Jul 29 16:04:37.391: INFO: Service test-service-cxdw9 patched
STEP: deleting the service 07/29/23 16:04:37.391
STEP: watching for the Service to be deleted 07/29/23 16:04:37.42
Jul 29 16:04:37.423: INFO: Observed event: ADDED
Jul 29 16:04:37.423: INFO: Observed event: MODIFIED
Jul 29 16:04:37.424: INFO: Observed event: MODIFIED
Jul 29 16:04:37.424: INFO: Observed event: MODIFIED
Jul 29 16:04:37.424: INFO: Found Service test-service-cxdw9 in namespace services-3479 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Jul 29 16:04:37.424: INFO: Service test-service-cxdw9 deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jul 29 16:04:37.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3479" for this suite. 07/29/23 16:04:37.434
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should complete a service status lifecycle [Conformance]","completed":142,"skipped":2626,"failed":0}
------------------------------
â€¢ [0.221 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3394

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:04:37.227
    Jul 29 16:04:37.228: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename services 07/29/23 16:04:37.235
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:04:37.272
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:04:37.277
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should complete a service status lifecycle [Conformance]
      test/e2e/network/service.go:3394
    STEP: creating a Service 07/29/23 16:04:37.287
    STEP: watching for the Service to be added 07/29/23 16:04:37.306
    Jul 29 16:04:37.309: INFO: Found Service test-service-cxdw9 in namespace services-3479 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
    Jul 29 16:04:37.309: INFO: Service test-service-cxdw9 created
    STEP: Getting /status 07/29/23 16:04:37.309
    Jul 29 16:04:37.319: INFO: Service test-service-cxdw9 has LoadBalancer: {[]}
    STEP: patching the ServiceStatus 07/29/23 16:04:37.32
    STEP: watching for the Service to be patched 07/29/23 16:04:37.336
    Jul 29 16:04:37.339: INFO: observed Service test-service-cxdw9 in namespace services-3479 with annotations: map[] & LoadBalancer: {[]}
    Jul 29 16:04:37.340: INFO: Found Service test-service-cxdw9 in namespace services-3479 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
    Jul 29 16:04:37.340: INFO: Service test-service-cxdw9 has service status patched
    STEP: updating the ServiceStatus 07/29/23 16:04:37.34
    Jul 29 16:04:37.357: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Service to be updated 07/29/23 16:04:37.358
    Jul 29 16:04:37.363: INFO: Observed Service test-service-cxdw9 in namespace services-3479 with annotations: map[] & Conditions: {[]}
    Jul 29 16:04:37.363: INFO: Observed event: &Service{ObjectMeta:{test-service-cxdw9  services-3479  f7e622d9-2de0-41cb-85c8-6d729486560a 13196 0 2023-07-29 16:04:37 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-07-29 16:04:37 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-07-29 16:04:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.233.29.154,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.233.29.154],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
    Jul 29 16:04:37.364: INFO: Found Service test-service-cxdw9 in namespace services-3479 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jul 29 16:04:37.364: INFO: Service test-service-cxdw9 has service status updated
    STEP: patching the service 07/29/23 16:04:37.364
    STEP: watching for the Service to be patched 07/29/23 16:04:37.387
    Jul 29 16:04:37.391: INFO: observed Service test-service-cxdw9 in namespace services-3479 with labels: map[test-service-static:true]
    Jul 29 16:04:37.391: INFO: observed Service test-service-cxdw9 in namespace services-3479 with labels: map[test-service-static:true]
    Jul 29 16:04:37.391: INFO: observed Service test-service-cxdw9 in namespace services-3479 with labels: map[test-service-static:true]
    Jul 29 16:04:37.391: INFO: Found Service test-service-cxdw9 in namespace services-3479 with labels: map[test-service:patched test-service-static:true]
    Jul 29 16:04:37.391: INFO: Service test-service-cxdw9 patched
    STEP: deleting the service 07/29/23 16:04:37.391
    STEP: watching for the Service to be deleted 07/29/23 16:04:37.42
    Jul 29 16:04:37.423: INFO: Observed event: ADDED
    Jul 29 16:04:37.423: INFO: Observed event: MODIFIED
    Jul 29 16:04:37.424: INFO: Observed event: MODIFIED
    Jul 29 16:04:37.424: INFO: Observed event: MODIFIED
    Jul 29 16:04:37.424: INFO: Found Service test-service-cxdw9 in namespace services-3479 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
    Jul 29 16:04:37.424: INFO: Service test-service-cxdw9 deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jul 29 16:04:37.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-3479" for this suite. 07/29/23 16:04:37.434
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] ConfigMap
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:04:37.449
Jul 29 16:04:37.449: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename configmap 07/29/23 16:04:37.456
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:04:37.484
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:04:37.49
[It] should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137
STEP: Creating configMap that has name configmap-test-emptyKey-3bf0a029-a2c4-4100-a333-7ad8b63f6098 07/29/23 16:04:37.495
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Jul 29 16:04:37.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2582" for this suite. 07/29/23 16:04:37.509
{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","completed":143,"skipped":2629,"failed":0}
------------------------------
â€¢ [0.071 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:04:37.449
    Jul 29 16:04:37.449: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename configmap 07/29/23 16:04:37.456
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:04:37.484
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:04:37.49
    [It] should fail to create ConfigMap with empty key [Conformance]
      test/e2e/common/node/configmap.go:137
    STEP: Creating configMap that has name configmap-test-emptyKey-3bf0a029-a2c4-4100-a333-7ad8b63f6098 07/29/23 16:04:37.495
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Jul 29 16:04:37.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-2582" for this suite. 07/29/23 16:04:37.509
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:04:37.522
Jul 29 16:04:37.522: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename container-probe 07/29/23 16:04:37.527
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:04:37.561
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:04:37.57
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180
STEP: Creating pod liveness-321c0d7b-3e48-487c-942e-6479ed1c9116 in namespace container-probe-6678 07/29/23 16:04:37.579
Jul 29 16:04:37.624: INFO: Waiting up to 5m0s for pod "liveness-321c0d7b-3e48-487c-942e-6479ed1c9116" in namespace "container-probe-6678" to be "not pending"
Jul 29 16:04:37.649: INFO: Pod "liveness-321c0d7b-3e48-487c-942e-6479ed1c9116": Phase="Pending", Reason="", readiness=false. Elapsed: 24.583497ms
Jul 29 16:04:39.659: INFO: Pod "liveness-321c0d7b-3e48-487c-942e-6479ed1c9116": Phase="Running", Reason="", readiness=true. Elapsed: 2.034146165s
Jul 29 16:04:39.659: INFO: Pod "liveness-321c0d7b-3e48-487c-942e-6479ed1c9116" satisfied condition "not pending"
Jul 29 16:04:39.659: INFO: Started pod liveness-321c0d7b-3e48-487c-942e-6479ed1c9116 in namespace container-probe-6678
STEP: checking the pod's current state and verifying that restartCount is present 07/29/23 16:04:39.659
Jul 29 16:04:39.668: INFO: Initial restart count of pod liveness-321c0d7b-3e48-487c-942e-6479ed1c9116 is 0
STEP: deleting the pod 07/29/23 16:08:40.741
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jul 29 16:08:40.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6678" for this suite. 07/29/23 16:08:40.793
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","completed":144,"skipped":2639,"failed":0}
------------------------------
â€¢ [SLOW TEST] [243.285 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:04:37.522
    Jul 29 16:04:37.522: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename container-probe 07/29/23 16:04:37.527
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:04:37.561
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:04:37.57
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:180
    STEP: Creating pod liveness-321c0d7b-3e48-487c-942e-6479ed1c9116 in namespace container-probe-6678 07/29/23 16:04:37.579
    Jul 29 16:04:37.624: INFO: Waiting up to 5m0s for pod "liveness-321c0d7b-3e48-487c-942e-6479ed1c9116" in namespace "container-probe-6678" to be "not pending"
    Jul 29 16:04:37.649: INFO: Pod "liveness-321c0d7b-3e48-487c-942e-6479ed1c9116": Phase="Pending", Reason="", readiness=false. Elapsed: 24.583497ms
    Jul 29 16:04:39.659: INFO: Pod "liveness-321c0d7b-3e48-487c-942e-6479ed1c9116": Phase="Running", Reason="", readiness=true. Elapsed: 2.034146165s
    Jul 29 16:04:39.659: INFO: Pod "liveness-321c0d7b-3e48-487c-942e-6479ed1c9116" satisfied condition "not pending"
    Jul 29 16:04:39.659: INFO: Started pod liveness-321c0d7b-3e48-487c-942e-6479ed1c9116 in namespace container-probe-6678
    STEP: checking the pod's current state and verifying that restartCount is present 07/29/23 16:04:39.659
    Jul 29 16:04:39.668: INFO: Initial restart count of pod liveness-321c0d7b-3e48-487c-942e-6479ed1c9116 is 0
    STEP: deleting the pod 07/29/23 16:08:40.741
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jul 29 16:08:40.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-6678" for this suite. 07/29/23 16:08:40.793
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions
  should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:822
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:08:40.808
Jul 29 16:08:40.809: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename kubectl 07/29/23 16:08:40.811
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:08:40.854
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:08:40.86
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:822
STEP: validating api versions 07/29/23 16:08:40.865
Jul 29 16:08:40.866: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-8316 api-versions'
Jul 29 16:08:41.037: INFO: stderr: ""
Jul 29 16:08:41.037: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nautoscaling/v2beta2\nbatch/v1\ncertificates.k8s.io/v1\ncilium.io/v2\ncilium.io/v2alpha1\ncoordination.k8s.io/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta2\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jul 29 16:08:41.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8316" for this suite. 07/29/23 16:08:41.047
{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","completed":145,"skipped":2643,"failed":0}
------------------------------
â€¢ [0.254 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  test/e2e/kubectl/kubectl.go:816
    should check if v1 is in available api versions  [Conformance]
    test/e2e/kubectl/kubectl.go:822

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:08:40.808
    Jul 29 16:08:40.809: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename kubectl 07/29/23 16:08:40.811
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:08:40.854
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:08:40.86
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if v1 is in available api versions  [Conformance]
      test/e2e/kubectl/kubectl.go:822
    STEP: validating api versions 07/29/23 16:08:40.865
    Jul 29 16:08:40.866: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-8316 api-versions'
    Jul 29 16:08:41.037: INFO: stderr: ""
    Jul 29 16:08:41.037: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nautoscaling/v2beta2\nbatch/v1\ncertificates.k8s.io/v1\ncilium.io/v2\ncilium.io/v2alpha1\ncoordination.k8s.io/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta2\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jul 29 16:08:41.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-8316" for this suite. 07/29/23 16:08:41.047
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Lease
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[BeforeEach] [sig-node] Lease
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:08:41.065
Jul 29 16:08:41.065: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename lease-test 07/29/23 16:08:41.069
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:08:41.116
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:08:41.122
[It] lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[AfterEach] [sig-node] Lease
  test/e2e/framework/framework.go:187
Jul 29 16:08:41.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-5442" for this suite. 07/29/23 16:08:41.28
{"msg":"PASSED [sig-node] Lease lease API should be available [Conformance]","completed":146,"skipped":2661,"failed":0}
------------------------------
â€¢ [0.233 seconds]
[sig-node] Lease
test/e2e/common/node/framework.go:23
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Lease
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:08:41.065
    Jul 29 16:08:41.065: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename lease-test 07/29/23 16:08:41.069
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:08:41.116
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:08:41.122
    [It] lease API should be available [Conformance]
      test/e2e/common/node/lease.go:72
    [AfterEach] [sig-node] Lease
      test/e2e/framework/framework.go:187
    Jul 29 16:08:41.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "lease-test-5442" for this suite. 07/29/23 16:08:41.28
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:08:41.308
Jul 29 16:08:41.309: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename emptydir 07/29/23 16:08:41.311
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:08:41.34
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:08:41.348
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176
STEP: Creating a pod to test emptydir 0666 on node default medium 07/29/23 16:08:41.352
Jul 29 16:08:41.368: INFO: Waiting up to 5m0s for pod "pod-5ed23708-4ba2-45a3-9c6d-3dcc4aee8706" in namespace "emptydir-9151" to be "Succeeded or Failed"
Jul 29 16:08:41.373: INFO: Pod "pod-5ed23708-4ba2-45a3-9c6d-3dcc4aee8706": Phase="Pending", Reason="", readiness=false. Elapsed: 5.716869ms
Jul 29 16:08:43.380: INFO: Pod "pod-5ed23708-4ba2-45a3-9c6d-3dcc4aee8706": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011875188s
Jul 29 16:08:45.382: INFO: Pod "pod-5ed23708-4ba2-45a3-9c6d-3dcc4aee8706": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01388804s
STEP: Saw pod success 07/29/23 16:08:45.382
Jul 29 16:08:45.382: INFO: Pod "pod-5ed23708-4ba2-45a3-9c6d-3dcc4aee8706" satisfied condition "Succeeded or Failed"
Jul 29 16:08:45.387: INFO: Trying to get logs from node wa4quivohpee-3 pod pod-5ed23708-4ba2-45a3-9c6d-3dcc4aee8706 container test-container: <nil>
STEP: delete the pod 07/29/23 16:08:45.42
Jul 29 16:08:45.441: INFO: Waiting for pod pod-5ed23708-4ba2-45a3-9c6d-3dcc4aee8706 to disappear
Jul 29 16:08:45.446: INFO: Pod pod-5ed23708-4ba2-45a3-9c6d-3dcc4aee8706 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jul 29 16:08:45.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9151" for this suite. 07/29/23 16:08:45.455
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":147,"skipped":2691,"failed":0}
------------------------------
â€¢ [4.158 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:08:41.308
    Jul 29 16:08:41.309: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename emptydir 07/29/23 16:08:41.311
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:08:41.34
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:08:41.348
    [It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:176
    STEP: Creating a pod to test emptydir 0666 on node default medium 07/29/23 16:08:41.352
    Jul 29 16:08:41.368: INFO: Waiting up to 5m0s for pod "pod-5ed23708-4ba2-45a3-9c6d-3dcc4aee8706" in namespace "emptydir-9151" to be "Succeeded or Failed"
    Jul 29 16:08:41.373: INFO: Pod "pod-5ed23708-4ba2-45a3-9c6d-3dcc4aee8706": Phase="Pending", Reason="", readiness=false. Elapsed: 5.716869ms
    Jul 29 16:08:43.380: INFO: Pod "pod-5ed23708-4ba2-45a3-9c6d-3dcc4aee8706": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011875188s
    Jul 29 16:08:45.382: INFO: Pod "pod-5ed23708-4ba2-45a3-9c6d-3dcc4aee8706": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01388804s
    STEP: Saw pod success 07/29/23 16:08:45.382
    Jul 29 16:08:45.382: INFO: Pod "pod-5ed23708-4ba2-45a3-9c6d-3dcc4aee8706" satisfied condition "Succeeded or Failed"
    Jul 29 16:08:45.387: INFO: Trying to get logs from node wa4quivohpee-3 pod pod-5ed23708-4ba2-45a3-9c6d-3dcc4aee8706 container test-container: <nil>
    STEP: delete the pod 07/29/23 16:08:45.42
    Jul 29 16:08:45.441: INFO: Waiting for pod pod-5ed23708-4ba2-45a3-9c6d-3dcc4aee8706 to disappear
    Jul 29 16:08:45.446: INFO: Pod pod-5ed23708-4ba2-45a3-9c6d-3dcc4aee8706 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jul 29 16:08:45.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-9151" for this suite. 07/29/23 16:08:45.455
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:08:45.471
Jul 29 16:08:45.471: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename job 07/29/23 16:08:45.473
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:08:45.502
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:08:45.507
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254
STEP: Creating a job 07/29/23 16:08:45.512
STEP: Ensuring job reaches completions 07/29/23 16:08:45.523
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Jul 29 16:08:57.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-9571" for this suite. 07/29/23 16:08:57.541
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","completed":148,"skipped":2707,"failed":0}
------------------------------
â€¢ [SLOW TEST] [12.084 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:08:45.471
    Jul 29 16:08:45.471: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename job 07/29/23 16:08:45.473
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:08:45.502
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:08:45.507
    [It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
      test/e2e/apps/job.go:254
    STEP: Creating a job 07/29/23 16:08:45.512
    STEP: Ensuring job reaches completions 07/29/23 16:08:45.523
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Jul 29 16:08:57.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-9571" for this suite. 07/29/23 16:08:57.541
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:08:57.561
Jul 29 16:08:57.561: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename crd-publish-openapi 07/29/23 16:08:57.564
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:08:57.595
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:08:57.6
[It] works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 07/29/23 16:08:57.605
Jul 29 16:08:57.608: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
Jul 29 16:09:01.697: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jul 29 16:09:22.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8946" for this suite. 07/29/23 16:09:22.028
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","completed":149,"skipped":2714,"failed":0}
------------------------------
â€¢ [SLOW TEST] [24.477 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:08:57.561
    Jul 29 16:08:57.561: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename crd-publish-openapi 07/29/23 16:08:57.564
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:08:57.595
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:08:57.6
    [It] works for multiple CRDs of different groups [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:275
    STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 07/29/23 16:08:57.605
    Jul 29 16:08:57.608: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    Jul 29 16:09:01.697: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jul 29 16:09:22.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-8946" for this suite. 07/29/23 16:09:22.028
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Pods
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:09:22.04
Jul 29 16:09:22.040: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename pods 07/29/23 16:09:22.043
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:09:22.071
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:09:22.076
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397
STEP: creating the pod 07/29/23 16:09:22.081
STEP: submitting the pod to kubernetes 07/29/23 16:09:22.081
Jul 29 16:09:22.095: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-cef710c9-be1f-47bd-aece-fcf00f4cbb25" in namespace "pods-6997" to be "running and ready"
Jul 29 16:09:22.101: INFO: Pod "pod-update-activedeadlineseconds-cef710c9-be1f-47bd-aece-fcf00f4cbb25": Phase="Pending", Reason="", readiness=false. Elapsed: 6.352112ms
Jul 29 16:09:22.102: INFO: The phase of Pod pod-update-activedeadlineseconds-cef710c9-be1f-47bd-aece-fcf00f4cbb25 is Pending, waiting for it to be Running (with Ready = true)
Jul 29 16:09:24.111: INFO: Pod "pod-update-activedeadlineseconds-cef710c9-be1f-47bd-aece-fcf00f4cbb25": Phase="Running", Reason="", readiness=true. Elapsed: 2.015899046s
Jul 29 16:09:24.111: INFO: The phase of Pod pod-update-activedeadlineseconds-cef710c9-be1f-47bd-aece-fcf00f4cbb25 is Running (Ready = true)
Jul 29 16:09:24.111: INFO: Pod "pod-update-activedeadlineseconds-cef710c9-be1f-47bd-aece-fcf00f4cbb25" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 07/29/23 16:09:24.122
STEP: updating the pod 07/29/23 16:09:24.13
Jul 29 16:09:24.651: INFO: Successfully updated pod "pod-update-activedeadlineseconds-cef710c9-be1f-47bd-aece-fcf00f4cbb25"
Jul 29 16:09:24.652: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-cef710c9-be1f-47bd-aece-fcf00f4cbb25" in namespace "pods-6997" to be "terminated with reason DeadlineExceeded"
Jul 29 16:09:24.657: INFO: Pod "pod-update-activedeadlineseconds-cef710c9-be1f-47bd-aece-fcf00f4cbb25": Phase="Running", Reason="", readiness=true. Elapsed: 5.62649ms
Jul 29 16:09:26.665: INFO: Pod "pod-update-activedeadlineseconds-cef710c9-be1f-47bd-aece-fcf00f4cbb25": Phase="Running", Reason="", readiness=true. Elapsed: 2.013383816s
Jul 29 16:09:28.667: INFO: Pod "pod-update-activedeadlineseconds-cef710c9-be1f-47bd-aece-fcf00f4cbb25": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.015054023s
Jul 29 16:09:28.667: INFO: Pod "pod-update-activedeadlineseconds-cef710c9-be1f-47bd-aece-fcf00f4cbb25" satisfied condition "terminated with reason DeadlineExceeded"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jul 29 16:09:28.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6997" for this suite. 07/29/23 16:09:28.677
{"msg":"PASSED [sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","completed":150,"skipped":2718,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.652 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:09:22.04
    Jul 29 16:09:22.040: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename pods 07/29/23 16:09:22.043
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:09:22.071
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:09:22.076
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:397
    STEP: creating the pod 07/29/23 16:09:22.081
    STEP: submitting the pod to kubernetes 07/29/23 16:09:22.081
    Jul 29 16:09:22.095: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-cef710c9-be1f-47bd-aece-fcf00f4cbb25" in namespace "pods-6997" to be "running and ready"
    Jul 29 16:09:22.101: INFO: Pod "pod-update-activedeadlineseconds-cef710c9-be1f-47bd-aece-fcf00f4cbb25": Phase="Pending", Reason="", readiness=false. Elapsed: 6.352112ms
    Jul 29 16:09:22.102: INFO: The phase of Pod pod-update-activedeadlineseconds-cef710c9-be1f-47bd-aece-fcf00f4cbb25 is Pending, waiting for it to be Running (with Ready = true)
    Jul 29 16:09:24.111: INFO: Pod "pod-update-activedeadlineseconds-cef710c9-be1f-47bd-aece-fcf00f4cbb25": Phase="Running", Reason="", readiness=true. Elapsed: 2.015899046s
    Jul 29 16:09:24.111: INFO: The phase of Pod pod-update-activedeadlineseconds-cef710c9-be1f-47bd-aece-fcf00f4cbb25 is Running (Ready = true)
    Jul 29 16:09:24.111: INFO: Pod "pod-update-activedeadlineseconds-cef710c9-be1f-47bd-aece-fcf00f4cbb25" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 07/29/23 16:09:24.122
    STEP: updating the pod 07/29/23 16:09:24.13
    Jul 29 16:09:24.651: INFO: Successfully updated pod "pod-update-activedeadlineseconds-cef710c9-be1f-47bd-aece-fcf00f4cbb25"
    Jul 29 16:09:24.652: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-cef710c9-be1f-47bd-aece-fcf00f4cbb25" in namespace "pods-6997" to be "terminated with reason DeadlineExceeded"
    Jul 29 16:09:24.657: INFO: Pod "pod-update-activedeadlineseconds-cef710c9-be1f-47bd-aece-fcf00f4cbb25": Phase="Running", Reason="", readiness=true. Elapsed: 5.62649ms
    Jul 29 16:09:26.665: INFO: Pod "pod-update-activedeadlineseconds-cef710c9-be1f-47bd-aece-fcf00f4cbb25": Phase="Running", Reason="", readiness=true. Elapsed: 2.013383816s
    Jul 29 16:09:28.667: INFO: Pod "pod-update-activedeadlineseconds-cef710c9-be1f-47bd-aece-fcf00f4cbb25": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.015054023s
    Jul 29 16:09:28.667: INFO: Pod "pod-update-activedeadlineseconds-cef710c9-be1f-47bd-aece-fcf00f4cbb25" satisfied condition "terminated with reason DeadlineExceeded"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jul 29 16:09:28.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-6997" for this suite. 07/29/23 16:09:28.677
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Downward API
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:09:28.702
Jul 29 16:09:28.702: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename downward-api 07/29/23 16:09:28.704
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:09:28.73
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:09:28.734
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165
STEP: Creating a pod to test downward api env vars 07/29/23 16:09:28.741
Jul 29 16:09:28.755: INFO: Waiting up to 5m0s for pod "downward-api-32a7c9eb-4370-4653-8753-26cebffe155d" in namespace "downward-api-1569" to be "Succeeded or Failed"
Jul 29 16:09:28.766: INFO: Pod "downward-api-32a7c9eb-4370-4653-8753-26cebffe155d": Phase="Pending", Reason="", readiness=false. Elapsed: 10.617507ms
Jul 29 16:09:30.782: INFO: Pod "downward-api-32a7c9eb-4370-4653-8753-26cebffe155d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026622058s
Jul 29 16:09:32.775: INFO: Pod "downward-api-32a7c9eb-4370-4653-8753-26cebffe155d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019778663s
STEP: Saw pod success 07/29/23 16:09:32.775
Jul 29 16:09:32.776: INFO: Pod "downward-api-32a7c9eb-4370-4653-8753-26cebffe155d" satisfied condition "Succeeded or Failed"
Jul 29 16:09:32.781: INFO: Trying to get logs from node wa4quivohpee-3 pod downward-api-32a7c9eb-4370-4653-8753-26cebffe155d container dapi-container: <nil>
STEP: delete the pod 07/29/23 16:09:32.809
Jul 29 16:09:32.826: INFO: Waiting for pod downward-api-32a7c9eb-4370-4653-8753-26cebffe155d to disappear
Jul 29 16:09:32.831: INFO: Pod downward-api-32a7c9eb-4370-4653-8753-26cebffe155d no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Jul 29 16:09:32.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1569" for this suite. 07/29/23 16:09:32.844
{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","completed":151,"skipped":2726,"failed":0}
------------------------------
â€¢ [4.153 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:09:28.702
    Jul 29 16:09:28.702: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename downward-api 07/29/23 16:09:28.704
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:09:28.73
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:09:28.734
    [It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:165
    STEP: Creating a pod to test downward api env vars 07/29/23 16:09:28.741
    Jul 29 16:09:28.755: INFO: Waiting up to 5m0s for pod "downward-api-32a7c9eb-4370-4653-8753-26cebffe155d" in namespace "downward-api-1569" to be "Succeeded or Failed"
    Jul 29 16:09:28.766: INFO: Pod "downward-api-32a7c9eb-4370-4653-8753-26cebffe155d": Phase="Pending", Reason="", readiness=false. Elapsed: 10.617507ms
    Jul 29 16:09:30.782: INFO: Pod "downward-api-32a7c9eb-4370-4653-8753-26cebffe155d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026622058s
    Jul 29 16:09:32.775: INFO: Pod "downward-api-32a7c9eb-4370-4653-8753-26cebffe155d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019778663s
    STEP: Saw pod success 07/29/23 16:09:32.775
    Jul 29 16:09:32.776: INFO: Pod "downward-api-32a7c9eb-4370-4653-8753-26cebffe155d" satisfied condition "Succeeded or Failed"
    Jul 29 16:09:32.781: INFO: Trying to get logs from node wa4quivohpee-3 pod downward-api-32a7c9eb-4370-4653-8753-26cebffe155d container dapi-container: <nil>
    STEP: delete the pod 07/29/23 16:09:32.809
    Jul 29 16:09:32.826: INFO: Waiting for pod downward-api-32a7c9eb-4370-4653-8753-26cebffe155d to disappear
    Jul 29 16:09:32.831: INFO: Pod downward-api-32a7c9eb-4370-4653-8753-26cebffe155d no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Jul 29 16:09:32.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-1569" for this suite. 07/29/23 16:09:32.844
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:09:32.884
Jul 29 16:09:32.884: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename daemonsets 07/29/23 16:09:32.885
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:09:32.914
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:09:32.918
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373
Jul 29 16:09:32.955: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster. 07/29/23 16:09:32.966
Jul 29 16:09:32.982: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jul 29 16:09:32.982: INFO: Node wa4quivohpee-1 is running 0 daemon pod, expected 1
Jul 29 16:09:34.029: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jul 29 16:09:34.029: INFO: Node wa4quivohpee-1 is running 0 daemon pod, expected 1
Jul 29 16:09:35.023: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jul 29 16:09:35.023: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Update daemon pods image. 07/29/23 16:09:35.051
STEP: Check that daemon pods images are updated. 07/29/23 16:09:35.074
Jul 29 16:09:35.102: INFO: Wrong image for pod: daemon-set-7bq5p. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jul 29 16:09:35.102: INFO: Wrong image for pod: daemon-set-shfm2. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jul 29 16:09:36.133: INFO: Wrong image for pod: daemon-set-7bq5p. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jul 29 16:09:36.134: INFO: Wrong image for pod: daemon-set-shfm2. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jul 29 16:09:37.151: INFO: Wrong image for pod: daemon-set-7bq5p. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jul 29 16:09:37.152: INFO: Wrong image for pod: daemon-set-shfm2. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jul 29 16:09:38.126: INFO: Pod daemon-set-2lbdq is not available
Jul 29 16:09:38.126: INFO: Wrong image for pod: daemon-set-7bq5p. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jul 29 16:09:38.126: INFO: Wrong image for pod: daemon-set-shfm2. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jul 29 16:09:39.147: INFO: Wrong image for pod: daemon-set-7bq5p. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jul 29 16:09:39.147: INFO: Pod daemon-set-fp27m is not available
Jul 29 16:09:40.123: INFO: Wrong image for pod: daemon-set-7bq5p. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jul 29 16:09:40.123: INFO: Pod daemon-set-fp27m is not available
Jul 29 16:09:42.123: INFO: Pod daemon-set-sdfkq is not available
STEP: Check that daemon pods are still running on every node of the cluster. 07/29/23 16:09:42.13
Jul 29 16:09:42.150: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jul 29 16:09:42.150: INFO: Node wa4quivohpee-3 is running 0 daemon pod, expected 1
Jul 29 16:09:43.166: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jul 29 16:09:43.167: INFO: Node wa4quivohpee-3 is running 0 daemon pod, expected 1
Jul 29 16:09:44.167: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jul 29 16:09:44.167: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 07/29/23 16:09:44.203
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1479, will wait for the garbage collector to delete the pods 07/29/23 16:09:44.203
Jul 29 16:09:44.270: INFO: Deleting DaemonSet.extensions daemon-set took: 10.783981ms
Jul 29 16:09:44.471: INFO: Terminating DaemonSet.extensions daemon-set pods took: 201.184934ms
Jul 29 16:09:46.580: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jul 29 16:09:46.581: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jul 29 16:09:46.587: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"14341"},"items":null}

Jul 29 16:09:46.594: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"14341"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Jul 29 16:09:46.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1479" for this suite. 07/29/23 16:09:46.634
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","completed":152,"skipped":2800,"failed":0}
------------------------------
â€¢ [SLOW TEST] [13.760 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:09:32.884
    Jul 29 16:09:32.884: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename daemonsets 07/29/23 16:09:32.885
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:09:32.914
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:09:32.918
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
      test/e2e/apps/daemon_set.go:373
    Jul 29 16:09:32.955: INFO: Creating simple daemon set daemon-set
    STEP: Check that daemon pods launch on every node of the cluster. 07/29/23 16:09:32.966
    Jul 29 16:09:32.982: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jul 29 16:09:32.982: INFO: Node wa4quivohpee-1 is running 0 daemon pod, expected 1
    Jul 29 16:09:34.029: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jul 29 16:09:34.029: INFO: Node wa4quivohpee-1 is running 0 daemon pod, expected 1
    Jul 29 16:09:35.023: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jul 29 16:09:35.023: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Update daemon pods image. 07/29/23 16:09:35.051
    STEP: Check that daemon pods images are updated. 07/29/23 16:09:35.074
    Jul 29 16:09:35.102: INFO: Wrong image for pod: daemon-set-7bq5p. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jul 29 16:09:35.102: INFO: Wrong image for pod: daemon-set-shfm2. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jul 29 16:09:36.133: INFO: Wrong image for pod: daemon-set-7bq5p. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jul 29 16:09:36.134: INFO: Wrong image for pod: daemon-set-shfm2. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jul 29 16:09:37.151: INFO: Wrong image for pod: daemon-set-7bq5p. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jul 29 16:09:37.152: INFO: Wrong image for pod: daemon-set-shfm2. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jul 29 16:09:38.126: INFO: Pod daemon-set-2lbdq is not available
    Jul 29 16:09:38.126: INFO: Wrong image for pod: daemon-set-7bq5p. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jul 29 16:09:38.126: INFO: Wrong image for pod: daemon-set-shfm2. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jul 29 16:09:39.147: INFO: Wrong image for pod: daemon-set-7bq5p. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jul 29 16:09:39.147: INFO: Pod daemon-set-fp27m is not available
    Jul 29 16:09:40.123: INFO: Wrong image for pod: daemon-set-7bq5p. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jul 29 16:09:40.123: INFO: Pod daemon-set-fp27m is not available
    Jul 29 16:09:42.123: INFO: Pod daemon-set-sdfkq is not available
    STEP: Check that daemon pods are still running on every node of the cluster. 07/29/23 16:09:42.13
    Jul 29 16:09:42.150: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jul 29 16:09:42.150: INFO: Node wa4quivohpee-3 is running 0 daemon pod, expected 1
    Jul 29 16:09:43.166: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jul 29 16:09:43.167: INFO: Node wa4quivohpee-3 is running 0 daemon pod, expected 1
    Jul 29 16:09:44.167: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jul 29 16:09:44.167: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 07/29/23 16:09:44.203
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1479, will wait for the garbage collector to delete the pods 07/29/23 16:09:44.203
    Jul 29 16:09:44.270: INFO: Deleting DaemonSet.extensions daemon-set took: 10.783981ms
    Jul 29 16:09:44.471: INFO: Terminating DaemonSet.extensions daemon-set pods took: 201.184934ms
    Jul 29 16:09:46.580: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jul 29 16:09:46.581: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jul 29 16:09:46.587: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"14341"},"items":null}

    Jul 29 16:09:46.594: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"14341"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Jul 29 16:09:46.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-1479" for this suite. 07/29/23 16:09:46.634
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] HostPort
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
[BeforeEach] [sig-network] HostPort
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:09:46.651
Jul 29 16:09:46.651: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename hostport 07/29/23 16:09:46.653
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:09:46.705
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:09:46.715
[BeforeEach] [sig-network] HostPort
  test/e2e/network/hostport.go:49
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 07/29/23 16:09:46.725
Jul 29 16:09:46.753: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-888" to be "running and ready"
Jul 29 16:09:46.759: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.999156ms
Jul 29 16:09:46.759: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jul 29 16:09:48.767: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.014031158s
Jul 29 16:09:48.767: INFO: The phase of Pod pod1 is Running (Ready = true)
Jul 29 16:09:48.767: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 192.168.121.234 on the node which pod1 resides and expect scheduled 07/29/23 16:09:48.767
Jul 29 16:09:48.781: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-888" to be "running and ready"
Jul 29 16:09:48.787: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.822129ms
Jul 29 16:09:48.787: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jul 29 16:09:50.795: INFO: Pod "pod2": Phase="Running", Reason="", readiness=false. Elapsed: 2.014843529s
Jul 29 16:09:50.796: INFO: The phase of Pod pod2 is Running (Ready = false)
Jul 29 16:09:52.800: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.018955616s
Jul 29 16:09:52.800: INFO: The phase of Pod pod2 is Running (Ready = true)
Jul 29 16:09:52.800: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 192.168.121.234 but use UDP protocol on the node which pod2 resides 07/29/23 16:09:52.8
Jul 29 16:09:52.812: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-888" to be "running and ready"
Jul 29 16:09:52.821: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 9.324459ms
Jul 29 16:09:52.821: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Jul 29 16:09:54.834: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.02240153s
Jul 29 16:09:54.835: INFO: The phase of Pod pod3 is Running (Ready = true)
Jul 29 16:09:54.835: INFO: Pod "pod3" satisfied condition "running and ready"
Jul 29 16:09:54.851: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-888" to be "running and ready"
Jul 29 16:09:54.857: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 6.580893ms
Jul 29 16:09:54.857: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Jul 29 16:09:56.866: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.014928663s
Jul 29 16:09:56.866: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
Jul 29 16:09:56.866: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 07/29/23 16:09:56.871
Jul 29 16:09:56.871: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 192.168.121.234 http://127.0.0.1:54323/hostname] Namespace:hostport-888 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 29 16:09:56.871: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
Jul 29 16:09:56.875: INFO: ExecWithOptions: Clientset creation
Jul 29 16:09:56.876: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-888/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+192.168.121.234+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.121.234, port: 54323 07/29/23 16:09:57.048
Jul 29 16:09:57.049: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://192.168.121.234:54323/hostname] Namespace:hostport-888 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 29 16:09:57.049: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
Jul 29 16:09:57.051: INFO: ExecWithOptions: Clientset creation
Jul 29 16:09:57.051: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-888/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F192.168.121.234%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.121.234, port: 54323 UDP 07/29/23 16:09:57.168
Jul 29 16:09:57.169: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 192.168.121.234 54323] Namespace:hostport-888 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 29 16:09:57.169: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
Jul 29 16:09:57.171: INFO: ExecWithOptions: Clientset creation
Jul 29 16:09:57.172: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-888/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+192.168.121.234+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
[AfterEach] [sig-network] HostPort
  test/e2e/framework/framework.go:187
Jul 29 16:10:02.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostport-888" for this suite. 07/29/23 16:10:02.286
{"msg":"PASSED [sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]","completed":153,"skipped":2833,"failed":0}
------------------------------
â€¢ [SLOW TEST] [15.650 seconds]
[sig-network] HostPort
test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] HostPort
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:09:46.651
    Jul 29 16:09:46.651: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename hostport 07/29/23 16:09:46.653
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:09:46.705
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:09:46.715
    [BeforeEach] [sig-network] HostPort
      test/e2e/network/hostport.go:49
    [It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
      test/e2e/network/hostport.go:63
    STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 07/29/23 16:09:46.725
    Jul 29 16:09:46.753: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-888" to be "running and ready"
    Jul 29 16:09:46.759: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.999156ms
    Jul 29 16:09:46.759: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jul 29 16:09:48.767: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.014031158s
    Jul 29 16:09:48.767: INFO: The phase of Pod pod1 is Running (Ready = true)
    Jul 29 16:09:48.767: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 192.168.121.234 on the node which pod1 resides and expect scheduled 07/29/23 16:09:48.767
    Jul 29 16:09:48.781: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-888" to be "running and ready"
    Jul 29 16:09:48.787: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.822129ms
    Jul 29 16:09:48.787: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jul 29 16:09:50.795: INFO: Pod "pod2": Phase="Running", Reason="", readiness=false. Elapsed: 2.014843529s
    Jul 29 16:09:50.796: INFO: The phase of Pod pod2 is Running (Ready = false)
    Jul 29 16:09:52.800: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.018955616s
    Jul 29 16:09:52.800: INFO: The phase of Pod pod2 is Running (Ready = true)
    Jul 29 16:09:52.800: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 192.168.121.234 but use UDP protocol on the node which pod2 resides 07/29/23 16:09:52.8
    Jul 29 16:09:52.812: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-888" to be "running and ready"
    Jul 29 16:09:52.821: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 9.324459ms
    Jul 29 16:09:52.821: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
    Jul 29 16:09:54.834: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.02240153s
    Jul 29 16:09:54.835: INFO: The phase of Pod pod3 is Running (Ready = true)
    Jul 29 16:09:54.835: INFO: Pod "pod3" satisfied condition "running and ready"
    Jul 29 16:09:54.851: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-888" to be "running and ready"
    Jul 29 16:09:54.857: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 6.580893ms
    Jul 29 16:09:54.857: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
    Jul 29 16:09:56.866: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.014928663s
    Jul 29 16:09:56.866: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
    Jul 29 16:09:56.866: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 07/29/23 16:09:56.871
    Jul 29 16:09:56.871: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 192.168.121.234 http://127.0.0.1:54323/hostname] Namespace:hostport-888 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jul 29 16:09:56.871: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    Jul 29 16:09:56.875: INFO: ExecWithOptions: Clientset creation
    Jul 29 16:09:56.876: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-888/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+192.168.121.234+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.121.234, port: 54323 07/29/23 16:09:57.048
    Jul 29 16:09:57.049: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://192.168.121.234:54323/hostname] Namespace:hostport-888 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jul 29 16:09:57.049: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    Jul 29 16:09:57.051: INFO: ExecWithOptions: Clientset creation
    Jul 29 16:09:57.051: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-888/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F192.168.121.234%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.121.234, port: 54323 UDP 07/29/23 16:09:57.168
    Jul 29 16:09:57.169: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 192.168.121.234 54323] Namespace:hostport-888 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jul 29 16:09:57.169: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    Jul 29 16:09:57.171: INFO: ExecWithOptions: Clientset creation
    Jul 29 16:09:57.172: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-888/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+192.168.121.234+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    [AfterEach] [sig-network] HostPort
      test/e2e/framework/framework.go:187
    Jul 29 16:10:02.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "hostport-888" for this suite. 07/29/23 16:10:02.286
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:10:02.311
Jul 29 16:10:02.311: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename downward-api 07/29/23 16:10:02.313
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:10:02.346
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:10:02.35
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89
STEP: Creating a pod to test downward api env vars 07/29/23 16:10:02.355
Jul 29 16:10:02.369: INFO: Waiting up to 5m0s for pod "downward-api-78f4c5cc-0bae-4eae-b16f-5ac5364d7914" in namespace "downward-api-6755" to be "Succeeded or Failed"
Jul 29 16:10:02.374: INFO: Pod "downward-api-78f4c5cc-0bae-4eae-b16f-5ac5364d7914": Phase="Pending", Reason="", readiness=false. Elapsed: 4.700355ms
Jul 29 16:10:04.387: INFO: Pod "downward-api-78f4c5cc-0bae-4eae-b16f-5ac5364d7914": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017228619s
Jul 29 16:10:06.383: INFO: Pod "downward-api-78f4c5cc-0bae-4eae-b16f-5ac5364d7914": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013865946s
Jul 29 16:10:08.382: INFO: Pod "downward-api-78f4c5cc-0bae-4eae-b16f-5ac5364d7914": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012128071s
STEP: Saw pod success 07/29/23 16:10:08.382
Jul 29 16:10:08.382: INFO: Pod "downward-api-78f4c5cc-0bae-4eae-b16f-5ac5364d7914" satisfied condition "Succeeded or Failed"
Jul 29 16:10:08.388: INFO: Trying to get logs from node wa4quivohpee-1 pod downward-api-78f4c5cc-0bae-4eae-b16f-5ac5364d7914 container dapi-container: <nil>
STEP: delete the pod 07/29/23 16:10:08.426
Jul 29 16:10:08.452: INFO: Waiting for pod downward-api-78f4c5cc-0bae-4eae-b16f-5ac5364d7914 to disappear
Jul 29 16:10:08.463: INFO: Pod downward-api-78f4c5cc-0bae-4eae-b16f-5ac5364d7914 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Jul 29 16:10:08.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6755" for this suite. 07/29/23 16:10:08.471
{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","completed":154,"skipped":2859,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.203 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:10:02.311
    Jul 29 16:10:02.311: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename downward-api 07/29/23 16:10:02.313
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:10:02.346
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:10:02.35
    [It] should provide host IP as an env var [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:89
    STEP: Creating a pod to test downward api env vars 07/29/23 16:10:02.355
    Jul 29 16:10:02.369: INFO: Waiting up to 5m0s for pod "downward-api-78f4c5cc-0bae-4eae-b16f-5ac5364d7914" in namespace "downward-api-6755" to be "Succeeded or Failed"
    Jul 29 16:10:02.374: INFO: Pod "downward-api-78f4c5cc-0bae-4eae-b16f-5ac5364d7914": Phase="Pending", Reason="", readiness=false. Elapsed: 4.700355ms
    Jul 29 16:10:04.387: INFO: Pod "downward-api-78f4c5cc-0bae-4eae-b16f-5ac5364d7914": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017228619s
    Jul 29 16:10:06.383: INFO: Pod "downward-api-78f4c5cc-0bae-4eae-b16f-5ac5364d7914": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013865946s
    Jul 29 16:10:08.382: INFO: Pod "downward-api-78f4c5cc-0bae-4eae-b16f-5ac5364d7914": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012128071s
    STEP: Saw pod success 07/29/23 16:10:08.382
    Jul 29 16:10:08.382: INFO: Pod "downward-api-78f4c5cc-0bae-4eae-b16f-5ac5364d7914" satisfied condition "Succeeded or Failed"
    Jul 29 16:10:08.388: INFO: Trying to get logs from node wa4quivohpee-1 pod downward-api-78f4c5cc-0bae-4eae-b16f-5ac5364d7914 container dapi-container: <nil>
    STEP: delete the pod 07/29/23 16:10:08.426
    Jul 29 16:10:08.452: INFO: Waiting for pod downward-api-78f4c5cc-0bae-4eae-b16f-5ac5364d7914 to disappear
    Jul 29 16:10:08.463: INFO: Pod downward-api-78f4c5cc-0bae-4eae-b16f-5ac5364d7914 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Jul 29 16:10:08.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-6755" for this suite. 07/29/23 16:10:08.471
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ControllerRevision [Serial]
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:10:08.523
Jul 29 16:10:08.524: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename controllerrevisions 07/29/23 16:10:08.526
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:10:08.553
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:10:08.557
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:93
[It] should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
STEP: Creating DaemonSet "e2e-8lx96-daemon-set" 07/29/23 16:10:08.645
STEP: Check that daemon pods launch on every node of the cluster. 07/29/23 16:10:08.672
Jul 29 16:10:08.692: INFO: Number of nodes with available pods controlled by daemonset e2e-8lx96-daemon-set: 0
Jul 29 16:10:08.692: INFO: Node wa4quivohpee-1 is running 0 daemon pod, expected 1
Jul 29 16:10:09.712: INFO: Number of nodes with available pods controlled by daemonset e2e-8lx96-daemon-set: 0
Jul 29 16:10:09.712: INFO: Node wa4quivohpee-1 is running 0 daemon pod, expected 1
Jul 29 16:10:10.707: INFO: Number of nodes with available pods controlled by daemonset e2e-8lx96-daemon-set: 3
Jul 29 16:10:10.707: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-8lx96-daemon-set
STEP: Confirm DaemonSet "e2e-8lx96-daemon-set" successfully created with "daemonset-name=e2e-8lx96-daemon-set" label 07/29/23 16:10:10.713
STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-8lx96-daemon-set" 07/29/23 16:10:10.725
Jul 29 16:10:10.731: INFO: Located ControllerRevision: "e2e-8lx96-daemon-set-58ddc6479b"
STEP: Patching ControllerRevision "e2e-8lx96-daemon-set-58ddc6479b" 07/29/23 16:10:10.736
Jul 29 16:10:10.746: INFO: e2e-8lx96-daemon-set-58ddc6479b has been patched
STEP: Create a new ControllerRevision 07/29/23 16:10:10.746
Jul 29 16:10:10.755: INFO: Created ControllerRevision: e2e-8lx96-daemon-set-84cb6c44f4
STEP: Confirm that there are two ControllerRevisions 07/29/23 16:10:10.756
Jul 29 16:10:10.756: INFO: Requesting list of ControllerRevisions to confirm quantity
Jul 29 16:10:10.762: INFO: Found 2 ControllerRevisions
STEP: Deleting ControllerRevision "e2e-8lx96-daemon-set-58ddc6479b" 07/29/23 16:10:10.762
STEP: Confirm that there is only one ControllerRevision 07/29/23 16:10:10.771
Jul 29 16:10:10.772: INFO: Requesting list of ControllerRevisions to confirm quantity
Jul 29 16:10:10.776: INFO: Found 1 ControllerRevisions
STEP: Updating ControllerRevision "e2e-8lx96-daemon-set-84cb6c44f4" 07/29/23 16:10:10.782
Jul 29 16:10:10.797: INFO: e2e-8lx96-daemon-set-84cb6c44f4 has been updated
STEP: Generate another ControllerRevision by patching the Daemonset 07/29/23 16:10:10.797
W0729 16:10:10.808346      13 warnings.go:70] unknown field "updateStrategy"
STEP: Confirm that there are two ControllerRevisions 07/29/23 16:10:10.808
Jul 29 16:10:10.808: INFO: Requesting list of ControllerRevisions to confirm quantity
Jul 29 16:10:11.816: INFO: Requesting list of ControllerRevisions to confirm quantity
Jul 29 16:10:11.822: INFO: Found 2 ControllerRevisions
STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-8lx96-daemon-set-84cb6c44f4=updated" 07/29/23 16:10:11.822
STEP: Confirm that there is only one ControllerRevision 07/29/23 16:10:11.835
Jul 29 16:10:11.836: INFO: Requesting list of ControllerRevisions to confirm quantity
Jul 29 16:10:11.844: INFO: Found 1 ControllerRevisions
Jul 29 16:10:11.849: INFO: ControllerRevision "e2e-8lx96-daemon-set-6db9f4cd5f" has revision 3
[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:58
STEP: Deleting DaemonSet "e2e-8lx96-daemon-set" 07/29/23 16:10:11.854
STEP: deleting DaemonSet.extensions e2e-8lx96-daemon-set in namespace controllerrevisions-8154, will wait for the garbage collector to delete the pods 07/29/23 16:10:11.855
Jul 29 16:10:11.923: INFO: Deleting DaemonSet.extensions e2e-8lx96-daemon-set took: 13.628719ms
Jul 29 16:10:12.024: INFO: Terminating DaemonSet.extensions e2e-8lx96-daemon-set pods took: 101.366049ms
Jul 29 16:10:13.732: INFO: Number of nodes with available pods controlled by daemonset e2e-8lx96-daemon-set: 0
Jul 29 16:10:13.732: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-8lx96-daemon-set
Jul 29 16:10:13.740: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"14610"},"items":null}

Jul 29 16:10:13.744: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"14610"},"items":null}

[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/framework.go:187
Jul 29 16:10:13.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "controllerrevisions-8154" for this suite. 07/29/23 16:10:13.788
{"msg":"PASSED [sig-apps] ControllerRevision [Serial] should manage the lifecycle of a ControllerRevision [Conformance]","completed":155,"skipped":2877,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.282 seconds]
[sig-apps] ControllerRevision [Serial]
test/e2e/apps/framework.go:23
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:10:08.523
    Jul 29 16:10:08.524: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename controllerrevisions 07/29/23 16:10:08.526
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:10:08.553
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:10:08.557
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:93
    [It] should manage the lifecycle of a ControllerRevision [Conformance]
      test/e2e/apps/controller_revision.go:124
    STEP: Creating DaemonSet "e2e-8lx96-daemon-set" 07/29/23 16:10:08.645
    STEP: Check that daemon pods launch on every node of the cluster. 07/29/23 16:10:08.672
    Jul 29 16:10:08.692: INFO: Number of nodes with available pods controlled by daemonset e2e-8lx96-daemon-set: 0
    Jul 29 16:10:08.692: INFO: Node wa4quivohpee-1 is running 0 daemon pod, expected 1
    Jul 29 16:10:09.712: INFO: Number of nodes with available pods controlled by daemonset e2e-8lx96-daemon-set: 0
    Jul 29 16:10:09.712: INFO: Node wa4quivohpee-1 is running 0 daemon pod, expected 1
    Jul 29 16:10:10.707: INFO: Number of nodes with available pods controlled by daemonset e2e-8lx96-daemon-set: 3
    Jul 29 16:10:10.707: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-8lx96-daemon-set
    STEP: Confirm DaemonSet "e2e-8lx96-daemon-set" successfully created with "daemonset-name=e2e-8lx96-daemon-set" label 07/29/23 16:10:10.713
    STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-8lx96-daemon-set" 07/29/23 16:10:10.725
    Jul 29 16:10:10.731: INFO: Located ControllerRevision: "e2e-8lx96-daemon-set-58ddc6479b"
    STEP: Patching ControllerRevision "e2e-8lx96-daemon-set-58ddc6479b" 07/29/23 16:10:10.736
    Jul 29 16:10:10.746: INFO: e2e-8lx96-daemon-set-58ddc6479b has been patched
    STEP: Create a new ControllerRevision 07/29/23 16:10:10.746
    Jul 29 16:10:10.755: INFO: Created ControllerRevision: e2e-8lx96-daemon-set-84cb6c44f4
    STEP: Confirm that there are two ControllerRevisions 07/29/23 16:10:10.756
    Jul 29 16:10:10.756: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jul 29 16:10:10.762: INFO: Found 2 ControllerRevisions
    STEP: Deleting ControllerRevision "e2e-8lx96-daemon-set-58ddc6479b" 07/29/23 16:10:10.762
    STEP: Confirm that there is only one ControllerRevision 07/29/23 16:10:10.771
    Jul 29 16:10:10.772: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jul 29 16:10:10.776: INFO: Found 1 ControllerRevisions
    STEP: Updating ControllerRevision "e2e-8lx96-daemon-set-84cb6c44f4" 07/29/23 16:10:10.782
    Jul 29 16:10:10.797: INFO: e2e-8lx96-daemon-set-84cb6c44f4 has been updated
    STEP: Generate another ControllerRevision by patching the Daemonset 07/29/23 16:10:10.797
    W0729 16:10:10.808346      13 warnings.go:70] unknown field "updateStrategy"
    STEP: Confirm that there are two ControllerRevisions 07/29/23 16:10:10.808
    Jul 29 16:10:10.808: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jul 29 16:10:11.816: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jul 29 16:10:11.822: INFO: Found 2 ControllerRevisions
    STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-8lx96-daemon-set-84cb6c44f4=updated" 07/29/23 16:10:11.822
    STEP: Confirm that there is only one ControllerRevision 07/29/23 16:10:11.835
    Jul 29 16:10:11.836: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jul 29 16:10:11.844: INFO: Found 1 ControllerRevisions
    Jul 29 16:10:11.849: INFO: ControllerRevision "e2e-8lx96-daemon-set-6db9f4cd5f" has revision 3
    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:58
    STEP: Deleting DaemonSet "e2e-8lx96-daemon-set" 07/29/23 16:10:11.854
    STEP: deleting DaemonSet.extensions e2e-8lx96-daemon-set in namespace controllerrevisions-8154, will wait for the garbage collector to delete the pods 07/29/23 16:10:11.855
    Jul 29 16:10:11.923: INFO: Deleting DaemonSet.extensions e2e-8lx96-daemon-set took: 13.628719ms
    Jul 29 16:10:12.024: INFO: Terminating DaemonSet.extensions e2e-8lx96-daemon-set pods took: 101.366049ms
    Jul 29 16:10:13.732: INFO: Number of nodes with available pods controlled by daemonset e2e-8lx96-daemon-set: 0
    Jul 29 16:10:13.732: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-8lx96-daemon-set
    Jul 29 16:10:13.740: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"14610"},"items":null}

    Jul 29 16:10:13.744: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"14610"},"items":null}

    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/framework.go:187
    Jul 29 16:10:13.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "controllerrevisions-8154" for this suite. 07/29/23 16:10:13.788
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:10:13.806
Jul 29 16:10:13.806: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename resourcequota 07/29/23 16:10:13.812
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:10:13.851
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:10:13.858
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65
STEP: Counting existing ResourceQuota 07/29/23 16:10:13.864
STEP: Creating a ResourceQuota 07/29/23 16:10:18.872
STEP: Ensuring resource quota status is calculated 07/29/23 16:10:18.889
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jul 29 16:10:20.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9796" for this suite. 07/29/23 16:10:20.906
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","completed":156,"skipped":2879,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.114 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:10:13.806
    Jul 29 16:10:13.806: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename resourcequota 07/29/23 16:10:13.812
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:10:13.851
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:10:13.858
    [It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
      test/e2e/apimachinery/resource_quota.go:65
    STEP: Counting existing ResourceQuota 07/29/23 16:10:13.864
    STEP: Creating a ResourceQuota 07/29/23 16:10:18.872
    STEP: Ensuring resource quota status is calculated 07/29/23 16:10:18.889
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jul 29 16:10:20.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-9796" for this suite. 07/29/23 16:10:20.906
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:10:20.926
Jul 29 16:10:20.926: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename configmap 07/29/23 16:10:20.927
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:10:20.95
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:10:20.955
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98
STEP: Creating configMap with name configmap-test-volume-map-484ecf18-ce88-4859-8d6f-65e1e48ecc05 07/29/23 16:10:20.959
STEP: Creating a pod to test consume configMaps 07/29/23 16:10:20.971
Jul 29 16:10:20.993: INFO: Waiting up to 5m0s for pod "pod-configmaps-68e8c806-0608-4c27-8881-abdeef26e77a" in namespace "configmap-7318" to be "Succeeded or Failed"
Jul 29 16:10:21.000: INFO: Pod "pod-configmaps-68e8c806-0608-4c27-8881-abdeef26e77a": Phase="Pending", Reason="", readiness=false. Elapsed: 7.285313ms
Jul 29 16:10:23.010: INFO: Pod "pod-configmaps-68e8c806-0608-4c27-8881-abdeef26e77a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017007206s
Jul 29 16:10:25.009: INFO: Pod "pod-configmaps-68e8c806-0608-4c27-8881-abdeef26e77a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016022485s
STEP: Saw pod success 07/29/23 16:10:25.009
Jul 29 16:10:25.010: INFO: Pod "pod-configmaps-68e8c806-0608-4c27-8881-abdeef26e77a" satisfied condition "Succeeded or Failed"
Jul 29 16:10:25.015: INFO: Trying to get logs from node wa4quivohpee-3 pod pod-configmaps-68e8c806-0608-4c27-8881-abdeef26e77a container agnhost-container: <nil>
STEP: delete the pod 07/29/23 16:10:25.027
Jul 29 16:10:25.050: INFO: Waiting for pod pod-configmaps-68e8c806-0608-4c27-8881-abdeef26e77a to disappear
Jul 29 16:10:25.055: INFO: Pod pod-configmaps-68e8c806-0608-4c27-8881-abdeef26e77a no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jul 29 16:10:25.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7318" for this suite. 07/29/23 16:10:25.072
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":157,"skipped":2936,"failed":0}
------------------------------
â€¢ [4.160 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:10:20.926
    Jul 29 16:10:20.926: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename configmap 07/29/23 16:10:20.927
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:10:20.95
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:10:20.955
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:98
    STEP: Creating configMap with name configmap-test-volume-map-484ecf18-ce88-4859-8d6f-65e1e48ecc05 07/29/23 16:10:20.959
    STEP: Creating a pod to test consume configMaps 07/29/23 16:10:20.971
    Jul 29 16:10:20.993: INFO: Waiting up to 5m0s for pod "pod-configmaps-68e8c806-0608-4c27-8881-abdeef26e77a" in namespace "configmap-7318" to be "Succeeded or Failed"
    Jul 29 16:10:21.000: INFO: Pod "pod-configmaps-68e8c806-0608-4c27-8881-abdeef26e77a": Phase="Pending", Reason="", readiness=false. Elapsed: 7.285313ms
    Jul 29 16:10:23.010: INFO: Pod "pod-configmaps-68e8c806-0608-4c27-8881-abdeef26e77a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017007206s
    Jul 29 16:10:25.009: INFO: Pod "pod-configmaps-68e8c806-0608-4c27-8881-abdeef26e77a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016022485s
    STEP: Saw pod success 07/29/23 16:10:25.009
    Jul 29 16:10:25.010: INFO: Pod "pod-configmaps-68e8c806-0608-4c27-8881-abdeef26e77a" satisfied condition "Succeeded or Failed"
    Jul 29 16:10:25.015: INFO: Trying to get logs from node wa4quivohpee-3 pod pod-configmaps-68e8c806-0608-4c27-8881-abdeef26e77a container agnhost-container: <nil>
    STEP: delete the pod 07/29/23 16:10:25.027
    Jul 29 16:10:25.050: INFO: Waiting for pod pod-configmaps-68e8c806-0608-4c27-8881-abdeef26e77a to disappear
    Jul 29 16:10:25.055: INFO: Pod pod-configmaps-68e8c806-0608-4c27-8881-abdeef26e77a no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jul 29 16:10:25.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-7318" for this suite. 07/29/23 16:10:25.072
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:10:25.088
Jul 29 16:10:25.088: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename var-expansion 07/29/23 16:10:25.091
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:10:25.124
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:10:25.128
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72
STEP: Creating a pod to test substitution in container's command 07/29/23 16:10:25.133
Jul 29 16:10:25.211: INFO: Waiting up to 5m0s for pod "var-expansion-7558f493-a75e-444f-9605-2553b771410d" in namespace "var-expansion-4806" to be "Succeeded or Failed"
Jul 29 16:10:25.227: INFO: Pod "var-expansion-7558f493-a75e-444f-9605-2553b771410d": Phase="Pending", Reason="", readiness=false. Elapsed: 15.134122ms
Jul 29 16:10:27.235: INFO: Pod "var-expansion-7558f493-a75e-444f-9605-2553b771410d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023313663s
Jul 29 16:10:29.238: INFO: Pod "var-expansion-7558f493-a75e-444f-9605-2553b771410d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026306054s
STEP: Saw pod success 07/29/23 16:10:29.238
Jul 29 16:10:29.238: INFO: Pod "var-expansion-7558f493-a75e-444f-9605-2553b771410d" satisfied condition "Succeeded or Failed"
Jul 29 16:10:29.244: INFO: Trying to get logs from node wa4quivohpee-3 pod var-expansion-7558f493-a75e-444f-9605-2553b771410d container dapi-container: <nil>
STEP: delete the pod 07/29/23 16:10:29.254
Jul 29 16:10:29.271: INFO: Waiting for pod var-expansion-7558f493-a75e-444f-9605-2553b771410d to disappear
Jul 29 16:10:29.275: INFO: Pod var-expansion-7558f493-a75e-444f-9605-2553b771410d no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jul 29 16:10:29.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4806" for this suite. 07/29/23 16:10:29.283
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","completed":158,"skipped":2937,"failed":0}
------------------------------
â€¢ [4.206 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:10:25.088
    Jul 29 16:10:25.088: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename var-expansion 07/29/23 16:10:25.091
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:10:25.124
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:10:25.128
    [It] should allow substituting values in a container's command [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:72
    STEP: Creating a pod to test substitution in container's command 07/29/23 16:10:25.133
    Jul 29 16:10:25.211: INFO: Waiting up to 5m0s for pod "var-expansion-7558f493-a75e-444f-9605-2553b771410d" in namespace "var-expansion-4806" to be "Succeeded or Failed"
    Jul 29 16:10:25.227: INFO: Pod "var-expansion-7558f493-a75e-444f-9605-2553b771410d": Phase="Pending", Reason="", readiness=false. Elapsed: 15.134122ms
    Jul 29 16:10:27.235: INFO: Pod "var-expansion-7558f493-a75e-444f-9605-2553b771410d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023313663s
    Jul 29 16:10:29.238: INFO: Pod "var-expansion-7558f493-a75e-444f-9605-2553b771410d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026306054s
    STEP: Saw pod success 07/29/23 16:10:29.238
    Jul 29 16:10:29.238: INFO: Pod "var-expansion-7558f493-a75e-444f-9605-2553b771410d" satisfied condition "Succeeded or Failed"
    Jul 29 16:10:29.244: INFO: Trying to get logs from node wa4quivohpee-3 pod var-expansion-7558f493-a75e-444f-9605-2553b771410d container dapi-container: <nil>
    STEP: delete the pod 07/29/23 16:10:29.254
    Jul 29 16:10:29.271: INFO: Waiting for pod var-expansion-7558f493-a75e-444f-9605-2553b771410d to disappear
    Jul 29 16:10:29.275: INFO: Pod var-expansion-7558f493-a75e-444f-9605-2553b771410d no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jul 29 16:10:29.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-4806" for this suite. 07/29/23 16:10:29.283
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:10:29.313
Jul 29 16:10:29.313: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename watch 07/29/23 16:10:29.315
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:10:29.345
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:10:29.348
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
STEP: creating a watch on configmaps with a certain label 07/29/23 16:10:29.352
STEP: creating a new configmap 07/29/23 16:10:29.353
STEP: modifying the configmap once 07/29/23 16:10:29.362
STEP: changing the label value of the configmap 07/29/23 16:10:29.374
STEP: Expecting to observe a delete notification for the watched object 07/29/23 16:10:29.391
Jul 29 16:10:29.391: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2691  1633f3ef-7a1f-4ae1-93a3-22b1be8404f6 14724 0 2023-07-29 16:10:29 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-07-29 16:10:29 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 29 16:10:29.391: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2691  1633f3ef-7a1f-4ae1-93a3-22b1be8404f6 14725 0 2023-07-29 16:10:29 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-07-29 16:10:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 29 16:10:29.392: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2691  1633f3ef-7a1f-4ae1-93a3-22b1be8404f6 14726 0 2023-07-29 16:10:29 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-07-29 16:10:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time 07/29/23 16:10:29.392
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 07/29/23 16:10:29.404
STEP: changing the label value of the configmap back 07/29/23 16:10:39.406
STEP: modifying the configmap a third time 07/29/23 16:10:39.422
STEP: deleting the configmap 07/29/23 16:10:39.433
STEP: Expecting to observe an add notification for the watched object when the label value was restored 07/29/23 16:10:39.442
Jul 29 16:10:39.442: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2691  1633f3ef-7a1f-4ae1-93a3-22b1be8404f6 14766 0 2023-07-29 16:10:29 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-07-29 16:10:39 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 29 16:10:39.443: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2691  1633f3ef-7a1f-4ae1-93a3-22b1be8404f6 14767 0 2023-07-29 16:10:29 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-07-29 16:10:39 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 29 16:10:39.443: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2691  1633f3ef-7a1f-4ae1-93a3-22b1be8404f6 14768 0 2023-07-29 16:10:29 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-07-29 16:10:39 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Jul 29 16:10:39.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2691" for this suite. 07/29/23 16:10:39.451
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","completed":159,"skipped":2994,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.150 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:10:29.313
    Jul 29 16:10:29.313: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename watch 07/29/23 16:10:29.315
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:10:29.345
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:10:29.348
    [It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
      test/e2e/apimachinery/watch.go:257
    STEP: creating a watch on configmaps with a certain label 07/29/23 16:10:29.352
    STEP: creating a new configmap 07/29/23 16:10:29.353
    STEP: modifying the configmap once 07/29/23 16:10:29.362
    STEP: changing the label value of the configmap 07/29/23 16:10:29.374
    STEP: Expecting to observe a delete notification for the watched object 07/29/23 16:10:29.391
    Jul 29 16:10:29.391: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2691  1633f3ef-7a1f-4ae1-93a3-22b1be8404f6 14724 0 2023-07-29 16:10:29 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-07-29 16:10:29 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jul 29 16:10:29.391: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2691  1633f3ef-7a1f-4ae1-93a3-22b1be8404f6 14725 0 2023-07-29 16:10:29 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-07-29 16:10:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jul 29 16:10:29.392: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2691  1633f3ef-7a1f-4ae1-93a3-22b1be8404f6 14726 0 2023-07-29 16:10:29 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-07-29 16:10:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time 07/29/23 16:10:29.392
    STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 07/29/23 16:10:29.404
    STEP: changing the label value of the configmap back 07/29/23 16:10:39.406
    STEP: modifying the configmap a third time 07/29/23 16:10:39.422
    STEP: deleting the configmap 07/29/23 16:10:39.433
    STEP: Expecting to observe an add notification for the watched object when the label value was restored 07/29/23 16:10:39.442
    Jul 29 16:10:39.442: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2691  1633f3ef-7a1f-4ae1-93a3-22b1be8404f6 14766 0 2023-07-29 16:10:29 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-07-29 16:10:39 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jul 29 16:10:39.443: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2691  1633f3ef-7a1f-4ae1-93a3-22b1be8404f6 14767 0 2023-07-29 16:10:29 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-07-29 16:10:39 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jul 29 16:10:39.443: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2691  1633f3ef-7a1f-4ae1-93a3-22b1be8404f6 14768 0 2023-07-29 16:10:29 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-07-29 16:10:39 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Jul 29 16:10:39.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-2691" for this suite. 07/29/23 16:10:39.451
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:10:39.471
Jul 29 16:10:39.471: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename webhook 07/29/23 16:10:39.473
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:10:39.504
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:10:39.508
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 07/29/23 16:10:39.537
STEP: Create role binding to let webhook read extension-apiserver-authentication 07/29/23 16:10:40.396
STEP: Deploying the webhook pod 07/29/23 16:10:40.406
STEP: Wait for the deployment to be ready 07/29/23 16:10:40.431
Jul 29 16:10:40.453: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 07/29/23 16:10:42.478
STEP: Verifying the service has paired with the endpoint 07/29/23 16:10:42.515
Jul 29 16:10:43.516: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340
Jul 29 16:10:43.526: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1393-crds.webhook.example.com via the AdmissionRegistration API 07/29/23 16:10:44.046
STEP: Creating a custom resource that should be mutated by the webhook 07/29/23 16:10:44.086
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jul 29 16:10:46.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6442" for this suite. 07/29/23 16:10:46.902
STEP: Destroying namespace "webhook-6442-markers" for this suite. 07/29/23 16:10:46.917
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","completed":160,"skipped":3004,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.590 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:10:39.471
    Jul 29 16:10:39.471: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename webhook 07/29/23 16:10:39.473
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:10:39.504
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:10:39.508
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 07/29/23 16:10:39.537
    STEP: Create role binding to let webhook read extension-apiserver-authentication 07/29/23 16:10:40.396
    STEP: Deploying the webhook pod 07/29/23 16:10:40.406
    STEP: Wait for the deployment to be ready 07/29/23 16:10:40.431
    Jul 29 16:10:40.453: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 07/29/23 16:10:42.478
    STEP: Verifying the service has paired with the endpoint 07/29/23 16:10:42.515
    Jul 29 16:10:43.516: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with pruning [Conformance]
      test/e2e/apimachinery/webhook.go:340
    Jul 29 16:10:43.526: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1393-crds.webhook.example.com via the AdmissionRegistration API 07/29/23 16:10:44.046
    STEP: Creating a custom resource that should be mutated by the webhook 07/29/23 16:10:44.086
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jul 29 16:10:46.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-6442" for this suite. 07/29/23 16:10:46.902
    STEP: Destroying namespace "webhook-6442-markers" for this suite. 07/29/23 16:10:46.917
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:10:47.064
Jul 29 16:10:47.065: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename configmap 07/29/23 16:10:47.068
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:10:47.113
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:10:47.132
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56
STEP: Creating configMap with name configmap-test-volume-82c0ec5c-767b-4e92-ac36-42eb585ae4d0 07/29/23 16:10:47.137
STEP: Creating a pod to test consume configMaps 07/29/23 16:10:47.145
Jul 29 16:10:47.158: INFO: Waiting up to 5m0s for pod "pod-configmaps-5bada10b-f9bc-4e49-89bb-bbe411a08931" in namespace "configmap-2544" to be "Succeeded or Failed"
Jul 29 16:10:47.166: INFO: Pod "pod-configmaps-5bada10b-f9bc-4e49-89bb-bbe411a08931": Phase="Pending", Reason="", readiness=false. Elapsed: 7.589998ms
Jul 29 16:10:49.183: INFO: Pod "pod-configmaps-5bada10b-f9bc-4e49-89bb-bbe411a08931": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02465008s
Jul 29 16:10:51.175: INFO: Pod "pod-configmaps-5bada10b-f9bc-4e49-89bb-bbe411a08931": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016695749s
STEP: Saw pod success 07/29/23 16:10:51.175
Jul 29 16:10:51.176: INFO: Pod "pod-configmaps-5bada10b-f9bc-4e49-89bb-bbe411a08931" satisfied condition "Succeeded or Failed"
Jul 29 16:10:51.182: INFO: Trying to get logs from node wa4quivohpee-3 pod pod-configmaps-5bada10b-f9bc-4e49-89bb-bbe411a08931 container agnhost-container: <nil>
STEP: delete the pod 07/29/23 16:10:51.193
Jul 29 16:10:51.223: INFO: Waiting for pod pod-configmaps-5bada10b-f9bc-4e49-89bb-bbe411a08931 to disappear
Jul 29 16:10:51.229: INFO: Pod pod-configmaps-5bada10b-f9bc-4e49-89bb-bbe411a08931 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jul 29 16:10:51.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2544" for this suite. 07/29/23 16:10:51.238
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":161,"skipped":3018,"failed":0}
------------------------------
â€¢ [4.183 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:10:47.064
    Jul 29 16:10:47.065: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename configmap 07/29/23 16:10:47.068
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:10:47.113
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:10:47.132
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:56
    STEP: Creating configMap with name configmap-test-volume-82c0ec5c-767b-4e92-ac36-42eb585ae4d0 07/29/23 16:10:47.137
    STEP: Creating a pod to test consume configMaps 07/29/23 16:10:47.145
    Jul 29 16:10:47.158: INFO: Waiting up to 5m0s for pod "pod-configmaps-5bada10b-f9bc-4e49-89bb-bbe411a08931" in namespace "configmap-2544" to be "Succeeded or Failed"
    Jul 29 16:10:47.166: INFO: Pod "pod-configmaps-5bada10b-f9bc-4e49-89bb-bbe411a08931": Phase="Pending", Reason="", readiness=false. Elapsed: 7.589998ms
    Jul 29 16:10:49.183: INFO: Pod "pod-configmaps-5bada10b-f9bc-4e49-89bb-bbe411a08931": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02465008s
    Jul 29 16:10:51.175: INFO: Pod "pod-configmaps-5bada10b-f9bc-4e49-89bb-bbe411a08931": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016695749s
    STEP: Saw pod success 07/29/23 16:10:51.175
    Jul 29 16:10:51.176: INFO: Pod "pod-configmaps-5bada10b-f9bc-4e49-89bb-bbe411a08931" satisfied condition "Succeeded or Failed"
    Jul 29 16:10:51.182: INFO: Trying to get logs from node wa4quivohpee-3 pod pod-configmaps-5bada10b-f9bc-4e49-89bb-bbe411a08931 container agnhost-container: <nil>
    STEP: delete the pod 07/29/23 16:10:51.193
    Jul 29 16:10:51.223: INFO: Waiting for pod pod-configmaps-5bada10b-f9bc-4e49-89bb-bbe411a08931 to disappear
    Jul 29 16:10:51.229: INFO: Pod pod-configmaps-5bada10b-f9bc-4e49-89bb-bbe411a08931 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jul 29 16:10:51.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-2544" for this suite. 07/29/23 16:10:51.238
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:10:51.288
Jul 29 16:10:51.289: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename endpointslice 07/29/23 16:10:51.29
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:10:51.326
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:10:51.333
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204
STEP: referencing a single matching pod 07/29/23 16:10:56.491
STEP: referencing matching pods with named port 07/29/23 16:11:01.523
STEP: creating empty Endpoints and EndpointSlices for no matching Pods 07/29/23 16:11:06.541
STEP: recreating EndpointSlices after they've been deleted 07/29/23 16:11:11.562
Jul 29 16:11:11.613: INFO: EndpointSlice for Service endpointslice-4883/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Jul 29 16:11:21.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-4883" for this suite. 07/29/23 16:11:21.643
{"msg":"PASSED [sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]","completed":162,"skipped":3163,"failed":0}
------------------------------
â€¢ [SLOW TEST] [30.372 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:10:51.288
    Jul 29 16:10:51.289: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename endpointslice 07/29/23 16:10:51.29
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:10:51.326
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:10:51.333
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
      test/e2e/network/endpointslice.go:204
    STEP: referencing a single matching pod 07/29/23 16:10:56.491
    STEP: referencing matching pods with named port 07/29/23 16:11:01.523
    STEP: creating empty Endpoints and EndpointSlices for no matching Pods 07/29/23 16:11:06.541
    STEP: recreating EndpointSlices after they've been deleted 07/29/23 16:11:11.562
    Jul 29 16:11:11.613: INFO: EndpointSlice for Service endpointslice-4883/example-named-port not found
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Jul 29 16:11:21.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-4883" for this suite. 07/29/23 16:11:21.643
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
[BeforeEach] [sig-network] Service endpoints latency
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:11:21.663
Jul 29 16:11:21.664: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename svc-latency 07/29/23 16:11:21.666
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:11:21.694
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:11:21.699
[It] should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
Jul 29 16:11:21.703: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: creating replication controller svc-latency-rc in namespace svc-latency-5126 07/29/23 16:11:21.705
I0729 16:11:21.719168      13 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-5126, replica count: 1
I0729 16:11:22.771948      13 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0729 16:11:23.772268      13 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 29 16:11:23.900: INFO: Created: latency-svc-gq9qs
Jul 29 16:11:23.912: INFO: Got endpoints: latency-svc-gq9qs [38.579643ms]
Jul 29 16:11:23.952: INFO: Created: latency-svc-vkksr
Jul 29 16:11:23.957: INFO: Got endpoints: latency-svc-vkksr [41.107874ms]
Jul 29 16:11:23.975: INFO: Created: latency-svc-pxcp2
Jul 29 16:11:23.991: INFO: Got endpoints: latency-svc-pxcp2 [73.850272ms]
Jul 29 16:11:24.001: INFO: Created: latency-svc-9pwdk
Jul 29 16:11:24.008: INFO: Got endpoints: latency-svc-9pwdk [88.798377ms]
Jul 29 16:11:24.054: INFO: Created: latency-svc-spcz4
Jul 29 16:11:24.078: INFO: Created: latency-svc-c89k9
Jul 29 16:11:24.079: INFO: Got endpoints: latency-svc-spcz4 [160.732454ms]
Jul 29 16:11:24.094: INFO: Got endpoints: latency-svc-c89k9 [175.329768ms]
Jul 29 16:11:24.107: INFO: Created: latency-svc-gwrs4
Jul 29 16:11:24.122: INFO: Got endpoints: latency-svc-gwrs4 [203.229354ms]
Jul 29 16:11:24.252: INFO: Created: latency-svc-tmqmw
Jul 29 16:11:24.295: INFO: Created: latency-svc-nt7tx
Jul 29 16:11:24.295: INFO: Created: latency-svc-vddj8
Jul 29 16:11:24.296: INFO: Created: latency-svc-4ndjf
Jul 29 16:11:24.296: INFO: Created: latency-svc-4kn9f
Jul 29 16:11:24.297: INFO: Created: latency-svc-5cdmt
Jul 29 16:11:24.297: INFO: Created: latency-svc-x476s
Jul 29 16:11:24.297: INFO: Created: latency-svc-dd5rn
Jul 29 16:11:24.297: INFO: Created: latency-svc-b75cx
Jul 29 16:11:24.297: INFO: Got endpoints: latency-svc-tmqmw [218.108223ms]
Jul 29 16:11:24.297: INFO: Created: latency-svc-pqdd5
Jul 29 16:11:24.303: INFO: Created: latency-svc-26hv6
Jul 29 16:11:24.306: INFO: Created: latency-svc-b6kk2
Jul 29 16:11:24.306: INFO: Created: latency-svc-t9zdv
Jul 29 16:11:24.306: INFO: Created: latency-svc-zqnvf
Jul 29 16:11:24.307: INFO: Created: latency-svc-k2lb5
Jul 29 16:11:24.323: INFO: Got endpoints: latency-svc-4kn9f [402.744606ms]
Jul 29 16:11:24.349: INFO: Got endpoints: latency-svc-vddj8 [429.206402ms]
Jul 29 16:11:24.349: INFO: Got endpoints: latency-svc-4ndjf [430.141421ms]
Jul 29 16:11:24.350: INFO: Got endpoints: latency-svc-t9zdv [430.89577ms]
Jul 29 16:11:24.350: INFO: Got endpoints: latency-svc-5cdmt [429.78959ms]
Jul 29 16:11:24.372: INFO: Created: latency-svc-v82x6
Jul 29 16:11:24.384: INFO: Got endpoints: latency-svc-k2lb5 [463.916407ms]
Jul 29 16:11:24.395: INFO: Got endpoints: latency-svc-26hv6 [403.261659ms]
Jul 29 16:11:24.395: INFO: Got endpoints: latency-svc-zqnvf [387.131016ms]
Jul 29 16:11:24.396: INFO: Got endpoints: latency-svc-nt7tx [273.725892ms]
Jul 29 16:11:24.396: INFO: Got endpoints: latency-svc-b6kk2 [438.272002ms]
Jul 29 16:11:24.402: INFO: Created: latency-svc-jj42c
Jul 29 16:11:24.440: INFO: Got endpoints: latency-svc-dd5rn [520.191355ms]
Jul 29 16:11:24.455: INFO: Got endpoints: latency-svc-pqdd5 [536.325865ms]
Jul 29 16:11:24.471: INFO: Got endpoints: latency-svc-x476s [550.443612ms]
Jul 29 16:11:24.472: INFO: Got endpoints: latency-svc-b75cx [378.097403ms]
Jul 29 16:11:24.494: INFO: Created: latency-svc-gdhs4
Jul 29 16:11:24.496: INFO: Got endpoints: latency-svc-jj42c [172.03353ms]
Jul 29 16:11:24.496: INFO: Got endpoints: latency-svc-v82x6 [198.120745ms]
Jul 29 16:11:24.521: INFO: Got endpoints: latency-svc-gdhs4 [171.807184ms]
Jul 29 16:11:24.523: INFO: Created: latency-svc-tr4sv
Jul 29 16:11:24.543: INFO: Created: latency-svc-rthl6
Jul 29 16:11:24.564: INFO: Got endpoints: latency-svc-tr4sv [213.070521ms]
Jul 29 16:11:24.567: INFO: Got endpoints: latency-svc-rthl6 [217.304772ms]
Jul 29 16:11:24.580: INFO: Created: latency-svc-l8dpb
Jul 29 16:11:24.601: INFO: Created: latency-svc-2nnml
Jul 29 16:11:24.613: INFO: Got endpoints: latency-svc-l8dpb [262.836815ms]
Jul 29 16:11:24.621: INFO: Got endpoints: latency-svc-2nnml [226.425733ms]
Jul 29 16:11:24.631: INFO: Created: latency-svc-jhztd
Jul 29 16:11:24.645: INFO: Got endpoints: latency-svc-jhztd [249.065977ms]
Jul 29 16:11:24.651: INFO: Created: latency-svc-n6wpr
Jul 29 16:11:24.668: INFO: Created: latency-svc-hkw9j
Jul 29 16:11:24.673: INFO: Got endpoints: latency-svc-n6wpr [277.112504ms]
Jul 29 16:11:24.686: INFO: Got endpoints: latency-svc-hkw9j [289.957888ms]
Jul 29 16:11:24.688: INFO: Created: latency-svc-8nq25
Jul 29 16:11:24.709: INFO: Got endpoints: latency-svc-8nq25 [324.42821ms]
Jul 29 16:11:24.716: INFO: Created: latency-svc-2wj59
Jul 29 16:11:24.727: INFO: Got endpoints: latency-svc-2wj59 [286.937325ms]
Jul 29 16:11:24.728: INFO: Created: latency-svc-5cnd4
Jul 29 16:11:24.733: INFO: Got endpoints: latency-svc-5cnd4 [278.425185ms]
Jul 29 16:11:24.751: INFO: Created: latency-svc-g4pw5
Jul 29 16:11:24.766: INFO: Created: latency-svc-t2mzl
Jul 29 16:11:24.768: INFO: Got endpoints: latency-svc-g4pw5 [296.032199ms]
Jul 29 16:11:24.775: INFO: Got endpoints: latency-svc-t2mzl [302.691327ms]
Jul 29 16:11:24.785: INFO: Created: latency-svc-tzvvx
Jul 29 16:11:24.803: INFO: Created: latency-svc-66crd
Jul 29 16:11:24.808: INFO: Got endpoints: latency-svc-tzvvx [312.559484ms]
Jul 29 16:11:24.817: INFO: Got endpoints: latency-svc-66crd [321.316043ms]
Jul 29 16:11:24.819: INFO: Created: latency-svc-j2bxb
Jul 29 16:11:24.826: INFO: Got endpoints: latency-svc-j2bxb [304.585733ms]
Jul 29 16:11:24.834: INFO: Created: latency-svc-82d9j
Jul 29 16:11:24.836: INFO: Got endpoints: latency-svc-82d9j [271.624065ms]
Jul 29 16:11:24.846: INFO: Created: latency-svc-skskr
Jul 29 16:11:24.854: INFO: Got endpoints: latency-svc-skskr [286.68961ms]
Jul 29 16:11:24.869: INFO: Created: latency-svc-knh7v
Jul 29 16:11:24.878: INFO: Created: latency-svc-9tf2b
Jul 29 16:11:24.884: INFO: Got endpoints: latency-svc-knh7v [271.262459ms]
Jul 29 16:11:24.895: INFO: Got endpoints: latency-svc-9tf2b [272.918428ms]
Jul 29 16:11:24.897: INFO: Created: latency-svc-8lk5h
Jul 29 16:11:24.917: INFO: Created: latency-svc-chtn8
Jul 29 16:11:24.918: INFO: Got endpoints: latency-svc-8lk5h [272.669817ms]
Jul 29 16:11:24.928: INFO: Got endpoints: latency-svc-chtn8 [255.164934ms]
Jul 29 16:11:24.952: INFO: Created: latency-svc-l4h7h
Jul 29 16:11:24.975: INFO: Created: latency-svc-255nm
Jul 29 16:11:24.975: INFO: Got endpoints: latency-svc-l4h7h [289.104743ms]
Jul 29 16:11:25.000: INFO: Created: latency-svc-vnnfp
Jul 29 16:11:25.008: INFO: Got endpoints: latency-svc-255nm [299.015794ms]
Jul 29 16:11:25.024: INFO: Got endpoints: latency-svc-vnnfp [296.301054ms]
Jul 29 16:11:25.026: INFO: Created: latency-svc-vhnkd
Jul 29 16:11:25.041: INFO: Got endpoints: latency-svc-vhnkd [306.975156ms]
Jul 29 16:11:25.060: INFO: Created: latency-svc-d9zrx
Jul 29 16:11:25.069: INFO: Got endpoints: latency-svc-d9zrx [301.442046ms]
Jul 29 16:11:25.078: INFO: Created: latency-svc-zfjtd
Jul 29 16:11:25.079: INFO: Got endpoints: latency-svc-zfjtd [303.690214ms]
Jul 29 16:11:25.091: INFO: Created: latency-svc-rh7ww
Jul 29 16:11:25.111: INFO: Created: latency-svc-lqs8x
Jul 29 16:11:25.126: INFO: Got endpoints: latency-svc-rh7ww [317.142192ms]
Jul 29 16:11:25.148: INFO: Got endpoints: latency-svc-lqs8x [330.55686ms]
Jul 29 16:11:25.152: INFO: Created: latency-svc-pz9z7
Jul 29 16:11:25.205: INFO: Created: latency-svc-lctxf
Jul 29 16:11:25.211: INFO: Got endpoints: latency-svc-pz9z7 [384.233723ms]
Jul 29 16:11:25.219: INFO: Got endpoints: latency-svc-lctxf [381.778306ms]
Jul 29 16:11:25.230: INFO: Created: latency-svc-bg9cb
Jul 29 16:11:25.241: INFO: Created: latency-svc-wgb5s
Jul 29 16:11:25.260: INFO: Created: latency-svc-bxhmk
Jul 29 16:11:25.267: INFO: Got endpoints: latency-svc-bg9cb [412.937963ms]
Jul 29 16:11:25.284: INFO: Created: latency-svc-8fl4k
Jul 29 16:11:25.291: INFO: Created: latency-svc-lz7fc
Jul 29 16:11:25.298: INFO: Created: latency-svc-skd85
Jul 29 16:11:25.315: INFO: Got endpoints: latency-svc-wgb5s [430.224143ms]
Jul 29 16:11:25.321: INFO: Created: latency-svc-lxbkh
Jul 29 16:11:25.334: INFO: Created: latency-svc-dbrbn
Jul 29 16:11:25.346: INFO: Created: latency-svc-cxhlq
Jul 29 16:11:25.359: INFO: Created: latency-svc-25sw7
Jul 29 16:11:25.362: INFO: Got endpoints: latency-svc-bxhmk [467.672315ms]
Jul 29 16:11:25.383: INFO: Created: latency-svc-4hbmp
Jul 29 16:11:25.389: INFO: Created: latency-svc-m4pjg
Jul 29 16:11:25.404: INFO: Created: latency-svc-9bttm
Jul 29 16:11:25.416: INFO: Got endpoints: latency-svc-8fl4k [498.361282ms]
Jul 29 16:11:25.418: INFO: Created: latency-svc-p9wdp
Jul 29 16:11:25.430: INFO: Created: latency-svc-d2zv8
Jul 29 16:11:25.466: INFO: Created: latency-svc-kfwj2
Jul 29 16:11:25.466: INFO: Got endpoints: latency-svc-lz7fc [538.315845ms]
Jul 29 16:11:25.473: INFO: Created: latency-svc-kkhx2
Jul 29 16:11:25.475: INFO: Created: latency-svc-q9z88
Jul 29 16:11:25.476: INFO: Created: latency-svc-nfzqr
Jul 29 16:11:25.489: INFO: Created: latency-svc-htbl7
Jul 29 16:11:25.514: INFO: Got endpoints: latency-svc-skd85 [538.54758ms]
Jul 29 16:11:25.535: INFO: Created: latency-svc-zdzv9
Jul 29 16:11:25.564: INFO: Got endpoints: latency-svc-lxbkh [540.253047ms]
Jul 29 16:11:25.587: INFO: Created: latency-svc-shl2r
Jul 29 16:11:25.614: INFO: Got endpoints: latency-svc-dbrbn [605.965876ms]
Jul 29 16:11:25.640: INFO: Created: latency-svc-j8jjx
Jul 29 16:11:25.665: INFO: Got endpoints: latency-svc-cxhlq [623.902163ms]
Jul 29 16:11:25.693: INFO: Created: latency-svc-dzcwj
Jul 29 16:11:25.713: INFO: Got endpoints: latency-svc-25sw7 [643.413568ms]
Jul 29 16:11:25.730: INFO: Created: latency-svc-7twlx
Jul 29 16:11:25.791: INFO: Got endpoints: latency-svc-4hbmp [712.292295ms]
Jul 29 16:11:25.849: INFO: Created: latency-svc-wch5k
Jul 29 16:11:25.868: INFO: Got endpoints: latency-svc-m4pjg [741.946323ms]
Jul 29 16:11:25.872: INFO: Got endpoints: latency-svc-9bttm [723.978574ms]
Jul 29 16:11:25.891: INFO: Created: latency-svc-qmfjp
Jul 29 16:11:25.897: INFO: Created: latency-svc-tjz54
Jul 29 16:11:25.920: INFO: Got endpoints: latency-svc-p9wdp [708.998333ms]
Jul 29 16:11:25.947: INFO: Created: latency-svc-qsdfb
Jul 29 16:11:25.967: INFO: Got endpoints: latency-svc-d2zv8 [748.170799ms]
Jul 29 16:11:25.994: INFO: Created: latency-svc-dk6bn
Jul 29 16:11:26.015: INFO: Got endpoints: latency-svc-kfwj2 [598.629422ms]
Jul 29 16:11:26.035: INFO: Created: latency-svc-jq86h
Jul 29 16:11:26.073: INFO: Got endpoints: latency-svc-kkhx2 [758.262316ms]
Jul 29 16:11:26.106: INFO: Created: latency-svc-x6hd8
Jul 29 16:11:26.116: INFO: Got endpoints: latency-svc-q9z88 [848.473583ms]
Jul 29 16:11:26.144: INFO: Created: latency-svc-f9q9x
Jul 29 16:11:26.163: INFO: Got endpoints: latency-svc-nfzqr [800.791784ms]
Jul 29 16:11:26.198: INFO: Created: latency-svc-ng9gd
Jul 29 16:11:26.214: INFO: Got endpoints: latency-svc-htbl7 [747.916596ms]
Jul 29 16:11:26.238: INFO: Created: latency-svc-w5vqt
Jul 29 16:11:26.263: INFO: Got endpoints: latency-svc-zdzv9 [749.031687ms]
Jul 29 16:11:26.284: INFO: Created: latency-svc-9dppk
Jul 29 16:11:26.311: INFO: Got endpoints: latency-svc-shl2r [746.419901ms]
Jul 29 16:11:26.338: INFO: Created: latency-svc-sg6ln
Jul 29 16:11:26.366: INFO: Got endpoints: latency-svc-j8jjx [751.60029ms]
Jul 29 16:11:26.388: INFO: Created: latency-svc-bpnw4
Jul 29 16:11:26.410: INFO: Got endpoints: latency-svc-dzcwj [745.306373ms]
Jul 29 16:11:26.434: INFO: Created: latency-svc-nszvf
Jul 29 16:11:26.467: INFO: Got endpoints: latency-svc-7twlx [753.621931ms]
Jul 29 16:11:26.487: INFO: Created: latency-svc-s4q5b
Jul 29 16:11:26.511: INFO: Got endpoints: latency-svc-wch5k [719.781808ms]
Jul 29 16:11:26.535: INFO: Created: latency-svc-dgt2n
Jul 29 16:11:26.564: INFO: Got endpoints: latency-svc-qmfjp [695.251138ms]
Jul 29 16:11:26.583: INFO: Created: latency-svc-hdv7b
Jul 29 16:11:26.632: INFO: Got endpoints: latency-svc-tjz54 [759.339912ms]
Jul 29 16:11:26.665: INFO: Created: latency-svc-ng5vd
Jul 29 16:11:26.672: INFO: Got endpoints: latency-svc-qsdfb [751.525939ms]
Jul 29 16:11:26.705: INFO: Created: latency-svc-g4mdd
Jul 29 16:11:26.718: INFO: Got endpoints: latency-svc-dk6bn [750.584556ms]
Jul 29 16:11:26.752: INFO: Created: latency-svc-f2jlf
Jul 29 16:11:26.762: INFO: Got endpoints: latency-svc-jq86h [746.713311ms]
Jul 29 16:11:26.798: INFO: Created: latency-svc-4pkq2
Jul 29 16:11:26.812: INFO: Got endpoints: latency-svc-x6hd8 [738.371787ms]
Jul 29 16:11:26.833: INFO: Created: latency-svc-jf8n2
Jul 29 16:11:26.865: INFO: Got endpoints: latency-svc-f9q9x [749.376995ms]
Jul 29 16:11:26.881: INFO: Created: latency-svc-9c5pq
Jul 29 16:11:26.911: INFO: Got endpoints: latency-svc-ng9gd [747.950866ms]
Jul 29 16:11:26.934: INFO: Created: latency-svc-gvnsw
Jul 29 16:11:26.965: INFO: Got endpoints: latency-svc-w5vqt [750.419878ms]
Jul 29 16:11:27.012: INFO: Got endpoints: latency-svc-9dppk [748.058244ms]
Jul 29 16:11:27.026: INFO: Created: latency-svc-pm8cp
Jul 29 16:11:27.062: INFO: Got endpoints: latency-svc-sg6ln [750.868549ms]
Jul 29 16:11:27.073: INFO: Created: latency-svc-fdz46
Jul 29 16:11:27.091: INFO: Created: latency-svc-z8qjd
Jul 29 16:11:27.108: INFO: Got endpoints: latency-svc-bpnw4 [741.438006ms]
Jul 29 16:11:27.137: INFO: Created: latency-svc-dczdk
Jul 29 16:11:27.171: INFO: Got endpoints: latency-svc-nszvf [760.21245ms]
Jul 29 16:11:27.186: INFO: Created: latency-svc-pzxm9
Jul 29 16:11:27.210: INFO: Got endpoints: latency-svc-s4q5b [742.912028ms]
Jul 29 16:11:27.233: INFO: Created: latency-svc-9kp22
Jul 29 16:11:27.266: INFO: Got endpoints: latency-svc-dgt2n [755.295593ms]
Jul 29 16:11:27.286: INFO: Created: latency-svc-l9qxn
Jul 29 16:11:27.323: INFO: Got endpoints: latency-svc-hdv7b [759.498915ms]
Jul 29 16:11:27.345: INFO: Created: latency-svc-78spc
Jul 29 16:11:27.361: INFO: Got endpoints: latency-svc-ng5vd [728.827523ms]
Jul 29 16:11:27.385: INFO: Created: latency-svc-6dxcv
Jul 29 16:11:27.431: INFO: Got endpoints: latency-svc-g4mdd [759.245397ms]
Jul 29 16:11:27.501: INFO: Got endpoints: latency-svc-f2jlf [782.940647ms]
Jul 29 16:11:27.501: INFO: Created: latency-svc-98bq5
Jul 29 16:11:27.513: INFO: Got endpoints: latency-svc-4pkq2 [751.012276ms]
Jul 29 16:11:27.534: INFO: Created: latency-svc-qw2hp
Jul 29 16:11:27.545: INFO: Created: latency-svc-x64jt
Jul 29 16:11:27.581: INFO: Got endpoints: latency-svc-jf8n2 [769.062454ms]
Jul 29 16:11:27.614: INFO: Got endpoints: latency-svc-9c5pq [748.340643ms]
Jul 29 16:11:27.623: INFO: Created: latency-svc-5hd9c
Jul 29 16:11:27.642: INFO: Created: latency-svc-5qn9h
Jul 29 16:11:27.686: INFO: Got endpoints: latency-svc-gvnsw [774.748391ms]
Jul 29 16:11:27.711: INFO: Created: latency-svc-7zmjz
Jul 29 16:11:27.812: INFO: Got endpoints: latency-svc-pm8cp [846.715297ms]
Jul 29 16:11:27.833: INFO: Created: latency-svc-2xscg
Jul 29 16:11:27.870: INFO: Got endpoints: latency-svc-fdz46 [858.707669ms]
Jul 29 16:11:27.887: INFO: Created: latency-svc-k7qcz
Jul 29 16:11:27.917: INFO: Got endpoints: latency-svc-z8qjd [854.730355ms]
Jul 29 16:11:27.953: INFO: Created: latency-svc-4c67w
Jul 29 16:11:27.975: INFO: Got endpoints: latency-svc-dczdk [866.653597ms]
Jul 29 16:11:28.011: INFO: Created: latency-svc-vn4c7
Jul 29 16:11:28.059: INFO: Got endpoints: latency-svc-pzxm9 [888.054532ms]
Jul 29 16:11:28.094: INFO: Created: latency-svc-p8rxl
Jul 29 16:11:28.111: INFO: Got endpoints: latency-svc-9kp22 [901.215666ms]
Jul 29 16:11:28.130: INFO: Created: latency-svc-snnjc
Jul 29 16:11:28.162: INFO: Got endpoints: latency-svc-l9qxn [895.518295ms]
Jul 29 16:11:28.187: INFO: Created: latency-svc-dxrq2
Jul 29 16:11:28.213: INFO: Got endpoints: latency-svc-78spc [888.930848ms]
Jul 29 16:11:28.235: INFO: Created: latency-svc-6fpbg
Jul 29 16:11:28.271: INFO: Got endpoints: latency-svc-6dxcv [910.101234ms]
Jul 29 16:11:28.289: INFO: Created: latency-svc-bqhqz
Jul 29 16:11:28.310: INFO: Got endpoints: latency-svc-98bq5 [878.911799ms]
Jul 29 16:11:28.330: INFO: Created: latency-svc-q5f2w
Jul 29 16:11:28.362: INFO: Got endpoints: latency-svc-qw2hp [861.44802ms]
Jul 29 16:11:28.388: INFO: Created: latency-svc-ftw2g
Jul 29 16:11:28.417: INFO: Got endpoints: latency-svc-x64jt [903.921251ms]
Jul 29 16:11:28.442: INFO: Created: latency-svc-wj78q
Jul 29 16:11:28.468: INFO: Got endpoints: latency-svc-5hd9c [886.651276ms]
Jul 29 16:11:28.501: INFO: Created: latency-svc-htp7m
Jul 29 16:11:28.527: INFO: Got endpoints: latency-svc-5qn9h [913.034274ms]
Jul 29 16:11:28.554: INFO: Created: latency-svc-7x8fz
Jul 29 16:11:28.564: INFO: Got endpoints: latency-svc-7zmjz [877.188106ms]
Jul 29 16:11:28.584: INFO: Created: latency-svc-lb9sg
Jul 29 16:11:28.611: INFO: Got endpoints: latency-svc-2xscg [798.623337ms]
Jul 29 16:11:28.636: INFO: Created: latency-svc-ppn2q
Jul 29 16:11:28.674: INFO: Got endpoints: latency-svc-k7qcz [802.809222ms]
Jul 29 16:11:28.701: INFO: Created: latency-svc-mpp25
Jul 29 16:11:28.718: INFO: Got endpoints: latency-svc-4c67w [799.712701ms]
Jul 29 16:11:28.735: INFO: Created: latency-svc-w4w2w
Jul 29 16:11:28.758: INFO: Got endpoints: latency-svc-vn4c7 [783.002688ms]
Jul 29 16:11:28.781: INFO: Created: latency-svc-kb8xs
Jul 29 16:11:28.813: INFO: Got endpoints: latency-svc-p8rxl [753.214572ms]
Jul 29 16:11:28.832: INFO: Created: latency-svc-h8db8
Jul 29 16:11:28.861: INFO: Got endpoints: latency-svc-snnjc [749.275272ms]
Jul 29 16:11:28.883: INFO: Created: latency-svc-mds87
Jul 29 16:11:28.912: INFO: Got endpoints: latency-svc-dxrq2 [750.41341ms]
Jul 29 16:11:28.936: INFO: Created: latency-svc-zft8d
Jul 29 16:11:28.962: INFO: Got endpoints: latency-svc-6fpbg [749.237572ms]
Jul 29 16:11:28.987: INFO: Created: latency-svc-22crk
Jul 29 16:11:29.019: INFO: Got endpoints: latency-svc-bqhqz [747.90424ms]
Jul 29 16:11:29.048: INFO: Created: latency-svc-9j2mv
Jul 29 16:11:29.064: INFO: Got endpoints: latency-svc-q5f2w [753.446349ms]
Jul 29 16:11:29.099: INFO: Created: latency-svc-b92gt
Jul 29 16:11:29.111: INFO: Got endpoints: latency-svc-ftw2g [748.454529ms]
Jul 29 16:11:29.220: INFO: Got endpoints: latency-svc-wj78q [802.23187ms]
Jul 29 16:11:29.221: INFO: Got endpoints: latency-svc-htp7m [752.314602ms]
Jul 29 16:11:29.235: INFO: Created: latency-svc-pdn4d
Jul 29 16:11:29.266: INFO: Created: latency-svc-dtzfq
Jul 29 16:11:29.273: INFO: Got endpoints: latency-svc-7x8fz [746.403932ms]
Jul 29 16:11:29.284: INFO: Created: latency-svc-4bkql
Jul 29 16:11:29.307: INFO: Created: latency-svc-4b7l2
Jul 29 16:11:29.324: INFO: Got endpoints: latency-svc-lb9sg [760.406865ms]
Jul 29 16:11:29.347: INFO: Created: latency-svc-m5ljl
Jul 29 16:11:29.360: INFO: Got endpoints: latency-svc-ppn2q [748.9196ms]
Jul 29 16:11:29.379: INFO: Created: latency-svc-zxzt4
Jul 29 16:11:29.409: INFO: Got endpoints: latency-svc-mpp25 [734.799261ms]
Jul 29 16:11:29.439: INFO: Created: latency-svc-6kh6g
Jul 29 16:11:29.469: INFO: Got endpoints: latency-svc-w4w2w [751.273248ms]
Jul 29 16:11:29.485: INFO: Created: latency-svc-nghrx
Jul 29 16:11:29.508: INFO: Got endpoints: latency-svc-kb8xs [749.558707ms]
Jul 29 16:11:29.525: INFO: Created: latency-svc-pkg9c
Jul 29 16:11:29.559: INFO: Got endpoints: latency-svc-h8db8 [746.51949ms]
Jul 29 16:11:29.584: INFO: Created: latency-svc-fmvbs
Jul 29 16:11:29.615: INFO: Got endpoints: latency-svc-mds87 [754.126013ms]
Jul 29 16:11:29.638: INFO: Created: latency-svc-dj9b8
Jul 29 16:11:29.660: INFO: Got endpoints: latency-svc-zft8d [746.615765ms]
Jul 29 16:11:29.678: INFO: Created: latency-svc-72zff
Jul 29 16:11:29.711: INFO: Got endpoints: latency-svc-22crk [748.257607ms]
Jul 29 16:11:29.730: INFO: Created: latency-svc-m78bj
Jul 29 16:11:29.759: INFO: Got endpoints: latency-svc-9j2mv [739.491302ms]
Jul 29 16:11:29.777: INFO: Created: latency-svc-fnhjv
Jul 29 16:11:29.818: INFO: Got endpoints: latency-svc-b92gt [754.040157ms]
Jul 29 16:11:29.836: INFO: Created: latency-svc-mn6zw
Jul 29 16:11:29.860: INFO: Got endpoints: latency-svc-pdn4d [748.607355ms]
Jul 29 16:11:29.877: INFO: Created: latency-svc-pqqxt
Jul 29 16:11:29.914: INFO: Got endpoints: latency-svc-dtzfq [693.835807ms]
Jul 29 16:11:29.929: INFO: Created: latency-svc-mwbkl
Jul 29 16:11:29.960: INFO: Got endpoints: latency-svc-4bkql [739.176887ms]
Jul 29 16:11:29.984: INFO: Created: latency-svc-bqb28
Jul 29 16:11:30.013: INFO: Got endpoints: latency-svc-4b7l2 [739.865001ms]
Jul 29 16:11:30.035: INFO: Created: latency-svc-s5rrm
Jul 29 16:11:30.061: INFO: Got endpoints: latency-svc-m5ljl [736.278208ms]
Jul 29 16:11:30.077: INFO: Created: latency-svc-ktw8l
Jul 29 16:11:30.115: INFO: Got endpoints: latency-svc-zxzt4 [755.586756ms]
Jul 29 16:11:30.133: INFO: Created: latency-svc-vrmnw
Jul 29 16:11:30.166: INFO: Got endpoints: latency-svc-6kh6g [756.655065ms]
Jul 29 16:11:30.184: INFO: Created: latency-svc-zkswt
Jul 29 16:11:30.213: INFO: Got endpoints: latency-svc-nghrx [743.344125ms]
Jul 29 16:11:30.232: INFO: Created: latency-svc-z8x5g
Jul 29 16:11:30.261: INFO: Got endpoints: latency-svc-pkg9c [753.019868ms]
Jul 29 16:11:30.281: INFO: Created: latency-svc-xrkbs
Jul 29 16:11:30.313: INFO: Got endpoints: latency-svc-fmvbs [753.211049ms]
Jul 29 16:11:30.330: INFO: Created: latency-svc-lmfbk
Jul 29 16:11:30.363: INFO: Got endpoints: latency-svc-dj9b8 [746.963026ms]
Jul 29 16:11:30.384: INFO: Created: latency-svc-r6dzn
Jul 29 16:11:30.413: INFO: Got endpoints: latency-svc-72zff [753.641ms]
Jul 29 16:11:30.435: INFO: Created: latency-svc-7qggv
Jul 29 16:11:30.460: INFO: Got endpoints: latency-svc-m78bj [749.018905ms]
Jul 29 16:11:30.481: INFO: Created: latency-svc-ln7hq
Jul 29 16:11:30.511: INFO: Got endpoints: latency-svc-fnhjv [751.764388ms]
Jul 29 16:11:30.530: INFO: Created: latency-svc-s8z78
Jul 29 16:11:30.563: INFO: Got endpoints: latency-svc-mn6zw [744.870596ms]
Jul 29 16:11:30.583: INFO: Created: latency-svc-glw2f
Jul 29 16:11:30.617: INFO: Got endpoints: latency-svc-pqqxt [756.630036ms]
Jul 29 16:11:30.646: INFO: Created: latency-svc-x6p98
Jul 29 16:11:30.668: INFO: Got endpoints: latency-svc-mwbkl [754.098838ms]
Jul 29 16:11:30.758: INFO: Created: latency-svc-7grnw
Jul 29 16:11:30.767: INFO: Got endpoints: latency-svc-bqb28 [806.719242ms]
Jul 29 16:11:30.770: INFO: Got endpoints: latency-svc-s5rrm [756.013752ms]
Jul 29 16:11:30.788: INFO: Created: latency-svc-mj6x5
Jul 29 16:11:30.803: INFO: Created: latency-svc-7s4m9
Jul 29 16:11:30.815: INFO: Got endpoints: latency-svc-ktw8l [753.863391ms]
Jul 29 16:11:30.836: INFO: Created: latency-svc-c9pf9
Jul 29 16:11:30.862: INFO: Got endpoints: latency-svc-vrmnw [745.096526ms]
Jul 29 16:11:30.876: INFO: Created: latency-svc-5ctlb
Jul 29 16:11:30.912: INFO: Got endpoints: latency-svc-zkswt [745.63093ms]
Jul 29 16:11:30.937: INFO: Created: latency-svc-5t9mv
Jul 29 16:11:30.961: INFO: Got endpoints: latency-svc-z8x5g [748.075273ms]
Jul 29 16:11:30.982: INFO: Created: latency-svc-mskwd
Jul 29 16:11:31.013: INFO: Got endpoints: latency-svc-xrkbs [750.779743ms]
Jul 29 16:11:31.045: INFO: Created: latency-svc-66rxh
Jul 29 16:11:31.069: INFO: Got endpoints: latency-svc-lmfbk [756.314788ms]
Jul 29 16:11:31.097: INFO: Created: latency-svc-6lh99
Jul 29 16:11:31.116: INFO: Got endpoints: latency-svc-r6dzn [753.391187ms]
Jul 29 16:11:31.139: INFO: Created: latency-svc-r76sz
Jul 29 16:11:31.168: INFO: Got endpoints: latency-svc-7qggv [754.042005ms]
Jul 29 16:11:31.184: INFO: Created: latency-svc-fdc6s
Jul 29 16:11:31.215: INFO: Got endpoints: latency-svc-ln7hq [754.619172ms]
Jul 29 16:11:31.231: INFO: Created: latency-svc-wd9hb
Jul 29 16:11:31.277: INFO: Got endpoints: latency-svc-s8z78 [766.328956ms]
Jul 29 16:11:31.308: INFO: Created: latency-svc-wjcqz
Jul 29 16:11:31.325: INFO: Got endpoints: latency-svc-glw2f [761.901614ms]
Jul 29 16:11:31.349: INFO: Created: latency-svc-wqvtv
Jul 29 16:11:31.361: INFO: Got endpoints: latency-svc-x6p98 [744.116757ms]
Jul 29 16:11:31.385: INFO: Created: latency-svc-c2smw
Jul 29 16:11:31.410: INFO: Got endpoints: latency-svc-7grnw [741.696306ms]
Jul 29 16:11:31.433: INFO: Created: latency-svc-j77vt
Jul 29 16:11:31.462: INFO: Got endpoints: latency-svc-mj6x5 [692.033694ms]
Jul 29 16:11:31.485: INFO: Created: latency-svc-n4hx9
Jul 29 16:11:31.509: INFO: Got endpoints: latency-svc-7s4m9 [741.427383ms]
Jul 29 16:11:31.531: INFO: Created: latency-svc-6rq5q
Jul 29 16:11:31.563: INFO: Got endpoints: latency-svc-c9pf9 [748.046501ms]
Jul 29 16:11:31.588: INFO: Created: latency-svc-jx4dn
Jul 29 16:11:31.614: INFO: Got endpoints: latency-svc-5ctlb [752.497348ms]
Jul 29 16:11:31.640: INFO: Created: latency-svc-m77gf
Jul 29 16:11:31.665: INFO: Got endpoints: latency-svc-5t9mv [753.429113ms]
Jul 29 16:11:31.688: INFO: Created: latency-svc-r687q
Jul 29 16:11:31.711: INFO: Got endpoints: latency-svc-mskwd [749.805915ms]
Jul 29 16:11:31.739: INFO: Created: latency-svc-z579n
Jul 29 16:11:31.766: INFO: Got endpoints: latency-svc-66rxh [752.25607ms]
Jul 29 16:11:31.796: INFO: Created: latency-svc-wzvnj
Jul 29 16:11:31.817: INFO: Got endpoints: latency-svc-6lh99 [747.478258ms]
Jul 29 16:11:31.843: INFO: Created: latency-svc-wr687
Jul 29 16:11:31.868: INFO: Got endpoints: latency-svc-r76sz [752.166567ms]
Jul 29 16:11:31.889: INFO: Created: latency-svc-4lw5t
Jul 29 16:11:31.919: INFO: Got endpoints: latency-svc-fdc6s [750.746415ms]
Jul 29 16:11:31.961: INFO: Got endpoints: latency-svc-wd9hb [746.647898ms]
Jul 29 16:11:32.012: INFO: Got endpoints: latency-svc-wjcqz [735.110301ms]
Jul 29 16:11:32.071: INFO: Got endpoints: latency-svc-wqvtv [746.274178ms]
Jul 29 16:11:32.115: INFO: Got endpoints: latency-svc-c2smw [753.790122ms]
Jul 29 16:11:32.163: INFO: Got endpoints: latency-svc-j77vt [753.254888ms]
Jul 29 16:11:32.210: INFO: Got endpoints: latency-svc-n4hx9 [748.098932ms]
Jul 29 16:11:32.261: INFO: Got endpoints: latency-svc-6rq5q [752.177128ms]
Jul 29 16:11:32.339: INFO: Got endpoints: latency-svc-jx4dn [775.15788ms]
Jul 29 16:11:32.366: INFO: Got endpoints: latency-svc-m77gf [751.924744ms]
Jul 29 16:11:32.420: INFO: Got endpoints: latency-svc-r687q [754.802991ms]
Jul 29 16:11:32.496: INFO: Got endpoints: latency-svc-z579n [784.566526ms]
Jul 29 16:11:32.513: INFO: Got endpoints: latency-svc-wzvnj [747.289954ms]
Jul 29 16:11:32.564: INFO: Got endpoints: latency-svc-wr687 [747.708493ms]
Jul 29 16:11:32.619: INFO: Got endpoints: latency-svc-4lw5t [750.651464ms]
Jul 29 16:11:32.619: INFO: Latencies: [41.107874ms 73.850272ms 88.798377ms 160.732454ms 171.807184ms 172.03353ms 175.329768ms 198.120745ms 203.229354ms 213.070521ms 217.304772ms 218.108223ms 226.425733ms 249.065977ms 255.164934ms 262.836815ms 271.262459ms 271.624065ms 272.669817ms 272.918428ms 273.725892ms 277.112504ms 278.425185ms 286.68961ms 286.937325ms 289.104743ms 289.957888ms 296.032199ms 296.301054ms 299.015794ms 301.442046ms 302.691327ms 303.690214ms 304.585733ms 306.975156ms 312.559484ms 317.142192ms 321.316043ms 324.42821ms 330.55686ms 378.097403ms 381.778306ms 384.233723ms 387.131016ms 402.744606ms 403.261659ms 412.937963ms 429.206402ms 429.78959ms 430.141421ms 430.224143ms 430.89577ms 438.272002ms 463.916407ms 467.672315ms 498.361282ms 520.191355ms 536.325865ms 538.315845ms 538.54758ms 540.253047ms 550.443612ms 598.629422ms 605.965876ms 623.902163ms 643.413568ms 692.033694ms 693.835807ms 695.251138ms 708.998333ms 712.292295ms 719.781808ms 723.978574ms 728.827523ms 734.799261ms 735.110301ms 736.278208ms 738.371787ms 739.176887ms 739.491302ms 739.865001ms 741.427383ms 741.438006ms 741.696306ms 741.946323ms 742.912028ms 743.344125ms 744.116757ms 744.870596ms 745.096526ms 745.306373ms 745.63093ms 746.274178ms 746.403932ms 746.419901ms 746.51949ms 746.615765ms 746.647898ms 746.713311ms 746.963026ms 747.289954ms 747.478258ms 747.708493ms 747.90424ms 747.916596ms 747.950866ms 748.046501ms 748.058244ms 748.075273ms 748.098932ms 748.170799ms 748.257607ms 748.340643ms 748.454529ms 748.607355ms 748.9196ms 749.018905ms 749.031687ms 749.237572ms 749.275272ms 749.376995ms 749.558707ms 749.805915ms 750.41341ms 750.419878ms 750.584556ms 750.651464ms 750.746415ms 750.779743ms 750.868549ms 751.012276ms 751.273248ms 751.525939ms 751.60029ms 751.764388ms 751.924744ms 752.166567ms 752.177128ms 752.25607ms 752.314602ms 752.497348ms 753.019868ms 753.211049ms 753.214572ms 753.254888ms 753.391187ms 753.429113ms 753.446349ms 753.621931ms 753.641ms 753.790122ms 753.863391ms 754.040157ms 754.042005ms 754.098838ms 754.126013ms 754.619172ms 754.802991ms 755.295593ms 755.586756ms 756.013752ms 756.314788ms 756.630036ms 756.655065ms 758.262316ms 759.245397ms 759.339912ms 759.498915ms 760.21245ms 760.406865ms 761.901614ms 766.328956ms 769.062454ms 774.748391ms 775.15788ms 782.940647ms 783.002688ms 784.566526ms 798.623337ms 799.712701ms 800.791784ms 802.23187ms 802.809222ms 806.719242ms 846.715297ms 848.473583ms 854.730355ms 858.707669ms 861.44802ms 866.653597ms 877.188106ms 878.911799ms 886.651276ms 888.054532ms 888.930848ms 895.518295ms 901.215666ms 903.921251ms 910.101234ms 913.034274ms]
Jul 29 16:11:32.619: INFO: 50 %ile: 747.289954ms
Jul 29 16:11:32.619: INFO: 90 %ile: 800.791784ms
Jul 29 16:11:32.619: INFO: 99 %ile: 910.101234ms
Jul 29 16:11:32.620: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  test/e2e/framework/framework.go:187
Jul 29 16:11:32.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-5126" for this suite. 07/29/23 16:11:32.639
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","completed":163,"skipped":3185,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.994 seconds]
[sig-network] Service endpoints latency
test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Service endpoints latency
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:11:21.663
    Jul 29 16:11:21.664: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename svc-latency 07/29/23 16:11:21.666
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:11:21.694
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:11:21.699
    [It] should not be very high  [Conformance]
      test/e2e/network/service_latency.go:59
    Jul 29 16:11:21.703: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: creating replication controller svc-latency-rc in namespace svc-latency-5126 07/29/23 16:11:21.705
    I0729 16:11:21.719168      13 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-5126, replica count: 1
    I0729 16:11:22.771948      13 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0729 16:11:23.772268      13 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jul 29 16:11:23.900: INFO: Created: latency-svc-gq9qs
    Jul 29 16:11:23.912: INFO: Got endpoints: latency-svc-gq9qs [38.579643ms]
    Jul 29 16:11:23.952: INFO: Created: latency-svc-vkksr
    Jul 29 16:11:23.957: INFO: Got endpoints: latency-svc-vkksr [41.107874ms]
    Jul 29 16:11:23.975: INFO: Created: latency-svc-pxcp2
    Jul 29 16:11:23.991: INFO: Got endpoints: latency-svc-pxcp2 [73.850272ms]
    Jul 29 16:11:24.001: INFO: Created: latency-svc-9pwdk
    Jul 29 16:11:24.008: INFO: Got endpoints: latency-svc-9pwdk [88.798377ms]
    Jul 29 16:11:24.054: INFO: Created: latency-svc-spcz4
    Jul 29 16:11:24.078: INFO: Created: latency-svc-c89k9
    Jul 29 16:11:24.079: INFO: Got endpoints: latency-svc-spcz4 [160.732454ms]
    Jul 29 16:11:24.094: INFO: Got endpoints: latency-svc-c89k9 [175.329768ms]
    Jul 29 16:11:24.107: INFO: Created: latency-svc-gwrs4
    Jul 29 16:11:24.122: INFO: Got endpoints: latency-svc-gwrs4 [203.229354ms]
    Jul 29 16:11:24.252: INFO: Created: latency-svc-tmqmw
    Jul 29 16:11:24.295: INFO: Created: latency-svc-nt7tx
    Jul 29 16:11:24.295: INFO: Created: latency-svc-vddj8
    Jul 29 16:11:24.296: INFO: Created: latency-svc-4ndjf
    Jul 29 16:11:24.296: INFO: Created: latency-svc-4kn9f
    Jul 29 16:11:24.297: INFO: Created: latency-svc-5cdmt
    Jul 29 16:11:24.297: INFO: Created: latency-svc-x476s
    Jul 29 16:11:24.297: INFO: Created: latency-svc-dd5rn
    Jul 29 16:11:24.297: INFO: Created: latency-svc-b75cx
    Jul 29 16:11:24.297: INFO: Got endpoints: latency-svc-tmqmw [218.108223ms]
    Jul 29 16:11:24.297: INFO: Created: latency-svc-pqdd5
    Jul 29 16:11:24.303: INFO: Created: latency-svc-26hv6
    Jul 29 16:11:24.306: INFO: Created: latency-svc-b6kk2
    Jul 29 16:11:24.306: INFO: Created: latency-svc-t9zdv
    Jul 29 16:11:24.306: INFO: Created: latency-svc-zqnvf
    Jul 29 16:11:24.307: INFO: Created: latency-svc-k2lb5
    Jul 29 16:11:24.323: INFO: Got endpoints: latency-svc-4kn9f [402.744606ms]
    Jul 29 16:11:24.349: INFO: Got endpoints: latency-svc-vddj8 [429.206402ms]
    Jul 29 16:11:24.349: INFO: Got endpoints: latency-svc-4ndjf [430.141421ms]
    Jul 29 16:11:24.350: INFO: Got endpoints: latency-svc-t9zdv [430.89577ms]
    Jul 29 16:11:24.350: INFO: Got endpoints: latency-svc-5cdmt [429.78959ms]
    Jul 29 16:11:24.372: INFO: Created: latency-svc-v82x6
    Jul 29 16:11:24.384: INFO: Got endpoints: latency-svc-k2lb5 [463.916407ms]
    Jul 29 16:11:24.395: INFO: Got endpoints: latency-svc-26hv6 [403.261659ms]
    Jul 29 16:11:24.395: INFO: Got endpoints: latency-svc-zqnvf [387.131016ms]
    Jul 29 16:11:24.396: INFO: Got endpoints: latency-svc-nt7tx [273.725892ms]
    Jul 29 16:11:24.396: INFO: Got endpoints: latency-svc-b6kk2 [438.272002ms]
    Jul 29 16:11:24.402: INFO: Created: latency-svc-jj42c
    Jul 29 16:11:24.440: INFO: Got endpoints: latency-svc-dd5rn [520.191355ms]
    Jul 29 16:11:24.455: INFO: Got endpoints: latency-svc-pqdd5 [536.325865ms]
    Jul 29 16:11:24.471: INFO: Got endpoints: latency-svc-x476s [550.443612ms]
    Jul 29 16:11:24.472: INFO: Got endpoints: latency-svc-b75cx [378.097403ms]
    Jul 29 16:11:24.494: INFO: Created: latency-svc-gdhs4
    Jul 29 16:11:24.496: INFO: Got endpoints: latency-svc-jj42c [172.03353ms]
    Jul 29 16:11:24.496: INFO: Got endpoints: latency-svc-v82x6 [198.120745ms]
    Jul 29 16:11:24.521: INFO: Got endpoints: latency-svc-gdhs4 [171.807184ms]
    Jul 29 16:11:24.523: INFO: Created: latency-svc-tr4sv
    Jul 29 16:11:24.543: INFO: Created: latency-svc-rthl6
    Jul 29 16:11:24.564: INFO: Got endpoints: latency-svc-tr4sv [213.070521ms]
    Jul 29 16:11:24.567: INFO: Got endpoints: latency-svc-rthl6 [217.304772ms]
    Jul 29 16:11:24.580: INFO: Created: latency-svc-l8dpb
    Jul 29 16:11:24.601: INFO: Created: latency-svc-2nnml
    Jul 29 16:11:24.613: INFO: Got endpoints: latency-svc-l8dpb [262.836815ms]
    Jul 29 16:11:24.621: INFO: Got endpoints: latency-svc-2nnml [226.425733ms]
    Jul 29 16:11:24.631: INFO: Created: latency-svc-jhztd
    Jul 29 16:11:24.645: INFO: Got endpoints: latency-svc-jhztd [249.065977ms]
    Jul 29 16:11:24.651: INFO: Created: latency-svc-n6wpr
    Jul 29 16:11:24.668: INFO: Created: latency-svc-hkw9j
    Jul 29 16:11:24.673: INFO: Got endpoints: latency-svc-n6wpr [277.112504ms]
    Jul 29 16:11:24.686: INFO: Got endpoints: latency-svc-hkw9j [289.957888ms]
    Jul 29 16:11:24.688: INFO: Created: latency-svc-8nq25
    Jul 29 16:11:24.709: INFO: Got endpoints: latency-svc-8nq25 [324.42821ms]
    Jul 29 16:11:24.716: INFO: Created: latency-svc-2wj59
    Jul 29 16:11:24.727: INFO: Got endpoints: latency-svc-2wj59 [286.937325ms]
    Jul 29 16:11:24.728: INFO: Created: latency-svc-5cnd4
    Jul 29 16:11:24.733: INFO: Got endpoints: latency-svc-5cnd4 [278.425185ms]
    Jul 29 16:11:24.751: INFO: Created: latency-svc-g4pw5
    Jul 29 16:11:24.766: INFO: Created: latency-svc-t2mzl
    Jul 29 16:11:24.768: INFO: Got endpoints: latency-svc-g4pw5 [296.032199ms]
    Jul 29 16:11:24.775: INFO: Got endpoints: latency-svc-t2mzl [302.691327ms]
    Jul 29 16:11:24.785: INFO: Created: latency-svc-tzvvx
    Jul 29 16:11:24.803: INFO: Created: latency-svc-66crd
    Jul 29 16:11:24.808: INFO: Got endpoints: latency-svc-tzvvx [312.559484ms]
    Jul 29 16:11:24.817: INFO: Got endpoints: latency-svc-66crd [321.316043ms]
    Jul 29 16:11:24.819: INFO: Created: latency-svc-j2bxb
    Jul 29 16:11:24.826: INFO: Got endpoints: latency-svc-j2bxb [304.585733ms]
    Jul 29 16:11:24.834: INFO: Created: latency-svc-82d9j
    Jul 29 16:11:24.836: INFO: Got endpoints: latency-svc-82d9j [271.624065ms]
    Jul 29 16:11:24.846: INFO: Created: latency-svc-skskr
    Jul 29 16:11:24.854: INFO: Got endpoints: latency-svc-skskr [286.68961ms]
    Jul 29 16:11:24.869: INFO: Created: latency-svc-knh7v
    Jul 29 16:11:24.878: INFO: Created: latency-svc-9tf2b
    Jul 29 16:11:24.884: INFO: Got endpoints: latency-svc-knh7v [271.262459ms]
    Jul 29 16:11:24.895: INFO: Got endpoints: latency-svc-9tf2b [272.918428ms]
    Jul 29 16:11:24.897: INFO: Created: latency-svc-8lk5h
    Jul 29 16:11:24.917: INFO: Created: latency-svc-chtn8
    Jul 29 16:11:24.918: INFO: Got endpoints: latency-svc-8lk5h [272.669817ms]
    Jul 29 16:11:24.928: INFO: Got endpoints: latency-svc-chtn8 [255.164934ms]
    Jul 29 16:11:24.952: INFO: Created: latency-svc-l4h7h
    Jul 29 16:11:24.975: INFO: Created: latency-svc-255nm
    Jul 29 16:11:24.975: INFO: Got endpoints: latency-svc-l4h7h [289.104743ms]
    Jul 29 16:11:25.000: INFO: Created: latency-svc-vnnfp
    Jul 29 16:11:25.008: INFO: Got endpoints: latency-svc-255nm [299.015794ms]
    Jul 29 16:11:25.024: INFO: Got endpoints: latency-svc-vnnfp [296.301054ms]
    Jul 29 16:11:25.026: INFO: Created: latency-svc-vhnkd
    Jul 29 16:11:25.041: INFO: Got endpoints: latency-svc-vhnkd [306.975156ms]
    Jul 29 16:11:25.060: INFO: Created: latency-svc-d9zrx
    Jul 29 16:11:25.069: INFO: Got endpoints: latency-svc-d9zrx [301.442046ms]
    Jul 29 16:11:25.078: INFO: Created: latency-svc-zfjtd
    Jul 29 16:11:25.079: INFO: Got endpoints: latency-svc-zfjtd [303.690214ms]
    Jul 29 16:11:25.091: INFO: Created: latency-svc-rh7ww
    Jul 29 16:11:25.111: INFO: Created: latency-svc-lqs8x
    Jul 29 16:11:25.126: INFO: Got endpoints: latency-svc-rh7ww [317.142192ms]
    Jul 29 16:11:25.148: INFO: Got endpoints: latency-svc-lqs8x [330.55686ms]
    Jul 29 16:11:25.152: INFO: Created: latency-svc-pz9z7
    Jul 29 16:11:25.205: INFO: Created: latency-svc-lctxf
    Jul 29 16:11:25.211: INFO: Got endpoints: latency-svc-pz9z7 [384.233723ms]
    Jul 29 16:11:25.219: INFO: Got endpoints: latency-svc-lctxf [381.778306ms]
    Jul 29 16:11:25.230: INFO: Created: latency-svc-bg9cb
    Jul 29 16:11:25.241: INFO: Created: latency-svc-wgb5s
    Jul 29 16:11:25.260: INFO: Created: latency-svc-bxhmk
    Jul 29 16:11:25.267: INFO: Got endpoints: latency-svc-bg9cb [412.937963ms]
    Jul 29 16:11:25.284: INFO: Created: latency-svc-8fl4k
    Jul 29 16:11:25.291: INFO: Created: latency-svc-lz7fc
    Jul 29 16:11:25.298: INFO: Created: latency-svc-skd85
    Jul 29 16:11:25.315: INFO: Got endpoints: latency-svc-wgb5s [430.224143ms]
    Jul 29 16:11:25.321: INFO: Created: latency-svc-lxbkh
    Jul 29 16:11:25.334: INFO: Created: latency-svc-dbrbn
    Jul 29 16:11:25.346: INFO: Created: latency-svc-cxhlq
    Jul 29 16:11:25.359: INFO: Created: latency-svc-25sw7
    Jul 29 16:11:25.362: INFO: Got endpoints: latency-svc-bxhmk [467.672315ms]
    Jul 29 16:11:25.383: INFO: Created: latency-svc-4hbmp
    Jul 29 16:11:25.389: INFO: Created: latency-svc-m4pjg
    Jul 29 16:11:25.404: INFO: Created: latency-svc-9bttm
    Jul 29 16:11:25.416: INFO: Got endpoints: latency-svc-8fl4k [498.361282ms]
    Jul 29 16:11:25.418: INFO: Created: latency-svc-p9wdp
    Jul 29 16:11:25.430: INFO: Created: latency-svc-d2zv8
    Jul 29 16:11:25.466: INFO: Created: latency-svc-kfwj2
    Jul 29 16:11:25.466: INFO: Got endpoints: latency-svc-lz7fc [538.315845ms]
    Jul 29 16:11:25.473: INFO: Created: latency-svc-kkhx2
    Jul 29 16:11:25.475: INFO: Created: latency-svc-q9z88
    Jul 29 16:11:25.476: INFO: Created: latency-svc-nfzqr
    Jul 29 16:11:25.489: INFO: Created: latency-svc-htbl7
    Jul 29 16:11:25.514: INFO: Got endpoints: latency-svc-skd85 [538.54758ms]
    Jul 29 16:11:25.535: INFO: Created: latency-svc-zdzv9
    Jul 29 16:11:25.564: INFO: Got endpoints: latency-svc-lxbkh [540.253047ms]
    Jul 29 16:11:25.587: INFO: Created: latency-svc-shl2r
    Jul 29 16:11:25.614: INFO: Got endpoints: latency-svc-dbrbn [605.965876ms]
    Jul 29 16:11:25.640: INFO: Created: latency-svc-j8jjx
    Jul 29 16:11:25.665: INFO: Got endpoints: latency-svc-cxhlq [623.902163ms]
    Jul 29 16:11:25.693: INFO: Created: latency-svc-dzcwj
    Jul 29 16:11:25.713: INFO: Got endpoints: latency-svc-25sw7 [643.413568ms]
    Jul 29 16:11:25.730: INFO: Created: latency-svc-7twlx
    Jul 29 16:11:25.791: INFO: Got endpoints: latency-svc-4hbmp [712.292295ms]
    Jul 29 16:11:25.849: INFO: Created: latency-svc-wch5k
    Jul 29 16:11:25.868: INFO: Got endpoints: latency-svc-m4pjg [741.946323ms]
    Jul 29 16:11:25.872: INFO: Got endpoints: latency-svc-9bttm [723.978574ms]
    Jul 29 16:11:25.891: INFO: Created: latency-svc-qmfjp
    Jul 29 16:11:25.897: INFO: Created: latency-svc-tjz54
    Jul 29 16:11:25.920: INFO: Got endpoints: latency-svc-p9wdp [708.998333ms]
    Jul 29 16:11:25.947: INFO: Created: latency-svc-qsdfb
    Jul 29 16:11:25.967: INFO: Got endpoints: latency-svc-d2zv8 [748.170799ms]
    Jul 29 16:11:25.994: INFO: Created: latency-svc-dk6bn
    Jul 29 16:11:26.015: INFO: Got endpoints: latency-svc-kfwj2 [598.629422ms]
    Jul 29 16:11:26.035: INFO: Created: latency-svc-jq86h
    Jul 29 16:11:26.073: INFO: Got endpoints: latency-svc-kkhx2 [758.262316ms]
    Jul 29 16:11:26.106: INFO: Created: latency-svc-x6hd8
    Jul 29 16:11:26.116: INFO: Got endpoints: latency-svc-q9z88 [848.473583ms]
    Jul 29 16:11:26.144: INFO: Created: latency-svc-f9q9x
    Jul 29 16:11:26.163: INFO: Got endpoints: latency-svc-nfzqr [800.791784ms]
    Jul 29 16:11:26.198: INFO: Created: latency-svc-ng9gd
    Jul 29 16:11:26.214: INFO: Got endpoints: latency-svc-htbl7 [747.916596ms]
    Jul 29 16:11:26.238: INFO: Created: latency-svc-w5vqt
    Jul 29 16:11:26.263: INFO: Got endpoints: latency-svc-zdzv9 [749.031687ms]
    Jul 29 16:11:26.284: INFO: Created: latency-svc-9dppk
    Jul 29 16:11:26.311: INFO: Got endpoints: latency-svc-shl2r [746.419901ms]
    Jul 29 16:11:26.338: INFO: Created: latency-svc-sg6ln
    Jul 29 16:11:26.366: INFO: Got endpoints: latency-svc-j8jjx [751.60029ms]
    Jul 29 16:11:26.388: INFO: Created: latency-svc-bpnw4
    Jul 29 16:11:26.410: INFO: Got endpoints: latency-svc-dzcwj [745.306373ms]
    Jul 29 16:11:26.434: INFO: Created: latency-svc-nszvf
    Jul 29 16:11:26.467: INFO: Got endpoints: latency-svc-7twlx [753.621931ms]
    Jul 29 16:11:26.487: INFO: Created: latency-svc-s4q5b
    Jul 29 16:11:26.511: INFO: Got endpoints: latency-svc-wch5k [719.781808ms]
    Jul 29 16:11:26.535: INFO: Created: latency-svc-dgt2n
    Jul 29 16:11:26.564: INFO: Got endpoints: latency-svc-qmfjp [695.251138ms]
    Jul 29 16:11:26.583: INFO: Created: latency-svc-hdv7b
    Jul 29 16:11:26.632: INFO: Got endpoints: latency-svc-tjz54 [759.339912ms]
    Jul 29 16:11:26.665: INFO: Created: latency-svc-ng5vd
    Jul 29 16:11:26.672: INFO: Got endpoints: latency-svc-qsdfb [751.525939ms]
    Jul 29 16:11:26.705: INFO: Created: latency-svc-g4mdd
    Jul 29 16:11:26.718: INFO: Got endpoints: latency-svc-dk6bn [750.584556ms]
    Jul 29 16:11:26.752: INFO: Created: latency-svc-f2jlf
    Jul 29 16:11:26.762: INFO: Got endpoints: latency-svc-jq86h [746.713311ms]
    Jul 29 16:11:26.798: INFO: Created: latency-svc-4pkq2
    Jul 29 16:11:26.812: INFO: Got endpoints: latency-svc-x6hd8 [738.371787ms]
    Jul 29 16:11:26.833: INFO: Created: latency-svc-jf8n2
    Jul 29 16:11:26.865: INFO: Got endpoints: latency-svc-f9q9x [749.376995ms]
    Jul 29 16:11:26.881: INFO: Created: latency-svc-9c5pq
    Jul 29 16:11:26.911: INFO: Got endpoints: latency-svc-ng9gd [747.950866ms]
    Jul 29 16:11:26.934: INFO: Created: latency-svc-gvnsw
    Jul 29 16:11:26.965: INFO: Got endpoints: latency-svc-w5vqt [750.419878ms]
    Jul 29 16:11:27.012: INFO: Got endpoints: latency-svc-9dppk [748.058244ms]
    Jul 29 16:11:27.026: INFO: Created: latency-svc-pm8cp
    Jul 29 16:11:27.062: INFO: Got endpoints: latency-svc-sg6ln [750.868549ms]
    Jul 29 16:11:27.073: INFO: Created: latency-svc-fdz46
    Jul 29 16:11:27.091: INFO: Created: latency-svc-z8qjd
    Jul 29 16:11:27.108: INFO: Got endpoints: latency-svc-bpnw4 [741.438006ms]
    Jul 29 16:11:27.137: INFO: Created: latency-svc-dczdk
    Jul 29 16:11:27.171: INFO: Got endpoints: latency-svc-nszvf [760.21245ms]
    Jul 29 16:11:27.186: INFO: Created: latency-svc-pzxm9
    Jul 29 16:11:27.210: INFO: Got endpoints: latency-svc-s4q5b [742.912028ms]
    Jul 29 16:11:27.233: INFO: Created: latency-svc-9kp22
    Jul 29 16:11:27.266: INFO: Got endpoints: latency-svc-dgt2n [755.295593ms]
    Jul 29 16:11:27.286: INFO: Created: latency-svc-l9qxn
    Jul 29 16:11:27.323: INFO: Got endpoints: latency-svc-hdv7b [759.498915ms]
    Jul 29 16:11:27.345: INFO: Created: latency-svc-78spc
    Jul 29 16:11:27.361: INFO: Got endpoints: latency-svc-ng5vd [728.827523ms]
    Jul 29 16:11:27.385: INFO: Created: latency-svc-6dxcv
    Jul 29 16:11:27.431: INFO: Got endpoints: latency-svc-g4mdd [759.245397ms]
    Jul 29 16:11:27.501: INFO: Got endpoints: latency-svc-f2jlf [782.940647ms]
    Jul 29 16:11:27.501: INFO: Created: latency-svc-98bq5
    Jul 29 16:11:27.513: INFO: Got endpoints: latency-svc-4pkq2 [751.012276ms]
    Jul 29 16:11:27.534: INFO: Created: latency-svc-qw2hp
    Jul 29 16:11:27.545: INFO: Created: latency-svc-x64jt
    Jul 29 16:11:27.581: INFO: Got endpoints: latency-svc-jf8n2 [769.062454ms]
    Jul 29 16:11:27.614: INFO: Got endpoints: latency-svc-9c5pq [748.340643ms]
    Jul 29 16:11:27.623: INFO: Created: latency-svc-5hd9c
    Jul 29 16:11:27.642: INFO: Created: latency-svc-5qn9h
    Jul 29 16:11:27.686: INFO: Got endpoints: latency-svc-gvnsw [774.748391ms]
    Jul 29 16:11:27.711: INFO: Created: latency-svc-7zmjz
    Jul 29 16:11:27.812: INFO: Got endpoints: latency-svc-pm8cp [846.715297ms]
    Jul 29 16:11:27.833: INFO: Created: latency-svc-2xscg
    Jul 29 16:11:27.870: INFO: Got endpoints: latency-svc-fdz46 [858.707669ms]
    Jul 29 16:11:27.887: INFO: Created: latency-svc-k7qcz
    Jul 29 16:11:27.917: INFO: Got endpoints: latency-svc-z8qjd [854.730355ms]
    Jul 29 16:11:27.953: INFO: Created: latency-svc-4c67w
    Jul 29 16:11:27.975: INFO: Got endpoints: latency-svc-dczdk [866.653597ms]
    Jul 29 16:11:28.011: INFO: Created: latency-svc-vn4c7
    Jul 29 16:11:28.059: INFO: Got endpoints: latency-svc-pzxm9 [888.054532ms]
    Jul 29 16:11:28.094: INFO: Created: latency-svc-p8rxl
    Jul 29 16:11:28.111: INFO: Got endpoints: latency-svc-9kp22 [901.215666ms]
    Jul 29 16:11:28.130: INFO: Created: latency-svc-snnjc
    Jul 29 16:11:28.162: INFO: Got endpoints: latency-svc-l9qxn [895.518295ms]
    Jul 29 16:11:28.187: INFO: Created: latency-svc-dxrq2
    Jul 29 16:11:28.213: INFO: Got endpoints: latency-svc-78spc [888.930848ms]
    Jul 29 16:11:28.235: INFO: Created: latency-svc-6fpbg
    Jul 29 16:11:28.271: INFO: Got endpoints: latency-svc-6dxcv [910.101234ms]
    Jul 29 16:11:28.289: INFO: Created: latency-svc-bqhqz
    Jul 29 16:11:28.310: INFO: Got endpoints: latency-svc-98bq5 [878.911799ms]
    Jul 29 16:11:28.330: INFO: Created: latency-svc-q5f2w
    Jul 29 16:11:28.362: INFO: Got endpoints: latency-svc-qw2hp [861.44802ms]
    Jul 29 16:11:28.388: INFO: Created: latency-svc-ftw2g
    Jul 29 16:11:28.417: INFO: Got endpoints: latency-svc-x64jt [903.921251ms]
    Jul 29 16:11:28.442: INFO: Created: latency-svc-wj78q
    Jul 29 16:11:28.468: INFO: Got endpoints: latency-svc-5hd9c [886.651276ms]
    Jul 29 16:11:28.501: INFO: Created: latency-svc-htp7m
    Jul 29 16:11:28.527: INFO: Got endpoints: latency-svc-5qn9h [913.034274ms]
    Jul 29 16:11:28.554: INFO: Created: latency-svc-7x8fz
    Jul 29 16:11:28.564: INFO: Got endpoints: latency-svc-7zmjz [877.188106ms]
    Jul 29 16:11:28.584: INFO: Created: latency-svc-lb9sg
    Jul 29 16:11:28.611: INFO: Got endpoints: latency-svc-2xscg [798.623337ms]
    Jul 29 16:11:28.636: INFO: Created: latency-svc-ppn2q
    Jul 29 16:11:28.674: INFO: Got endpoints: latency-svc-k7qcz [802.809222ms]
    Jul 29 16:11:28.701: INFO: Created: latency-svc-mpp25
    Jul 29 16:11:28.718: INFO: Got endpoints: latency-svc-4c67w [799.712701ms]
    Jul 29 16:11:28.735: INFO: Created: latency-svc-w4w2w
    Jul 29 16:11:28.758: INFO: Got endpoints: latency-svc-vn4c7 [783.002688ms]
    Jul 29 16:11:28.781: INFO: Created: latency-svc-kb8xs
    Jul 29 16:11:28.813: INFO: Got endpoints: latency-svc-p8rxl [753.214572ms]
    Jul 29 16:11:28.832: INFO: Created: latency-svc-h8db8
    Jul 29 16:11:28.861: INFO: Got endpoints: latency-svc-snnjc [749.275272ms]
    Jul 29 16:11:28.883: INFO: Created: latency-svc-mds87
    Jul 29 16:11:28.912: INFO: Got endpoints: latency-svc-dxrq2 [750.41341ms]
    Jul 29 16:11:28.936: INFO: Created: latency-svc-zft8d
    Jul 29 16:11:28.962: INFO: Got endpoints: latency-svc-6fpbg [749.237572ms]
    Jul 29 16:11:28.987: INFO: Created: latency-svc-22crk
    Jul 29 16:11:29.019: INFO: Got endpoints: latency-svc-bqhqz [747.90424ms]
    Jul 29 16:11:29.048: INFO: Created: latency-svc-9j2mv
    Jul 29 16:11:29.064: INFO: Got endpoints: latency-svc-q5f2w [753.446349ms]
    Jul 29 16:11:29.099: INFO: Created: latency-svc-b92gt
    Jul 29 16:11:29.111: INFO: Got endpoints: latency-svc-ftw2g [748.454529ms]
    Jul 29 16:11:29.220: INFO: Got endpoints: latency-svc-wj78q [802.23187ms]
    Jul 29 16:11:29.221: INFO: Got endpoints: latency-svc-htp7m [752.314602ms]
    Jul 29 16:11:29.235: INFO: Created: latency-svc-pdn4d
    Jul 29 16:11:29.266: INFO: Created: latency-svc-dtzfq
    Jul 29 16:11:29.273: INFO: Got endpoints: latency-svc-7x8fz [746.403932ms]
    Jul 29 16:11:29.284: INFO: Created: latency-svc-4bkql
    Jul 29 16:11:29.307: INFO: Created: latency-svc-4b7l2
    Jul 29 16:11:29.324: INFO: Got endpoints: latency-svc-lb9sg [760.406865ms]
    Jul 29 16:11:29.347: INFO: Created: latency-svc-m5ljl
    Jul 29 16:11:29.360: INFO: Got endpoints: latency-svc-ppn2q [748.9196ms]
    Jul 29 16:11:29.379: INFO: Created: latency-svc-zxzt4
    Jul 29 16:11:29.409: INFO: Got endpoints: latency-svc-mpp25 [734.799261ms]
    Jul 29 16:11:29.439: INFO: Created: latency-svc-6kh6g
    Jul 29 16:11:29.469: INFO: Got endpoints: latency-svc-w4w2w [751.273248ms]
    Jul 29 16:11:29.485: INFO: Created: latency-svc-nghrx
    Jul 29 16:11:29.508: INFO: Got endpoints: latency-svc-kb8xs [749.558707ms]
    Jul 29 16:11:29.525: INFO: Created: latency-svc-pkg9c
    Jul 29 16:11:29.559: INFO: Got endpoints: latency-svc-h8db8 [746.51949ms]
    Jul 29 16:11:29.584: INFO: Created: latency-svc-fmvbs
    Jul 29 16:11:29.615: INFO: Got endpoints: latency-svc-mds87 [754.126013ms]
    Jul 29 16:11:29.638: INFO: Created: latency-svc-dj9b8
    Jul 29 16:11:29.660: INFO: Got endpoints: latency-svc-zft8d [746.615765ms]
    Jul 29 16:11:29.678: INFO: Created: latency-svc-72zff
    Jul 29 16:11:29.711: INFO: Got endpoints: latency-svc-22crk [748.257607ms]
    Jul 29 16:11:29.730: INFO: Created: latency-svc-m78bj
    Jul 29 16:11:29.759: INFO: Got endpoints: latency-svc-9j2mv [739.491302ms]
    Jul 29 16:11:29.777: INFO: Created: latency-svc-fnhjv
    Jul 29 16:11:29.818: INFO: Got endpoints: latency-svc-b92gt [754.040157ms]
    Jul 29 16:11:29.836: INFO: Created: latency-svc-mn6zw
    Jul 29 16:11:29.860: INFO: Got endpoints: latency-svc-pdn4d [748.607355ms]
    Jul 29 16:11:29.877: INFO: Created: latency-svc-pqqxt
    Jul 29 16:11:29.914: INFO: Got endpoints: latency-svc-dtzfq [693.835807ms]
    Jul 29 16:11:29.929: INFO: Created: latency-svc-mwbkl
    Jul 29 16:11:29.960: INFO: Got endpoints: latency-svc-4bkql [739.176887ms]
    Jul 29 16:11:29.984: INFO: Created: latency-svc-bqb28
    Jul 29 16:11:30.013: INFO: Got endpoints: latency-svc-4b7l2 [739.865001ms]
    Jul 29 16:11:30.035: INFO: Created: latency-svc-s5rrm
    Jul 29 16:11:30.061: INFO: Got endpoints: latency-svc-m5ljl [736.278208ms]
    Jul 29 16:11:30.077: INFO: Created: latency-svc-ktw8l
    Jul 29 16:11:30.115: INFO: Got endpoints: latency-svc-zxzt4 [755.586756ms]
    Jul 29 16:11:30.133: INFO: Created: latency-svc-vrmnw
    Jul 29 16:11:30.166: INFO: Got endpoints: latency-svc-6kh6g [756.655065ms]
    Jul 29 16:11:30.184: INFO: Created: latency-svc-zkswt
    Jul 29 16:11:30.213: INFO: Got endpoints: latency-svc-nghrx [743.344125ms]
    Jul 29 16:11:30.232: INFO: Created: latency-svc-z8x5g
    Jul 29 16:11:30.261: INFO: Got endpoints: latency-svc-pkg9c [753.019868ms]
    Jul 29 16:11:30.281: INFO: Created: latency-svc-xrkbs
    Jul 29 16:11:30.313: INFO: Got endpoints: latency-svc-fmvbs [753.211049ms]
    Jul 29 16:11:30.330: INFO: Created: latency-svc-lmfbk
    Jul 29 16:11:30.363: INFO: Got endpoints: latency-svc-dj9b8 [746.963026ms]
    Jul 29 16:11:30.384: INFO: Created: latency-svc-r6dzn
    Jul 29 16:11:30.413: INFO: Got endpoints: latency-svc-72zff [753.641ms]
    Jul 29 16:11:30.435: INFO: Created: latency-svc-7qggv
    Jul 29 16:11:30.460: INFO: Got endpoints: latency-svc-m78bj [749.018905ms]
    Jul 29 16:11:30.481: INFO: Created: latency-svc-ln7hq
    Jul 29 16:11:30.511: INFO: Got endpoints: latency-svc-fnhjv [751.764388ms]
    Jul 29 16:11:30.530: INFO: Created: latency-svc-s8z78
    Jul 29 16:11:30.563: INFO: Got endpoints: latency-svc-mn6zw [744.870596ms]
    Jul 29 16:11:30.583: INFO: Created: latency-svc-glw2f
    Jul 29 16:11:30.617: INFO: Got endpoints: latency-svc-pqqxt [756.630036ms]
    Jul 29 16:11:30.646: INFO: Created: latency-svc-x6p98
    Jul 29 16:11:30.668: INFO: Got endpoints: latency-svc-mwbkl [754.098838ms]
    Jul 29 16:11:30.758: INFO: Created: latency-svc-7grnw
    Jul 29 16:11:30.767: INFO: Got endpoints: latency-svc-bqb28 [806.719242ms]
    Jul 29 16:11:30.770: INFO: Got endpoints: latency-svc-s5rrm [756.013752ms]
    Jul 29 16:11:30.788: INFO: Created: latency-svc-mj6x5
    Jul 29 16:11:30.803: INFO: Created: latency-svc-7s4m9
    Jul 29 16:11:30.815: INFO: Got endpoints: latency-svc-ktw8l [753.863391ms]
    Jul 29 16:11:30.836: INFO: Created: latency-svc-c9pf9
    Jul 29 16:11:30.862: INFO: Got endpoints: latency-svc-vrmnw [745.096526ms]
    Jul 29 16:11:30.876: INFO: Created: latency-svc-5ctlb
    Jul 29 16:11:30.912: INFO: Got endpoints: latency-svc-zkswt [745.63093ms]
    Jul 29 16:11:30.937: INFO: Created: latency-svc-5t9mv
    Jul 29 16:11:30.961: INFO: Got endpoints: latency-svc-z8x5g [748.075273ms]
    Jul 29 16:11:30.982: INFO: Created: latency-svc-mskwd
    Jul 29 16:11:31.013: INFO: Got endpoints: latency-svc-xrkbs [750.779743ms]
    Jul 29 16:11:31.045: INFO: Created: latency-svc-66rxh
    Jul 29 16:11:31.069: INFO: Got endpoints: latency-svc-lmfbk [756.314788ms]
    Jul 29 16:11:31.097: INFO: Created: latency-svc-6lh99
    Jul 29 16:11:31.116: INFO: Got endpoints: latency-svc-r6dzn [753.391187ms]
    Jul 29 16:11:31.139: INFO: Created: latency-svc-r76sz
    Jul 29 16:11:31.168: INFO: Got endpoints: latency-svc-7qggv [754.042005ms]
    Jul 29 16:11:31.184: INFO: Created: latency-svc-fdc6s
    Jul 29 16:11:31.215: INFO: Got endpoints: latency-svc-ln7hq [754.619172ms]
    Jul 29 16:11:31.231: INFO: Created: latency-svc-wd9hb
    Jul 29 16:11:31.277: INFO: Got endpoints: latency-svc-s8z78 [766.328956ms]
    Jul 29 16:11:31.308: INFO: Created: latency-svc-wjcqz
    Jul 29 16:11:31.325: INFO: Got endpoints: latency-svc-glw2f [761.901614ms]
    Jul 29 16:11:31.349: INFO: Created: latency-svc-wqvtv
    Jul 29 16:11:31.361: INFO: Got endpoints: latency-svc-x6p98 [744.116757ms]
    Jul 29 16:11:31.385: INFO: Created: latency-svc-c2smw
    Jul 29 16:11:31.410: INFO: Got endpoints: latency-svc-7grnw [741.696306ms]
    Jul 29 16:11:31.433: INFO: Created: latency-svc-j77vt
    Jul 29 16:11:31.462: INFO: Got endpoints: latency-svc-mj6x5 [692.033694ms]
    Jul 29 16:11:31.485: INFO: Created: latency-svc-n4hx9
    Jul 29 16:11:31.509: INFO: Got endpoints: latency-svc-7s4m9 [741.427383ms]
    Jul 29 16:11:31.531: INFO: Created: latency-svc-6rq5q
    Jul 29 16:11:31.563: INFO: Got endpoints: latency-svc-c9pf9 [748.046501ms]
    Jul 29 16:11:31.588: INFO: Created: latency-svc-jx4dn
    Jul 29 16:11:31.614: INFO: Got endpoints: latency-svc-5ctlb [752.497348ms]
    Jul 29 16:11:31.640: INFO: Created: latency-svc-m77gf
    Jul 29 16:11:31.665: INFO: Got endpoints: latency-svc-5t9mv [753.429113ms]
    Jul 29 16:11:31.688: INFO: Created: latency-svc-r687q
    Jul 29 16:11:31.711: INFO: Got endpoints: latency-svc-mskwd [749.805915ms]
    Jul 29 16:11:31.739: INFO: Created: latency-svc-z579n
    Jul 29 16:11:31.766: INFO: Got endpoints: latency-svc-66rxh [752.25607ms]
    Jul 29 16:11:31.796: INFO: Created: latency-svc-wzvnj
    Jul 29 16:11:31.817: INFO: Got endpoints: latency-svc-6lh99 [747.478258ms]
    Jul 29 16:11:31.843: INFO: Created: latency-svc-wr687
    Jul 29 16:11:31.868: INFO: Got endpoints: latency-svc-r76sz [752.166567ms]
    Jul 29 16:11:31.889: INFO: Created: latency-svc-4lw5t
    Jul 29 16:11:31.919: INFO: Got endpoints: latency-svc-fdc6s [750.746415ms]
    Jul 29 16:11:31.961: INFO: Got endpoints: latency-svc-wd9hb [746.647898ms]
    Jul 29 16:11:32.012: INFO: Got endpoints: latency-svc-wjcqz [735.110301ms]
    Jul 29 16:11:32.071: INFO: Got endpoints: latency-svc-wqvtv [746.274178ms]
    Jul 29 16:11:32.115: INFO: Got endpoints: latency-svc-c2smw [753.790122ms]
    Jul 29 16:11:32.163: INFO: Got endpoints: latency-svc-j77vt [753.254888ms]
    Jul 29 16:11:32.210: INFO: Got endpoints: latency-svc-n4hx9 [748.098932ms]
    Jul 29 16:11:32.261: INFO: Got endpoints: latency-svc-6rq5q [752.177128ms]
    Jul 29 16:11:32.339: INFO: Got endpoints: latency-svc-jx4dn [775.15788ms]
    Jul 29 16:11:32.366: INFO: Got endpoints: latency-svc-m77gf [751.924744ms]
    Jul 29 16:11:32.420: INFO: Got endpoints: latency-svc-r687q [754.802991ms]
    Jul 29 16:11:32.496: INFO: Got endpoints: latency-svc-z579n [784.566526ms]
    Jul 29 16:11:32.513: INFO: Got endpoints: latency-svc-wzvnj [747.289954ms]
    Jul 29 16:11:32.564: INFO: Got endpoints: latency-svc-wr687 [747.708493ms]
    Jul 29 16:11:32.619: INFO: Got endpoints: latency-svc-4lw5t [750.651464ms]
    Jul 29 16:11:32.619: INFO: Latencies: [41.107874ms 73.850272ms 88.798377ms 160.732454ms 171.807184ms 172.03353ms 175.329768ms 198.120745ms 203.229354ms 213.070521ms 217.304772ms 218.108223ms 226.425733ms 249.065977ms 255.164934ms 262.836815ms 271.262459ms 271.624065ms 272.669817ms 272.918428ms 273.725892ms 277.112504ms 278.425185ms 286.68961ms 286.937325ms 289.104743ms 289.957888ms 296.032199ms 296.301054ms 299.015794ms 301.442046ms 302.691327ms 303.690214ms 304.585733ms 306.975156ms 312.559484ms 317.142192ms 321.316043ms 324.42821ms 330.55686ms 378.097403ms 381.778306ms 384.233723ms 387.131016ms 402.744606ms 403.261659ms 412.937963ms 429.206402ms 429.78959ms 430.141421ms 430.224143ms 430.89577ms 438.272002ms 463.916407ms 467.672315ms 498.361282ms 520.191355ms 536.325865ms 538.315845ms 538.54758ms 540.253047ms 550.443612ms 598.629422ms 605.965876ms 623.902163ms 643.413568ms 692.033694ms 693.835807ms 695.251138ms 708.998333ms 712.292295ms 719.781808ms 723.978574ms 728.827523ms 734.799261ms 735.110301ms 736.278208ms 738.371787ms 739.176887ms 739.491302ms 739.865001ms 741.427383ms 741.438006ms 741.696306ms 741.946323ms 742.912028ms 743.344125ms 744.116757ms 744.870596ms 745.096526ms 745.306373ms 745.63093ms 746.274178ms 746.403932ms 746.419901ms 746.51949ms 746.615765ms 746.647898ms 746.713311ms 746.963026ms 747.289954ms 747.478258ms 747.708493ms 747.90424ms 747.916596ms 747.950866ms 748.046501ms 748.058244ms 748.075273ms 748.098932ms 748.170799ms 748.257607ms 748.340643ms 748.454529ms 748.607355ms 748.9196ms 749.018905ms 749.031687ms 749.237572ms 749.275272ms 749.376995ms 749.558707ms 749.805915ms 750.41341ms 750.419878ms 750.584556ms 750.651464ms 750.746415ms 750.779743ms 750.868549ms 751.012276ms 751.273248ms 751.525939ms 751.60029ms 751.764388ms 751.924744ms 752.166567ms 752.177128ms 752.25607ms 752.314602ms 752.497348ms 753.019868ms 753.211049ms 753.214572ms 753.254888ms 753.391187ms 753.429113ms 753.446349ms 753.621931ms 753.641ms 753.790122ms 753.863391ms 754.040157ms 754.042005ms 754.098838ms 754.126013ms 754.619172ms 754.802991ms 755.295593ms 755.586756ms 756.013752ms 756.314788ms 756.630036ms 756.655065ms 758.262316ms 759.245397ms 759.339912ms 759.498915ms 760.21245ms 760.406865ms 761.901614ms 766.328956ms 769.062454ms 774.748391ms 775.15788ms 782.940647ms 783.002688ms 784.566526ms 798.623337ms 799.712701ms 800.791784ms 802.23187ms 802.809222ms 806.719242ms 846.715297ms 848.473583ms 854.730355ms 858.707669ms 861.44802ms 866.653597ms 877.188106ms 878.911799ms 886.651276ms 888.054532ms 888.930848ms 895.518295ms 901.215666ms 903.921251ms 910.101234ms 913.034274ms]
    Jul 29 16:11:32.619: INFO: 50 %ile: 747.289954ms
    Jul 29 16:11:32.619: INFO: 90 %ile: 800.791784ms
    Jul 29 16:11:32.619: INFO: 99 %ile: 910.101234ms
    Jul 29 16:11:32.620: INFO: Total sample count: 200
    [AfterEach] [sig-network] Service endpoints latency
      test/e2e/framework/framework.go:187
    Jul 29 16:11:32.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svc-latency-5126" for this suite. 07/29/23 16:11:32.639
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:11:32.659
Jul 29 16:11:32.659: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename gc 07/29/23 16:11:32.663
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:11:32.703
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:11:32.709
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
STEP: create the deployment 07/29/23 16:11:32.712
STEP: Wait for the Deployment to create new ReplicaSet 07/29/23 16:11:32.728
STEP: delete the deployment 07/29/23 16:11:33.251
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 07/29/23 16:11:33.283
STEP: Gathering metrics 07/29/23 16:11:33.907
Jul 29 16:11:34.097: INFO: Waiting up to 5m0s for pod "kube-controller-manager-wa4quivohpee-2" in namespace "kube-system" to be "running and ready"
Jul 29 16:11:34.111: INFO: Pod "kube-controller-manager-wa4quivohpee-2": Phase="Running", Reason="", readiness=true. Elapsed: 13.705753ms
Jul 29 16:11:34.111: INFO: The phase of Pod kube-controller-manager-wa4quivohpee-2 is Running (Ready = true)
Jul 29 16:11:34.111: INFO: Pod "kube-controller-manager-wa4quivohpee-2" satisfied condition "running and ready"
Jul 29 16:11:34.335: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Jul 29 16:11:34.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7514" for this suite. 07/29/23 16:11:34.41
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","completed":164,"skipped":3188,"failed":0}
------------------------------
â€¢ [1.788 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:11:32.659
    Jul 29 16:11:32.659: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename gc 07/29/23 16:11:32.663
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:11:32.703
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:11:32.709
    [It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
      test/e2e/apimachinery/garbage_collector.go:550
    STEP: create the deployment 07/29/23 16:11:32.712
    STEP: Wait for the Deployment to create new ReplicaSet 07/29/23 16:11:32.728
    STEP: delete the deployment 07/29/23 16:11:33.251
    STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 07/29/23 16:11:33.283
    STEP: Gathering metrics 07/29/23 16:11:33.907
    Jul 29 16:11:34.097: INFO: Waiting up to 5m0s for pod "kube-controller-manager-wa4quivohpee-2" in namespace "kube-system" to be "running and ready"
    Jul 29 16:11:34.111: INFO: Pod "kube-controller-manager-wa4quivohpee-2": Phase="Running", Reason="", readiness=true. Elapsed: 13.705753ms
    Jul 29 16:11:34.111: INFO: The phase of Pod kube-controller-manager-wa4quivohpee-2 is Running (Ready = true)
    Jul 29 16:11:34.111: INFO: Pod "kube-controller-manager-wa4quivohpee-2" satisfied condition "running and ready"
    Jul 29 16:11:34.335: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Jul 29 16:11:34.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-7514" for this suite. 07/29/23 16:11:34.41
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:11:34.453
Jul 29 16:11:34.453: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename var-expansion 07/29/23 16:11:34.455
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:11:34.498
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:11:34.503
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43
STEP: Creating a pod to test env composition 07/29/23 16:11:34.516
Jul 29 16:11:34.542: INFO: Waiting up to 5m0s for pod "var-expansion-a783154d-7ddc-4be0-995f-034745b7a35f" in namespace "var-expansion-1247" to be "Succeeded or Failed"
Jul 29 16:11:34.548: INFO: Pod "var-expansion-a783154d-7ddc-4be0-995f-034745b7a35f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.491741ms
Jul 29 16:11:36.556: INFO: Pod "var-expansion-a783154d-7ddc-4be0-995f-034745b7a35f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014670235s
Jul 29 16:11:38.562: INFO: Pod "var-expansion-a783154d-7ddc-4be0-995f-034745b7a35f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020788394s
STEP: Saw pod success 07/29/23 16:11:38.563
Jul 29 16:11:38.563: INFO: Pod "var-expansion-a783154d-7ddc-4be0-995f-034745b7a35f" satisfied condition "Succeeded or Failed"
Jul 29 16:11:38.569: INFO: Trying to get logs from node wa4quivohpee-3 pod var-expansion-a783154d-7ddc-4be0-995f-034745b7a35f container dapi-container: <nil>
STEP: delete the pod 07/29/23 16:11:38.624
Jul 29 16:11:38.650: INFO: Waiting for pod var-expansion-a783154d-7ddc-4be0-995f-034745b7a35f to disappear
Jul 29 16:11:38.671: INFO: Pod var-expansion-a783154d-7ddc-4be0-995f-034745b7a35f no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jul 29 16:11:38.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1247" for this suite. 07/29/23 16:11:38.684
{"msg":"PASSED [sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","completed":165,"skipped":3204,"failed":0}
------------------------------
â€¢ [4.246 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:11:34.453
    Jul 29 16:11:34.453: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename var-expansion 07/29/23 16:11:34.455
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:11:34.498
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:11:34.503
    [It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:43
    STEP: Creating a pod to test env composition 07/29/23 16:11:34.516
    Jul 29 16:11:34.542: INFO: Waiting up to 5m0s for pod "var-expansion-a783154d-7ddc-4be0-995f-034745b7a35f" in namespace "var-expansion-1247" to be "Succeeded or Failed"
    Jul 29 16:11:34.548: INFO: Pod "var-expansion-a783154d-7ddc-4be0-995f-034745b7a35f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.491741ms
    Jul 29 16:11:36.556: INFO: Pod "var-expansion-a783154d-7ddc-4be0-995f-034745b7a35f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014670235s
    Jul 29 16:11:38.562: INFO: Pod "var-expansion-a783154d-7ddc-4be0-995f-034745b7a35f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020788394s
    STEP: Saw pod success 07/29/23 16:11:38.563
    Jul 29 16:11:38.563: INFO: Pod "var-expansion-a783154d-7ddc-4be0-995f-034745b7a35f" satisfied condition "Succeeded or Failed"
    Jul 29 16:11:38.569: INFO: Trying to get logs from node wa4quivohpee-3 pod var-expansion-a783154d-7ddc-4be0-995f-034745b7a35f container dapi-container: <nil>
    STEP: delete the pod 07/29/23 16:11:38.624
    Jul 29 16:11:38.650: INFO: Waiting for pod var-expansion-a783154d-7ddc-4be0-995f-034745b7a35f to disappear
    Jul 29 16:11:38.671: INFO: Pod var-expansion-a783154d-7ddc-4be0-995f-034745b7a35f no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jul 29 16:11:38.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-1247" for this suite. 07/29/23 16:11:38.684
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:11:38.701
Jul 29 16:11:38.701: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename secrets 07/29/23 16:11:38.704
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:11:38.747
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:11:38.753
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46
STEP: Creating secret with name secret-test-03322d86-528d-49df-8842-9d67367c0261 07/29/23 16:11:38.758
STEP: Creating a pod to test consume secrets 07/29/23 16:11:38.768
Jul 29 16:11:38.784: INFO: Waiting up to 5m0s for pod "pod-secrets-0c0c61a0-3fb3-4b2d-8b14-d2905d7f4d6e" in namespace "secrets-1800" to be "Succeeded or Failed"
Jul 29 16:11:38.789: INFO: Pod "pod-secrets-0c0c61a0-3fb3-4b2d-8b14-d2905d7f4d6e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.445758ms
Jul 29 16:11:40.800: INFO: Pod "pod-secrets-0c0c61a0-3fb3-4b2d-8b14-d2905d7f4d6e": Phase="Running", Reason="", readiness=true. Elapsed: 2.016013865s
Jul 29 16:11:42.803: INFO: Pod "pod-secrets-0c0c61a0-3fb3-4b2d-8b14-d2905d7f4d6e": Phase="Running", Reason="", readiness=false. Elapsed: 4.019589413s
Jul 29 16:11:44.799: INFO: Pod "pod-secrets-0c0c61a0-3fb3-4b2d-8b14-d2905d7f4d6e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.014703876s
STEP: Saw pod success 07/29/23 16:11:44.799
Jul 29 16:11:44.799: INFO: Pod "pod-secrets-0c0c61a0-3fb3-4b2d-8b14-d2905d7f4d6e" satisfied condition "Succeeded or Failed"
Jul 29 16:11:44.806: INFO: Trying to get logs from node wa4quivohpee-3 pod pod-secrets-0c0c61a0-3fb3-4b2d-8b14-d2905d7f4d6e container secret-volume-test: <nil>
STEP: delete the pod 07/29/23 16:11:44.824
Jul 29 16:11:44.856: INFO: Waiting for pod pod-secrets-0c0c61a0-3fb3-4b2d-8b14-d2905d7f4d6e to disappear
Jul 29 16:11:44.866: INFO: Pod pod-secrets-0c0c61a0-3fb3-4b2d-8b14-d2905d7f4d6e no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jul 29 16:11:44.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1800" for this suite. 07/29/23 16:11:44.88
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","completed":166,"skipped":3207,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.197 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:11:38.701
    Jul 29 16:11:38.701: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename secrets 07/29/23 16:11:38.704
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:11:38.747
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:11:38.753
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:46
    STEP: Creating secret with name secret-test-03322d86-528d-49df-8842-9d67367c0261 07/29/23 16:11:38.758
    STEP: Creating a pod to test consume secrets 07/29/23 16:11:38.768
    Jul 29 16:11:38.784: INFO: Waiting up to 5m0s for pod "pod-secrets-0c0c61a0-3fb3-4b2d-8b14-d2905d7f4d6e" in namespace "secrets-1800" to be "Succeeded or Failed"
    Jul 29 16:11:38.789: INFO: Pod "pod-secrets-0c0c61a0-3fb3-4b2d-8b14-d2905d7f4d6e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.445758ms
    Jul 29 16:11:40.800: INFO: Pod "pod-secrets-0c0c61a0-3fb3-4b2d-8b14-d2905d7f4d6e": Phase="Running", Reason="", readiness=true. Elapsed: 2.016013865s
    Jul 29 16:11:42.803: INFO: Pod "pod-secrets-0c0c61a0-3fb3-4b2d-8b14-d2905d7f4d6e": Phase="Running", Reason="", readiness=false. Elapsed: 4.019589413s
    Jul 29 16:11:44.799: INFO: Pod "pod-secrets-0c0c61a0-3fb3-4b2d-8b14-d2905d7f4d6e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.014703876s
    STEP: Saw pod success 07/29/23 16:11:44.799
    Jul 29 16:11:44.799: INFO: Pod "pod-secrets-0c0c61a0-3fb3-4b2d-8b14-d2905d7f4d6e" satisfied condition "Succeeded or Failed"
    Jul 29 16:11:44.806: INFO: Trying to get logs from node wa4quivohpee-3 pod pod-secrets-0c0c61a0-3fb3-4b2d-8b14-d2905d7f4d6e container secret-volume-test: <nil>
    STEP: delete the pod 07/29/23 16:11:44.824
    Jul 29 16:11:44.856: INFO: Waiting for pod pod-secrets-0c0c61a0-3fb3-4b2d-8b14-d2905d7f4d6e to disappear
    Jul 29 16:11:44.866: INFO: Pod pod-secrets-0c0c61a0-3fb3-4b2d-8b14-d2905d7f4d6e no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jul 29 16:11:44.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-1800" for this suite. 07/29/23 16:11:44.88
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:11:44.901
Jul 29 16:11:44.906: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename sched-pred 07/29/23 16:11:44.909
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:11:44.945
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:11:44.951
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Jul 29 16:11:44.955: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jul 29 16:11:44.987: INFO: Waiting for terminating namespaces to be deleted...
Jul 29 16:11:44.994: INFO: 
Logging pods the apiserver thinks is on node wa4quivohpee-1 before test
Jul 29 16:11:45.037: INFO: cilium-gfkpl from kube-system started at 2023-07-29 15:23:18 +0000 UTC (1 container statuses recorded)
Jul 29 16:11:45.037: INFO: 	Container cilium-agent ready: true, restart count 0
Jul 29 16:11:45.037: INFO: cilium-node-init-n8876 from kube-system started at 2023-07-29 15:23:18 +0000 UTC (1 container statuses recorded)
Jul 29 16:11:45.037: INFO: 	Container node-init ready: true, restart count 0
Jul 29 16:11:45.037: INFO: coredns-565d847f94-xm9ff from kube-system started at 2023-07-29 15:34:18 +0000 UTC (1 container statuses recorded)
Jul 29 16:11:45.037: INFO: 	Container coredns ready: true, restart count 0
Jul 29 16:11:45.037: INFO: kube-addon-manager-wa4quivohpee-1 from kube-system started at 2023-07-29 15:22:59 +0000 UTC (1 container statuses recorded)
Jul 29 16:11:45.038: INFO: 	Container kube-addon-manager ready: true, restart count 0
Jul 29 16:11:45.038: INFO: kube-apiserver-wa4quivohpee-1 from kube-system started at 2023-07-29 15:14:24 +0000 UTC (1 container statuses recorded)
Jul 29 16:11:45.038: INFO: 	Container kube-apiserver ready: true, restart count 0
Jul 29 16:11:45.038: INFO: kube-controller-manager-wa4quivohpee-1 from kube-system started at 2023-07-29 15:14:24 +0000 UTC (1 container statuses recorded)
Jul 29 16:11:45.038: INFO: 	Container kube-controller-manager ready: true, restart count 0
Jul 29 16:11:45.038: INFO: kube-proxy-gdffl from kube-system started at 2023-07-29 15:14:37 +0000 UTC (1 container statuses recorded)
Jul 29 16:11:45.038: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 29 16:11:45.038: INFO: kube-scheduler-wa4quivohpee-1 from kube-system started at 2023-07-29 15:14:23 +0000 UTC (1 container statuses recorded)
Jul 29 16:11:45.038: INFO: 	Container kube-scheduler ready: true, restart count 0
Jul 29 16:11:45.038: INFO: sonobuoy-systemd-logs-daemon-set-b6f0dae2c3174900-mtck5 from sonobuoy started at 2023-07-29 15:24:46 +0000 UTC (2 container statuses recorded)
Jul 29 16:11:45.038: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 29 16:11:45.039: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 29 16:11:45.039: INFO: 
Logging pods the apiserver thinks is on node wa4quivohpee-2 before test
Jul 29 16:11:45.076: INFO: cilium-node-init-946z9 from kube-system started at 2023-07-29 15:23:18 +0000 UTC (1 container statuses recorded)
Jul 29 16:11:45.077: INFO: 	Container node-init ready: true, restart count 0
Jul 29 16:11:45.077: INFO: cilium-rmjkx from kube-system started at 2023-07-29 15:23:18 +0000 UTC (1 container statuses recorded)
Jul 29 16:11:45.077: INFO: 	Container cilium-agent ready: true, restart count 0
Jul 29 16:11:45.077: INFO: coredns-565d847f94-wmg75 from kube-system started at 2023-07-29 15:24:27 +0000 UTC (1 container statuses recorded)
Jul 29 16:11:45.077: INFO: 	Container coredns ready: true, restart count 0
Jul 29 16:11:45.078: INFO: kube-addon-manager-wa4quivohpee-2 from kube-system started at 2023-07-29 15:22:59 +0000 UTC (1 container statuses recorded)
Jul 29 16:11:45.078: INFO: 	Container kube-addon-manager ready: true, restart count 0
Jul 29 16:11:45.078: INFO: kube-apiserver-wa4quivohpee-2 from kube-system started at 2023-07-29 15:15:22 +0000 UTC (1 container statuses recorded)
Jul 29 16:11:45.079: INFO: 	Container kube-apiserver ready: true, restart count 0
Jul 29 16:11:45.079: INFO: kube-controller-manager-wa4quivohpee-2 from kube-system started at 2023-07-29 15:15:22 +0000 UTC (1 container statuses recorded)
Jul 29 16:11:45.079: INFO: 	Container kube-controller-manager ready: true, restart count 0
Jul 29 16:11:45.079: INFO: kube-proxy-74hn2 from kube-system started at 2023-07-29 15:15:19 +0000 UTC (1 container statuses recorded)
Jul 29 16:11:45.079: INFO: 	Container kube-proxy ready: true, restart count 1
Jul 29 16:11:45.079: INFO: kube-scheduler-wa4quivohpee-2 from kube-system started at 2023-07-29 15:15:22 +0000 UTC (1 container statuses recorded)
Jul 29 16:11:45.080: INFO: 	Container kube-scheduler ready: true, restart count 0
Jul 29 16:11:45.080: INFO: sonobuoy-systemd-logs-daemon-set-b6f0dae2c3174900-cnfp2 from sonobuoy started at 2023-07-29 15:24:46 +0000 UTC (2 container statuses recorded)
Jul 29 16:11:45.080: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 29 16:11:45.081: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 29 16:11:45.081: INFO: 
Logging pods the apiserver thinks is on node wa4quivohpee-3 before test
Jul 29 16:11:45.102: INFO: cilium-node-init-j4pkd from kube-system started at 2023-07-29 15:23:18 +0000 UTC (1 container statuses recorded)
Jul 29 16:11:45.102: INFO: 	Container node-init ready: true, restart count 0
Jul 29 16:11:45.102: INFO: cilium-operator-69cffcb958-48gkl from kube-system started at 2023-07-29 15:23:19 +0000 UTC (1 container statuses recorded)
Jul 29 16:11:45.102: INFO: 	Container cilium-operator ready: true, restart count 0
Jul 29 16:11:45.102: INFO: cilium-zl28x from kube-system started at 2023-07-29 15:23:18 +0000 UTC (1 container statuses recorded)
Jul 29 16:11:45.103: INFO: 	Container cilium-agent ready: true, restart count 0
Jul 29 16:11:45.103: INFO: kube-proxy-ts2pg from kube-system started at 2023-07-29 15:16:17 +0000 UTC (1 container statuses recorded)
Jul 29 16:11:45.103: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 29 16:11:45.103: INFO: sonobuoy from sonobuoy started at 2023-07-29 15:24:31 +0000 UTC (1 container statuses recorded)
Jul 29 16:11:45.103: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jul 29 16:11:45.103: INFO: sonobuoy-e2e-job-5c9a7da3e6b74e15 from sonobuoy started at 2023-07-29 15:24:45 +0000 UTC (2 container statuses recorded)
Jul 29 16:11:45.103: INFO: 	Container e2e ready: true, restart count 0
Jul 29 16:11:45.104: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 29 16:11:45.104: INFO: sonobuoy-systemd-logs-daemon-set-b6f0dae2c3174900-cqnfx from sonobuoy started at 2023-07-29 15:24:46 +0000 UTC (2 container statuses recorded)
Jul 29 16:11:45.104: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 29 16:11:45.104: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 29 16:11:45.104: INFO: svc-latency-rc-gksv7 from svc-latency-5126 started at 2023-07-29 16:11:21 +0000 UTC (1 container statuses recorded)
Jul 29 16:11:45.104: INFO: 	Container svc-latency-rc ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326
STEP: verifying the node has the label node wa4quivohpee-1 07/29/23 16:11:45.24
STEP: verifying the node has the label node wa4quivohpee-2 07/29/23 16:11:45.302
STEP: verifying the node has the label node wa4quivohpee-3 07/29/23 16:11:45.377
Jul 29 16:11:45.472: INFO: Pod cilium-gfkpl requesting resource cpu=0m on Node wa4quivohpee-1
Jul 29 16:11:45.472: INFO: Pod cilium-node-init-946z9 requesting resource cpu=100m on Node wa4quivohpee-2
Jul 29 16:11:45.472: INFO: Pod cilium-node-init-j4pkd requesting resource cpu=100m on Node wa4quivohpee-3
Jul 29 16:11:45.472: INFO: Pod cilium-node-init-n8876 requesting resource cpu=100m on Node wa4quivohpee-1
Jul 29 16:11:45.472: INFO: Pod cilium-operator-69cffcb958-48gkl requesting resource cpu=0m on Node wa4quivohpee-3
Jul 29 16:11:45.472: INFO: Pod cilium-rmjkx requesting resource cpu=0m on Node wa4quivohpee-2
Jul 29 16:11:45.472: INFO: Pod cilium-zl28x requesting resource cpu=0m on Node wa4quivohpee-3
Jul 29 16:11:45.472: INFO: Pod coredns-565d847f94-wmg75 requesting resource cpu=100m on Node wa4quivohpee-2
Jul 29 16:11:45.472: INFO: Pod coredns-565d847f94-xm9ff requesting resource cpu=100m on Node wa4quivohpee-1
Jul 29 16:11:45.472: INFO: Pod kube-addon-manager-wa4quivohpee-1 requesting resource cpu=5m on Node wa4quivohpee-1
Jul 29 16:11:45.472: INFO: Pod kube-addon-manager-wa4quivohpee-2 requesting resource cpu=5m on Node wa4quivohpee-2
Jul 29 16:11:45.472: INFO: Pod kube-apiserver-wa4quivohpee-1 requesting resource cpu=250m on Node wa4quivohpee-1
Jul 29 16:11:45.472: INFO: Pod kube-apiserver-wa4quivohpee-2 requesting resource cpu=250m on Node wa4quivohpee-2
Jul 29 16:11:45.472: INFO: Pod kube-controller-manager-wa4quivohpee-1 requesting resource cpu=200m on Node wa4quivohpee-1
Jul 29 16:11:45.472: INFO: Pod kube-controller-manager-wa4quivohpee-2 requesting resource cpu=200m on Node wa4quivohpee-2
Jul 29 16:11:45.472: INFO: Pod kube-proxy-74hn2 requesting resource cpu=0m on Node wa4quivohpee-2
Jul 29 16:11:45.472: INFO: Pod kube-proxy-gdffl requesting resource cpu=0m on Node wa4quivohpee-1
Jul 29 16:11:45.472: INFO: Pod kube-proxy-ts2pg requesting resource cpu=0m on Node wa4quivohpee-3
Jul 29 16:11:45.472: INFO: Pod kube-scheduler-wa4quivohpee-1 requesting resource cpu=100m on Node wa4quivohpee-1
Jul 29 16:11:45.472: INFO: Pod kube-scheduler-wa4quivohpee-2 requesting resource cpu=100m on Node wa4quivohpee-2
Jul 29 16:11:45.472: INFO: Pod sonobuoy requesting resource cpu=0m on Node wa4quivohpee-3
Jul 29 16:11:45.472: INFO: Pod sonobuoy-e2e-job-5c9a7da3e6b74e15 requesting resource cpu=0m on Node wa4quivohpee-3
Jul 29 16:11:45.473: INFO: Pod sonobuoy-systemd-logs-daemon-set-b6f0dae2c3174900-cnfp2 requesting resource cpu=0m on Node wa4quivohpee-2
Jul 29 16:11:45.473: INFO: Pod sonobuoy-systemd-logs-daemon-set-b6f0dae2c3174900-cqnfx requesting resource cpu=0m on Node wa4quivohpee-3
Jul 29 16:11:45.473: INFO: Pod sonobuoy-systemd-logs-daemon-set-b6f0dae2c3174900-mtck5 requesting resource cpu=0m on Node wa4quivohpee-1
Jul 29 16:11:45.473: INFO: Pod svc-latency-rc-gksv7 requesting resource cpu=0m on Node wa4quivohpee-3
STEP: Starting Pods to consume most of the cluster CPU. 07/29/23 16:11:45.473
Jul 29 16:11:45.473: INFO: Creating a pod which consumes cpu=591m on Node wa4quivohpee-1
Jul 29 16:11:45.517: INFO: Creating a pod which consumes cpu=591m on Node wa4quivohpee-2
Jul 29 16:11:45.578: INFO: Creating a pod which consumes cpu=1050m on Node wa4quivohpee-3
Jul 29 16:11:45.617: INFO: Waiting up to 5m0s for pod "filler-pod-e0f2cd11-f442-406d-876d-2066b8057bc7" in namespace "sched-pred-6898" to be "running"
Jul 29 16:11:45.643: INFO: Pod "filler-pod-e0f2cd11-f442-406d-876d-2066b8057bc7": Phase="Pending", Reason="", readiness=false. Elapsed: 25.646156ms
Jul 29 16:11:47.650: INFO: Pod "filler-pod-e0f2cd11-f442-406d-876d-2066b8057bc7": Phase="Running", Reason="", readiness=true. Elapsed: 2.032611886s
Jul 29 16:11:47.650: INFO: Pod "filler-pod-e0f2cd11-f442-406d-876d-2066b8057bc7" satisfied condition "running"
Jul 29 16:11:47.650: INFO: Waiting up to 5m0s for pod "filler-pod-a39ddc09-1a95-4c98-a200-3aceff858021" in namespace "sched-pred-6898" to be "running"
Jul 29 16:11:47.662: INFO: Pod "filler-pod-a39ddc09-1a95-4c98-a200-3aceff858021": Phase="Running", Reason="", readiness=true. Elapsed: 11.581532ms
Jul 29 16:11:47.662: INFO: Pod "filler-pod-a39ddc09-1a95-4c98-a200-3aceff858021" satisfied condition "running"
Jul 29 16:11:47.662: INFO: Waiting up to 5m0s for pod "filler-pod-fa931e98-a855-4607-bcb6-7a5525a0fb65" in namespace "sched-pred-6898" to be "running"
Jul 29 16:11:47.671: INFO: Pod "filler-pod-fa931e98-a855-4607-bcb6-7a5525a0fb65": Phase="Running", Reason="", readiness=true. Elapsed: 8.979879ms
Jul 29 16:11:47.671: INFO: Pod "filler-pod-fa931e98-a855-4607-bcb6-7a5525a0fb65" satisfied condition "running"
STEP: Creating another pod that requires unavailable amount of CPU. 07/29/23 16:11:47.671
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a39ddc09-1a95-4c98-a200-3aceff858021.17766295caf57f8e], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6898/filler-pod-a39ddc09-1a95-4c98-a200-3aceff858021 to wa4quivohpee-2] 07/29/23 16:11:47.679
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a39ddc09-1a95-4c98-a200-3aceff858021.17766296056cd033], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 07/29/23 16:11:47.68
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a39ddc09-1a95-4c98-a200-3aceff858021.1776629614e1a582], Reason = [Created], Message = [Created container filler-pod-a39ddc09-1a95-4c98-a200-3aceff858021] 07/29/23 16:11:47.68
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a39ddc09-1a95-4c98-a200-3aceff858021.1776629617460296], Reason = [Started], Message = [Started container filler-pod-a39ddc09-1a95-4c98-a200-3aceff858021] 07/29/23 16:11:47.68
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e0f2cd11-f442-406d-876d-2066b8057bc7.17766295c5863296], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6898/filler-pod-e0f2cd11-f442-406d-876d-2066b8057bc7 to wa4quivohpee-1] 07/29/23 16:11:47.68
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e0f2cd11-f442-406d-876d-2066b8057bc7.177662960d20c976], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 07/29/23 16:11:47.681
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e0f2cd11-f442-406d-876d-2066b8057bc7.177662962a3e9ddd], Reason = [Created], Message = [Created container filler-pod-e0f2cd11-f442-406d-876d-2066b8057bc7] 07/29/23 16:11:47.681
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e0f2cd11-f442-406d-876d-2066b8057bc7.177662962eaa6c15], Reason = [Started], Message = [Started container filler-pod-e0f2cd11-f442-406d-876d-2066b8057bc7] 07/29/23 16:11:47.681
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-fa931e98-a855-4607-bcb6-7a5525a0fb65.17766295cd15071c], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6898/filler-pod-fa931e98-a855-4607-bcb6-7a5525a0fb65 to wa4quivohpee-3] 07/29/23 16:11:47.682
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-fa931e98-a855-4607-bcb6-7a5525a0fb65.177662960834beb9], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 07/29/23 16:11:47.682
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-fa931e98-a855-4607-bcb6-7a5525a0fb65.17766296155e9872], Reason = [Created], Message = [Created container filler-pod-fa931e98-a855-4607-bcb6-7a5525a0fb65] 07/29/23 16:11:47.683
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-fa931e98-a855-4607-bcb6-7a5525a0fb65.1776629618febaef], Reason = [Started], Message = [Started container filler-pod-fa931e98-a855-4607-bcb6-7a5525a0fb65] 07/29/23 16:11:47.683
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.1776629646bdd2bd], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu. preemption: 0/3 nodes are available: 3 No preemption victims found for incoming pod.] 07/29/23 16:11:47.721
STEP: removing the label node off the node wa4quivohpee-3 07/29/23 16:11:48.708
STEP: verifying the node doesn't have the label node 07/29/23 16:11:48.756
STEP: removing the label node off the node wa4quivohpee-1 07/29/23 16:11:48.784
STEP: verifying the node doesn't have the label node 07/29/23 16:11:48.856
STEP: removing the label node off the node wa4quivohpee-2 07/29/23 16:11:48.868
STEP: verifying the node doesn't have the label node 07/29/23 16:11:48.927
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Jul 29 16:11:48.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6898" for this suite. 07/29/23 16:11:48.968
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","completed":167,"skipped":3214,"failed":0}
------------------------------
â€¢ [4.103 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:11:44.901
    Jul 29 16:11:44.906: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename sched-pred 07/29/23 16:11:44.909
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:11:44.945
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:11:44.951
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Jul 29 16:11:44.955: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jul 29 16:11:44.987: INFO: Waiting for terminating namespaces to be deleted...
    Jul 29 16:11:44.994: INFO: 
    Logging pods the apiserver thinks is on node wa4quivohpee-1 before test
    Jul 29 16:11:45.037: INFO: cilium-gfkpl from kube-system started at 2023-07-29 15:23:18 +0000 UTC (1 container statuses recorded)
    Jul 29 16:11:45.037: INFO: 	Container cilium-agent ready: true, restart count 0
    Jul 29 16:11:45.037: INFO: cilium-node-init-n8876 from kube-system started at 2023-07-29 15:23:18 +0000 UTC (1 container statuses recorded)
    Jul 29 16:11:45.037: INFO: 	Container node-init ready: true, restart count 0
    Jul 29 16:11:45.037: INFO: coredns-565d847f94-xm9ff from kube-system started at 2023-07-29 15:34:18 +0000 UTC (1 container statuses recorded)
    Jul 29 16:11:45.037: INFO: 	Container coredns ready: true, restart count 0
    Jul 29 16:11:45.037: INFO: kube-addon-manager-wa4quivohpee-1 from kube-system started at 2023-07-29 15:22:59 +0000 UTC (1 container statuses recorded)
    Jul 29 16:11:45.038: INFO: 	Container kube-addon-manager ready: true, restart count 0
    Jul 29 16:11:45.038: INFO: kube-apiserver-wa4quivohpee-1 from kube-system started at 2023-07-29 15:14:24 +0000 UTC (1 container statuses recorded)
    Jul 29 16:11:45.038: INFO: 	Container kube-apiserver ready: true, restart count 0
    Jul 29 16:11:45.038: INFO: kube-controller-manager-wa4quivohpee-1 from kube-system started at 2023-07-29 15:14:24 +0000 UTC (1 container statuses recorded)
    Jul 29 16:11:45.038: INFO: 	Container kube-controller-manager ready: true, restart count 0
    Jul 29 16:11:45.038: INFO: kube-proxy-gdffl from kube-system started at 2023-07-29 15:14:37 +0000 UTC (1 container statuses recorded)
    Jul 29 16:11:45.038: INFO: 	Container kube-proxy ready: true, restart count 0
    Jul 29 16:11:45.038: INFO: kube-scheduler-wa4quivohpee-1 from kube-system started at 2023-07-29 15:14:23 +0000 UTC (1 container statuses recorded)
    Jul 29 16:11:45.038: INFO: 	Container kube-scheduler ready: true, restart count 0
    Jul 29 16:11:45.038: INFO: sonobuoy-systemd-logs-daemon-set-b6f0dae2c3174900-mtck5 from sonobuoy started at 2023-07-29 15:24:46 +0000 UTC (2 container statuses recorded)
    Jul 29 16:11:45.038: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jul 29 16:11:45.039: INFO: 	Container systemd-logs ready: true, restart count 0
    Jul 29 16:11:45.039: INFO: 
    Logging pods the apiserver thinks is on node wa4quivohpee-2 before test
    Jul 29 16:11:45.076: INFO: cilium-node-init-946z9 from kube-system started at 2023-07-29 15:23:18 +0000 UTC (1 container statuses recorded)
    Jul 29 16:11:45.077: INFO: 	Container node-init ready: true, restart count 0
    Jul 29 16:11:45.077: INFO: cilium-rmjkx from kube-system started at 2023-07-29 15:23:18 +0000 UTC (1 container statuses recorded)
    Jul 29 16:11:45.077: INFO: 	Container cilium-agent ready: true, restart count 0
    Jul 29 16:11:45.077: INFO: coredns-565d847f94-wmg75 from kube-system started at 2023-07-29 15:24:27 +0000 UTC (1 container statuses recorded)
    Jul 29 16:11:45.077: INFO: 	Container coredns ready: true, restart count 0
    Jul 29 16:11:45.078: INFO: kube-addon-manager-wa4quivohpee-2 from kube-system started at 2023-07-29 15:22:59 +0000 UTC (1 container statuses recorded)
    Jul 29 16:11:45.078: INFO: 	Container kube-addon-manager ready: true, restart count 0
    Jul 29 16:11:45.078: INFO: kube-apiserver-wa4quivohpee-2 from kube-system started at 2023-07-29 15:15:22 +0000 UTC (1 container statuses recorded)
    Jul 29 16:11:45.079: INFO: 	Container kube-apiserver ready: true, restart count 0
    Jul 29 16:11:45.079: INFO: kube-controller-manager-wa4quivohpee-2 from kube-system started at 2023-07-29 15:15:22 +0000 UTC (1 container statuses recorded)
    Jul 29 16:11:45.079: INFO: 	Container kube-controller-manager ready: true, restart count 0
    Jul 29 16:11:45.079: INFO: kube-proxy-74hn2 from kube-system started at 2023-07-29 15:15:19 +0000 UTC (1 container statuses recorded)
    Jul 29 16:11:45.079: INFO: 	Container kube-proxy ready: true, restart count 1
    Jul 29 16:11:45.079: INFO: kube-scheduler-wa4quivohpee-2 from kube-system started at 2023-07-29 15:15:22 +0000 UTC (1 container statuses recorded)
    Jul 29 16:11:45.080: INFO: 	Container kube-scheduler ready: true, restart count 0
    Jul 29 16:11:45.080: INFO: sonobuoy-systemd-logs-daemon-set-b6f0dae2c3174900-cnfp2 from sonobuoy started at 2023-07-29 15:24:46 +0000 UTC (2 container statuses recorded)
    Jul 29 16:11:45.080: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jul 29 16:11:45.081: INFO: 	Container systemd-logs ready: true, restart count 0
    Jul 29 16:11:45.081: INFO: 
    Logging pods the apiserver thinks is on node wa4quivohpee-3 before test
    Jul 29 16:11:45.102: INFO: cilium-node-init-j4pkd from kube-system started at 2023-07-29 15:23:18 +0000 UTC (1 container statuses recorded)
    Jul 29 16:11:45.102: INFO: 	Container node-init ready: true, restart count 0
    Jul 29 16:11:45.102: INFO: cilium-operator-69cffcb958-48gkl from kube-system started at 2023-07-29 15:23:19 +0000 UTC (1 container statuses recorded)
    Jul 29 16:11:45.102: INFO: 	Container cilium-operator ready: true, restart count 0
    Jul 29 16:11:45.102: INFO: cilium-zl28x from kube-system started at 2023-07-29 15:23:18 +0000 UTC (1 container statuses recorded)
    Jul 29 16:11:45.103: INFO: 	Container cilium-agent ready: true, restart count 0
    Jul 29 16:11:45.103: INFO: kube-proxy-ts2pg from kube-system started at 2023-07-29 15:16:17 +0000 UTC (1 container statuses recorded)
    Jul 29 16:11:45.103: INFO: 	Container kube-proxy ready: true, restart count 0
    Jul 29 16:11:45.103: INFO: sonobuoy from sonobuoy started at 2023-07-29 15:24:31 +0000 UTC (1 container statuses recorded)
    Jul 29 16:11:45.103: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jul 29 16:11:45.103: INFO: sonobuoy-e2e-job-5c9a7da3e6b74e15 from sonobuoy started at 2023-07-29 15:24:45 +0000 UTC (2 container statuses recorded)
    Jul 29 16:11:45.103: INFO: 	Container e2e ready: true, restart count 0
    Jul 29 16:11:45.104: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jul 29 16:11:45.104: INFO: sonobuoy-systemd-logs-daemon-set-b6f0dae2c3174900-cqnfx from sonobuoy started at 2023-07-29 15:24:46 +0000 UTC (2 container statuses recorded)
    Jul 29 16:11:45.104: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jul 29 16:11:45.104: INFO: 	Container systemd-logs ready: true, restart count 0
    Jul 29 16:11:45.104: INFO: svc-latency-rc-gksv7 from svc-latency-5126 started at 2023-07-29 16:11:21 +0000 UTC (1 container statuses recorded)
    Jul 29 16:11:45.104: INFO: 	Container svc-latency-rc ready: true, restart count 0
    [It] validates resource limits of pods that are allowed to run  [Conformance]
      test/e2e/scheduling/predicates.go:326
    STEP: verifying the node has the label node wa4quivohpee-1 07/29/23 16:11:45.24
    STEP: verifying the node has the label node wa4quivohpee-2 07/29/23 16:11:45.302
    STEP: verifying the node has the label node wa4quivohpee-3 07/29/23 16:11:45.377
    Jul 29 16:11:45.472: INFO: Pod cilium-gfkpl requesting resource cpu=0m on Node wa4quivohpee-1
    Jul 29 16:11:45.472: INFO: Pod cilium-node-init-946z9 requesting resource cpu=100m on Node wa4quivohpee-2
    Jul 29 16:11:45.472: INFO: Pod cilium-node-init-j4pkd requesting resource cpu=100m on Node wa4quivohpee-3
    Jul 29 16:11:45.472: INFO: Pod cilium-node-init-n8876 requesting resource cpu=100m on Node wa4quivohpee-1
    Jul 29 16:11:45.472: INFO: Pod cilium-operator-69cffcb958-48gkl requesting resource cpu=0m on Node wa4quivohpee-3
    Jul 29 16:11:45.472: INFO: Pod cilium-rmjkx requesting resource cpu=0m on Node wa4quivohpee-2
    Jul 29 16:11:45.472: INFO: Pod cilium-zl28x requesting resource cpu=0m on Node wa4quivohpee-3
    Jul 29 16:11:45.472: INFO: Pod coredns-565d847f94-wmg75 requesting resource cpu=100m on Node wa4quivohpee-2
    Jul 29 16:11:45.472: INFO: Pod coredns-565d847f94-xm9ff requesting resource cpu=100m on Node wa4quivohpee-1
    Jul 29 16:11:45.472: INFO: Pod kube-addon-manager-wa4quivohpee-1 requesting resource cpu=5m on Node wa4quivohpee-1
    Jul 29 16:11:45.472: INFO: Pod kube-addon-manager-wa4quivohpee-2 requesting resource cpu=5m on Node wa4quivohpee-2
    Jul 29 16:11:45.472: INFO: Pod kube-apiserver-wa4quivohpee-1 requesting resource cpu=250m on Node wa4quivohpee-1
    Jul 29 16:11:45.472: INFO: Pod kube-apiserver-wa4quivohpee-2 requesting resource cpu=250m on Node wa4quivohpee-2
    Jul 29 16:11:45.472: INFO: Pod kube-controller-manager-wa4quivohpee-1 requesting resource cpu=200m on Node wa4quivohpee-1
    Jul 29 16:11:45.472: INFO: Pod kube-controller-manager-wa4quivohpee-2 requesting resource cpu=200m on Node wa4quivohpee-2
    Jul 29 16:11:45.472: INFO: Pod kube-proxy-74hn2 requesting resource cpu=0m on Node wa4quivohpee-2
    Jul 29 16:11:45.472: INFO: Pod kube-proxy-gdffl requesting resource cpu=0m on Node wa4quivohpee-1
    Jul 29 16:11:45.472: INFO: Pod kube-proxy-ts2pg requesting resource cpu=0m on Node wa4quivohpee-3
    Jul 29 16:11:45.472: INFO: Pod kube-scheduler-wa4quivohpee-1 requesting resource cpu=100m on Node wa4quivohpee-1
    Jul 29 16:11:45.472: INFO: Pod kube-scheduler-wa4quivohpee-2 requesting resource cpu=100m on Node wa4quivohpee-2
    Jul 29 16:11:45.472: INFO: Pod sonobuoy requesting resource cpu=0m on Node wa4quivohpee-3
    Jul 29 16:11:45.472: INFO: Pod sonobuoy-e2e-job-5c9a7da3e6b74e15 requesting resource cpu=0m on Node wa4quivohpee-3
    Jul 29 16:11:45.473: INFO: Pod sonobuoy-systemd-logs-daemon-set-b6f0dae2c3174900-cnfp2 requesting resource cpu=0m on Node wa4quivohpee-2
    Jul 29 16:11:45.473: INFO: Pod sonobuoy-systemd-logs-daemon-set-b6f0dae2c3174900-cqnfx requesting resource cpu=0m on Node wa4quivohpee-3
    Jul 29 16:11:45.473: INFO: Pod sonobuoy-systemd-logs-daemon-set-b6f0dae2c3174900-mtck5 requesting resource cpu=0m on Node wa4quivohpee-1
    Jul 29 16:11:45.473: INFO: Pod svc-latency-rc-gksv7 requesting resource cpu=0m on Node wa4quivohpee-3
    STEP: Starting Pods to consume most of the cluster CPU. 07/29/23 16:11:45.473
    Jul 29 16:11:45.473: INFO: Creating a pod which consumes cpu=591m on Node wa4quivohpee-1
    Jul 29 16:11:45.517: INFO: Creating a pod which consumes cpu=591m on Node wa4quivohpee-2
    Jul 29 16:11:45.578: INFO: Creating a pod which consumes cpu=1050m on Node wa4quivohpee-3
    Jul 29 16:11:45.617: INFO: Waiting up to 5m0s for pod "filler-pod-e0f2cd11-f442-406d-876d-2066b8057bc7" in namespace "sched-pred-6898" to be "running"
    Jul 29 16:11:45.643: INFO: Pod "filler-pod-e0f2cd11-f442-406d-876d-2066b8057bc7": Phase="Pending", Reason="", readiness=false. Elapsed: 25.646156ms
    Jul 29 16:11:47.650: INFO: Pod "filler-pod-e0f2cd11-f442-406d-876d-2066b8057bc7": Phase="Running", Reason="", readiness=true. Elapsed: 2.032611886s
    Jul 29 16:11:47.650: INFO: Pod "filler-pod-e0f2cd11-f442-406d-876d-2066b8057bc7" satisfied condition "running"
    Jul 29 16:11:47.650: INFO: Waiting up to 5m0s for pod "filler-pod-a39ddc09-1a95-4c98-a200-3aceff858021" in namespace "sched-pred-6898" to be "running"
    Jul 29 16:11:47.662: INFO: Pod "filler-pod-a39ddc09-1a95-4c98-a200-3aceff858021": Phase="Running", Reason="", readiness=true. Elapsed: 11.581532ms
    Jul 29 16:11:47.662: INFO: Pod "filler-pod-a39ddc09-1a95-4c98-a200-3aceff858021" satisfied condition "running"
    Jul 29 16:11:47.662: INFO: Waiting up to 5m0s for pod "filler-pod-fa931e98-a855-4607-bcb6-7a5525a0fb65" in namespace "sched-pred-6898" to be "running"
    Jul 29 16:11:47.671: INFO: Pod "filler-pod-fa931e98-a855-4607-bcb6-7a5525a0fb65": Phase="Running", Reason="", readiness=true. Elapsed: 8.979879ms
    Jul 29 16:11:47.671: INFO: Pod "filler-pod-fa931e98-a855-4607-bcb6-7a5525a0fb65" satisfied condition "running"
    STEP: Creating another pod that requires unavailable amount of CPU. 07/29/23 16:11:47.671
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-a39ddc09-1a95-4c98-a200-3aceff858021.17766295caf57f8e], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6898/filler-pod-a39ddc09-1a95-4c98-a200-3aceff858021 to wa4quivohpee-2] 07/29/23 16:11:47.679
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-a39ddc09-1a95-4c98-a200-3aceff858021.17766296056cd033], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 07/29/23 16:11:47.68
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-a39ddc09-1a95-4c98-a200-3aceff858021.1776629614e1a582], Reason = [Created], Message = [Created container filler-pod-a39ddc09-1a95-4c98-a200-3aceff858021] 07/29/23 16:11:47.68
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-a39ddc09-1a95-4c98-a200-3aceff858021.1776629617460296], Reason = [Started], Message = [Started container filler-pod-a39ddc09-1a95-4c98-a200-3aceff858021] 07/29/23 16:11:47.68
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-e0f2cd11-f442-406d-876d-2066b8057bc7.17766295c5863296], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6898/filler-pod-e0f2cd11-f442-406d-876d-2066b8057bc7 to wa4quivohpee-1] 07/29/23 16:11:47.68
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-e0f2cd11-f442-406d-876d-2066b8057bc7.177662960d20c976], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 07/29/23 16:11:47.681
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-e0f2cd11-f442-406d-876d-2066b8057bc7.177662962a3e9ddd], Reason = [Created], Message = [Created container filler-pod-e0f2cd11-f442-406d-876d-2066b8057bc7] 07/29/23 16:11:47.681
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-e0f2cd11-f442-406d-876d-2066b8057bc7.177662962eaa6c15], Reason = [Started], Message = [Started container filler-pod-e0f2cd11-f442-406d-876d-2066b8057bc7] 07/29/23 16:11:47.681
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-fa931e98-a855-4607-bcb6-7a5525a0fb65.17766295cd15071c], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6898/filler-pod-fa931e98-a855-4607-bcb6-7a5525a0fb65 to wa4quivohpee-3] 07/29/23 16:11:47.682
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-fa931e98-a855-4607-bcb6-7a5525a0fb65.177662960834beb9], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 07/29/23 16:11:47.682
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-fa931e98-a855-4607-bcb6-7a5525a0fb65.17766296155e9872], Reason = [Created], Message = [Created container filler-pod-fa931e98-a855-4607-bcb6-7a5525a0fb65] 07/29/23 16:11:47.683
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-fa931e98-a855-4607-bcb6-7a5525a0fb65.1776629618febaef], Reason = [Started], Message = [Started container filler-pod-fa931e98-a855-4607-bcb6-7a5525a0fb65] 07/29/23 16:11:47.683
    STEP: Considering event: 
    Type = [Warning], Name = [additional-pod.1776629646bdd2bd], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu. preemption: 0/3 nodes are available: 3 No preemption victims found for incoming pod.] 07/29/23 16:11:47.721
    STEP: removing the label node off the node wa4quivohpee-3 07/29/23 16:11:48.708
    STEP: verifying the node doesn't have the label node 07/29/23 16:11:48.756
    STEP: removing the label node off the node wa4quivohpee-1 07/29/23 16:11:48.784
    STEP: verifying the node doesn't have the label node 07/29/23 16:11:48.856
    STEP: removing the label node off the node wa4quivohpee-2 07/29/23 16:11:48.868
    STEP: verifying the node doesn't have the label node 07/29/23 16:11:48.927
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Jul 29 16:11:48.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-6898" for this suite. 07/29/23 16:11:48.968
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:11:49.059
Jul 29 16:11:49.060: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename init-container 07/29/23 16:11:49.118
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:11:49.212
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:11:49.227
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333
STEP: creating the pod 07/29/23 16:11:49.241
Jul 29 16:11:49.242: INFO: PodSpec: initContainers in spec.initContainers
Jul 29 16:12:35.813: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-0d055477-bce0-439c-b54f-a8c173813758", GenerateName:"", Namespace:"init-container-1588", SelfLink:"", UID:"9989131d-2396-4d61-beb9-27b40c5193e5", ResourceVersion:"17076", Generation:0, CreationTimestamp:time.Date(2023, time.July, 29, 16, 11, 49, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"242358110"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.July, 29, 16, 11, 49, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000fd9530), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.July, 29, 16, 12, 35, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000fd9578), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-pwrss", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc0038c12e0), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-pwrss", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-pwrss", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.8", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-pwrss", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc002c16f10), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"wa4quivohpee-2", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0037e1d50), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002c16fa0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002c16fc0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc002c16fc8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc002c16fcc), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc000a59590), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.July, 29, 16, 11, 49, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.July, 29, 16, 11, 49, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.July, 29, 16, 11, 49, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.July, 29, 16, 11, 49, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.121.206", PodIP:"10.233.66.22", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.233.66.22"}}, StartTime:time.Date(2023, time.July, 29, 16, 11, 49, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0037e1e30)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0037e1ea0)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:c318242786b139d18676b1c09a0ad7f15fc17f8f16a5b2e625cd0dc8c9703daf", ContainerID:"cri-o://539820b831f12a9538e5999e75326e10c0684b2ef4df536e012ddf17032cd7f8", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0038c1440), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0038c1380), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.8", ImageID:"", ContainerID:"", Started:(*bool)(0xc002c1704f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Jul 29 16:12:35.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-1588" for this suite. 07/29/23 16:12:35.847
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","completed":168,"skipped":3231,"failed":0}
------------------------------
â€¢ [SLOW TEST] [46.814 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:11:49.059
    Jul 29 16:11:49.060: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename init-container 07/29/23 16:11:49.118
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:11:49.212
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:11:49.227
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:333
    STEP: creating the pod 07/29/23 16:11:49.241
    Jul 29 16:11:49.242: INFO: PodSpec: initContainers in spec.initContainers
    Jul 29 16:12:35.813: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-0d055477-bce0-439c-b54f-a8c173813758", GenerateName:"", Namespace:"init-container-1588", SelfLink:"", UID:"9989131d-2396-4d61-beb9-27b40c5193e5", ResourceVersion:"17076", Generation:0, CreationTimestamp:time.Date(2023, time.July, 29, 16, 11, 49, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"242358110"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.July, 29, 16, 11, 49, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000fd9530), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.July, 29, 16, 12, 35, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000fd9578), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-pwrss", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc0038c12e0), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-pwrss", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-pwrss", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.8", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-pwrss", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc002c16f10), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"wa4quivohpee-2", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0037e1d50), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002c16fa0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002c16fc0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc002c16fc8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc002c16fcc), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc000a59590), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.July, 29, 16, 11, 49, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.July, 29, 16, 11, 49, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.July, 29, 16, 11, 49, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.July, 29, 16, 11, 49, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.121.206", PodIP:"10.233.66.22", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.233.66.22"}}, StartTime:time.Date(2023, time.July, 29, 16, 11, 49, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0037e1e30)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0037e1ea0)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:c318242786b139d18676b1c09a0ad7f15fc17f8f16a5b2e625cd0dc8c9703daf", ContainerID:"cri-o://539820b831f12a9538e5999e75326e10c0684b2ef4df536e012ddf17032cd7f8", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0038c1440), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0038c1380), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.8", ImageID:"", ContainerID:"", Started:(*bool)(0xc002c1704f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Jul 29 16:12:35.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-1588" for this suite. 07/29/23 16:12:35.847
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:12:35.876
Jul 29 16:12:35.877: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename crd-publish-openapi 07/29/23 16:12:35.88
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:12:35.911
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:12:35.917
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235
Jul 29 16:12:35.922: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 07/29/23 16:12:41.992
Jul 29 16:12:41.993: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=crd-publish-openapi-3792 --namespace=crd-publish-openapi-3792 create -f -'
Jul 29 16:12:43.623: INFO: stderr: ""
Jul 29 16:12:43.623: INFO: stdout: "e2e-test-crd-publish-openapi-8787-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jul 29 16:12:43.623: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=crd-publish-openapi-3792 --namespace=crd-publish-openapi-3792 delete e2e-test-crd-publish-openapi-8787-crds test-cr'
Jul 29 16:12:43.756: INFO: stderr: ""
Jul 29 16:12:43.756: INFO: stdout: "e2e-test-crd-publish-openapi-8787-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Jul 29 16:12:43.756: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=crd-publish-openapi-3792 --namespace=crd-publish-openapi-3792 apply -f -'
Jul 29 16:12:44.971: INFO: stderr: ""
Jul 29 16:12:44.971: INFO: stdout: "e2e-test-crd-publish-openapi-8787-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jul 29 16:12:44.972: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=crd-publish-openapi-3792 --namespace=crd-publish-openapi-3792 delete e2e-test-crd-publish-openapi-8787-crds test-cr'
Jul 29 16:12:45.165: INFO: stderr: ""
Jul 29 16:12:45.165: INFO: stdout: "e2e-test-crd-publish-openapi-8787-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 07/29/23 16:12:45.165
Jul 29 16:12:45.166: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=crd-publish-openapi-3792 explain e2e-test-crd-publish-openapi-8787-crds'
Jul 29 16:12:45.657: INFO: stderr: ""
Jul 29 16:12:45.658: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8787-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jul 29 16:12:51.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3792" for this suite. 07/29/23 16:12:51.531
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","completed":169,"skipped":3234,"failed":0}
------------------------------
â€¢ [SLOW TEST] [15.668 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:12:35.876
    Jul 29 16:12:35.877: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename crd-publish-openapi 07/29/23 16:12:35.88
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:12:35.911
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:12:35.917
    [It] works for CRD preserving unknown fields in an embedded object [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:235
    Jul 29 16:12:35.922: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 07/29/23 16:12:41.992
    Jul 29 16:12:41.993: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=crd-publish-openapi-3792 --namespace=crd-publish-openapi-3792 create -f -'
    Jul 29 16:12:43.623: INFO: stderr: ""
    Jul 29 16:12:43.623: INFO: stdout: "e2e-test-crd-publish-openapi-8787-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Jul 29 16:12:43.623: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=crd-publish-openapi-3792 --namespace=crd-publish-openapi-3792 delete e2e-test-crd-publish-openapi-8787-crds test-cr'
    Jul 29 16:12:43.756: INFO: stderr: ""
    Jul 29 16:12:43.756: INFO: stdout: "e2e-test-crd-publish-openapi-8787-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    Jul 29 16:12:43.756: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=crd-publish-openapi-3792 --namespace=crd-publish-openapi-3792 apply -f -'
    Jul 29 16:12:44.971: INFO: stderr: ""
    Jul 29 16:12:44.971: INFO: stdout: "e2e-test-crd-publish-openapi-8787-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Jul 29 16:12:44.972: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=crd-publish-openapi-3792 --namespace=crd-publish-openapi-3792 delete e2e-test-crd-publish-openapi-8787-crds test-cr'
    Jul 29 16:12:45.165: INFO: stderr: ""
    Jul 29 16:12:45.165: INFO: stdout: "e2e-test-crd-publish-openapi-8787-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 07/29/23 16:12:45.165
    Jul 29 16:12:45.166: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=crd-publish-openapi-3792 explain e2e-test-crd-publish-openapi-8787-crds'
    Jul 29 16:12:45.657: INFO: stderr: ""
    Jul 29 16:12:45.658: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8787-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jul 29 16:12:51.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-3792" for this suite. 07/29/23 16:12:51.531
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:12:51.549
Jul 29 16:12:51.549: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename downward-api 07/29/23 16:12:51.551
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:12:51.582
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:12:51.587
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67
STEP: Creating a pod to test downward API volume plugin 07/29/23 16:12:51.591
Jul 29 16:12:51.606: INFO: Waiting up to 5m0s for pod "downwardapi-volume-43502cc7-2221-46b0-a926-3c61a899e9f1" in namespace "downward-api-7233" to be "Succeeded or Failed"
Jul 29 16:12:51.611: INFO: Pod "downwardapi-volume-43502cc7-2221-46b0-a926-3c61a899e9f1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.626959ms
Jul 29 16:12:53.620: INFO: Pod "downwardapi-volume-43502cc7-2221-46b0-a926-3c61a899e9f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014195991s
Jul 29 16:12:55.625: INFO: Pod "downwardapi-volume-43502cc7-2221-46b0-a926-3c61a899e9f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018798582s
STEP: Saw pod success 07/29/23 16:12:55.625
Jul 29 16:12:55.626: INFO: Pod "downwardapi-volume-43502cc7-2221-46b0-a926-3c61a899e9f1" satisfied condition "Succeeded or Failed"
Jul 29 16:12:55.633: INFO: Trying to get logs from node wa4quivohpee-3 pod downwardapi-volume-43502cc7-2221-46b0-a926-3c61a899e9f1 container client-container: <nil>
STEP: delete the pod 07/29/23 16:12:55.659
Jul 29 16:12:55.679: INFO: Waiting for pod downwardapi-volume-43502cc7-2221-46b0-a926-3c61a899e9f1 to disappear
Jul 29 16:12:55.685: INFO: Pod downwardapi-volume-43502cc7-2221-46b0-a926-3c61a899e9f1 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jul 29 16:12:55.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7233" for this suite. 07/29/23 16:12:55.695
{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","completed":170,"skipped":3240,"failed":0}
------------------------------
â€¢ [4.159 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:12:51.549
    Jul 29 16:12:51.549: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename downward-api 07/29/23 16:12:51.551
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:12:51.582
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:12:51.587
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:67
    STEP: Creating a pod to test downward API volume plugin 07/29/23 16:12:51.591
    Jul 29 16:12:51.606: INFO: Waiting up to 5m0s for pod "downwardapi-volume-43502cc7-2221-46b0-a926-3c61a899e9f1" in namespace "downward-api-7233" to be "Succeeded or Failed"
    Jul 29 16:12:51.611: INFO: Pod "downwardapi-volume-43502cc7-2221-46b0-a926-3c61a899e9f1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.626959ms
    Jul 29 16:12:53.620: INFO: Pod "downwardapi-volume-43502cc7-2221-46b0-a926-3c61a899e9f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014195991s
    Jul 29 16:12:55.625: INFO: Pod "downwardapi-volume-43502cc7-2221-46b0-a926-3c61a899e9f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018798582s
    STEP: Saw pod success 07/29/23 16:12:55.625
    Jul 29 16:12:55.626: INFO: Pod "downwardapi-volume-43502cc7-2221-46b0-a926-3c61a899e9f1" satisfied condition "Succeeded or Failed"
    Jul 29 16:12:55.633: INFO: Trying to get logs from node wa4quivohpee-3 pod downwardapi-volume-43502cc7-2221-46b0-a926-3c61a899e9f1 container client-container: <nil>
    STEP: delete the pod 07/29/23 16:12:55.659
    Jul 29 16:12:55.679: INFO: Waiting for pod downwardapi-volume-43502cc7-2221-46b0-a926-3c61a899e9f1 to disappear
    Jul 29 16:12:55.685: INFO: Pod downwardapi-volume-43502cc7-2221-46b0-a926-3c61a899e9f1 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jul 29 16:12:55.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-7233" for this suite. 07/29/23 16:12:55.695
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:12:55.719
Jul 29 16:12:55.719: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename replicaset 07/29/23 16:12:55.721
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:12:55.748
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:12:55.754
[It] should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
STEP: Create a ReplicaSet 07/29/23 16:12:55.761
STEP: Verify that the required pods have come up 07/29/23 16:12:55.774
Jul 29 16:12:55.779: INFO: Pod name sample-pod: Found 0 pods out of 3
Jul 29 16:13:00.786: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running 07/29/23 16:13:00.787
Jul 29 16:13:00.795: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets 07/29/23 16:13:00.795
STEP: DeleteCollection of the ReplicaSets 07/29/23 16:13:00.801
STEP: After DeleteCollection verify that ReplicaSets have been deleted 07/29/23 16:13:00.817
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Jul 29 16:13:00.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-8403" for this suite. 07/29/23 16:13:00.835
{"msg":"PASSED [sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]","completed":171,"skipped":3269,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.127 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:12:55.719
    Jul 29 16:12:55.719: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename replicaset 07/29/23 16:12:55.721
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:12:55.748
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:12:55.754
    [It] should list and delete a collection of ReplicaSets [Conformance]
      test/e2e/apps/replica_set.go:165
    STEP: Create a ReplicaSet 07/29/23 16:12:55.761
    STEP: Verify that the required pods have come up 07/29/23 16:12:55.774
    Jul 29 16:12:55.779: INFO: Pod name sample-pod: Found 0 pods out of 3
    Jul 29 16:13:00.786: INFO: Pod name sample-pod: Found 3 pods out of 3
    STEP: ensuring each pod is running 07/29/23 16:13:00.787
    Jul 29 16:13:00.795: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
    STEP: Listing all ReplicaSets 07/29/23 16:13:00.795
    STEP: DeleteCollection of the ReplicaSets 07/29/23 16:13:00.801
    STEP: After DeleteCollection verify that ReplicaSets have been deleted 07/29/23 16:13:00.817
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Jul 29 16:13:00.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-8403" for this suite. 07/29/23 16:13:00.835
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:13:00.847
Jul 29 16:13:00.847: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename webhook 07/29/23 16:13:00.849
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:13:00.913
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:13:00.917
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 07/29/23 16:13:01.008
STEP: Create role binding to let webhook read extension-apiserver-authentication 07/29/23 16:13:01.469
STEP: Deploying the webhook pod 07/29/23 16:13:01.48
STEP: Wait for the deployment to be ready 07/29/23 16:13:01.5
Jul 29 16:13:01.512: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jul 29 16:13:03.530: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 16, 13, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 16, 13, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 16, 13, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 16, 13, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 07/29/23 16:13:05.536
STEP: Verifying the service has paired with the endpoint 07/29/23 16:13:05.559
Jul 29 16:13:06.561: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380
STEP: Setting timeout (1s) shorter than webhook latency (5s) 07/29/23 16:13:06.569
STEP: Registering slow webhook via the AdmissionRegistration API 07/29/23 16:13:06.569
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 07/29/23 16:13:06.604
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 07/29/23 16:13:07.623
STEP: Registering slow webhook via the AdmissionRegistration API 07/29/23 16:13:07.624
STEP: Having no error when timeout is longer than webhook latency 07/29/23 16:13:08.675
STEP: Registering slow webhook via the AdmissionRegistration API 07/29/23 16:13:08.675
STEP: Having no error when timeout is empty (defaulted to 10s in v1) 07/29/23 16:13:13.734
STEP: Registering slow webhook via the AdmissionRegistration API 07/29/23 16:13:13.734
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jul 29 16:13:18.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9508" for this suite. 07/29/23 16:13:18.789
STEP: Destroying namespace "webhook-9508-markers" for this suite. 07/29/23 16:13:18.799
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","completed":172,"skipped":3272,"failed":0}
------------------------------
â€¢ [SLOW TEST] [18.029 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:13:00.847
    Jul 29 16:13:00.847: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename webhook 07/29/23 16:13:00.849
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:13:00.913
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:13:00.917
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 07/29/23 16:13:01.008
    STEP: Create role binding to let webhook read extension-apiserver-authentication 07/29/23 16:13:01.469
    STEP: Deploying the webhook pod 07/29/23 16:13:01.48
    STEP: Wait for the deployment to be ready 07/29/23 16:13:01.5
    Jul 29 16:13:01.512: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    Jul 29 16:13:03.530: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 16, 13, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 16, 13, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 16, 13, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 16, 13, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 07/29/23 16:13:05.536
    STEP: Verifying the service has paired with the endpoint 07/29/23 16:13:05.559
    Jul 29 16:13:06.561: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should honor timeout [Conformance]
      test/e2e/apimachinery/webhook.go:380
    STEP: Setting timeout (1s) shorter than webhook latency (5s) 07/29/23 16:13:06.569
    STEP: Registering slow webhook via the AdmissionRegistration API 07/29/23 16:13:06.569
    STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 07/29/23 16:13:06.604
    STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 07/29/23 16:13:07.623
    STEP: Registering slow webhook via the AdmissionRegistration API 07/29/23 16:13:07.624
    STEP: Having no error when timeout is longer than webhook latency 07/29/23 16:13:08.675
    STEP: Registering slow webhook via the AdmissionRegistration API 07/29/23 16:13:08.675
    STEP: Having no error when timeout is empty (defaulted to 10s in v1) 07/29/23 16:13:13.734
    STEP: Registering slow webhook via the AdmissionRegistration API 07/29/23 16:13:13.734
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jul 29 16:13:18.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-9508" for this suite. 07/29/23 16:13:18.789
    STEP: Destroying namespace "webhook-9508-markers" for this suite. 07/29/23 16:13:18.799
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:13:18.888
Jul 29 16:13:18.889: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename secrets 07/29/23 16:13:18.903
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:13:18.965
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:13:19.017
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45
STEP: Creating secret with name secret-test-96d7efe7-e7a6-4386-95f5-70e5e9288dc1 07/29/23 16:13:19.029
STEP: Creating a pod to test consume secrets 07/29/23 16:13:19.04
Jul 29 16:13:19.061: INFO: Waiting up to 5m0s for pod "pod-secrets-ec5a08c2-0cac-4d7a-9831-99c8cf3a69d4" in namespace "secrets-988" to be "Succeeded or Failed"
Jul 29 16:13:19.076: INFO: Pod "pod-secrets-ec5a08c2-0cac-4d7a-9831-99c8cf3a69d4": Phase="Pending", Reason="", readiness=false. Elapsed: 14.824594ms
Jul 29 16:13:21.091: INFO: Pod "pod-secrets-ec5a08c2-0cac-4d7a-9831-99c8cf3a69d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03009006s
Jul 29 16:13:23.094: INFO: Pod "pod-secrets-ec5a08c2-0cac-4d7a-9831-99c8cf3a69d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032832527s
STEP: Saw pod success 07/29/23 16:13:23.094
Jul 29 16:13:23.095: INFO: Pod "pod-secrets-ec5a08c2-0cac-4d7a-9831-99c8cf3a69d4" satisfied condition "Succeeded or Failed"
Jul 29 16:13:23.102: INFO: Trying to get logs from node wa4quivohpee-3 pod pod-secrets-ec5a08c2-0cac-4d7a-9831-99c8cf3a69d4 container secret-env-test: <nil>
STEP: delete the pod 07/29/23 16:13:23.122
Jul 29 16:13:23.147: INFO: Waiting for pod pod-secrets-ec5a08c2-0cac-4d7a-9831-99c8cf3a69d4 to disappear
Jul 29 16:13:23.152: INFO: Pod pod-secrets-ec5a08c2-0cac-4d7a-9831-99c8cf3a69d4 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Jul 29 16:13:23.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-988" for this suite. 07/29/23 16:13:23.16
{"msg":"PASSED [sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","completed":173,"skipped":3303,"failed":0}
------------------------------
â€¢ [4.306 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:13:18.888
    Jul 29 16:13:18.889: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename secrets 07/29/23 16:13:18.903
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:13:18.965
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:13:19.017
    [It] should be consumable from pods in env vars [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:45
    STEP: Creating secret with name secret-test-96d7efe7-e7a6-4386-95f5-70e5e9288dc1 07/29/23 16:13:19.029
    STEP: Creating a pod to test consume secrets 07/29/23 16:13:19.04
    Jul 29 16:13:19.061: INFO: Waiting up to 5m0s for pod "pod-secrets-ec5a08c2-0cac-4d7a-9831-99c8cf3a69d4" in namespace "secrets-988" to be "Succeeded or Failed"
    Jul 29 16:13:19.076: INFO: Pod "pod-secrets-ec5a08c2-0cac-4d7a-9831-99c8cf3a69d4": Phase="Pending", Reason="", readiness=false. Elapsed: 14.824594ms
    Jul 29 16:13:21.091: INFO: Pod "pod-secrets-ec5a08c2-0cac-4d7a-9831-99c8cf3a69d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03009006s
    Jul 29 16:13:23.094: INFO: Pod "pod-secrets-ec5a08c2-0cac-4d7a-9831-99c8cf3a69d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032832527s
    STEP: Saw pod success 07/29/23 16:13:23.094
    Jul 29 16:13:23.095: INFO: Pod "pod-secrets-ec5a08c2-0cac-4d7a-9831-99c8cf3a69d4" satisfied condition "Succeeded or Failed"
    Jul 29 16:13:23.102: INFO: Trying to get logs from node wa4quivohpee-3 pod pod-secrets-ec5a08c2-0cac-4d7a-9831-99c8cf3a69d4 container secret-env-test: <nil>
    STEP: delete the pod 07/29/23 16:13:23.122
    Jul 29 16:13:23.147: INFO: Waiting for pod pod-secrets-ec5a08c2-0cac-4d7a-9831-99c8cf3a69d4 to disappear
    Jul 29 16:13:23.152: INFO: Pod pod-secrets-ec5a08c2-0cac-4d7a-9831-99c8cf3a69d4 no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Jul 29 16:13:23.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-988" for this suite. 07/29/23 16:13:23.16
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:13:23.197
Jul 29 16:13:23.201: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename runtimeclass 07/29/23 16:13:23.204
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:13:23.24
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:13:23.245
[It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
Jul 29 16:13:23.270: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-3021 to be scheduled
Jul 29 16:13:23.276: INFO: 1 pods are not scheduled: [runtimeclass-3021/test-runtimeclass-runtimeclass-3021-preconfigured-handler-dcz6c(f3b9e517-5437-4f01-b715-203caf94ecea)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Jul 29 16:13:25.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-3021" for this suite. 07/29/23 16:13:25.303
{"msg":"PASSED [sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]","completed":174,"skipped":3304,"failed":0}
------------------------------
â€¢ [2.114 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:13:23.197
    Jul 29 16:13:23.201: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename runtimeclass 07/29/23 16:13:23.204
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:13:23.24
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:13:23.245
    [It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:129
    Jul 29 16:13:23.270: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-3021 to be scheduled
    Jul 29 16:13:23.276: INFO: 1 pods are not scheduled: [runtimeclass-3021/test-runtimeclass-runtimeclass-3021-preconfigured-handler-dcz6c(f3b9e517-5437-4f01-b715-203caf94ecea)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Jul 29 16:13:25.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-3021" for this suite. 07/29/23 16:13:25.303
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:13:25.316
Jul 29 16:13:25.316: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename custom-resource-definition 07/29/23 16:13:25.32
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:13:25.347
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:13:25.355
[It] custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
Jul 29 16:13:25.359: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jul 29 16:13:28.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-9136" for this suite. 07/29/23 16:13:28.89
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","completed":175,"skipped":3307,"failed":0}
------------------------------
â€¢ [3.583 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:13:25.316
    Jul 29 16:13:25.316: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename custom-resource-definition 07/29/23 16:13:25.32
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:13:25.347
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:13:25.355
    [It] custom resource defaulting for requests and from storage works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:269
    Jul 29 16:13:25.359: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jul 29 16:13:28.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-9136" for this suite. 07/29/23 16:13:28.89
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:13:28.901
Jul 29 16:13:28.901: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename webhook 07/29/23 16:13:28.903
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:13:28.931
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:13:28.935
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 07/29/23 16:13:28.957
STEP: Create role binding to let webhook read extension-apiserver-authentication 07/29/23 16:13:29.548
STEP: Deploying the webhook pod 07/29/23 16:13:29.558
STEP: Wait for the deployment to be ready 07/29/23 16:13:29.577
Jul 29 16:13:29.604: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jul 29 16:13:31.629: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 16, 13, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 16, 13, 29, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 16, 13, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 16, 13, 29, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 07/29/23 16:13:33.638
STEP: Verifying the service has paired with the endpoint 07/29/23 16:13:33.657
Jul 29 16:13:34.658: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290
Jul 29 16:13:34.665: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-5049-crds.webhook.example.com via the AdmissionRegistration API 07/29/23 16:13:35.18
STEP: Creating a custom resource that should be mutated by the webhook 07/29/23 16:13:35.205
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jul 29 16:13:37.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3531" for this suite. 07/29/23 16:13:37.917
STEP: Destroying namespace "webhook-3531-markers" for this suite. 07/29/23 16:13:37.932
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","completed":176,"skipped":3321,"failed":0}
------------------------------
â€¢ [SLOW TEST] [9.108 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:13:28.901
    Jul 29 16:13:28.901: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename webhook 07/29/23 16:13:28.903
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:13:28.931
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:13:28.935
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 07/29/23 16:13:28.957
    STEP: Create role binding to let webhook read extension-apiserver-authentication 07/29/23 16:13:29.548
    STEP: Deploying the webhook pod 07/29/23 16:13:29.558
    STEP: Wait for the deployment to be ready 07/29/23 16:13:29.577
    Jul 29 16:13:29.604: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jul 29 16:13:31.629: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 16, 13, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 16, 13, 29, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 16, 13, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 16, 13, 29, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 07/29/23 16:13:33.638
    STEP: Verifying the service has paired with the endpoint 07/29/23 16:13:33.657
    Jul 29 16:13:34.658: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource [Conformance]
      test/e2e/apimachinery/webhook.go:290
    Jul 29 16:13:34.665: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-5049-crds.webhook.example.com via the AdmissionRegistration API 07/29/23 16:13:35.18
    STEP: Creating a custom resource that should be mutated by the webhook 07/29/23 16:13:35.205
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jul 29 16:13:37.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-3531" for this suite. 07/29/23 16:13:37.917
    STEP: Destroying namespace "webhook-3531-markers" for this suite. 07/29/23 16:13:37.932
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Probing container
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:13:38.018
Jul 29 16:13:38.022: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename container-probe 07/29/23 16:13:38.024
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:13:38.071
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:13:38.077
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165
STEP: Creating pod liveness-83c300bb-8ab2-4502-b49a-774c11cf9692 in namespace container-probe-7452 07/29/23 16:13:38.083
Jul 29 16:13:38.115: INFO: Waiting up to 5m0s for pod "liveness-83c300bb-8ab2-4502-b49a-774c11cf9692" in namespace "container-probe-7452" to be "not pending"
Jul 29 16:13:38.131: INFO: Pod "liveness-83c300bb-8ab2-4502-b49a-774c11cf9692": Phase="Pending", Reason="", readiness=false. Elapsed: 5.54275ms
Jul 29 16:13:40.139: INFO: Pod "liveness-83c300bb-8ab2-4502-b49a-774c11cf9692": Phase="Running", Reason="", readiness=true. Elapsed: 2.013572175s
Jul 29 16:13:40.139: INFO: Pod "liveness-83c300bb-8ab2-4502-b49a-774c11cf9692" satisfied condition "not pending"
Jul 29 16:13:40.139: INFO: Started pod liveness-83c300bb-8ab2-4502-b49a-774c11cf9692 in namespace container-probe-7452
STEP: checking the pod's current state and verifying that restartCount is present 07/29/23 16:13:40.139
Jul 29 16:13:40.146: INFO: Initial restart count of pod liveness-83c300bb-8ab2-4502-b49a-774c11cf9692 is 0
Jul 29 16:14:00.245: INFO: Restart count of pod container-probe-7452/liveness-83c300bb-8ab2-4502-b49a-774c11cf9692 is now 1 (20.099271016s elapsed)
STEP: deleting the pod 07/29/23 16:14:00.245
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jul 29 16:14:00.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7452" for this suite. 07/29/23 16:14:00.278
{"msg":"PASSED [sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","completed":177,"skipped":3321,"failed":0}
------------------------------
â€¢ [SLOW TEST] [22.273 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:13:38.018
    Jul 29 16:13:38.022: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename container-probe 07/29/23 16:13:38.024
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:13:38.071
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:13:38.077
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:165
    STEP: Creating pod liveness-83c300bb-8ab2-4502-b49a-774c11cf9692 in namespace container-probe-7452 07/29/23 16:13:38.083
    Jul 29 16:13:38.115: INFO: Waiting up to 5m0s for pod "liveness-83c300bb-8ab2-4502-b49a-774c11cf9692" in namespace "container-probe-7452" to be "not pending"
    Jul 29 16:13:38.131: INFO: Pod "liveness-83c300bb-8ab2-4502-b49a-774c11cf9692": Phase="Pending", Reason="", readiness=false. Elapsed: 5.54275ms
    Jul 29 16:13:40.139: INFO: Pod "liveness-83c300bb-8ab2-4502-b49a-774c11cf9692": Phase="Running", Reason="", readiness=true. Elapsed: 2.013572175s
    Jul 29 16:13:40.139: INFO: Pod "liveness-83c300bb-8ab2-4502-b49a-774c11cf9692" satisfied condition "not pending"
    Jul 29 16:13:40.139: INFO: Started pod liveness-83c300bb-8ab2-4502-b49a-774c11cf9692 in namespace container-probe-7452
    STEP: checking the pod's current state and verifying that restartCount is present 07/29/23 16:13:40.139
    Jul 29 16:13:40.146: INFO: Initial restart count of pod liveness-83c300bb-8ab2-4502-b49a-774c11cf9692 is 0
    Jul 29 16:14:00.245: INFO: Restart count of pod container-probe-7452/liveness-83c300bb-8ab2-4502-b49a-774c11cf9692 is now 1 (20.099271016s elapsed)
    STEP: deleting the pod 07/29/23 16:14:00.245
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jul 29 16:14:00.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-7452" for this suite. 07/29/23 16:14:00.278
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:14:00.299
Jul 29 16:14:00.299: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename projected 07/29/23 16:14:00.302
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:14:00.338
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:14:00.346
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83
STEP: Creating a pod to test downward API volume plugin 07/29/23 16:14:00.352
Jul 29 16:14:00.370: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a3a2909b-0302-43c7-b81a-cb8ab3eabbb2" in namespace "projected-7252" to be "Succeeded or Failed"
Jul 29 16:14:00.378: INFO: Pod "downwardapi-volume-a3a2909b-0302-43c7-b81a-cb8ab3eabbb2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.378593ms
Jul 29 16:14:02.386: INFO: Pod "downwardapi-volume-a3a2909b-0302-43c7-b81a-cb8ab3eabbb2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016055836s
Jul 29 16:14:04.386: INFO: Pod "downwardapi-volume-a3a2909b-0302-43c7-b81a-cb8ab3eabbb2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016330236s
STEP: Saw pod success 07/29/23 16:14:04.386
Jul 29 16:14:04.387: INFO: Pod "downwardapi-volume-a3a2909b-0302-43c7-b81a-cb8ab3eabbb2" satisfied condition "Succeeded or Failed"
Jul 29 16:14:04.392: INFO: Trying to get logs from node wa4quivohpee-3 pod downwardapi-volume-a3a2909b-0302-43c7-b81a-cb8ab3eabbb2 container client-container: <nil>
STEP: delete the pod 07/29/23 16:14:04.406
Jul 29 16:14:04.428: INFO: Waiting for pod downwardapi-volume-a3a2909b-0302-43c7-b81a-cb8ab3eabbb2 to disappear
Jul 29 16:14:04.433: INFO: Pod downwardapi-volume-a3a2909b-0302-43c7-b81a-cb8ab3eabbb2 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jul 29 16:14:04.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7252" for this suite. 07/29/23 16:14:04.452
{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","completed":178,"skipped":3350,"failed":0}
------------------------------
â€¢ [4.164 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:14:00.299
    Jul 29 16:14:00.299: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename projected 07/29/23 16:14:00.302
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:14:00.338
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:14:00.346
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:83
    STEP: Creating a pod to test downward API volume plugin 07/29/23 16:14:00.352
    Jul 29 16:14:00.370: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a3a2909b-0302-43c7-b81a-cb8ab3eabbb2" in namespace "projected-7252" to be "Succeeded or Failed"
    Jul 29 16:14:00.378: INFO: Pod "downwardapi-volume-a3a2909b-0302-43c7-b81a-cb8ab3eabbb2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.378593ms
    Jul 29 16:14:02.386: INFO: Pod "downwardapi-volume-a3a2909b-0302-43c7-b81a-cb8ab3eabbb2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016055836s
    Jul 29 16:14:04.386: INFO: Pod "downwardapi-volume-a3a2909b-0302-43c7-b81a-cb8ab3eabbb2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016330236s
    STEP: Saw pod success 07/29/23 16:14:04.386
    Jul 29 16:14:04.387: INFO: Pod "downwardapi-volume-a3a2909b-0302-43c7-b81a-cb8ab3eabbb2" satisfied condition "Succeeded or Failed"
    Jul 29 16:14:04.392: INFO: Trying to get logs from node wa4quivohpee-3 pod downwardapi-volume-a3a2909b-0302-43c7-b81a-cb8ab3eabbb2 container client-container: <nil>
    STEP: delete the pod 07/29/23 16:14:04.406
    Jul 29 16:14:04.428: INFO: Waiting for pod downwardapi-volume-a3a2909b-0302-43c7-b81a-cb8ab3eabbb2 to disappear
    Jul 29 16:14:04.433: INFO: Pod downwardapi-volume-a3a2909b-0302-43c7-b81a-cb8ab3eabbb2 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jul 29 16:14:04.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-7252" for this suite. 07/29/23 16:14:04.452
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:247
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:14:04.467
Jul 29 16:14:04.467: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename container-runtime 07/29/23 16:14:04.47
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:14:04.503
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:14:04.506
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:247
STEP: create the container 07/29/23 16:14:04.512
STEP: wait for the container to reach Succeeded 07/29/23 16:14:04.531
STEP: get the container status 07/29/23 16:14:08.566
STEP: the container should be terminated 07/29/23 16:14:08.571
STEP: the termination message should be set 07/29/23 16:14:08.571
Jul 29 16:14:08.572: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container 07/29/23 16:14:08.572
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Jul 29 16:14:08.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8093" for this suite. 07/29/23 16:14:08.63
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":179,"skipped":3370,"failed":0}
------------------------------
â€¢ [4.177 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:247

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:14:04.467
    Jul 29 16:14:04.467: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename container-runtime 07/29/23 16:14:04.47
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:14:04.503
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:14:04.506
    [It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:247
    STEP: create the container 07/29/23 16:14:04.512
    STEP: wait for the container to reach Succeeded 07/29/23 16:14:04.531
    STEP: get the container status 07/29/23 16:14:08.566
    STEP: the container should be terminated 07/29/23 16:14:08.571
    STEP: the termination message should be set 07/29/23 16:14:08.571
    Jul 29 16:14:08.572: INFO: Expected: &{OK} to match Container's Termination Message: OK --
    STEP: delete the container 07/29/23 16:14:08.572
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Jul 29 16:14:08.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-8093" for this suite. 07/29/23 16:14:08.63
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:14:08.646
Jul 29 16:14:08.646: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename webhook 07/29/23 16:14:08.648
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:14:08.679
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:14:08.685
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 07/29/23 16:14:08.72
STEP: Create role binding to let webhook read extension-apiserver-authentication 07/29/23 16:14:09.601
STEP: Deploying the webhook pod 07/29/23 16:14:09.619
STEP: Wait for the deployment to be ready 07/29/23 16:14:09.643
Jul 29 16:14:09.666: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 07/29/23 16:14:11.682
STEP: Verifying the service has paired with the endpoint 07/29/23 16:14:11.704
Jul 29 16:14:12.705: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 07/29/23 16:14:12.714
Jul 29 16:14:12.748: INFO: Waiting for webhook configuration to be ready...
STEP: create a namespace for the webhook 07/29/23 16:14:12.871
STEP: create a configmap should be unconditionally rejected by the webhook 07/29/23 16:14:12.89
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jul 29 16:14:12.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4351" for this suite. 07/29/23 16:14:12.969
STEP: Destroying namespace "webhook-4351-markers" for this suite. 07/29/23 16:14:12.98
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","completed":180,"skipped":3379,"failed":0}
------------------------------
â€¢ [4.426 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:14:08.646
    Jul 29 16:14:08.646: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename webhook 07/29/23 16:14:08.648
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:14:08.679
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:14:08.685
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 07/29/23 16:14:08.72
    STEP: Create role binding to let webhook read extension-apiserver-authentication 07/29/23 16:14:09.601
    STEP: Deploying the webhook pod 07/29/23 16:14:09.619
    STEP: Wait for the deployment to be ready 07/29/23 16:14:09.643
    Jul 29 16:14:09.666: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 07/29/23 16:14:11.682
    STEP: Verifying the service has paired with the endpoint 07/29/23 16:14:11.704
    Jul 29 16:14:12.705: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should unconditionally reject operations on fail closed webhook [Conformance]
      test/e2e/apimachinery/webhook.go:238
    STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 07/29/23 16:14:12.714
    Jul 29 16:14:12.748: INFO: Waiting for webhook configuration to be ready...
    STEP: create a namespace for the webhook 07/29/23 16:14:12.871
    STEP: create a configmap should be unconditionally rejected by the webhook 07/29/23 16:14:12.89
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jul 29 16:14:12.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-4351" for this suite. 07/29/23 16:14:12.969
    STEP: Destroying namespace "webhook-4351-markers" for this suite. 07/29/23 16:14:12.98
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:14:13.083
Jul 29 16:14:13.084: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename secrets 07/29/23 16:14:13.09
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:14:13.132
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:14:13.14
[It] should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153
STEP: creating a secret 07/29/23 16:14:13.145
STEP: listing secrets in all namespaces to ensure that there are more than zero 07/29/23 16:14:13.212
STEP: patching the secret 07/29/23 16:14:13.229
STEP: deleting the secret using a LabelSelector 07/29/23 16:14:13.255
STEP: listing secrets in all namespaces, searching for label name and value in patch 07/29/23 16:14:13.273
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Jul 29 16:14:13.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6564" for this suite. 07/29/23 16:14:13.291
{"msg":"PASSED [sig-node] Secrets should patch a secret [Conformance]","completed":181,"skipped":3397,"failed":0}
------------------------------
â€¢ [0.220 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:14:13.083
    Jul 29 16:14:13.084: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename secrets 07/29/23 16:14:13.09
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:14:13.132
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:14:13.14
    [It] should patch a secret [Conformance]
      test/e2e/common/node/secrets.go:153
    STEP: creating a secret 07/29/23 16:14:13.145
    STEP: listing secrets in all namespaces to ensure that there are more than zero 07/29/23 16:14:13.212
    STEP: patching the secret 07/29/23 16:14:13.229
    STEP: deleting the secret using a LabelSelector 07/29/23 16:14:13.255
    STEP: listing secrets in all namespaces, searching for label name and value in patch 07/29/23 16:14:13.273
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Jul 29 16:14:13.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-6564" for this suite. 07/29/23 16:14:13.291
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:215
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:14:13.307
Jul 29 16:14:13.307: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename container-runtime 07/29/23 16:14:13.309
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:14:13.335
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:14:13.34
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:215
STEP: create the container 07/29/23 16:14:13.349
STEP: wait for the container to reach Failed 07/29/23 16:14:13.365
STEP: get the container status 07/29/23 16:14:17.408
STEP: the container should be terminated 07/29/23 16:14:17.413
STEP: the termination message should be set 07/29/23 16:14:17.413
Jul 29 16:14:17.413: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 07/29/23 16:14:17.413
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Jul 29 16:14:17.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3080" for this suite. 07/29/23 16:14:17.453
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":182,"skipped":3398,"failed":0}
------------------------------
â€¢ [4.159 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:14:13.307
    Jul 29 16:14:13.307: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename container-runtime 07/29/23 16:14:13.309
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:14:13.335
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:14:13.34
    [It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:215
    STEP: create the container 07/29/23 16:14:13.349
    STEP: wait for the container to reach Failed 07/29/23 16:14:13.365
    STEP: get the container status 07/29/23 16:14:17.408
    STEP: the container should be terminated 07/29/23 16:14:17.413
    STEP: the termination message should be set 07/29/23 16:14:17.413
    Jul 29 16:14:17.413: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 07/29/23 16:14:17.413
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Jul 29 16:14:17.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-3080" for this suite. 07/29/23 16:14:17.453
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:14:17.475
Jul 29 16:14:17.475: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename secrets 07/29/23 16:14:17.477
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:14:17.508
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:14:17.513
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56
STEP: Creating secret with name secret-test-ad79e801-7b8b-4e23-a357-aa5f5ef26fb0 07/29/23 16:14:17.518
STEP: Creating a pod to test consume secrets 07/29/23 16:14:17.527
Jul 29 16:14:17.541: INFO: Waiting up to 5m0s for pod "pod-secrets-bfd80800-8963-4ed7-b094-e4b8becb7a6d" in namespace "secrets-6287" to be "Succeeded or Failed"
Jul 29 16:14:17.548: INFO: Pod "pod-secrets-bfd80800-8963-4ed7-b094-e4b8becb7a6d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.992755ms
Jul 29 16:14:19.557: INFO: Pod "pod-secrets-bfd80800-8963-4ed7-b094-e4b8becb7a6d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015684745s
Jul 29 16:14:21.558: INFO: Pod "pod-secrets-bfd80800-8963-4ed7-b094-e4b8becb7a6d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016527636s
STEP: Saw pod success 07/29/23 16:14:21.558
Jul 29 16:14:21.558: INFO: Pod "pod-secrets-bfd80800-8963-4ed7-b094-e4b8becb7a6d" satisfied condition "Succeeded or Failed"
Jul 29 16:14:21.564: INFO: Trying to get logs from node wa4quivohpee-3 pod pod-secrets-bfd80800-8963-4ed7-b094-e4b8becb7a6d container secret-volume-test: <nil>
STEP: delete the pod 07/29/23 16:14:21.578
Jul 29 16:14:21.601: INFO: Waiting for pod pod-secrets-bfd80800-8963-4ed7-b094-e4b8becb7a6d to disappear
Jul 29 16:14:21.606: INFO: Pod pod-secrets-bfd80800-8963-4ed7-b094-e4b8becb7a6d no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jul 29 16:14:21.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6287" for this suite. 07/29/23 16:14:21.613
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":183,"skipped":3441,"failed":0}
------------------------------
â€¢ [4.155 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:14:17.475
    Jul 29 16:14:17.475: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename secrets 07/29/23 16:14:17.477
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:14:17.508
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:14:17.513
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:56
    STEP: Creating secret with name secret-test-ad79e801-7b8b-4e23-a357-aa5f5ef26fb0 07/29/23 16:14:17.518
    STEP: Creating a pod to test consume secrets 07/29/23 16:14:17.527
    Jul 29 16:14:17.541: INFO: Waiting up to 5m0s for pod "pod-secrets-bfd80800-8963-4ed7-b094-e4b8becb7a6d" in namespace "secrets-6287" to be "Succeeded or Failed"
    Jul 29 16:14:17.548: INFO: Pod "pod-secrets-bfd80800-8963-4ed7-b094-e4b8becb7a6d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.992755ms
    Jul 29 16:14:19.557: INFO: Pod "pod-secrets-bfd80800-8963-4ed7-b094-e4b8becb7a6d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015684745s
    Jul 29 16:14:21.558: INFO: Pod "pod-secrets-bfd80800-8963-4ed7-b094-e4b8becb7a6d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016527636s
    STEP: Saw pod success 07/29/23 16:14:21.558
    Jul 29 16:14:21.558: INFO: Pod "pod-secrets-bfd80800-8963-4ed7-b094-e4b8becb7a6d" satisfied condition "Succeeded or Failed"
    Jul 29 16:14:21.564: INFO: Trying to get logs from node wa4quivohpee-3 pod pod-secrets-bfd80800-8963-4ed7-b094-e4b8becb7a6d container secret-volume-test: <nil>
    STEP: delete the pod 07/29/23 16:14:21.578
    Jul 29 16:14:21.601: INFO: Waiting for pod pod-secrets-bfd80800-8963-4ed7-b094-e4b8becb7a6d to disappear
    Jul 29 16:14:21.606: INFO: Pod pod-secrets-bfd80800-8963-4ed7-b094-e4b8becb7a6d no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jul 29 16:14:21.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-6287" for this suite. 07/29/23 16:14:21.613
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:14:21.63
Jul 29 16:14:21.631: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename subpath 07/29/23 16:14:21.633
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:14:21.659
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:14:21.665
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 07/29/23 16:14:21.671
[It] should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
STEP: Creating pod pod-subpath-test-configmap-tllt 07/29/23 16:14:21.686
STEP: Creating a pod to test atomic-volume-subpath 07/29/23 16:14:21.687
Jul 29 16:14:21.700: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-tllt" in namespace "subpath-7363" to be "Succeeded or Failed"
Jul 29 16:14:21.719: INFO: Pod "pod-subpath-test-configmap-tllt": Phase="Pending", Reason="", readiness=false. Elapsed: 18.847504ms
Jul 29 16:14:23.728: INFO: Pod "pod-subpath-test-configmap-tllt": Phase="Running", Reason="", readiness=true. Elapsed: 2.028223985s
Jul 29 16:14:25.729: INFO: Pod "pod-subpath-test-configmap-tllt": Phase="Running", Reason="", readiness=true. Elapsed: 4.028774487s
Jul 29 16:14:27.727: INFO: Pod "pod-subpath-test-configmap-tllt": Phase="Running", Reason="", readiness=true. Elapsed: 6.02701919s
Jul 29 16:14:29.726: INFO: Pod "pod-subpath-test-configmap-tllt": Phase="Running", Reason="", readiness=true. Elapsed: 8.025329661s
Jul 29 16:14:31.727: INFO: Pod "pod-subpath-test-configmap-tllt": Phase="Running", Reason="", readiness=true. Elapsed: 10.026991881s
Jul 29 16:14:33.728: INFO: Pod "pod-subpath-test-configmap-tllt": Phase="Running", Reason="", readiness=true. Elapsed: 12.027359497s
Jul 29 16:14:35.726: INFO: Pod "pod-subpath-test-configmap-tllt": Phase="Running", Reason="", readiness=true. Elapsed: 14.025633634s
Jul 29 16:14:37.727: INFO: Pod "pod-subpath-test-configmap-tllt": Phase="Running", Reason="", readiness=true. Elapsed: 16.026842049s
Jul 29 16:14:39.727: INFO: Pod "pod-subpath-test-configmap-tllt": Phase="Running", Reason="", readiness=true. Elapsed: 18.026429996s
Jul 29 16:14:41.727: INFO: Pod "pod-subpath-test-configmap-tllt": Phase="Running", Reason="", readiness=true. Elapsed: 20.026518346s
Jul 29 16:14:43.726: INFO: Pod "pod-subpath-test-configmap-tllt": Phase="Running", Reason="", readiness=false. Elapsed: 22.026067886s
Jul 29 16:14:45.732: INFO: Pod "pod-subpath-test-configmap-tllt": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.032017284s
STEP: Saw pod success 07/29/23 16:14:45.732
Jul 29 16:14:45.733: INFO: Pod "pod-subpath-test-configmap-tllt" satisfied condition "Succeeded or Failed"
Jul 29 16:14:45.738: INFO: Trying to get logs from node wa4quivohpee-3 pod pod-subpath-test-configmap-tllt container test-container-subpath-configmap-tllt: <nil>
STEP: delete the pod 07/29/23 16:14:45.753
Jul 29 16:14:45.773: INFO: Waiting for pod pod-subpath-test-configmap-tllt to disappear
Jul 29 16:14:45.779: INFO: Pod pod-subpath-test-configmap-tllt no longer exists
STEP: Deleting pod pod-subpath-test-configmap-tllt 07/29/23 16:14:45.779
Jul 29 16:14:45.779: INFO: Deleting pod "pod-subpath-test-configmap-tllt" in namespace "subpath-7363"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Jul 29 16:14:45.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7363" for this suite. 07/29/23 16:14:45.789
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [Conformance]","completed":184,"skipped":3448,"failed":0}
------------------------------
â€¢ [SLOW TEST] [24.173 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod [Conformance]
    test/e2e/storage/subpath.go:70

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:14:21.63
    Jul 29 16:14:21.631: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename subpath 07/29/23 16:14:21.633
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:14:21.659
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:14:21.665
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 07/29/23 16:14:21.671
    [It] should support subpaths with configmap pod [Conformance]
      test/e2e/storage/subpath.go:70
    STEP: Creating pod pod-subpath-test-configmap-tllt 07/29/23 16:14:21.686
    STEP: Creating a pod to test atomic-volume-subpath 07/29/23 16:14:21.687
    Jul 29 16:14:21.700: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-tllt" in namespace "subpath-7363" to be "Succeeded or Failed"
    Jul 29 16:14:21.719: INFO: Pod "pod-subpath-test-configmap-tllt": Phase="Pending", Reason="", readiness=false. Elapsed: 18.847504ms
    Jul 29 16:14:23.728: INFO: Pod "pod-subpath-test-configmap-tllt": Phase="Running", Reason="", readiness=true. Elapsed: 2.028223985s
    Jul 29 16:14:25.729: INFO: Pod "pod-subpath-test-configmap-tllt": Phase="Running", Reason="", readiness=true. Elapsed: 4.028774487s
    Jul 29 16:14:27.727: INFO: Pod "pod-subpath-test-configmap-tllt": Phase="Running", Reason="", readiness=true. Elapsed: 6.02701919s
    Jul 29 16:14:29.726: INFO: Pod "pod-subpath-test-configmap-tllt": Phase="Running", Reason="", readiness=true. Elapsed: 8.025329661s
    Jul 29 16:14:31.727: INFO: Pod "pod-subpath-test-configmap-tllt": Phase="Running", Reason="", readiness=true. Elapsed: 10.026991881s
    Jul 29 16:14:33.728: INFO: Pod "pod-subpath-test-configmap-tllt": Phase="Running", Reason="", readiness=true. Elapsed: 12.027359497s
    Jul 29 16:14:35.726: INFO: Pod "pod-subpath-test-configmap-tllt": Phase="Running", Reason="", readiness=true. Elapsed: 14.025633634s
    Jul 29 16:14:37.727: INFO: Pod "pod-subpath-test-configmap-tllt": Phase="Running", Reason="", readiness=true. Elapsed: 16.026842049s
    Jul 29 16:14:39.727: INFO: Pod "pod-subpath-test-configmap-tllt": Phase="Running", Reason="", readiness=true. Elapsed: 18.026429996s
    Jul 29 16:14:41.727: INFO: Pod "pod-subpath-test-configmap-tllt": Phase="Running", Reason="", readiness=true. Elapsed: 20.026518346s
    Jul 29 16:14:43.726: INFO: Pod "pod-subpath-test-configmap-tllt": Phase="Running", Reason="", readiness=false. Elapsed: 22.026067886s
    Jul 29 16:14:45.732: INFO: Pod "pod-subpath-test-configmap-tllt": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.032017284s
    STEP: Saw pod success 07/29/23 16:14:45.732
    Jul 29 16:14:45.733: INFO: Pod "pod-subpath-test-configmap-tllt" satisfied condition "Succeeded or Failed"
    Jul 29 16:14:45.738: INFO: Trying to get logs from node wa4quivohpee-3 pod pod-subpath-test-configmap-tllt container test-container-subpath-configmap-tllt: <nil>
    STEP: delete the pod 07/29/23 16:14:45.753
    Jul 29 16:14:45.773: INFO: Waiting for pod pod-subpath-test-configmap-tllt to disappear
    Jul 29 16:14:45.779: INFO: Pod pod-subpath-test-configmap-tllt no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-tllt 07/29/23 16:14:45.779
    Jul 29 16:14:45.779: INFO: Deleting pod "pod-subpath-test-configmap-tllt" in namespace "subpath-7363"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Jul 29 16:14:45.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-7363" for this suite. 07/29/23 16:14:45.789
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:14:45.812
Jul 29 16:14:45.813: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename projected 07/29/23 16:14:45.814
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:14:45.849
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:14:45.853
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45
STEP: Creating projection with secret that has name projected-secret-test-c54a2e2e-20fa-442f-b678-0b8c0c4faa85 07/29/23 16:14:45.859
STEP: Creating a pod to test consume secrets 07/29/23 16:14:45.87
Jul 29 16:14:45.890: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e5168953-32b5-4f3e-852f-5134d9ef3545" in namespace "projected-6309" to be "Succeeded or Failed"
Jul 29 16:14:45.902: INFO: Pod "pod-projected-secrets-e5168953-32b5-4f3e-852f-5134d9ef3545": Phase="Pending", Reason="", readiness=false. Elapsed: 12.597638ms
Jul 29 16:14:47.910: INFO: Pod "pod-projected-secrets-e5168953-32b5-4f3e-852f-5134d9ef3545": Phase="Running", Reason="", readiness=true. Elapsed: 2.020259471s
Jul 29 16:14:49.909: INFO: Pod "pod-projected-secrets-e5168953-32b5-4f3e-852f-5134d9ef3545": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019607722s
STEP: Saw pod success 07/29/23 16:14:49.909
Jul 29 16:14:49.910: INFO: Pod "pod-projected-secrets-e5168953-32b5-4f3e-852f-5134d9ef3545" satisfied condition "Succeeded or Failed"
Jul 29 16:14:49.914: INFO: Trying to get logs from node wa4quivohpee-3 pod pod-projected-secrets-e5168953-32b5-4f3e-852f-5134d9ef3545 container projected-secret-volume-test: <nil>
STEP: delete the pod 07/29/23 16:14:49.926
Jul 29 16:14:49.945: INFO: Waiting for pod pod-projected-secrets-e5168953-32b5-4f3e-852f-5134d9ef3545 to disappear
Jul 29 16:14:49.950: INFO: Pod pod-projected-secrets-e5168953-32b5-4f3e-852f-5134d9ef3545 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Jul 29 16:14:49.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6309" for this suite. 07/29/23 16:14:49.957
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","completed":185,"skipped":3462,"failed":0}
------------------------------
â€¢ [4.157 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:14:45.812
    Jul 29 16:14:45.813: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename projected 07/29/23 16:14:45.814
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:14:45.849
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:14:45.853
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:45
    STEP: Creating projection with secret that has name projected-secret-test-c54a2e2e-20fa-442f-b678-0b8c0c4faa85 07/29/23 16:14:45.859
    STEP: Creating a pod to test consume secrets 07/29/23 16:14:45.87
    Jul 29 16:14:45.890: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e5168953-32b5-4f3e-852f-5134d9ef3545" in namespace "projected-6309" to be "Succeeded or Failed"
    Jul 29 16:14:45.902: INFO: Pod "pod-projected-secrets-e5168953-32b5-4f3e-852f-5134d9ef3545": Phase="Pending", Reason="", readiness=false. Elapsed: 12.597638ms
    Jul 29 16:14:47.910: INFO: Pod "pod-projected-secrets-e5168953-32b5-4f3e-852f-5134d9ef3545": Phase="Running", Reason="", readiness=true. Elapsed: 2.020259471s
    Jul 29 16:14:49.909: INFO: Pod "pod-projected-secrets-e5168953-32b5-4f3e-852f-5134d9ef3545": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019607722s
    STEP: Saw pod success 07/29/23 16:14:49.909
    Jul 29 16:14:49.910: INFO: Pod "pod-projected-secrets-e5168953-32b5-4f3e-852f-5134d9ef3545" satisfied condition "Succeeded or Failed"
    Jul 29 16:14:49.914: INFO: Trying to get logs from node wa4quivohpee-3 pod pod-projected-secrets-e5168953-32b5-4f3e-852f-5134d9ef3545 container projected-secret-volume-test: <nil>
    STEP: delete the pod 07/29/23 16:14:49.926
    Jul 29 16:14:49.945: INFO: Waiting for pod pod-projected-secrets-e5168953-32b5-4f3e-852f-5134d9ef3545 to disappear
    Jul 29 16:14:49.950: INFO: Pod pod-projected-secrets-e5168953-32b5-4f3e-852f-5134d9ef3545 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Jul 29 16:14:49.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6309" for this suite. 07/29/23 16:14:49.957
  << End Captured GinkgoWriter Output
------------------------------
[sig-auth] ServiceAccounts
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:14:49.972
Jul 29 16:14:49.972: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename svcaccounts 07/29/23 16:14:49.975
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:14:50.002
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:14:50.007
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646
STEP: creating a ServiceAccount 07/29/23 16:14:50.013
STEP: watching for the ServiceAccount to be added 07/29/23 16:14:50.027
STEP: patching the ServiceAccount 07/29/23 16:14:50.029
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 07/29/23 16:14:50.039
STEP: deleting the ServiceAccount 07/29/23 16:14:50.045
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Jul 29 16:14:50.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-1311" for this suite. 07/29/23 16:14:50.072
{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","completed":186,"skipped":3462,"failed":0}
------------------------------
â€¢ [0.110 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:14:49.972
    Jul 29 16:14:49.972: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename svcaccounts 07/29/23 16:14:49.975
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:14:50.002
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:14:50.007
    [It] should run through the lifecycle of a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:646
    STEP: creating a ServiceAccount 07/29/23 16:14:50.013
    STEP: watching for the ServiceAccount to be added 07/29/23 16:14:50.027
    STEP: patching the ServiceAccount 07/29/23 16:14:50.029
    STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 07/29/23 16:14:50.039
    STEP: deleting the ServiceAccount 07/29/23 16:14:50.045
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Jul 29 16:14:50.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-1311" for this suite. 07/29/23 16:14:50.072
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:14:50.083
Jul 29 16:14:50.084: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename webhook 07/29/23 16:14:50.086
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:14:50.111
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:14:50.115
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 07/29/23 16:14:50.141
STEP: Create role binding to let webhook read extension-apiserver-authentication 07/29/23 16:14:50.948
STEP: Deploying the webhook pod 07/29/23 16:14:50.964
STEP: Wait for the deployment to be ready 07/29/23 16:14:50.985
Jul 29 16:14:50.994: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 07/29/23 16:14:53.017
STEP: Verifying the service has paired with the endpoint 07/29/23 16:14:53.037
Jul 29 16:14:54.037: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322
Jul 29 16:14:54.043: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3259-crds.webhook.example.com via the AdmissionRegistration API 07/29/23 16:14:54.57
STEP: Creating a custom resource while v1 is storage version 07/29/23 16:14:54.603
STEP: Patching Custom Resource Definition to set v2 as storage 07/29/23 16:14:56.93
STEP: Patching the custom resource while v2 is storage version 07/29/23 16:14:56.953
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jul 29 16:14:57.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4727" for this suite. 07/29/23 16:14:57.785
STEP: Destroying namespace "webhook-4727-markers" for this suite. 07/29/23 16:14:57.799
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","completed":187,"skipped":3468,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.800 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:14:50.083
    Jul 29 16:14:50.084: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename webhook 07/29/23 16:14:50.086
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:14:50.111
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:14:50.115
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 07/29/23 16:14:50.141
    STEP: Create role binding to let webhook read extension-apiserver-authentication 07/29/23 16:14:50.948
    STEP: Deploying the webhook pod 07/29/23 16:14:50.964
    STEP: Wait for the deployment to be ready 07/29/23 16:14:50.985
    Jul 29 16:14:50.994: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 07/29/23 16:14:53.017
    STEP: Verifying the service has paired with the endpoint 07/29/23 16:14:53.037
    Jul 29 16:14:54.037: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with different stored version [Conformance]
      test/e2e/apimachinery/webhook.go:322
    Jul 29 16:14:54.043: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3259-crds.webhook.example.com via the AdmissionRegistration API 07/29/23 16:14:54.57
    STEP: Creating a custom resource while v1 is storage version 07/29/23 16:14:54.603
    STEP: Patching Custom Resource Definition to set v2 as storage 07/29/23 16:14:56.93
    STEP: Patching the custom resource while v2 is storage version 07/29/23 16:14:56.953
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jul 29 16:14:57.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-4727" for this suite. 07/29/23 16:14:57.785
    STEP: Destroying namespace "webhook-4727-markers" for this suite. 07/29/23 16:14:57.799
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:14:57.886
Jul 29 16:14:57.886: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename projected 07/29/23 16:14:57.894
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:14:57.932
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:14:57.946
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73
STEP: Creating configMap with name projected-configmap-test-volume-67630a30-246c-437d-a314-9bd8e502c5cb 07/29/23 16:14:57.963
STEP: Creating a pod to test consume configMaps 07/29/23 16:14:58.047
Jul 29 16:14:58.070: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ae504cac-3ed7-4c4b-9e35-425ebec4d52d" in namespace "projected-4057" to be "Succeeded or Failed"
Jul 29 16:14:58.089: INFO: Pod "pod-projected-configmaps-ae504cac-3ed7-4c4b-9e35-425ebec4d52d": Phase="Pending", Reason="", readiness=false. Elapsed: 19.320346ms
Jul 29 16:15:00.097: INFO: Pod "pod-projected-configmaps-ae504cac-3ed7-4c4b-9e35-425ebec4d52d": Phase="Running", Reason="", readiness=false. Elapsed: 2.027346595s
Jul 29 16:15:02.097: INFO: Pod "pod-projected-configmaps-ae504cac-3ed7-4c4b-9e35-425ebec4d52d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026595254s
STEP: Saw pod success 07/29/23 16:15:02.097
Jul 29 16:15:02.097: INFO: Pod "pod-projected-configmaps-ae504cac-3ed7-4c4b-9e35-425ebec4d52d" satisfied condition "Succeeded or Failed"
Jul 29 16:15:02.103: INFO: Trying to get logs from node wa4quivohpee-3 pod pod-projected-configmaps-ae504cac-3ed7-4c4b-9e35-425ebec4d52d container agnhost-container: <nil>
STEP: delete the pod 07/29/23 16:15:02.112
Jul 29 16:15:02.136: INFO: Waiting for pod pod-projected-configmaps-ae504cac-3ed7-4c4b-9e35-425ebec4d52d to disappear
Jul 29 16:15:02.142: INFO: Pod pod-projected-configmaps-ae504cac-3ed7-4c4b-9e35-425ebec4d52d no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jul 29 16:15:02.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4057" for this suite. 07/29/23 16:15:02.149
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","completed":188,"skipped":3472,"failed":0}
------------------------------
â€¢ [4.274 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:14:57.886
    Jul 29 16:14:57.886: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename projected 07/29/23 16:14:57.894
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:14:57.932
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:14:57.946
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:73
    STEP: Creating configMap with name projected-configmap-test-volume-67630a30-246c-437d-a314-9bd8e502c5cb 07/29/23 16:14:57.963
    STEP: Creating a pod to test consume configMaps 07/29/23 16:14:58.047
    Jul 29 16:14:58.070: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ae504cac-3ed7-4c4b-9e35-425ebec4d52d" in namespace "projected-4057" to be "Succeeded or Failed"
    Jul 29 16:14:58.089: INFO: Pod "pod-projected-configmaps-ae504cac-3ed7-4c4b-9e35-425ebec4d52d": Phase="Pending", Reason="", readiness=false. Elapsed: 19.320346ms
    Jul 29 16:15:00.097: INFO: Pod "pod-projected-configmaps-ae504cac-3ed7-4c4b-9e35-425ebec4d52d": Phase="Running", Reason="", readiness=false. Elapsed: 2.027346595s
    Jul 29 16:15:02.097: INFO: Pod "pod-projected-configmaps-ae504cac-3ed7-4c4b-9e35-425ebec4d52d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026595254s
    STEP: Saw pod success 07/29/23 16:15:02.097
    Jul 29 16:15:02.097: INFO: Pod "pod-projected-configmaps-ae504cac-3ed7-4c4b-9e35-425ebec4d52d" satisfied condition "Succeeded or Failed"
    Jul 29 16:15:02.103: INFO: Trying to get logs from node wa4quivohpee-3 pod pod-projected-configmaps-ae504cac-3ed7-4c4b-9e35-425ebec4d52d container agnhost-container: <nil>
    STEP: delete the pod 07/29/23 16:15:02.112
    Jul 29 16:15:02.136: INFO: Waiting for pod pod-projected-configmaps-ae504cac-3ed7-4c4b-9e35-425ebec4d52d to disappear
    Jul 29 16:15:02.142: INFO: Pod pod-projected-configmaps-ae504cac-3ed7-4c4b-9e35-425ebec4d52d no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jul 29 16:15:02.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4057" for this suite. 07/29/23 16:15:02.149
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:15:02.182
Jul 29 16:15:02.182: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename gc 07/29/23 16:15:02.185
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:15:02.213
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:15:02.22
[It] should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
STEP: create the deployment 07/29/23 16:15:02.226
STEP: Wait for the Deployment to create new ReplicaSet 07/29/23 16:15:02.235
STEP: delete the deployment 07/29/23 16:15:02.751
STEP: wait for all rs to be garbage collected 07/29/23 16:15:02.772
STEP: expected 0 rs, got 1 rs 07/29/23 16:15:02.79
STEP: expected 0 pods, got 2 pods 07/29/23 16:15:02.809
STEP: Gathering metrics 07/29/23 16:15:03.338
Jul 29 16:15:03.389: INFO: Waiting up to 5m0s for pod "kube-controller-manager-wa4quivohpee-2" in namespace "kube-system" to be "running and ready"
Jul 29 16:15:03.398: INFO: Pod "kube-controller-manager-wa4quivohpee-2": Phase="Running", Reason="", readiness=true. Elapsed: 8.351428ms
Jul 29 16:15:03.398: INFO: The phase of Pod kube-controller-manager-wa4quivohpee-2 is Running (Ready = true)
Jul 29 16:15:03.398: INFO: Pod "kube-controller-manager-wa4quivohpee-2" satisfied condition "running and ready"
Jul 29 16:15:03.519: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Jul 29 16:15:03.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8665" for this suite. 07/29/23 16:15:03.542
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","completed":189,"skipped":3538,"failed":0}
------------------------------
â€¢ [1.374 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:15:02.182
    Jul 29 16:15:02.182: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename gc 07/29/23 16:15:02.185
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:15:02.213
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:15:02.22
    [It] should delete RS created by deployment when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:491
    STEP: create the deployment 07/29/23 16:15:02.226
    STEP: Wait for the Deployment to create new ReplicaSet 07/29/23 16:15:02.235
    STEP: delete the deployment 07/29/23 16:15:02.751
    STEP: wait for all rs to be garbage collected 07/29/23 16:15:02.772
    STEP: expected 0 rs, got 1 rs 07/29/23 16:15:02.79
    STEP: expected 0 pods, got 2 pods 07/29/23 16:15:02.809
    STEP: Gathering metrics 07/29/23 16:15:03.338
    Jul 29 16:15:03.389: INFO: Waiting up to 5m0s for pod "kube-controller-manager-wa4quivohpee-2" in namespace "kube-system" to be "running and ready"
    Jul 29 16:15:03.398: INFO: Pod "kube-controller-manager-wa4quivohpee-2": Phase="Running", Reason="", readiness=true. Elapsed: 8.351428ms
    Jul 29 16:15:03.398: INFO: The phase of Pod kube-controller-manager-wa4quivohpee-2 is Running (Ready = true)
    Jul 29 16:15:03.398: INFO: Pod "kube-controller-manager-wa4quivohpee-2" satisfied condition "running and ready"
    Jul 29 16:15:03.519: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Jul 29 16:15:03.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-8665" for this suite. 07/29/23 16:15:03.542
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2216
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:15:03.558
Jul 29 16:15:03.558: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename services 07/29/23 16:15:03.561
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:15:03.647
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:15:03.653
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2216
STEP: creating service in namespace services-2160 07/29/23 16:15:03.66
STEP: creating service affinity-nodeport-transition in namespace services-2160 07/29/23 16:15:03.66
STEP: creating replication controller affinity-nodeport-transition in namespace services-2160 07/29/23 16:15:03.724
I0729 16:15:03.748280      13 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-2160, replica count: 3
I0729 16:15:06.799564      13 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 29 16:15:06.821: INFO: Creating new exec pod
Jul 29 16:15:06.833: INFO: Waiting up to 5m0s for pod "execpod-affinity66kv6" in namespace "services-2160" to be "running"
Jul 29 16:15:06.839: INFO: Pod "execpod-affinity66kv6": Phase="Pending", Reason="", readiness=false. Elapsed: 5.240674ms
Jul 29 16:15:08.845: INFO: Pod "execpod-affinity66kv6": Phase="Running", Reason="", readiness=true. Elapsed: 2.011020802s
Jul 29 16:15:08.845: INFO: Pod "execpod-affinity66kv6" satisfied condition "running"
Jul 29 16:15:09.853: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=services-2160 exec execpod-affinity66kv6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
Jul 29 16:15:10.157: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Jul 29 16:15:10.157: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul 29 16:15:10.158: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=services-2160 exec execpod-affinity66kv6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.6.3 80'
Jul 29 16:15:10.374: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.6.3 80\nConnection to 10.233.6.3 80 port [tcp/http] succeeded!\n"
Jul 29 16:15:10.374: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul 29 16:15:10.375: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=services-2160 exec execpod-affinity66kv6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.234 32343'
Jul 29 16:15:10.600: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.234 32343\nConnection to 192.168.121.234 32343 port [tcp/*] succeeded!\n"
Jul 29 16:15:10.600: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul 29 16:15:10.600: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=services-2160 exec execpod-affinity66kv6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.206 32343'
Jul 29 16:15:10.851: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.206 32343\nConnection to 192.168.121.206 32343 port [tcp/*] succeeded!\n"
Jul 29 16:15:10.851: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul 29 16:15:10.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=services-2160 exec execpod-affinity66kv6 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.121.28:32343/ ; done'
Jul 29 16:15:11.368: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.28:32343/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.28:32343/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.28:32343/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.28:32343/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.28:32343/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.28:32343/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.28:32343/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.28:32343/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.28:32343/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.28:32343/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.28:32343/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.28:32343/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.28:32343/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.28:32343/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.28:32343/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.28:32343/\n"
Jul 29 16:15:11.368: INFO: stdout: "\naffinity-nodeport-transition-t7k6v\naffinity-nodeport-transition-fssgn\naffinity-nodeport-transition-t7k6v\naffinity-nodeport-transition-wtdxl\naffinity-nodeport-transition-wtdxl\naffinity-nodeport-transition-t7k6v\naffinity-nodeport-transition-fssgn\naffinity-nodeport-transition-fssgn\naffinity-nodeport-transition-fssgn\naffinity-nodeport-transition-t7k6v\naffinity-nodeport-transition-fssgn\naffinity-nodeport-transition-t7k6v\naffinity-nodeport-transition-t7k6v\naffinity-nodeport-transition-wtdxl\naffinity-nodeport-transition-t7k6v\naffinity-nodeport-transition-t7k6v"
Jul 29 16:15:11.368: INFO: Received response from host: affinity-nodeport-transition-t7k6v
Jul 29 16:15:11.368: INFO: Received response from host: affinity-nodeport-transition-fssgn
Jul 29 16:15:11.368: INFO: Received response from host: affinity-nodeport-transition-t7k6v
Jul 29 16:15:11.368: INFO: Received response from host: affinity-nodeport-transition-wtdxl
Jul 29 16:15:11.368: INFO: Received response from host: affinity-nodeport-transition-wtdxl
Jul 29 16:15:11.368: INFO: Received response from host: affinity-nodeport-transition-t7k6v
Jul 29 16:15:11.368: INFO: Received response from host: affinity-nodeport-transition-fssgn
Jul 29 16:15:11.368: INFO: Received response from host: affinity-nodeport-transition-fssgn
Jul 29 16:15:11.368: INFO: Received response from host: affinity-nodeport-transition-fssgn
Jul 29 16:15:11.368: INFO: Received response from host: affinity-nodeport-transition-t7k6v
Jul 29 16:15:11.368: INFO: Received response from host: affinity-nodeport-transition-fssgn
Jul 29 16:15:11.368: INFO: Received response from host: affinity-nodeport-transition-t7k6v
Jul 29 16:15:11.368: INFO: Received response from host: affinity-nodeport-transition-t7k6v
Jul 29 16:15:11.368: INFO: Received response from host: affinity-nodeport-transition-wtdxl
Jul 29 16:15:11.368: INFO: Received response from host: affinity-nodeport-transition-t7k6v
Jul 29 16:15:11.368: INFO: Received response from host: affinity-nodeport-transition-t7k6v
Jul 29 16:15:11.385: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=services-2160 exec execpod-affinity66kv6 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.121.28:32343/ ; done'
Jul 29 16:15:11.846: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.28:32343/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.28:32343/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.28:32343/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.28:32343/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.28:32343/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.28:32343/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.28:32343/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.28:32343/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.28:32343/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.28:32343/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.28:32343/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.28:32343/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.28:32343/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.28:32343/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.28:32343/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.28:32343/\n"
Jul 29 16:15:11.846: INFO: stdout: "\naffinity-nodeport-transition-wtdxl\naffinity-nodeport-transition-wtdxl\naffinity-nodeport-transition-wtdxl\naffinity-nodeport-transition-wtdxl\naffinity-nodeport-transition-wtdxl\naffinity-nodeport-transition-wtdxl\naffinity-nodeport-transition-wtdxl\naffinity-nodeport-transition-wtdxl\naffinity-nodeport-transition-wtdxl\naffinity-nodeport-transition-wtdxl\naffinity-nodeport-transition-wtdxl\naffinity-nodeport-transition-wtdxl\naffinity-nodeport-transition-wtdxl\naffinity-nodeport-transition-wtdxl\naffinity-nodeport-transition-wtdxl\naffinity-nodeport-transition-wtdxl"
Jul 29 16:15:11.846: INFO: Received response from host: affinity-nodeport-transition-wtdxl
Jul 29 16:15:11.846: INFO: Received response from host: affinity-nodeport-transition-wtdxl
Jul 29 16:15:11.846: INFO: Received response from host: affinity-nodeport-transition-wtdxl
Jul 29 16:15:11.846: INFO: Received response from host: affinity-nodeport-transition-wtdxl
Jul 29 16:15:11.846: INFO: Received response from host: affinity-nodeport-transition-wtdxl
Jul 29 16:15:11.846: INFO: Received response from host: affinity-nodeport-transition-wtdxl
Jul 29 16:15:11.846: INFO: Received response from host: affinity-nodeport-transition-wtdxl
Jul 29 16:15:11.846: INFO: Received response from host: affinity-nodeport-transition-wtdxl
Jul 29 16:15:11.846: INFO: Received response from host: affinity-nodeport-transition-wtdxl
Jul 29 16:15:11.846: INFO: Received response from host: affinity-nodeport-transition-wtdxl
Jul 29 16:15:11.846: INFO: Received response from host: affinity-nodeport-transition-wtdxl
Jul 29 16:15:11.846: INFO: Received response from host: affinity-nodeport-transition-wtdxl
Jul 29 16:15:11.846: INFO: Received response from host: affinity-nodeport-transition-wtdxl
Jul 29 16:15:11.846: INFO: Received response from host: affinity-nodeport-transition-wtdxl
Jul 29 16:15:11.846: INFO: Received response from host: affinity-nodeport-transition-wtdxl
Jul 29 16:15:11.846: INFO: Received response from host: affinity-nodeport-transition-wtdxl
Jul 29 16:15:11.846: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-2160, will wait for the garbage collector to delete the pods 07/29/23 16:15:11.869
Jul 29 16:15:11.938: INFO: Deleting ReplicationController affinity-nodeport-transition took: 12.448896ms
Jul 29 16:15:12.039: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 101.141847ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jul 29 16:15:14.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2160" for this suite. 07/29/23 16:15:14.489
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","completed":190,"skipped":3541,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.940 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:15:03.558
    Jul 29 16:15:03.558: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename services 07/29/23 16:15:03.561
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:15:03.647
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:15:03.653
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2216
    STEP: creating service in namespace services-2160 07/29/23 16:15:03.66
    STEP: creating service affinity-nodeport-transition in namespace services-2160 07/29/23 16:15:03.66
    STEP: creating replication controller affinity-nodeport-transition in namespace services-2160 07/29/23 16:15:03.724
    I0729 16:15:03.748280      13 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-2160, replica count: 3
    I0729 16:15:06.799564      13 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jul 29 16:15:06.821: INFO: Creating new exec pod
    Jul 29 16:15:06.833: INFO: Waiting up to 5m0s for pod "execpod-affinity66kv6" in namespace "services-2160" to be "running"
    Jul 29 16:15:06.839: INFO: Pod "execpod-affinity66kv6": Phase="Pending", Reason="", readiness=false. Elapsed: 5.240674ms
    Jul 29 16:15:08.845: INFO: Pod "execpod-affinity66kv6": Phase="Running", Reason="", readiness=true. Elapsed: 2.011020802s
    Jul 29 16:15:08.845: INFO: Pod "execpod-affinity66kv6" satisfied condition "running"
    Jul 29 16:15:09.853: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=services-2160 exec execpod-affinity66kv6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
    Jul 29 16:15:10.157: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
    Jul 29 16:15:10.157: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jul 29 16:15:10.158: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=services-2160 exec execpod-affinity66kv6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.6.3 80'
    Jul 29 16:15:10.374: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.6.3 80\nConnection to 10.233.6.3 80 port [tcp/http] succeeded!\n"
    Jul 29 16:15:10.374: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jul 29 16:15:10.375: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=services-2160 exec execpod-affinity66kv6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.234 32343'
    Jul 29 16:15:10.600: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.234 32343\nConnection to 192.168.121.234 32343 port [tcp/*] succeeded!\n"
    Jul 29 16:15:10.600: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jul 29 16:15:10.600: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=services-2160 exec execpod-affinity66kv6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.206 32343'
    Jul 29 16:15:10.851: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.206 32343\nConnection to 192.168.121.206 32343 port [tcp/*] succeeded!\n"
    Jul 29 16:15:10.851: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jul 29 16:15:10.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=services-2160 exec execpod-affinity66kv6 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.121.28:32343/ ; done'
    Jul 29 16:15:11.368: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.28:32343/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.28:32343/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.28:32343/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.28:32343/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.28:32343/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.28:32343/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.28:32343/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.28:32343/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.28:32343/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.28:32343/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.28:32343/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.28:32343/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.28:32343/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.28:32343/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.28:32343/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.28:32343/\n"
    Jul 29 16:15:11.368: INFO: stdout: "\naffinity-nodeport-transition-t7k6v\naffinity-nodeport-transition-fssgn\naffinity-nodeport-transition-t7k6v\naffinity-nodeport-transition-wtdxl\naffinity-nodeport-transition-wtdxl\naffinity-nodeport-transition-t7k6v\naffinity-nodeport-transition-fssgn\naffinity-nodeport-transition-fssgn\naffinity-nodeport-transition-fssgn\naffinity-nodeport-transition-t7k6v\naffinity-nodeport-transition-fssgn\naffinity-nodeport-transition-t7k6v\naffinity-nodeport-transition-t7k6v\naffinity-nodeport-transition-wtdxl\naffinity-nodeport-transition-t7k6v\naffinity-nodeport-transition-t7k6v"
    Jul 29 16:15:11.368: INFO: Received response from host: affinity-nodeport-transition-t7k6v
    Jul 29 16:15:11.368: INFO: Received response from host: affinity-nodeport-transition-fssgn
    Jul 29 16:15:11.368: INFO: Received response from host: affinity-nodeport-transition-t7k6v
    Jul 29 16:15:11.368: INFO: Received response from host: affinity-nodeport-transition-wtdxl
    Jul 29 16:15:11.368: INFO: Received response from host: affinity-nodeport-transition-wtdxl
    Jul 29 16:15:11.368: INFO: Received response from host: affinity-nodeport-transition-t7k6v
    Jul 29 16:15:11.368: INFO: Received response from host: affinity-nodeport-transition-fssgn
    Jul 29 16:15:11.368: INFO: Received response from host: affinity-nodeport-transition-fssgn
    Jul 29 16:15:11.368: INFO: Received response from host: affinity-nodeport-transition-fssgn
    Jul 29 16:15:11.368: INFO: Received response from host: affinity-nodeport-transition-t7k6v
    Jul 29 16:15:11.368: INFO: Received response from host: affinity-nodeport-transition-fssgn
    Jul 29 16:15:11.368: INFO: Received response from host: affinity-nodeport-transition-t7k6v
    Jul 29 16:15:11.368: INFO: Received response from host: affinity-nodeport-transition-t7k6v
    Jul 29 16:15:11.368: INFO: Received response from host: affinity-nodeport-transition-wtdxl
    Jul 29 16:15:11.368: INFO: Received response from host: affinity-nodeport-transition-t7k6v
    Jul 29 16:15:11.368: INFO: Received response from host: affinity-nodeport-transition-t7k6v
    Jul 29 16:15:11.385: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=services-2160 exec execpod-affinity66kv6 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.121.28:32343/ ; done'
    Jul 29 16:15:11.846: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.28:32343/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.28:32343/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.28:32343/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.28:32343/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.28:32343/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.28:32343/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.28:32343/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.28:32343/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.28:32343/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.28:32343/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.28:32343/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.28:32343/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.28:32343/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.28:32343/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.28:32343/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.28:32343/\n"
    Jul 29 16:15:11.846: INFO: stdout: "\naffinity-nodeport-transition-wtdxl\naffinity-nodeport-transition-wtdxl\naffinity-nodeport-transition-wtdxl\naffinity-nodeport-transition-wtdxl\naffinity-nodeport-transition-wtdxl\naffinity-nodeport-transition-wtdxl\naffinity-nodeport-transition-wtdxl\naffinity-nodeport-transition-wtdxl\naffinity-nodeport-transition-wtdxl\naffinity-nodeport-transition-wtdxl\naffinity-nodeport-transition-wtdxl\naffinity-nodeport-transition-wtdxl\naffinity-nodeport-transition-wtdxl\naffinity-nodeport-transition-wtdxl\naffinity-nodeport-transition-wtdxl\naffinity-nodeport-transition-wtdxl"
    Jul 29 16:15:11.846: INFO: Received response from host: affinity-nodeport-transition-wtdxl
    Jul 29 16:15:11.846: INFO: Received response from host: affinity-nodeport-transition-wtdxl
    Jul 29 16:15:11.846: INFO: Received response from host: affinity-nodeport-transition-wtdxl
    Jul 29 16:15:11.846: INFO: Received response from host: affinity-nodeport-transition-wtdxl
    Jul 29 16:15:11.846: INFO: Received response from host: affinity-nodeport-transition-wtdxl
    Jul 29 16:15:11.846: INFO: Received response from host: affinity-nodeport-transition-wtdxl
    Jul 29 16:15:11.846: INFO: Received response from host: affinity-nodeport-transition-wtdxl
    Jul 29 16:15:11.846: INFO: Received response from host: affinity-nodeport-transition-wtdxl
    Jul 29 16:15:11.846: INFO: Received response from host: affinity-nodeport-transition-wtdxl
    Jul 29 16:15:11.846: INFO: Received response from host: affinity-nodeport-transition-wtdxl
    Jul 29 16:15:11.846: INFO: Received response from host: affinity-nodeport-transition-wtdxl
    Jul 29 16:15:11.846: INFO: Received response from host: affinity-nodeport-transition-wtdxl
    Jul 29 16:15:11.846: INFO: Received response from host: affinity-nodeport-transition-wtdxl
    Jul 29 16:15:11.846: INFO: Received response from host: affinity-nodeport-transition-wtdxl
    Jul 29 16:15:11.846: INFO: Received response from host: affinity-nodeport-transition-wtdxl
    Jul 29 16:15:11.846: INFO: Received response from host: affinity-nodeport-transition-wtdxl
    Jul 29 16:15:11.846: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-2160, will wait for the garbage collector to delete the pods 07/29/23 16:15:11.869
    Jul 29 16:15:11.938: INFO: Deleting ReplicationController affinity-nodeport-transition took: 12.448896ms
    Jul 29 16:15:12.039: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 101.141847ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jul 29 16:15:14.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-2160" for this suite. 07/29/23 16:15:14.489
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:15:14.501
Jul 29 16:15:14.501: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename projected 07/29/23 16:15:14.507
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:15:14.532
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:15:14.536
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98
STEP: Creating configMap with name projected-configmap-test-volume-map-98ba68a6-c76e-4067-befa-6efcb0379529 07/29/23 16:15:14.54
STEP: Creating a pod to test consume configMaps 07/29/23 16:15:14.547
Jul 29 16:15:14.559: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-46affa57-08e4-452e-ba2a-a9b6476a6f91" in namespace "projected-2202" to be "Succeeded or Failed"
Jul 29 16:15:14.580: INFO: Pod "pod-projected-configmaps-46affa57-08e4-452e-ba2a-a9b6476a6f91": Phase="Pending", Reason="", readiness=false. Elapsed: 21.17453ms
Jul 29 16:15:16.589: INFO: Pod "pod-projected-configmaps-46affa57-08e4-452e-ba2a-a9b6476a6f91": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030187332s
Jul 29 16:15:18.587: INFO: Pod "pod-projected-configmaps-46affa57-08e4-452e-ba2a-a9b6476a6f91": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027779314s
STEP: Saw pod success 07/29/23 16:15:18.587
Jul 29 16:15:18.587: INFO: Pod "pod-projected-configmaps-46affa57-08e4-452e-ba2a-a9b6476a6f91" satisfied condition "Succeeded or Failed"
Jul 29 16:15:18.592: INFO: Trying to get logs from node wa4quivohpee-3 pod pod-projected-configmaps-46affa57-08e4-452e-ba2a-a9b6476a6f91 container agnhost-container: <nil>
STEP: delete the pod 07/29/23 16:15:18.603
Jul 29 16:15:18.631: INFO: Waiting for pod pod-projected-configmaps-46affa57-08e4-452e-ba2a-a9b6476a6f91 to disappear
Jul 29 16:15:18.636: INFO: Pod pod-projected-configmaps-46affa57-08e4-452e-ba2a-a9b6476a6f91 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jul 29 16:15:18.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2202" for this suite. 07/29/23 16:15:18.644
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":191,"skipped":3545,"failed":0}
------------------------------
â€¢ [4.157 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:15:14.501
    Jul 29 16:15:14.501: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename projected 07/29/23 16:15:14.507
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:15:14.532
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:15:14.536
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:98
    STEP: Creating configMap with name projected-configmap-test-volume-map-98ba68a6-c76e-4067-befa-6efcb0379529 07/29/23 16:15:14.54
    STEP: Creating a pod to test consume configMaps 07/29/23 16:15:14.547
    Jul 29 16:15:14.559: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-46affa57-08e4-452e-ba2a-a9b6476a6f91" in namespace "projected-2202" to be "Succeeded or Failed"
    Jul 29 16:15:14.580: INFO: Pod "pod-projected-configmaps-46affa57-08e4-452e-ba2a-a9b6476a6f91": Phase="Pending", Reason="", readiness=false. Elapsed: 21.17453ms
    Jul 29 16:15:16.589: INFO: Pod "pod-projected-configmaps-46affa57-08e4-452e-ba2a-a9b6476a6f91": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030187332s
    Jul 29 16:15:18.587: INFO: Pod "pod-projected-configmaps-46affa57-08e4-452e-ba2a-a9b6476a6f91": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027779314s
    STEP: Saw pod success 07/29/23 16:15:18.587
    Jul 29 16:15:18.587: INFO: Pod "pod-projected-configmaps-46affa57-08e4-452e-ba2a-a9b6476a6f91" satisfied condition "Succeeded or Failed"
    Jul 29 16:15:18.592: INFO: Trying to get logs from node wa4quivohpee-3 pod pod-projected-configmaps-46affa57-08e4-452e-ba2a-a9b6476a6f91 container agnhost-container: <nil>
    STEP: delete the pod 07/29/23 16:15:18.603
    Jul 29 16:15:18.631: INFO: Waiting for pod pod-projected-configmaps-46affa57-08e4-452e-ba2a-a9b6476a6f91 to disappear
    Jul 29 16:15:18.636: INFO: Pod pod-projected-configmaps-46affa57-08e4-452e-ba2a-a9b6476a6f91 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jul 29 16:15:18.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-2202" for this suite. 07/29/23 16:15:18.644
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:15:18.667
Jul 29 16:15:18.667: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename gc 07/29/23 16:15:18.669
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:15:18.702
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:15:18.707
[It] should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
STEP: create the rc 07/29/23 16:15:18.723
STEP: delete the rc 07/29/23 16:15:23.779
STEP: wait for the rc to be deleted 07/29/23 16:15:23.854
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 07/29/23 16:15:28.908
STEP: Gathering metrics 07/29/23 16:15:58.934
Jul 29 16:15:58.979: INFO: Waiting up to 5m0s for pod "kube-controller-manager-wa4quivohpee-2" in namespace "kube-system" to be "running and ready"
Jul 29 16:15:58.990: INFO: Pod "kube-controller-manager-wa4quivohpee-2": Phase="Running", Reason="", readiness=true. Elapsed: 10.252596ms
Jul 29 16:15:58.990: INFO: The phase of Pod kube-controller-manager-wa4quivohpee-2 is Running (Ready = true)
Jul 29 16:15:58.991: INFO: Pod "kube-controller-manager-wa4quivohpee-2" satisfied condition "running and ready"
Jul 29 16:15:59.095: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Jul 29 16:15:59.095: INFO: Deleting pod "simpletest.rc-25qhn" in namespace "gc-6675"
Jul 29 16:15:59.134: INFO: Deleting pod "simpletest.rc-28sm9" in namespace "gc-6675"
Jul 29 16:15:59.193: INFO: Deleting pod "simpletest.rc-2k8n9" in namespace "gc-6675"
Jul 29 16:15:59.239: INFO: Deleting pod "simpletest.rc-2kf58" in namespace "gc-6675"
Jul 29 16:15:59.391: INFO: Deleting pod "simpletest.rc-2mdw6" in namespace "gc-6675"
Jul 29 16:15:59.518: INFO: Deleting pod "simpletest.rc-2n2m2" in namespace "gc-6675"
Jul 29 16:15:59.605: INFO: Deleting pod "simpletest.rc-2rmbd" in namespace "gc-6675"
Jul 29 16:15:59.673: INFO: Deleting pod "simpletest.rc-2v4fn" in namespace "gc-6675"
Jul 29 16:15:59.777: INFO: Deleting pod "simpletest.rc-4dgf6" in namespace "gc-6675"
Jul 29 16:15:59.830: INFO: Deleting pod "simpletest.rc-4zjlp" in namespace "gc-6675"
Jul 29 16:15:59.889: INFO: Deleting pod "simpletest.rc-56rvd" in namespace "gc-6675"
Jul 29 16:15:59.959: INFO: Deleting pod "simpletest.rc-5c9cw" in namespace "gc-6675"
Jul 29 16:16:00.001: INFO: Deleting pod "simpletest.rc-5wtdg" in namespace "gc-6675"
Jul 29 16:16:00.070: INFO: Deleting pod "simpletest.rc-5xxkq" in namespace "gc-6675"
Jul 29 16:16:00.151: INFO: Deleting pod "simpletest.rc-68jkj" in namespace "gc-6675"
Jul 29 16:16:00.235: INFO: Deleting pod "simpletest.rc-6f5c7" in namespace "gc-6675"
Jul 29 16:16:00.345: INFO: Deleting pod "simpletest.rc-6j8vx" in namespace "gc-6675"
Jul 29 16:16:00.398: INFO: Deleting pod "simpletest.rc-6ngzw" in namespace "gc-6675"
Jul 29 16:16:00.480: INFO: Deleting pod "simpletest.rc-6wnq8" in namespace "gc-6675"
Jul 29 16:16:00.603: INFO: Deleting pod "simpletest.rc-6xsgn" in namespace "gc-6675"
Jul 29 16:16:00.687: INFO: Deleting pod "simpletest.rc-756st" in namespace "gc-6675"
Jul 29 16:16:00.729: INFO: Deleting pod "simpletest.rc-7h8sw" in namespace "gc-6675"
Jul 29 16:16:00.785: INFO: Deleting pod "simpletest.rc-7nkrs" in namespace "gc-6675"
Jul 29 16:16:00.859: INFO: Deleting pod "simpletest.rc-88fbv" in namespace "gc-6675"
Jul 29 16:16:00.929: INFO: Deleting pod "simpletest.rc-8qgpx" in namespace "gc-6675"
Jul 29 16:16:01.035: INFO: Deleting pod "simpletest.rc-8wvv2" in namespace "gc-6675"
Jul 29 16:16:01.068: INFO: Deleting pod "simpletest.rc-96vdl" in namespace "gc-6675"
Jul 29 16:16:01.243: INFO: Deleting pod "simpletest.rc-9fmg5" in namespace "gc-6675"
Jul 29 16:16:01.511: INFO: Deleting pod "simpletest.rc-9gnpc" in namespace "gc-6675"
Jul 29 16:16:01.586: INFO: Deleting pod "simpletest.rc-ckm77" in namespace "gc-6675"
Jul 29 16:16:01.672: INFO: Deleting pod "simpletest.rc-dcf9x" in namespace "gc-6675"
Jul 29 16:16:01.823: INFO: Deleting pod "simpletest.rc-dfhk5" in namespace "gc-6675"
Jul 29 16:16:01.910: INFO: Deleting pod "simpletest.rc-dl272" in namespace "gc-6675"
Jul 29 16:16:02.059: INFO: Deleting pod "simpletest.rc-dllqk" in namespace "gc-6675"
Jul 29 16:16:02.106: INFO: Deleting pod "simpletest.rc-fglpf" in namespace "gc-6675"
Jul 29 16:16:02.176: INFO: Deleting pod "simpletest.rc-fx8xd" in namespace "gc-6675"
Jul 29 16:16:02.217: INFO: Deleting pod "simpletest.rc-fxggs" in namespace "gc-6675"
Jul 29 16:16:02.292: INFO: Deleting pod "simpletest.rc-g4cn2" in namespace "gc-6675"
Jul 29 16:16:02.405: INFO: Deleting pod "simpletest.rc-gfj8t" in namespace "gc-6675"
Jul 29 16:16:02.456: INFO: Deleting pod "simpletest.rc-ggb5g" in namespace "gc-6675"
Jul 29 16:16:02.534: INFO: Deleting pod "simpletest.rc-gqz4v" in namespace "gc-6675"
Jul 29 16:16:02.607: INFO: Deleting pod "simpletest.rc-grdbx" in namespace "gc-6675"
Jul 29 16:16:02.709: INFO: Deleting pod "simpletest.rc-h7s8w" in namespace "gc-6675"
Jul 29 16:16:02.772: INFO: Deleting pod "simpletest.rc-hlkwm" in namespace "gc-6675"
Jul 29 16:16:02.916: INFO: Deleting pod "simpletest.rc-hp4b7" in namespace "gc-6675"
Jul 29 16:16:03.045: INFO: Deleting pod "simpletest.rc-hqtln" in namespace "gc-6675"
Jul 29 16:16:03.135: INFO: Deleting pod "simpletest.rc-j4884" in namespace "gc-6675"
Jul 29 16:16:03.202: INFO: Deleting pod "simpletest.rc-j7fn2" in namespace "gc-6675"
Jul 29 16:16:03.270: INFO: Deleting pod "simpletest.rc-jfnzt" in namespace "gc-6675"
Jul 29 16:16:03.383: INFO: Deleting pod "simpletest.rc-jn52q" in namespace "gc-6675"
Jul 29 16:16:03.457: INFO: Deleting pod "simpletest.rc-jqchp" in namespace "gc-6675"
Jul 29 16:16:03.552: INFO: Deleting pod "simpletest.rc-jqlf2" in namespace "gc-6675"
Jul 29 16:16:03.637: INFO: Deleting pod "simpletest.rc-jz9vt" in namespace "gc-6675"
Jul 29 16:16:03.708: INFO: Deleting pod "simpletest.rc-k95hx" in namespace "gc-6675"
Jul 29 16:16:03.886: INFO: Deleting pod "simpletest.rc-knml4" in namespace "gc-6675"
Jul 29 16:16:03.931: INFO: Deleting pod "simpletest.rc-l76km" in namespace "gc-6675"
Jul 29 16:16:03.996: INFO: Deleting pod "simpletest.rc-lfvns" in namespace "gc-6675"
Jul 29 16:16:04.051: INFO: Deleting pod "simpletest.rc-lxfzf" in namespace "gc-6675"
Jul 29 16:16:04.102: INFO: Deleting pod "simpletest.rc-m2gqr" in namespace "gc-6675"
Jul 29 16:16:04.165: INFO: Deleting pod "simpletest.rc-m2j9k" in namespace "gc-6675"
Jul 29 16:16:04.221: INFO: Deleting pod "simpletest.rc-m62fx" in namespace "gc-6675"
Jul 29 16:16:04.282: INFO: Deleting pod "simpletest.rc-n4sqq" in namespace "gc-6675"
Jul 29 16:16:04.387: INFO: Deleting pod "simpletest.rc-n9mfj" in namespace "gc-6675"
Jul 29 16:16:04.442: INFO: Deleting pod "simpletest.rc-ngrn2" in namespace "gc-6675"
Jul 29 16:16:04.590: INFO: Deleting pod "simpletest.rc-nkhcl" in namespace "gc-6675"
Jul 29 16:16:04.680: INFO: Deleting pod "simpletest.rc-nqjfb" in namespace "gc-6675"
Jul 29 16:16:04.748: INFO: Deleting pod "simpletest.rc-pj4dr" in namespace "gc-6675"
Jul 29 16:16:04.835: INFO: Deleting pod "simpletest.rc-plmqm" in namespace "gc-6675"
Jul 29 16:16:04.884: INFO: Deleting pod "simpletest.rc-pnk2z" in namespace "gc-6675"
Jul 29 16:16:04.952: INFO: Deleting pod "simpletest.rc-pvc77" in namespace "gc-6675"
Jul 29 16:16:05.175: INFO: Deleting pod "simpletest.rc-q772l" in namespace "gc-6675"
Jul 29 16:16:05.355: INFO: Deleting pod "simpletest.rc-qm7s6" in namespace "gc-6675"
Jul 29 16:16:05.437: INFO: Deleting pod "simpletest.rc-qp7wb" in namespace "gc-6675"
Jul 29 16:16:05.520: INFO: Deleting pod "simpletest.rc-qvqb2" in namespace "gc-6675"
Jul 29 16:16:05.582: INFO: Deleting pod "simpletest.rc-r4zjs" in namespace "gc-6675"
Jul 29 16:16:05.703: INFO: Deleting pod "simpletest.rc-r65ct" in namespace "gc-6675"
Jul 29 16:16:05.793: INFO: Deleting pod "simpletest.rc-rb2nh" in namespace "gc-6675"
Jul 29 16:16:05.867: INFO: Deleting pod "simpletest.rc-rnbt6" in namespace "gc-6675"
Jul 29 16:16:05.983: INFO: Deleting pod "simpletest.rc-s4w2c" in namespace "gc-6675"
Jul 29 16:16:06.029: INFO: Deleting pod "simpletest.rc-sflf9" in namespace "gc-6675"
Jul 29 16:16:06.098: INFO: Deleting pod "simpletest.rc-sjq5p" in namespace "gc-6675"
Jul 29 16:16:06.158: INFO: Deleting pod "simpletest.rc-tm2xc" in namespace "gc-6675"
Jul 29 16:16:06.301: INFO: Deleting pod "simpletest.rc-tr7n5" in namespace "gc-6675"
Jul 29 16:16:06.364: INFO: Deleting pod "simpletest.rc-v4mzc" in namespace "gc-6675"
Jul 29 16:16:06.509: INFO: Deleting pod "simpletest.rc-v5tbd" in namespace "gc-6675"
Jul 29 16:16:06.597: INFO: Deleting pod "simpletest.rc-vdbgd" in namespace "gc-6675"
Jul 29 16:16:06.685: INFO: Deleting pod "simpletest.rc-vn6qp" in namespace "gc-6675"
Jul 29 16:16:06.735: INFO: Deleting pod "simpletest.rc-vsrmg" in namespace "gc-6675"
Jul 29 16:16:06.813: INFO: Deleting pod "simpletest.rc-vz6qw" in namespace "gc-6675"
Jul 29 16:16:06.878: INFO: Deleting pod "simpletest.rc-w22nk" in namespace "gc-6675"
Jul 29 16:16:06.966: INFO: Deleting pod "simpletest.rc-wms8f" in namespace "gc-6675"
Jul 29 16:16:07.024: INFO: Deleting pod "simpletest.rc-ww72m" in namespace "gc-6675"
Jul 29 16:16:07.119: INFO: Deleting pod "simpletest.rc-x48tb" in namespace "gc-6675"
Jul 29 16:16:07.194: INFO: Deleting pod "simpletest.rc-x5c42" in namespace "gc-6675"
Jul 29 16:16:07.332: INFO: Deleting pod "simpletest.rc-xgxkn" in namespace "gc-6675"
Jul 29 16:16:07.417: INFO: Deleting pod "simpletest.rc-xt682" in namespace "gc-6675"
Jul 29 16:16:07.496: INFO: Deleting pod "simpletest.rc-xtw6k" in namespace "gc-6675"
Jul 29 16:16:07.590: INFO: Deleting pod "simpletest.rc-xzw7x" in namespace "gc-6675"
Jul 29 16:16:07.656: INFO: Deleting pod "simpletest.rc-zbbfb" in namespace "gc-6675"
Jul 29 16:16:07.731: INFO: Deleting pod "simpletest.rc-zmw2r" in namespace "gc-6675"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Jul 29 16:16:07.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6675" for this suite. 07/29/23 16:16:07.817
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","completed":192,"skipped":3581,"failed":0}
------------------------------
â€¢ [SLOW TEST] [49.193 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:15:18.667
    Jul 29 16:15:18.667: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename gc 07/29/23 16:15:18.669
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:15:18.702
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:15:18.707
    [It] should orphan pods created by rc if delete options say so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:370
    STEP: create the rc 07/29/23 16:15:18.723
    STEP: delete the rc 07/29/23 16:15:23.779
    STEP: wait for the rc to be deleted 07/29/23 16:15:23.854
    STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 07/29/23 16:15:28.908
    STEP: Gathering metrics 07/29/23 16:15:58.934
    Jul 29 16:15:58.979: INFO: Waiting up to 5m0s for pod "kube-controller-manager-wa4quivohpee-2" in namespace "kube-system" to be "running and ready"
    Jul 29 16:15:58.990: INFO: Pod "kube-controller-manager-wa4quivohpee-2": Phase="Running", Reason="", readiness=true. Elapsed: 10.252596ms
    Jul 29 16:15:58.990: INFO: The phase of Pod kube-controller-manager-wa4quivohpee-2 is Running (Ready = true)
    Jul 29 16:15:58.991: INFO: Pod "kube-controller-manager-wa4quivohpee-2" satisfied condition "running and ready"
    Jul 29 16:15:59.095: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Jul 29 16:15:59.095: INFO: Deleting pod "simpletest.rc-25qhn" in namespace "gc-6675"
    Jul 29 16:15:59.134: INFO: Deleting pod "simpletest.rc-28sm9" in namespace "gc-6675"
    Jul 29 16:15:59.193: INFO: Deleting pod "simpletest.rc-2k8n9" in namespace "gc-6675"
    Jul 29 16:15:59.239: INFO: Deleting pod "simpletest.rc-2kf58" in namespace "gc-6675"
    Jul 29 16:15:59.391: INFO: Deleting pod "simpletest.rc-2mdw6" in namespace "gc-6675"
    Jul 29 16:15:59.518: INFO: Deleting pod "simpletest.rc-2n2m2" in namespace "gc-6675"
    Jul 29 16:15:59.605: INFO: Deleting pod "simpletest.rc-2rmbd" in namespace "gc-6675"
    Jul 29 16:15:59.673: INFO: Deleting pod "simpletest.rc-2v4fn" in namespace "gc-6675"
    Jul 29 16:15:59.777: INFO: Deleting pod "simpletest.rc-4dgf6" in namespace "gc-6675"
    Jul 29 16:15:59.830: INFO: Deleting pod "simpletest.rc-4zjlp" in namespace "gc-6675"
    Jul 29 16:15:59.889: INFO: Deleting pod "simpletest.rc-56rvd" in namespace "gc-6675"
    Jul 29 16:15:59.959: INFO: Deleting pod "simpletest.rc-5c9cw" in namespace "gc-6675"
    Jul 29 16:16:00.001: INFO: Deleting pod "simpletest.rc-5wtdg" in namespace "gc-6675"
    Jul 29 16:16:00.070: INFO: Deleting pod "simpletest.rc-5xxkq" in namespace "gc-6675"
    Jul 29 16:16:00.151: INFO: Deleting pod "simpletest.rc-68jkj" in namespace "gc-6675"
    Jul 29 16:16:00.235: INFO: Deleting pod "simpletest.rc-6f5c7" in namespace "gc-6675"
    Jul 29 16:16:00.345: INFO: Deleting pod "simpletest.rc-6j8vx" in namespace "gc-6675"
    Jul 29 16:16:00.398: INFO: Deleting pod "simpletest.rc-6ngzw" in namespace "gc-6675"
    Jul 29 16:16:00.480: INFO: Deleting pod "simpletest.rc-6wnq8" in namespace "gc-6675"
    Jul 29 16:16:00.603: INFO: Deleting pod "simpletest.rc-6xsgn" in namespace "gc-6675"
    Jul 29 16:16:00.687: INFO: Deleting pod "simpletest.rc-756st" in namespace "gc-6675"
    Jul 29 16:16:00.729: INFO: Deleting pod "simpletest.rc-7h8sw" in namespace "gc-6675"
    Jul 29 16:16:00.785: INFO: Deleting pod "simpletest.rc-7nkrs" in namespace "gc-6675"
    Jul 29 16:16:00.859: INFO: Deleting pod "simpletest.rc-88fbv" in namespace "gc-6675"
    Jul 29 16:16:00.929: INFO: Deleting pod "simpletest.rc-8qgpx" in namespace "gc-6675"
    Jul 29 16:16:01.035: INFO: Deleting pod "simpletest.rc-8wvv2" in namespace "gc-6675"
    Jul 29 16:16:01.068: INFO: Deleting pod "simpletest.rc-96vdl" in namespace "gc-6675"
    Jul 29 16:16:01.243: INFO: Deleting pod "simpletest.rc-9fmg5" in namespace "gc-6675"
    Jul 29 16:16:01.511: INFO: Deleting pod "simpletest.rc-9gnpc" in namespace "gc-6675"
    Jul 29 16:16:01.586: INFO: Deleting pod "simpletest.rc-ckm77" in namespace "gc-6675"
    Jul 29 16:16:01.672: INFO: Deleting pod "simpletest.rc-dcf9x" in namespace "gc-6675"
    Jul 29 16:16:01.823: INFO: Deleting pod "simpletest.rc-dfhk5" in namespace "gc-6675"
    Jul 29 16:16:01.910: INFO: Deleting pod "simpletest.rc-dl272" in namespace "gc-6675"
    Jul 29 16:16:02.059: INFO: Deleting pod "simpletest.rc-dllqk" in namespace "gc-6675"
    Jul 29 16:16:02.106: INFO: Deleting pod "simpletest.rc-fglpf" in namespace "gc-6675"
    Jul 29 16:16:02.176: INFO: Deleting pod "simpletest.rc-fx8xd" in namespace "gc-6675"
    Jul 29 16:16:02.217: INFO: Deleting pod "simpletest.rc-fxggs" in namespace "gc-6675"
    Jul 29 16:16:02.292: INFO: Deleting pod "simpletest.rc-g4cn2" in namespace "gc-6675"
    Jul 29 16:16:02.405: INFO: Deleting pod "simpletest.rc-gfj8t" in namespace "gc-6675"
    Jul 29 16:16:02.456: INFO: Deleting pod "simpletest.rc-ggb5g" in namespace "gc-6675"
    Jul 29 16:16:02.534: INFO: Deleting pod "simpletest.rc-gqz4v" in namespace "gc-6675"
    Jul 29 16:16:02.607: INFO: Deleting pod "simpletest.rc-grdbx" in namespace "gc-6675"
    Jul 29 16:16:02.709: INFO: Deleting pod "simpletest.rc-h7s8w" in namespace "gc-6675"
    Jul 29 16:16:02.772: INFO: Deleting pod "simpletest.rc-hlkwm" in namespace "gc-6675"
    Jul 29 16:16:02.916: INFO: Deleting pod "simpletest.rc-hp4b7" in namespace "gc-6675"
    Jul 29 16:16:03.045: INFO: Deleting pod "simpletest.rc-hqtln" in namespace "gc-6675"
    Jul 29 16:16:03.135: INFO: Deleting pod "simpletest.rc-j4884" in namespace "gc-6675"
    Jul 29 16:16:03.202: INFO: Deleting pod "simpletest.rc-j7fn2" in namespace "gc-6675"
    Jul 29 16:16:03.270: INFO: Deleting pod "simpletest.rc-jfnzt" in namespace "gc-6675"
    Jul 29 16:16:03.383: INFO: Deleting pod "simpletest.rc-jn52q" in namespace "gc-6675"
    Jul 29 16:16:03.457: INFO: Deleting pod "simpletest.rc-jqchp" in namespace "gc-6675"
    Jul 29 16:16:03.552: INFO: Deleting pod "simpletest.rc-jqlf2" in namespace "gc-6675"
    Jul 29 16:16:03.637: INFO: Deleting pod "simpletest.rc-jz9vt" in namespace "gc-6675"
    Jul 29 16:16:03.708: INFO: Deleting pod "simpletest.rc-k95hx" in namespace "gc-6675"
    Jul 29 16:16:03.886: INFO: Deleting pod "simpletest.rc-knml4" in namespace "gc-6675"
    Jul 29 16:16:03.931: INFO: Deleting pod "simpletest.rc-l76km" in namespace "gc-6675"
    Jul 29 16:16:03.996: INFO: Deleting pod "simpletest.rc-lfvns" in namespace "gc-6675"
    Jul 29 16:16:04.051: INFO: Deleting pod "simpletest.rc-lxfzf" in namespace "gc-6675"
    Jul 29 16:16:04.102: INFO: Deleting pod "simpletest.rc-m2gqr" in namespace "gc-6675"
    Jul 29 16:16:04.165: INFO: Deleting pod "simpletest.rc-m2j9k" in namespace "gc-6675"
    Jul 29 16:16:04.221: INFO: Deleting pod "simpletest.rc-m62fx" in namespace "gc-6675"
    Jul 29 16:16:04.282: INFO: Deleting pod "simpletest.rc-n4sqq" in namespace "gc-6675"
    Jul 29 16:16:04.387: INFO: Deleting pod "simpletest.rc-n9mfj" in namespace "gc-6675"
    Jul 29 16:16:04.442: INFO: Deleting pod "simpletest.rc-ngrn2" in namespace "gc-6675"
    Jul 29 16:16:04.590: INFO: Deleting pod "simpletest.rc-nkhcl" in namespace "gc-6675"
    Jul 29 16:16:04.680: INFO: Deleting pod "simpletest.rc-nqjfb" in namespace "gc-6675"
    Jul 29 16:16:04.748: INFO: Deleting pod "simpletest.rc-pj4dr" in namespace "gc-6675"
    Jul 29 16:16:04.835: INFO: Deleting pod "simpletest.rc-plmqm" in namespace "gc-6675"
    Jul 29 16:16:04.884: INFO: Deleting pod "simpletest.rc-pnk2z" in namespace "gc-6675"
    Jul 29 16:16:04.952: INFO: Deleting pod "simpletest.rc-pvc77" in namespace "gc-6675"
    Jul 29 16:16:05.175: INFO: Deleting pod "simpletest.rc-q772l" in namespace "gc-6675"
    Jul 29 16:16:05.355: INFO: Deleting pod "simpletest.rc-qm7s6" in namespace "gc-6675"
    Jul 29 16:16:05.437: INFO: Deleting pod "simpletest.rc-qp7wb" in namespace "gc-6675"
    Jul 29 16:16:05.520: INFO: Deleting pod "simpletest.rc-qvqb2" in namespace "gc-6675"
    Jul 29 16:16:05.582: INFO: Deleting pod "simpletest.rc-r4zjs" in namespace "gc-6675"
    Jul 29 16:16:05.703: INFO: Deleting pod "simpletest.rc-r65ct" in namespace "gc-6675"
    Jul 29 16:16:05.793: INFO: Deleting pod "simpletest.rc-rb2nh" in namespace "gc-6675"
    Jul 29 16:16:05.867: INFO: Deleting pod "simpletest.rc-rnbt6" in namespace "gc-6675"
    Jul 29 16:16:05.983: INFO: Deleting pod "simpletest.rc-s4w2c" in namespace "gc-6675"
    Jul 29 16:16:06.029: INFO: Deleting pod "simpletest.rc-sflf9" in namespace "gc-6675"
    Jul 29 16:16:06.098: INFO: Deleting pod "simpletest.rc-sjq5p" in namespace "gc-6675"
    Jul 29 16:16:06.158: INFO: Deleting pod "simpletest.rc-tm2xc" in namespace "gc-6675"
    Jul 29 16:16:06.301: INFO: Deleting pod "simpletest.rc-tr7n5" in namespace "gc-6675"
    Jul 29 16:16:06.364: INFO: Deleting pod "simpletest.rc-v4mzc" in namespace "gc-6675"
    Jul 29 16:16:06.509: INFO: Deleting pod "simpletest.rc-v5tbd" in namespace "gc-6675"
    Jul 29 16:16:06.597: INFO: Deleting pod "simpletest.rc-vdbgd" in namespace "gc-6675"
    Jul 29 16:16:06.685: INFO: Deleting pod "simpletest.rc-vn6qp" in namespace "gc-6675"
    Jul 29 16:16:06.735: INFO: Deleting pod "simpletest.rc-vsrmg" in namespace "gc-6675"
    Jul 29 16:16:06.813: INFO: Deleting pod "simpletest.rc-vz6qw" in namespace "gc-6675"
    Jul 29 16:16:06.878: INFO: Deleting pod "simpletest.rc-w22nk" in namespace "gc-6675"
    Jul 29 16:16:06.966: INFO: Deleting pod "simpletest.rc-wms8f" in namespace "gc-6675"
    Jul 29 16:16:07.024: INFO: Deleting pod "simpletest.rc-ww72m" in namespace "gc-6675"
    Jul 29 16:16:07.119: INFO: Deleting pod "simpletest.rc-x48tb" in namespace "gc-6675"
    Jul 29 16:16:07.194: INFO: Deleting pod "simpletest.rc-x5c42" in namespace "gc-6675"
    Jul 29 16:16:07.332: INFO: Deleting pod "simpletest.rc-xgxkn" in namespace "gc-6675"
    Jul 29 16:16:07.417: INFO: Deleting pod "simpletest.rc-xt682" in namespace "gc-6675"
    Jul 29 16:16:07.496: INFO: Deleting pod "simpletest.rc-xtw6k" in namespace "gc-6675"
    Jul 29 16:16:07.590: INFO: Deleting pod "simpletest.rc-xzw7x" in namespace "gc-6675"
    Jul 29 16:16:07.656: INFO: Deleting pod "simpletest.rc-zbbfb" in namespace "gc-6675"
    Jul 29 16:16:07.731: INFO: Deleting pod "simpletest.rc-zmw2r" in namespace "gc-6675"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Jul 29 16:16:07.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-6675" for this suite. 07/29/23 16:16:07.817
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:16:07.868
Jul 29 16:16:07.868: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename deployment 07/29/23 16:16:07.872
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:16:07.912
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:16:07.919
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
Jul 29 16:16:07.932: INFO: Creating deployment "webserver-deployment"
Jul 29 16:16:07.964: INFO: Waiting for observed generation 1
Jul 29 16:16:09.988: INFO: Waiting for all required pods to come up
Jul 29 16:16:10.023: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running 07/29/23 16:16:10.023
Jul 29 16:16:10.023: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-v2k5d" in namespace "deployment-7704" to be "running"
Jul 29 16:16:10.024: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-4l4mn" in namespace "deployment-7704" to be "running"
Jul 29 16:16:10.024: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-6bw72" in namespace "deployment-7704" to be "running"
Jul 29 16:16:10.025: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-b6pkk" in namespace "deployment-7704" to be "running"
Jul 29 16:16:10.025: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-dsghk" in namespace "deployment-7704" to be "running"
Jul 29 16:16:10.025: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-f7zkx" in namespace "deployment-7704" to be "running"
Jul 29 16:16:10.026: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-f8mx6" in namespace "deployment-7704" to be "running"
Jul 29 16:16:10.026: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-ghmgr" in namespace "deployment-7704" to be "running"
Jul 29 16:16:10.026: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-jr8gs" in namespace "deployment-7704" to be "running"
Jul 29 16:16:10.027: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-tbvfd" in namespace "deployment-7704" to be "running"
Jul 29 16:16:10.036: INFO: Pod "webserver-deployment-845c8977d9-v2k5d": Phase="Pending", Reason="", readiness=false. Elapsed: 12.622642ms
Jul 29 16:16:10.036: INFO: Pod "webserver-deployment-845c8977d9-4l4mn": Phase="Pending", Reason="", readiness=false. Elapsed: 12.177404ms
Jul 29 16:16:10.042: INFO: Pod "webserver-deployment-845c8977d9-b6pkk": Phase="Pending", Reason="", readiness=false. Elapsed: 16.827531ms
Jul 29 16:16:10.042: INFO: Pod "webserver-deployment-845c8977d9-jr8gs": Phase="Pending", Reason="", readiness=false. Elapsed: 15.309428ms
Jul 29 16:16:10.046: INFO: Pod "webserver-deployment-845c8977d9-dsghk": Phase="Pending", Reason="", readiness=false. Elapsed: 20.923706ms
Jul 29 16:16:10.047: INFO: Pod "webserver-deployment-845c8977d9-tbvfd": Phase="Pending", Reason="", readiness=false. Elapsed: 19.973738ms
Jul 29 16:16:10.047: INFO: Pod "webserver-deployment-845c8977d9-ghmgr": Phase="Pending", Reason="", readiness=false. Elapsed: 21.414746ms
Jul 29 16:16:10.053: INFO: Pod "webserver-deployment-845c8977d9-6bw72": Phase="Pending", Reason="", readiness=false. Elapsed: 28.219732ms
Jul 29 16:16:10.071: INFO: Pod "webserver-deployment-845c8977d9-f7zkx": Phase="Pending", Reason="", readiness=false. Elapsed: 45.825955ms
Jul 29 16:16:10.072: INFO: Pod "webserver-deployment-845c8977d9-f8mx6": Phase="Pending", Reason="", readiness=false. Elapsed: 45.775607ms
Jul 29 16:16:12.046: INFO: Pod "webserver-deployment-845c8977d9-v2k5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022058797s
Jul 29 16:16:12.047: INFO: Pod "webserver-deployment-845c8977d9-4l4mn": Phase="Running", Reason="", readiness=true. Elapsed: 2.02322558s
Jul 29 16:16:12.047: INFO: Pod "webserver-deployment-845c8977d9-4l4mn" satisfied condition "running"
Jul 29 16:16:12.051: INFO: Pod "webserver-deployment-845c8977d9-b6pkk": Phase="Running", Reason="", readiness=true. Elapsed: 2.026551403s
Jul 29 16:16:12.051: INFO: Pod "webserver-deployment-845c8977d9-b6pkk" satisfied condition "running"
Jul 29 16:16:12.053: INFO: Pod "webserver-deployment-845c8977d9-jr8gs": Phase="Running", Reason="", readiness=true. Elapsed: 2.026585966s
Jul 29 16:16:12.054: INFO: Pod "webserver-deployment-845c8977d9-jr8gs" satisfied condition "running"
Jul 29 16:16:12.060: INFO: Pod "webserver-deployment-845c8977d9-dsghk": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03499241s
Jul 29 16:16:12.061: INFO: Pod "webserver-deployment-845c8977d9-tbvfd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033727183s
Jul 29 16:16:12.063: INFO: Pod "webserver-deployment-845c8977d9-6bw72": Phase="Running", Reason="", readiness=true. Elapsed: 2.038561075s
Jul 29 16:16:12.063: INFO: Pod "webserver-deployment-845c8977d9-6bw72" satisfied condition "running"
Jul 29 16:16:12.066: INFO: Pod "webserver-deployment-845c8977d9-ghmgr": Phase="Running", Reason="", readiness=true. Elapsed: 2.039636853s
Jul 29 16:16:12.066: INFO: Pod "webserver-deployment-845c8977d9-ghmgr" satisfied condition "running"
Jul 29 16:16:12.087: INFO: Pod "webserver-deployment-845c8977d9-f7zkx": Phase="Running", Reason="", readiness=true. Elapsed: 2.06151291s
Jul 29 16:16:12.087: INFO: Pod "webserver-deployment-845c8977d9-f7zkx" satisfied condition "running"
Jul 29 16:16:12.088: INFO: Pod "webserver-deployment-845c8977d9-f8mx6": Phase="Running", Reason="", readiness=true. Elapsed: 2.061866743s
Jul 29 16:16:12.088: INFO: Pod "webserver-deployment-845c8977d9-f8mx6" satisfied condition "running"
Jul 29 16:16:14.044: INFO: Pod "webserver-deployment-845c8977d9-v2k5d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020942634s
Jul 29 16:16:14.052: INFO: Pod "webserver-deployment-845c8977d9-dsghk": Phase="Pending", Reason="", readiness=false. Elapsed: 4.02654114s
Jul 29 16:16:14.055: INFO: Pod "webserver-deployment-845c8977d9-tbvfd": Phase="Running", Reason="", readiness=true. Elapsed: 4.028132861s
Jul 29 16:16:14.055: INFO: Pod "webserver-deployment-845c8977d9-tbvfd" satisfied condition "running"
Jul 29 16:16:16.045: INFO: Pod "webserver-deployment-845c8977d9-v2k5d": Phase="Running", Reason="", readiness=true. Elapsed: 6.021335976s
Jul 29 16:16:16.045: INFO: Pod "webserver-deployment-845c8977d9-v2k5d" satisfied condition "running"
Jul 29 16:16:16.051: INFO: Pod "webserver-deployment-845c8977d9-dsghk": Phase="Running", Reason="", readiness=true. Elapsed: 6.02614028s
Jul 29 16:16:16.051: INFO: Pod "webserver-deployment-845c8977d9-dsghk" satisfied condition "running"
Jul 29 16:16:16.052: INFO: Waiting for deployment "webserver-deployment" to complete
Jul 29 16:16:16.066: INFO: Updating deployment "webserver-deployment" with a non-existent image
Jul 29 16:16:16.084: INFO: Updating deployment webserver-deployment
Jul 29 16:16:16.084: INFO: Waiting for observed generation 2
Jul 29 16:16:18.096: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Jul 29 16:16:18.103: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Jul 29 16:16:18.108: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jul 29 16:16:18.127: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Jul 29 16:16:18.127: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Jul 29 16:16:18.134: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jul 29 16:16:18.148: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Jul 29 16:16:18.148: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Jul 29 16:16:18.166: INFO: Updating deployment webserver-deployment
Jul 29 16:16:18.166: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Jul 29 16:16:18.202: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Jul 29 16:16:18.210: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jul 29 16:16:18.233: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-7704  2c9ced0f-bead-4558-97ab-c777ab8f4508 20479 3 2023-07-29 16:16:07 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{kube-controller-manager Update apps/v1 2023-07-29 16:16:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status} {e2e.test Update apps/v1 2023-07-29 16:16:18 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003295fb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-07-29 16:16:13 +0000 UTC,LastTransitionTime:2023-07-29 16:16:13 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-69b7448995" is progressing.,LastUpdateTime:2023-07-29 16:16:16 +0000 UTC,LastTransitionTime:2023-07-29 16:16:07 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Jul 29 16:16:18.252: INFO: New ReplicaSet "webserver-deployment-69b7448995" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-69b7448995  deployment-7704  415ace57-a1c0-42b0-9b4c-4fb335bf9e14 20483 3 2023-07-29 16:16:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 2c9ced0f-bead-4558-97ab-c777ab8f4508 0xc00311c9e7 0xc00311c9e8}] [] [{kube-controller-manager Update apps/v1 2023-07-29 16:16:16 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-07-29 16:16:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2c9ced0f-bead-4558-97ab-c777ab8f4508\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 69b7448995,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00311d1b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jul 29 16:16:18.265: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Jul 29 16:16:18.266: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-845c8977d9  deployment-7704  96abbba1-e89d-4f29-9448-882a0204331f 20480 3 2023-07-29 16:16:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 2c9ced0f-bead-4558-97ab-c777ab8f4508 0xc00311d217 0xc00311d218}] [] [{kube-controller-manager Update apps/v1 2023-07-29 16:16:16 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-07-29 16:16:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2c9ced0f-bead-4558-97ab-c777ab8f4508\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00311d2a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Jul 29 16:16:18.283: INFO: Pod "webserver-deployment-69b7448995-2b49k" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-2b49k webserver-deployment-69b7448995- deployment-7704  81108fe6-a3df-4115-95ae-84db39634d99 20405 0 2023-07-29 16:16:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 415ace57-a1c0-42b0-9b4c-4fb335bf9e14 0xc0033a43f7 0xc0033a43f8}] [] [{kube-controller-manager Update v1 2023-07-29 16:16:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"415ace57-a1c0-42b0-9b4c-4fb335bf9e14\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 16:16:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-49wnh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-49wnh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wa4quivohpee-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:16:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:16:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:16:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:16:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.28,PodIP:,StartTime:2023-07-29 16:16:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 29 16:16:18.287: INFO: Pod "webserver-deployment-69b7448995-56687" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-56687 webserver-deployment-69b7448995- deployment-7704  cb02abd9-000b-4e34-8eb7-36eb9e45249e 20368 0 2023-07-29 16:16:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 415ace57-a1c0-42b0-9b4c-4fb335bf9e14 0xc0033a4607 0xc0033a4608}] [] [{kube-controller-manager Update v1 2023-07-29 16:16:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"415ace57-a1c0-42b0-9b4c-4fb335bf9e14\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 16:16:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zrchf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zrchf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wa4quivohpee-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:16:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:16:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:16:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:16:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.206,PodIP:,StartTime:2023-07-29 16:16:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 29 16:16:18.288: INFO: Pod "webserver-deployment-69b7448995-9xjps" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-9xjps webserver-deployment-69b7448995- deployment-7704  4d5987f5-9641-44f3-b39a-d7400da8886e 20372 0 2023-07-29 16:16:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 415ace57-a1c0-42b0-9b4c-4fb335bf9e14 0xc0033a4817 0xc0033a4818}] [] [{kube-controller-manager Update v1 2023-07-29 16:16:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"415ace57-a1c0-42b0-9b4c-4fb335bf9e14\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 16:16:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-55hq4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-55hq4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wa4quivohpee-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:16:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:16:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:16:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:16:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.28,PodIP:,StartTime:2023-07-29 16:16:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 29 16:16:18.289: INFO: Pod "webserver-deployment-69b7448995-k4nxw" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-k4nxw webserver-deployment-69b7448995- deployment-7704  4140c8aa-012a-4eeb-a691-8e058762c15c 20496 0 2023-07-29 16:16:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 415ace57-a1c0-42b0-9b4c-4fb335bf9e14 0xc0033a4a37 0xc0033a4a38}] [] [{kube-controller-manager Update v1 2023-07-29 16:16:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"415ace57-a1c0-42b0-9b4c-4fb335bf9e14\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fk6zv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fk6zv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 29 16:16:18.289: INFO: Pod "webserver-deployment-69b7448995-qj2ct" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-qj2ct webserver-deployment-69b7448995- deployment-7704  f68397e5-ddb5-4084-b7a0-d0206d034870 20402 0 2023-07-29 16:16:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 415ace57-a1c0-42b0-9b4c-4fb335bf9e14 0xc0033a4b87 0xc0033a4b88}] [] [{kube-controller-manager Update v1 2023-07-29 16:16:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"415ace57-a1c0-42b0-9b4c-4fb335bf9e14\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 16:16:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c54mz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c54mz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wa4quivohpee-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:16:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:16:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:16:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:16:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.234,PodIP:,StartTime:2023-07-29 16:16:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 29 16:16:18.292: INFO: Pod "webserver-deployment-69b7448995-tfdfb" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-tfdfb webserver-deployment-69b7448995- deployment-7704  c37bf03d-5739-4ede-af16-ddb18ec6787f 20495 0 2023-07-29 16:16:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 415ace57-a1c0-42b0-9b4c-4fb335bf9e14 0xc0033a4d87 0xc0033a4d88}] [] [{kube-controller-manager Update v1 2023-07-29 16:16:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"415ace57-a1c0-42b0-9b4c-4fb335bf9e14\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wfgwh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wfgwh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wa4quivohpee-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:16:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 29 16:16:18.292: INFO: Pod "webserver-deployment-69b7448995-zfkpw" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-zfkpw webserver-deployment-69b7448995- deployment-7704  aeccf2ac-3f00-4927-b091-3146ee518c9d 20364 0 2023-07-29 16:16:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 415ace57-a1c0-42b0-9b4c-4fb335bf9e14 0xc0033a4f00 0xc0033a4f01}] [] [{kube-controller-manager Update v1 2023-07-29 16:16:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"415ace57-a1c0-42b0-9b4c-4fb335bf9e14\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 16:16:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hd6fm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hd6fm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wa4quivohpee-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:16:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:16:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:16:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:16:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.234,PodIP:,StartTime:2023-07-29 16:16:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 29 16:16:18.295: INFO: Pod "webserver-deployment-845c8977d9-45967" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-45967 webserver-deployment-845c8977d9- deployment-7704  30d16b65-fc85-42ac-8013-36d56ac016e7 20490 0 2023-07-29 16:16:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 96abbba1-e89d-4f29-9448-882a0204331f 0xc0033a50e7 0xc0033a50e8}] [] [{kube-controller-manager Update v1 2023-07-29 16:16:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"96abbba1-e89d-4f29-9448-882a0204331f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fc58x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fc58x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wa4quivohpee-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:16:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 29 16:16:18.300: INFO: Pod "webserver-deployment-845c8977d9-4l4mn" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-4l4mn webserver-deployment-845c8977d9- deployment-7704  10c7fbe1-e0e2-4eb4-8c6e-e5bdd4aabf3b 20046 0 2023-07-29 16:16:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 96abbba1-e89d-4f29-9448-882a0204331f 0xc0033a5250 0xc0033a5251}] [] [{kube-controller-manager Update v1 2023-07-29 16:16:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"96abbba1-e89d-4f29-9448-882a0204331f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 16:16:10 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.144\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4pd6p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4pd6p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wa4quivohpee-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:16:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:16:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:16:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:16:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.206,PodIP:10.233.66.144,StartTime:2023-07-29 16:16:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-29 16:16:10 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://7242c32a7910444c9d77f08c04aa79e9a6824cd4a30478cbb7babd5045b76461,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.144,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 29 16:16:18.300: INFO: Pod "webserver-deployment-845c8977d9-b6pkk" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-b6pkk webserver-deployment-845c8977d9- deployment-7704  94c53b72-b035-442d-ad20-aa3832507de8 20057 0 2023-07-29 16:16:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 96abbba1-e89d-4f29-9448-882a0204331f 0xc0033a5437 0xc0033a5438}] [] [{kube-controller-manager Update v1 2023-07-29 16:16:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"96abbba1-e89d-4f29-9448-882a0204331f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 16:16:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.122\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-45296,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-45296,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wa4quivohpee-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:16:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:16:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:16:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:16:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.206,PodIP:10.233.66.122,StartTime:2023-07-29 16:16:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-29 16:16:10 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://9ce6a6f368634e95a4ae8da371c12309564b97f11d8ac551a94c2b6bfd77d62d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.122,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 29 16:16:18.301: INFO: Pod "webserver-deployment-845c8977d9-dsghk" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-dsghk webserver-deployment-845c8977d9- deployment-7704  5afddcdb-3957-4202-901c-261f6b842a7c 20160 0 2023-07-29 16:16:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 96abbba1-e89d-4f29-9448-882a0204331f 0xc0033a5627 0xc0033a5628}] [] [{kube-controller-manager Update v1 2023-07-29 16:16:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"96abbba1-e89d-4f29-9448-882a0204331f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 16:16:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.64.13\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sgg6b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sgg6b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wa4quivohpee-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:16:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:16:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:16:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:16:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.28,PodIP:10.233.64.13,StartTime:2023-07-29 16:16:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-29 16:16:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://4892c6c47d57d6bc7425bd7400b7bdf1a7c94bf04bcf0dbd6954a353aa278a76,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.64.13,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 29 16:16:18.301: INFO: Pod "webserver-deployment-845c8977d9-f7zkx" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-f7zkx webserver-deployment-845c8977d9- deployment-7704  77372f6d-6dc5-4448-9e6d-7044e425f10a 20061 0 2023-07-29 16:16:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 96abbba1-e89d-4f29-9448-882a0204331f 0xc0033a5817 0xc0033a5818}] [] [{kube-controller-manager Update v1 2023-07-29 16:16:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"96abbba1-e89d-4f29-9448-882a0204331f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 16:16:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.65.33\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-q2lrl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-q2lrl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wa4quivohpee-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:16:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:16:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:16:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:16:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.234,PodIP:10.233.65.33,StartTime:2023-07-29 16:16:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-29 16:16:11 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://8afccd040489efb27592dc7aa2156e1d90a91bc8921682fa548a85d594ec3372,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.65.33,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 29 16:16:18.302: INFO: Pod "webserver-deployment-845c8977d9-f8mx6" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-f8mx6 webserver-deployment-845c8977d9- deployment-7704  1eaad250-965a-4577-93b5-67ea1a6209f9 20049 0 2023-07-29 16:16:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 96abbba1-e89d-4f29-9448-882a0204331f 0xc0033a5a47 0xc0033a5a48}] [] [{kube-controller-manager Update v1 2023-07-29 16:16:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"96abbba1-e89d-4f29-9448-882a0204331f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 16:16:10 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.54\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-72tvb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-72tvb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wa4quivohpee-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:16:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:16:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:16:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:16:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.206,PodIP:10.233.66.54,StartTime:2023-07-29 16:16:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-29 16:16:10 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://700cfe46f4e35e3fb41f868f88eca34a2ba9f4d178829d2b989f2a90e735df35,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.54,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 29 16:16:18.302: INFO: Pod "webserver-deployment-845c8977d9-jr8gs" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-jr8gs webserver-deployment-845c8977d9- deployment-7704  8ff605e3-aec1-4e74-a286-b583ba951182 20064 0 2023-07-29 16:16:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 96abbba1-e89d-4f29-9448-882a0204331f 0xc0033a5c47 0xc0033a5c48}] [] [{kube-controller-manager Update v1 2023-07-29 16:16:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"96abbba1-e89d-4f29-9448-882a0204331f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 16:16:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.65.17\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bnmjp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bnmjp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wa4quivohpee-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:16:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:16:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:16:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:16:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.234,PodIP:10.233.65.17,StartTime:2023-07-29 16:16:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-29 16:16:10 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://233a6adc2026cdfd8aad3213fb63f1be2914d8ef6817d59dc75f39865db1c04a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.65.17,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 29 16:16:18.308: INFO: Pod "webserver-deployment-845c8977d9-rj9wk" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-rj9wk webserver-deployment-845c8977d9- deployment-7704  e8aaa63d-a95d-4096-8557-8c4139dc5b23 20491 0 2023-07-29 16:16:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 96abbba1-e89d-4f29-9448-882a0204331f 0xc0033a5e57 0xc0033a5e58}] [] [{kube-controller-manager Update v1 2023-07-29 16:16:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"96abbba1-e89d-4f29-9448-882a0204331f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rcqw9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rcqw9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 29 16:16:18.308: INFO: Pod "webserver-deployment-845c8977d9-tbvfd" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-tbvfd webserver-deployment-845c8977d9- deployment-7704  7db70abb-64e7-459f-9e1a-8ec7055240f8 20126 0 2023-07-29 16:16:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 96abbba1-e89d-4f29-9448-882a0204331f 0xc0033a5f97 0xc0033a5f98}] [] [{kube-controller-manager Update v1 2023-07-29 16:16:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"96abbba1-e89d-4f29-9448-882a0204331f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 16:16:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.64.17\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dx968,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dx968,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wa4quivohpee-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:16:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:16:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:16:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:16:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.28,PodIP:10.233.64.17,StartTime:2023-07-29 16:16:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-29 16:16:12 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://48f65b301f3851720612b4e2d83c449fabbf8d09d58cbea2abe0fba694a5a950,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.64.17,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 29 16:16:18.308: INFO: Pod "webserver-deployment-845c8977d9-v2k5d" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-v2k5d webserver-deployment-845c8977d9- deployment-7704  6cf16ba6-50bf-4376-a6d4-4d560fb6dee3 20202 0 2023-07-29 16:16:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 96abbba1-e89d-4f29-9448-882a0204331f 0xc003ea6187 0xc003ea6188}] [] [{kube-controller-manager Update v1 2023-07-29 16:16:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"96abbba1-e89d-4f29-9448-882a0204331f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 16:16:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.64.226\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-krml4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-krml4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wa4quivohpee-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:16:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:16:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:16:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:16:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.28,PodIP:10.233.64.226,StartTime:2023-07-29 16:16:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-29 16:16:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://d811d2736a2c553aca0e5469615f5554602c209d2740b7985c62961e2c665dbd,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.64.226,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 29 16:16:18.309: INFO: Pod "webserver-deployment-845c8977d9-w6lrd" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-w6lrd webserver-deployment-845c8977d9- deployment-7704  74e10fe6-9ed5-4483-b201-068d963df868 20486 0 2023-07-29 16:16:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 96abbba1-e89d-4f29-9448-882a0204331f 0xc003ea6377 0xc003ea6378}] [] [{kube-controller-manager Update v1 2023-07-29 16:16:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"96abbba1-e89d-4f29-9448-882a0204331f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vhccq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vhccq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wa4quivohpee-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:16:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jul 29 16:16:18.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7704" for this suite. 07/29/23 16:16:18.323
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","completed":193,"skipped":3596,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.483 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:16:07.868
    Jul 29 16:16:07.868: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename deployment 07/29/23 16:16:07.872
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:16:07.912
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:16:07.919
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support proportional scaling [Conformance]
      test/e2e/apps/deployment.go:160
    Jul 29 16:16:07.932: INFO: Creating deployment "webserver-deployment"
    Jul 29 16:16:07.964: INFO: Waiting for observed generation 1
    Jul 29 16:16:09.988: INFO: Waiting for all required pods to come up
    Jul 29 16:16:10.023: INFO: Pod name httpd: Found 10 pods out of 10
    STEP: ensuring each pod is running 07/29/23 16:16:10.023
    Jul 29 16:16:10.023: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-v2k5d" in namespace "deployment-7704" to be "running"
    Jul 29 16:16:10.024: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-4l4mn" in namespace "deployment-7704" to be "running"
    Jul 29 16:16:10.024: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-6bw72" in namespace "deployment-7704" to be "running"
    Jul 29 16:16:10.025: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-b6pkk" in namespace "deployment-7704" to be "running"
    Jul 29 16:16:10.025: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-dsghk" in namespace "deployment-7704" to be "running"
    Jul 29 16:16:10.025: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-f7zkx" in namespace "deployment-7704" to be "running"
    Jul 29 16:16:10.026: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-f8mx6" in namespace "deployment-7704" to be "running"
    Jul 29 16:16:10.026: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-ghmgr" in namespace "deployment-7704" to be "running"
    Jul 29 16:16:10.026: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-jr8gs" in namespace "deployment-7704" to be "running"
    Jul 29 16:16:10.027: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-tbvfd" in namespace "deployment-7704" to be "running"
    Jul 29 16:16:10.036: INFO: Pod "webserver-deployment-845c8977d9-v2k5d": Phase="Pending", Reason="", readiness=false. Elapsed: 12.622642ms
    Jul 29 16:16:10.036: INFO: Pod "webserver-deployment-845c8977d9-4l4mn": Phase="Pending", Reason="", readiness=false. Elapsed: 12.177404ms
    Jul 29 16:16:10.042: INFO: Pod "webserver-deployment-845c8977d9-b6pkk": Phase="Pending", Reason="", readiness=false. Elapsed: 16.827531ms
    Jul 29 16:16:10.042: INFO: Pod "webserver-deployment-845c8977d9-jr8gs": Phase="Pending", Reason="", readiness=false. Elapsed: 15.309428ms
    Jul 29 16:16:10.046: INFO: Pod "webserver-deployment-845c8977d9-dsghk": Phase="Pending", Reason="", readiness=false. Elapsed: 20.923706ms
    Jul 29 16:16:10.047: INFO: Pod "webserver-deployment-845c8977d9-tbvfd": Phase="Pending", Reason="", readiness=false. Elapsed: 19.973738ms
    Jul 29 16:16:10.047: INFO: Pod "webserver-deployment-845c8977d9-ghmgr": Phase="Pending", Reason="", readiness=false. Elapsed: 21.414746ms
    Jul 29 16:16:10.053: INFO: Pod "webserver-deployment-845c8977d9-6bw72": Phase="Pending", Reason="", readiness=false. Elapsed: 28.219732ms
    Jul 29 16:16:10.071: INFO: Pod "webserver-deployment-845c8977d9-f7zkx": Phase="Pending", Reason="", readiness=false. Elapsed: 45.825955ms
    Jul 29 16:16:10.072: INFO: Pod "webserver-deployment-845c8977d9-f8mx6": Phase="Pending", Reason="", readiness=false. Elapsed: 45.775607ms
    Jul 29 16:16:12.046: INFO: Pod "webserver-deployment-845c8977d9-v2k5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022058797s
    Jul 29 16:16:12.047: INFO: Pod "webserver-deployment-845c8977d9-4l4mn": Phase="Running", Reason="", readiness=true. Elapsed: 2.02322558s
    Jul 29 16:16:12.047: INFO: Pod "webserver-deployment-845c8977d9-4l4mn" satisfied condition "running"
    Jul 29 16:16:12.051: INFO: Pod "webserver-deployment-845c8977d9-b6pkk": Phase="Running", Reason="", readiness=true. Elapsed: 2.026551403s
    Jul 29 16:16:12.051: INFO: Pod "webserver-deployment-845c8977d9-b6pkk" satisfied condition "running"
    Jul 29 16:16:12.053: INFO: Pod "webserver-deployment-845c8977d9-jr8gs": Phase="Running", Reason="", readiness=true. Elapsed: 2.026585966s
    Jul 29 16:16:12.054: INFO: Pod "webserver-deployment-845c8977d9-jr8gs" satisfied condition "running"
    Jul 29 16:16:12.060: INFO: Pod "webserver-deployment-845c8977d9-dsghk": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03499241s
    Jul 29 16:16:12.061: INFO: Pod "webserver-deployment-845c8977d9-tbvfd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033727183s
    Jul 29 16:16:12.063: INFO: Pod "webserver-deployment-845c8977d9-6bw72": Phase="Running", Reason="", readiness=true. Elapsed: 2.038561075s
    Jul 29 16:16:12.063: INFO: Pod "webserver-deployment-845c8977d9-6bw72" satisfied condition "running"
    Jul 29 16:16:12.066: INFO: Pod "webserver-deployment-845c8977d9-ghmgr": Phase="Running", Reason="", readiness=true. Elapsed: 2.039636853s
    Jul 29 16:16:12.066: INFO: Pod "webserver-deployment-845c8977d9-ghmgr" satisfied condition "running"
    Jul 29 16:16:12.087: INFO: Pod "webserver-deployment-845c8977d9-f7zkx": Phase="Running", Reason="", readiness=true. Elapsed: 2.06151291s
    Jul 29 16:16:12.087: INFO: Pod "webserver-deployment-845c8977d9-f7zkx" satisfied condition "running"
    Jul 29 16:16:12.088: INFO: Pod "webserver-deployment-845c8977d9-f8mx6": Phase="Running", Reason="", readiness=true. Elapsed: 2.061866743s
    Jul 29 16:16:12.088: INFO: Pod "webserver-deployment-845c8977d9-f8mx6" satisfied condition "running"
    Jul 29 16:16:14.044: INFO: Pod "webserver-deployment-845c8977d9-v2k5d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020942634s
    Jul 29 16:16:14.052: INFO: Pod "webserver-deployment-845c8977d9-dsghk": Phase="Pending", Reason="", readiness=false. Elapsed: 4.02654114s
    Jul 29 16:16:14.055: INFO: Pod "webserver-deployment-845c8977d9-tbvfd": Phase="Running", Reason="", readiness=true. Elapsed: 4.028132861s
    Jul 29 16:16:14.055: INFO: Pod "webserver-deployment-845c8977d9-tbvfd" satisfied condition "running"
    Jul 29 16:16:16.045: INFO: Pod "webserver-deployment-845c8977d9-v2k5d": Phase="Running", Reason="", readiness=true. Elapsed: 6.021335976s
    Jul 29 16:16:16.045: INFO: Pod "webserver-deployment-845c8977d9-v2k5d" satisfied condition "running"
    Jul 29 16:16:16.051: INFO: Pod "webserver-deployment-845c8977d9-dsghk": Phase="Running", Reason="", readiness=true. Elapsed: 6.02614028s
    Jul 29 16:16:16.051: INFO: Pod "webserver-deployment-845c8977d9-dsghk" satisfied condition "running"
    Jul 29 16:16:16.052: INFO: Waiting for deployment "webserver-deployment" to complete
    Jul 29 16:16:16.066: INFO: Updating deployment "webserver-deployment" with a non-existent image
    Jul 29 16:16:16.084: INFO: Updating deployment webserver-deployment
    Jul 29 16:16:16.084: INFO: Waiting for observed generation 2
    Jul 29 16:16:18.096: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
    Jul 29 16:16:18.103: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
    Jul 29 16:16:18.108: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Jul 29 16:16:18.127: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
    Jul 29 16:16:18.127: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
    Jul 29 16:16:18.134: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Jul 29 16:16:18.148: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
    Jul 29 16:16:18.148: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
    Jul 29 16:16:18.166: INFO: Updating deployment webserver-deployment
    Jul 29 16:16:18.166: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
    Jul 29 16:16:18.202: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
    Jul 29 16:16:18.210: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jul 29 16:16:18.233: INFO: Deployment "webserver-deployment":
    &Deployment{ObjectMeta:{webserver-deployment  deployment-7704  2c9ced0f-bead-4558-97ab-c777ab8f4508 20479 3 2023-07-29 16:16:07 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{kube-controller-manager Update apps/v1 2023-07-29 16:16:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status} {e2e.test Update apps/v1 2023-07-29 16:16:18 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003295fb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-07-29 16:16:13 +0000 UTC,LastTransitionTime:2023-07-29 16:16:13 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-69b7448995" is progressing.,LastUpdateTime:2023-07-29 16:16:16 +0000 UTC,LastTransitionTime:2023-07-29 16:16:07 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

    Jul 29 16:16:18.252: INFO: New ReplicaSet "webserver-deployment-69b7448995" of Deployment "webserver-deployment":
    &ReplicaSet{ObjectMeta:{webserver-deployment-69b7448995  deployment-7704  415ace57-a1c0-42b0-9b4c-4fb335bf9e14 20483 3 2023-07-29 16:16:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 2c9ced0f-bead-4558-97ab-c777ab8f4508 0xc00311c9e7 0xc00311c9e8}] [] [{kube-controller-manager Update apps/v1 2023-07-29 16:16:16 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-07-29 16:16:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2c9ced0f-bead-4558-97ab-c777ab8f4508\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 69b7448995,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00311d1b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jul 29 16:16:18.265: INFO: All old ReplicaSets of Deployment "webserver-deployment":
    Jul 29 16:16:18.266: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-845c8977d9  deployment-7704  96abbba1-e89d-4f29-9448-882a0204331f 20480 3 2023-07-29 16:16:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 2c9ced0f-bead-4558-97ab-c777ab8f4508 0xc00311d217 0xc00311d218}] [] [{kube-controller-manager Update apps/v1 2023-07-29 16:16:16 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-07-29 16:16:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2c9ced0f-bead-4558-97ab-c777ab8f4508\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00311d2a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
    Jul 29 16:16:18.283: INFO: Pod "webserver-deployment-69b7448995-2b49k" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-2b49k webserver-deployment-69b7448995- deployment-7704  81108fe6-a3df-4115-95ae-84db39634d99 20405 0 2023-07-29 16:16:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 415ace57-a1c0-42b0-9b4c-4fb335bf9e14 0xc0033a43f7 0xc0033a43f8}] [] [{kube-controller-manager Update v1 2023-07-29 16:16:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"415ace57-a1c0-42b0-9b4c-4fb335bf9e14\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 16:16:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-49wnh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-49wnh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wa4quivohpee-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:16:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:16:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:16:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:16:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.28,PodIP:,StartTime:2023-07-29 16:16:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jul 29 16:16:18.287: INFO: Pod "webserver-deployment-69b7448995-56687" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-56687 webserver-deployment-69b7448995- deployment-7704  cb02abd9-000b-4e34-8eb7-36eb9e45249e 20368 0 2023-07-29 16:16:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 415ace57-a1c0-42b0-9b4c-4fb335bf9e14 0xc0033a4607 0xc0033a4608}] [] [{kube-controller-manager Update v1 2023-07-29 16:16:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"415ace57-a1c0-42b0-9b4c-4fb335bf9e14\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 16:16:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zrchf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zrchf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wa4quivohpee-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:16:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:16:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:16:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:16:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.206,PodIP:,StartTime:2023-07-29 16:16:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jul 29 16:16:18.288: INFO: Pod "webserver-deployment-69b7448995-9xjps" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-9xjps webserver-deployment-69b7448995- deployment-7704  4d5987f5-9641-44f3-b39a-d7400da8886e 20372 0 2023-07-29 16:16:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 415ace57-a1c0-42b0-9b4c-4fb335bf9e14 0xc0033a4817 0xc0033a4818}] [] [{kube-controller-manager Update v1 2023-07-29 16:16:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"415ace57-a1c0-42b0-9b4c-4fb335bf9e14\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 16:16:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-55hq4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-55hq4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wa4quivohpee-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:16:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:16:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:16:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:16:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.28,PodIP:,StartTime:2023-07-29 16:16:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jul 29 16:16:18.289: INFO: Pod "webserver-deployment-69b7448995-k4nxw" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-k4nxw webserver-deployment-69b7448995- deployment-7704  4140c8aa-012a-4eeb-a691-8e058762c15c 20496 0 2023-07-29 16:16:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 415ace57-a1c0-42b0-9b4c-4fb335bf9e14 0xc0033a4a37 0xc0033a4a38}] [] [{kube-controller-manager Update v1 2023-07-29 16:16:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"415ace57-a1c0-42b0-9b4c-4fb335bf9e14\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fk6zv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fk6zv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jul 29 16:16:18.289: INFO: Pod "webserver-deployment-69b7448995-qj2ct" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-qj2ct webserver-deployment-69b7448995- deployment-7704  f68397e5-ddb5-4084-b7a0-d0206d034870 20402 0 2023-07-29 16:16:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 415ace57-a1c0-42b0-9b4c-4fb335bf9e14 0xc0033a4b87 0xc0033a4b88}] [] [{kube-controller-manager Update v1 2023-07-29 16:16:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"415ace57-a1c0-42b0-9b4c-4fb335bf9e14\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 16:16:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c54mz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c54mz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wa4quivohpee-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:16:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:16:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:16:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:16:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.234,PodIP:,StartTime:2023-07-29 16:16:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jul 29 16:16:18.292: INFO: Pod "webserver-deployment-69b7448995-tfdfb" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-tfdfb webserver-deployment-69b7448995- deployment-7704  c37bf03d-5739-4ede-af16-ddb18ec6787f 20495 0 2023-07-29 16:16:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 415ace57-a1c0-42b0-9b4c-4fb335bf9e14 0xc0033a4d87 0xc0033a4d88}] [] [{kube-controller-manager Update v1 2023-07-29 16:16:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"415ace57-a1c0-42b0-9b4c-4fb335bf9e14\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wfgwh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wfgwh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wa4quivohpee-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:16:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jul 29 16:16:18.292: INFO: Pod "webserver-deployment-69b7448995-zfkpw" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-zfkpw webserver-deployment-69b7448995- deployment-7704  aeccf2ac-3f00-4927-b091-3146ee518c9d 20364 0 2023-07-29 16:16:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 415ace57-a1c0-42b0-9b4c-4fb335bf9e14 0xc0033a4f00 0xc0033a4f01}] [] [{kube-controller-manager Update v1 2023-07-29 16:16:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"415ace57-a1c0-42b0-9b4c-4fb335bf9e14\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 16:16:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hd6fm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hd6fm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wa4quivohpee-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:16:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:16:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:16:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:16:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.234,PodIP:,StartTime:2023-07-29 16:16:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jul 29 16:16:18.295: INFO: Pod "webserver-deployment-845c8977d9-45967" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-45967 webserver-deployment-845c8977d9- deployment-7704  30d16b65-fc85-42ac-8013-36d56ac016e7 20490 0 2023-07-29 16:16:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 96abbba1-e89d-4f29-9448-882a0204331f 0xc0033a50e7 0xc0033a50e8}] [] [{kube-controller-manager Update v1 2023-07-29 16:16:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"96abbba1-e89d-4f29-9448-882a0204331f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fc58x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fc58x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wa4quivohpee-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:16:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jul 29 16:16:18.300: INFO: Pod "webserver-deployment-845c8977d9-4l4mn" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-4l4mn webserver-deployment-845c8977d9- deployment-7704  10c7fbe1-e0e2-4eb4-8c6e-e5bdd4aabf3b 20046 0 2023-07-29 16:16:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 96abbba1-e89d-4f29-9448-882a0204331f 0xc0033a5250 0xc0033a5251}] [] [{kube-controller-manager Update v1 2023-07-29 16:16:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"96abbba1-e89d-4f29-9448-882a0204331f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 16:16:10 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.144\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4pd6p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4pd6p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wa4quivohpee-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:16:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:16:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:16:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:16:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.206,PodIP:10.233.66.144,StartTime:2023-07-29 16:16:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-29 16:16:10 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://7242c32a7910444c9d77f08c04aa79e9a6824cd4a30478cbb7babd5045b76461,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.144,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jul 29 16:16:18.300: INFO: Pod "webserver-deployment-845c8977d9-b6pkk" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-b6pkk webserver-deployment-845c8977d9- deployment-7704  94c53b72-b035-442d-ad20-aa3832507de8 20057 0 2023-07-29 16:16:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 96abbba1-e89d-4f29-9448-882a0204331f 0xc0033a5437 0xc0033a5438}] [] [{kube-controller-manager Update v1 2023-07-29 16:16:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"96abbba1-e89d-4f29-9448-882a0204331f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 16:16:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.122\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-45296,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-45296,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wa4quivohpee-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:16:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:16:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:16:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:16:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.206,PodIP:10.233.66.122,StartTime:2023-07-29 16:16:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-29 16:16:10 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://9ce6a6f368634e95a4ae8da371c12309564b97f11d8ac551a94c2b6bfd77d62d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.122,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jul 29 16:16:18.301: INFO: Pod "webserver-deployment-845c8977d9-dsghk" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-dsghk webserver-deployment-845c8977d9- deployment-7704  5afddcdb-3957-4202-901c-261f6b842a7c 20160 0 2023-07-29 16:16:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 96abbba1-e89d-4f29-9448-882a0204331f 0xc0033a5627 0xc0033a5628}] [] [{kube-controller-manager Update v1 2023-07-29 16:16:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"96abbba1-e89d-4f29-9448-882a0204331f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 16:16:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.64.13\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sgg6b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sgg6b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wa4quivohpee-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:16:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:16:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:16:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:16:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.28,PodIP:10.233.64.13,StartTime:2023-07-29 16:16:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-29 16:16:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://4892c6c47d57d6bc7425bd7400b7bdf1a7c94bf04bcf0dbd6954a353aa278a76,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.64.13,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jul 29 16:16:18.301: INFO: Pod "webserver-deployment-845c8977d9-f7zkx" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-f7zkx webserver-deployment-845c8977d9- deployment-7704  77372f6d-6dc5-4448-9e6d-7044e425f10a 20061 0 2023-07-29 16:16:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 96abbba1-e89d-4f29-9448-882a0204331f 0xc0033a5817 0xc0033a5818}] [] [{kube-controller-manager Update v1 2023-07-29 16:16:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"96abbba1-e89d-4f29-9448-882a0204331f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 16:16:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.65.33\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-q2lrl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-q2lrl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wa4quivohpee-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:16:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:16:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:16:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:16:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.234,PodIP:10.233.65.33,StartTime:2023-07-29 16:16:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-29 16:16:11 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://8afccd040489efb27592dc7aa2156e1d90a91bc8921682fa548a85d594ec3372,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.65.33,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jul 29 16:16:18.302: INFO: Pod "webserver-deployment-845c8977d9-f8mx6" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-f8mx6 webserver-deployment-845c8977d9- deployment-7704  1eaad250-965a-4577-93b5-67ea1a6209f9 20049 0 2023-07-29 16:16:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 96abbba1-e89d-4f29-9448-882a0204331f 0xc0033a5a47 0xc0033a5a48}] [] [{kube-controller-manager Update v1 2023-07-29 16:16:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"96abbba1-e89d-4f29-9448-882a0204331f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 16:16:10 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.54\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-72tvb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-72tvb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wa4quivohpee-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:16:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:16:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:16:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:16:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.206,PodIP:10.233.66.54,StartTime:2023-07-29 16:16:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-29 16:16:10 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://700cfe46f4e35e3fb41f868f88eca34a2ba9f4d178829d2b989f2a90e735df35,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.54,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jul 29 16:16:18.302: INFO: Pod "webserver-deployment-845c8977d9-jr8gs" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-jr8gs webserver-deployment-845c8977d9- deployment-7704  8ff605e3-aec1-4e74-a286-b583ba951182 20064 0 2023-07-29 16:16:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 96abbba1-e89d-4f29-9448-882a0204331f 0xc0033a5c47 0xc0033a5c48}] [] [{kube-controller-manager Update v1 2023-07-29 16:16:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"96abbba1-e89d-4f29-9448-882a0204331f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 16:16:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.65.17\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bnmjp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bnmjp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wa4quivohpee-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:16:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:16:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:16:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:16:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.234,PodIP:10.233.65.17,StartTime:2023-07-29 16:16:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-29 16:16:10 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://233a6adc2026cdfd8aad3213fb63f1be2914d8ef6817d59dc75f39865db1c04a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.65.17,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jul 29 16:16:18.308: INFO: Pod "webserver-deployment-845c8977d9-rj9wk" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-rj9wk webserver-deployment-845c8977d9- deployment-7704  e8aaa63d-a95d-4096-8557-8c4139dc5b23 20491 0 2023-07-29 16:16:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 96abbba1-e89d-4f29-9448-882a0204331f 0xc0033a5e57 0xc0033a5e58}] [] [{kube-controller-manager Update v1 2023-07-29 16:16:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"96abbba1-e89d-4f29-9448-882a0204331f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rcqw9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rcqw9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jul 29 16:16:18.308: INFO: Pod "webserver-deployment-845c8977d9-tbvfd" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-tbvfd webserver-deployment-845c8977d9- deployment-7704  7db70abb-64e7-459f-9e1a-8ec7055240f8 20126 0 2023-07-29 16:16:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 96abbba1-e89d-4f29-9448-882a0204331f 0xc0033a5f97 0xc0033a5f98}] [] [{kube-controller-manager Update v1 2023-07-29 16:16:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"96abbba1-e89d-4f29-9448-882a0204331f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 16:16:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.64.17\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dx968,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dx968,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wa4quivohpee-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:16:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:16:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:16:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:16:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.28,PodIP:10.233.64.17,StartTime:2023-07-29 16:16:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-29 16:16:12 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://48f65b301f3851720612b4e2d83c449fabbf8d09d58cbea2abe0fba694a5a950,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.64.17,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jul 29 16:16:18.308: INFO: Pod "webserver-deployment-845c8977d9-v2k5d" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-v2k5d webserver-deployment-845c8977d9- deployment-7704  6cf16ba6-50bf-4376-a6d4-4d560fb6dee3 20202 0 2023-07-29 16:16:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 96abbba1-e89d-4f29-9448-882a0204331f 0xc003ea6187 0xc003ea6188}] [] [{kube-controller-manager Update v1 2023-07-29 16:16:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"96abbba1-e89d-4f29-9448-882a0204331f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 16:16:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.64.226\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-krml4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-krml4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wa4quivohpee-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:16:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:16:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:16:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:16:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.28,PodIP:10.233.64.226,StartTime:2023-07-29 16:16:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-29 16:16:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://d811d2736a2c553aca0e5469615f5554602c209d2740b7985c62961e2c665dbd,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.64.226,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jul 29 16:16:18.309: INFO: Pod "webserver-deployment-845c8977d9-w6lrd" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-w6lrd webserver-deployment-845c8977d9- deployment-7704  74e10fe6-9ed5-4483-b201-068d963df868 20486 0 2023-07-29 16:16:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 96abbba1-e89d-4f29-9448-882a0204331f 0xc003ea6377 0xc003ea6378}] [] [{kube-controller-manager Update v1 2023-07-29 16:16:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"96abbba1-e89d-4f29-9448-882a0204331f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vhccq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vhccq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wa4quivohpee-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:16:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jul 29 16:16:18.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-7704" for this suite. 07/29/23 16:16:18.323
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:16:18.354
Jul 29 16:16:18.354: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename sysctl 07/29/23 16:16:18.357
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:16:18.657
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:16:18.661
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 07/29/23 16:16:18.666
STEP: Watching for error events or started pod 07/29/23 16:16:18.688
STEP: Waiting for pod completion 07/29/23 16:16:22.699
Jul 29 16:16:22.699: INFO: Waiting up to 3m0s for pod "sysctl-0a765a84-8ac3-40c1-9581-2286dd6d5e34" in namespace "sysctl-5604" to be "completed"
Jul 29 16:16:22.705: INFO: Pod "sysctl-0a765a84-8ac3-40c1-9581-2286dd6d5e34": Phase="Pending", Reason="", readiness=false. Elapsed: 5.498867ms
Jul 29 16:16:24.729: INFO: Pod "sysctl-0a765a84-8ac3-40c1-9581-2286dd6d5e34": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.029479904s
Jul 29 16:16:24.729: INFO: Pod "sysctl-0a765a84-8ac3-40c1-9581-2286dd6d5e34" satisfied condition "completed"
STEP: Checking that the pod succeeded 07/29/23 16:16:24.761
STEP: Getting logs from the pod 07/29/23 16:16:24.761
STEP: Checking that the sysctl is actually updated 07/29/23 16:16:24.813
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:187
Jul 29 16:16:24.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-5604" for this suite. 07/29/23 16:16:24.872
{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]","completed":194,"skipped":3603,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.617 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:16:18.354
    Jul 29 16:16:18.354: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename sysctl 07/29/23 16:16:18.357
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:16:18.657
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:16:18.661
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:77
    STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 07/29/23 16:16:18.666
    STEP: Watching for error events or started pod 07/29/23 16:16:18.688
    STEP: Waiting for pod completion 07/29/23 16:16:22.699
    Jul 29 16:16:22.699: INFO: Waiting up to 3m0s for pod "sysctl-0a765a84-8ac3-40c1-9581-2286dd6d5e34" in namespace "sysctl-5604" to be "completed"
    Jul 29 16:16:22.705: INFO: Pod "sysctl-0a765a84-8ac3-40c1-9581-2286dd6d5e34": Phase="Pending", Reason="", readiness=false. Elapsed: 5.498867ms
    Jul 29 16:16:24.729: INFO: Pod "sysctl-0a765a84-8ac3-40c1-9581-2286dd6d5e34": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.029479904s
    Jul 29 16:16:24.729: INFO: Pod "sysctl-0a765a84-8ac3-40c1-9581-2286dd6d5e34" satisfied condition "completed"
    STEP: Checking that the pod succeeded 07/29/23 16:16:24.761
    STEP: Getting logs from the pod 07/29/23 16:16:24.761
    STEP: Checking that the sysctl is actually updated 07/29/23 16:16:24.813
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:187
    Jul 29 16:16:24.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sysctl-5604" for this suite. 07/29/23 16:16:24.872
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Probing container
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:16:24.975
Jul 29 16:16:24.975: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename container-probe 07/29/23 16:16:24.979
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:16:25.123
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:16:25.127
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195
STEP: Creating pod liveness-00f34114-a254-4d40-992b-10b72688f2ac in namespace container-probe-9895 07/29/23 16:16:25.134
Jul 29 16:16:25.247: INFO: Waiting up to 5m0s for pod "liveness-00f34114-a254-4d40-992b-10b72688f2ac" in namespace "container-probe-9895" to be "not pending"
Jul 29 16:16:25.275: INFO: Pod "liveness-00f34114-a254-4d40-992b-10b72688f2ac": Phase="Pending", Reason="", readiness=false. Elapsed: 28.210337ms
Jul 29 16:16:27.282: INFO: Pod "liveness-00f34114-a254-4d40-992b-10b72688f2ac": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035013511s
Jul 29 16:16:29.282: INFO: Pod "liveness-00f34114-a254-4d40-992b-10b72688f2ac": Phase="Running", Reason="", readiness=true. Elapsed: 4.034941094s
Jul 29 16:16:29.282: INFO: Pod "liveness-00f34114-a254-4d40-992b-10b72688f2ac" satisfied condition "not pending"
Jul 29 16:16:29.282: INFO: Started pod liveness-00f34114-a254-4d40-992b-10b72688f2ac in namespace container-probe-9895
STEP: checking the pod's current state and verifying that restartCount is present 07/29/23 16:16:29.282
Jul 29 16:16:29.288: INFO: Initial restart count of pod liveness-00f34114-a254-4d40-992b-10b72688f2ac is 0
Jul 29 16:16:47.389: INFO: Restart count of pod container-probe-9895/liveness-00f34114-a254-4d40-992b-10b72688f2ac is now 1 (18.10074748s elapsed)
Jul 29 16:17:07.482: INFO: Restart count of pod container-probe-9895/liveness-00f34114-a254-4d40-992b-10b72688f2ac is now 2 (38.194522767s elapsed)
Jul 29 16:17:27.563: INFO: Restart count of pod container-probe-9895/liveness-00f34114-a254-4d40-992b-10b72688f2ac is now 3 (58.275591133s elapsed)
Jul 29 16:17:47.643: INFO: Restart count of pod container-probe-9895/liveness-00f34114-a254-4d40-992b-10b72688f2ac is now 4 (1m18.355075396s elapsed)
Jul 29 16:18:49.929: INFO: Restart count of pod container-probe-9895/liveness-00f34114-a254-4d40-992b-10b72688f2ac is now 5 (2m20.641146993s elapsed)
STEP: deleting the pod 07/29/23 16:18:49.929
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jul 29 16:18:49.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9895" for this suite. 07/29/23 16:18:49.968
{"msg":"PASSED [sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","completed":195,"skipped":3607,"failed":0}
------------------------------
â€¢ [SLOW TEST] [145.006 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:16:24.975
    Jul 29 16:16:24.975: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename container-probe 07/29/23 16:16:24.979
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:16:25.123
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:16:25.127
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should have monotonically increasing restart count [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:195
    STEP: Creating pod liveness-00f34114-a254-4d40-992b-10b72688f2ac in namespace container-probe-9895 07/29/23 16:16:25.134
    Jul 29 16:16:25.247: INFO: Waiting up to 5m0s for pod "liveness-00f34114-a254-4d40-992b-10b72688f2ac" in namespace "container-probe-9895" to be "not pending"
    Jul 29 16:16:25.275: INFO: Pod "liveness-00f34114-a254-4d40-992b-10b72688f2ac": Phase="Pending", Reason="", readiness=false. Elapsed: 28.210337ms
    Jul 29 16:16:27.282: INFO: Pod "liveness-00f34114-a254-4d40-992b-10b72688f2ac": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035013511s
    Jul 29 16:16:29.282: INFO: Pod "liveness-00f34114-a254-4d40-992b-10b72688f2ac": Phase="Running", Reason="", readiness=true. Elapsed: 4.034941094s
    Jul 29 16:16:29.282: INFO: Pod "liveness-00f34114-a254-4d40-992b-10b72688f2ac" satisfied condition "not pending"
    Jul 29 16:16:29.282: INFO: Started pod liveness-00f34114-a254-4d40-992b-10b72688f2ac in namespace container-probe-9895
    STEP: checking the pod's current state and verifying that restartCount is present 07/29/23 16:16:29.282
    Jul 29 16:16:29.288: INFO: Initial restart count of pod liveness-00f34114-a254-4d40-992b-10b72688f2ac is 0
    Jul 29 16:16:47.389: INFO: Restart count of pod container-probe-9895/liveness-00f34114-a254-4d40-992b-10b72688f2ac is now 1 (18.10074748s elapsed)
    Jul 29 16:17:07.482: INFO: Restart count of pod container-probe-9895/liveness-00f34114-a254-4d40-992b-10b72688f2ac is now 2 (38.194522767s elapsed)
    Jul 29 16:17:27.563: INFO: Restart count of pod container-probe-9895/liveness-00f34114-a254-4d40-992b-10b72688f2ac is now 3 (58.275591133s elapsed)
    Jul 29 16:17:47.643: INFO: Restart count of pod container-probe-9895/liveness-00f34114-a254-4d40-992b-10b72688f2ac is now 4 (1m18.355075396s elapsed)
    Jul 29 16:18:49.929: INFO: Restart count of pod container-probe-9895/liveness-00f34114-a254-4d40-992b-10b72688f2ac is now 5 (2m20.641146993s elapsed)
    STEP: deleting the pod 07/29/23 16:18:49.929
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jul 29 16:18:49.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-9895" for this suite. 07/29/23 16:18:49.968
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1274
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:18:49.987
Jul 29 16:18:49.988: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename kubectl 07/29/23 16:18:50.015
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:18:50.045
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:18:50.049
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1274
Jul 29 16:18:50.056: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-2536 create -f -'
Jul 29 16:18:50.807: INFO: stderr: ""
Jul 29 16:18:50.807: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Jul 29 16:18:50.808: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-2536 create -f -'
Jul 29 16:18:52.652: INFO: stderr: ""
Jul 29 16:18:52.653: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 07/29/23 16:18:52.653
Jul 29 16:18:53.662: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 29 16:18:53.662: INFO: Found 1 / 1
Jul 29 16:18:53.662: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jul 29 16:18:53.670: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 29 16:18:53.670: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jul 29 16:18:53.670: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-2536 describe pod agnhost-primary-drxzm'
Jul 29 16:18:53.832: INFO: stderr: ""
Jul 29 16:18:53.832: INFO: stdout: "Name:             agnhost-primary-drxzm\nNamespace:        kubectl-2536\nPriority:         0\nService Account:  default\nNode:             wa4quivohpee-3/192.168.121.234\nStart Time:       Sat, 29 Jul 2023 16:18:50 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      <none>\nStatus:           Running\nIP:               10.233.65.30\nIPs:\n  IP:           10.233.65.30\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   cri-o://fbd8d17ab870ed513cff6ef8960bced476832cf17455dd18a20504b7888966f6\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.40\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:37dd9ad84b34913b4bc57e00b0a28e8b00f18ca6a5ecec69461aa57402e5c787\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Sat, 29 Jul 2023 16:18:51 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-wfngs (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-wfngs:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  3s    default-scheduler  Successfully assigned kubectl-2536/agnhost-primary-drxzm to wa4quivohpee-3\n  Normal  Pulled     2s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.40\" already present on machine\n  Normal  Created    2s    kubelet            Created container agnhost-primary\n  Normal  Started    2s    kubelet            Started container agnhost-primary\n"
Jul 29 16:18:53.833: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-2536 describe rc agnhost-primary'
Jul 29 16:18:53.987: INFO: stderr: ""
Jul 29 16:18:53.987: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-2536\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.40\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: agnhost-primary-drxzm\n"
Jul 29 16:18:53.988: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-2536 describe service agnhost-primary'
Jul 29 16:18:54.127: INFO: stderr: ""
Jul 29 16:18:54.127: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-2536\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.233.7.251\nIPs:               10.233.7.251\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.233.65.30:6379\nSession Affinity:  None\nEvents:            <none>\n"
Jul 29 16:18:54.136: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-2536 describe node wa4quivohpee-1'
Jul 29 16:18:54.319: INFO: stderr: ""
Jul 29 16:18:54.319: INFO: stdout: "Name:               wa4quivohpee-1\nRoles:              control-plane\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=wa4quivohpee-1\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node.kubernetes.io/exclude-from-external-load-balancers=\nAnnotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/crio/crio.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Sat, 29 Jul 2023 15:14:21 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  wa4quivohpee-1\n  AcquireTime:     <unset>\n  RenewTime:       Sat, 29 Jul 2023 16:18:53 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Sat, 29 Jul 2023 15:24:30 +0000   Sat, 29 Jul 2023 15:24:30 +0000   CiliumIsUp                   Cilium is running on this node\n  MemoryPressure       False   Sat, 29 Jul 2023 16:15:28 +0000   Sat, 29 Jul 2023 15:14:21 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Sat, 29 Jul 2023 16:15:28 +0000   Sat, 29 Jul 2023 15:14:21 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Sat, 29 Jul 2023 16:15:28 +0000   Sat, 29 Jul 2023 15:14:21 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Sat, 29 Jul 2023 16:15:28 +0000   Sat, 29 Jul 2023 15:15:49 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  192.168.121.28\n  Hostname:    wa4quivohpee-1\nCapacity:\n  cpu:                2\n  ephemeral-storage:  115008636Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             8127912Ki\n  pods:               110\nAllocatable:\n  cpu:                1600m\n  ephemeral-storage:  111880401014\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             3278248Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 46501bf33a244b06bbf1ca29267b7fcc\n  System UUID:                46501bf3-3a24-4b06-bbf1-ca29267b7fcc\n  Boot ID:                    25bd5ed4-e438-4d68-8dac-6771fd7cb68f\n  Kernel Version:             5.19.0-50-generic\n  OS Image:                   Ubuntu 22.04.2 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  cri-o://1.25.3\n  Kubelet Version:            v1.25.12\n  Kube-Proxy Version:         v1.25.12\nPodCIDR:                      10.233.64.0/24\nPodCIDRs:                     10.233.64.0/24\nNon-terminated Pods:          (9 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 cilium-gfkpl                                               100m (6%)     0 (0%)      100Mi (3%)       0 (0%)         55m\n  kube-system                 cilium-node-init-n8876                                     100m (6%)     0 (0%)      100Mi (3%)       0 (0%)         55m\n  kube-system                 coredns-565d847f94-xm9ff                                   100m (6%)     0 (0%)      70Mi (2%)        170Mi (5%)     44m\n  kube-system                 kube-addon-manager-wa4quivohpee-1                          5m (0%)       0 (0%)      50Mi (1%)        0 (0%)         55m\n  kube-system                 kube-apiserver-wa4quivohpee-1                              250m (15%)    0 (0%)      0 (0%)           0 (0%)         64m\n  kube-system                 kube-controller-manager-wa4quivohpee-1                     200m (12%)    0 (0%)      0 (0%)           0 (0%)         64m\n  kube-system                 kube-proxy-gdffl                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         64m\n  kube-system                 kube-scheduler-wa4quivohpee-1                              100m (6%)     0 (0%)      0 (0%)           0 (0%)         64m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-b6f0dae2c3174900-mtck5    0 (0%)        0 (0%)      0 (0%)           0 (0%)         54m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                855m (53%)  0 (0%)\n  memory             320Mi (9%)  170Mi (5%)\n  ephemeral-storage  0 (0%)      0 (0%)\n  hugepages-1Gi      0 (0%)      0 (0%)\n  hugepages-2Mi      0 (0%)      0 (0%)\nEvents:              <none>\n"
Jul 29 16:18:54.320: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-2536 describe namespace kubectl-2536'
Jul 29 16:18:54.492: INFO: stderr: ""
Jul 29 16:18:54.492: INFO: stdout: "Name:         kubectl-2536\nLabels:       e2e-framework=kubectl\n              e2e-run=fdedc122-77f5-414b-b179-f28fd2dcbd5c\n              kubernetes.io/metadata.name=kubectl-2536\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jul 29 16:18:54.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2536" for this suite. 07/29/23 16:18:54.501
{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","completed":196,"skipped":3614,"failed":0}
------------------------------
â€¢ [4.526 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl describe
  test/e2e/kubectl/kubectl.go:1268
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    test/e2e/kubectl/kubectl.go:1274

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:18:49.987
    Jul 29 16:18:49.988: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename kubectl 07/29/23 16:18:50.015
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:18:50.045
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:18:50.049
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
      test/e2e/kubectl/kubectl.go:1274
    Jul 29 16:18:50.056: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-2536 create -f -'
    Jul 29 16:18:50.807: INFO: stderr: ""
    Jul 29 16:18:50.807: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    Jul 29 16:18:50.808: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-2536 create -f -'
    Jul 29 16:18:52.652: INFO: stderr: ""
    Jul 29 16:18:52.653: INFO: stdout: "service/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 07/29/23 16:18:52.653
    Jul 29 16:18:53.662: INFO: Selector matched 1 pods for map[app:agnhost]
    Jul 29 16:18:53.662: INFO: Found 1 / 1
    Jul 29 16:18:53.662: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Jul 29 16:18:53.670: INFO: Selector matched 1 pods for map[app:agnhost]
    Jul 29 16:18:53.670: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Jul 29 16:18:53.670: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-2536 describe pod agnhost-primary-drxzm'
    Jul 29 16:18:53.832: INFO: stderr: ""
    Jul 29 16:18:53.832: INFO: stdout: "Name:             agnhost-primary-drxzm\nNamespace:        kubectl-2536\nPriority:         0\nService Account:  default\nNode:             wa4quivohpee-3/192.168.121.234\nStart Time:       Sat, 29 Jul 2023 16:18:50 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      <none>\nStatus:           Running\nIP:               10.233.65.30\nIPs:\n  IP:           10.233.65.30\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   cri-o://fbd8d17ab870ed513cff6ef8960bced476832cf17455dd18a20504b7888966f6\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.40\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:37dd9ad84b34913b4bc57e00b0a28e8b00f18ca6a5ecec69461aa57402e5c787\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Sat, 29 Jul 2023 16:18:51 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-wfngs (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-wfngs:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  3s    default-scheduler  Successfully assigned kubectl-2536/agnhost-primary-drxzm to wa4quivohpee-3\n  Normal  Pulled     2s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.40\" already present on machine\n  Normal  Created    2s    kubelet            Created container agnhost-primary\n  Normal  Started    2s    kubelet            Started container agnhost-primary\n"
    Jul 29 16:18:53.833: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-2536 describe rc agnhost-primary'
    Jul 29 16:18:53.987: INFO: stderr: ""
    Jul 29 16:18:53.987: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-2536\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.40\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: agnhost-primary-drxzm\n"
    Jul 29 16:18:53.988: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-2536 describe service agnhost-primary'
    Jul 29 16:18:54.127: INFO: stderr: ""
    Jul 29 16:18:54.127: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-2536\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.233.7.251\nIPs:               10.233.7.251\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.233.65.30:6379\nSession Affinity:  None\nEvents:            <none>\n"
    Jul 29 16:18:54.136: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-2536 describe node wa4quivohpee-1'
    Jul 29 16:18:54.319: INFO: stderr: ""
    Jul 29 16:18:54.319: INFO: stdout: "Name:               wa4quivohpee-1\nRoles:              control-plane\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=wa4quivohpee-1\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node.kubernetes.io/exclude-from-external-load-balancers=\nAnnotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/crio/crio.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Sat, 29 Jul 2023 15:14:21 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  wa4quivohpee-1\n  AcquireTime:     <unset>\n  RenewTime:       Sat, 29 Jul 2023 16:18:53 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Sat, 29 Jul 2023 15:24:30 +0000   Sat, 29 Jul 2023 15:24:30 +0000   CiliumIsUp                   Cilium is running on this node\n  MemoryPressure       False   Sat, 29 Jul 2023 16:15:28 +0000   Sat, 29 Jul 2023 15:14:21 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Sat, 29 Jul 2023 16:15:28 +0000   Sat, 29 Jul 2023 15:14:21 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Sat, 29 Jul 2023 16:15:28 +0000   Sat, 29 Jul 2023 15:14:21 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Sat, 29 Jul 2023 16:15:28 +0000   Sat, 29 Jul 2023 15:15:49 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  192.168.121.28\n  Hostname:    wa4quivohpee-1\nCapacity:\n  cpu:                2\n  ephemeral-storage:  115008636Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             8127912Ki\n  pods:               110\nAllocatable:\n  cpu:                1600m\n  ephemeral-storage:  111880401014\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             3278248Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 46501bf33a244b06bbf1ca29267b7fcc\n  System UUID:                46501bf3-3a24-4b06-bbf1-ca29267b7fcc\n  Boot ID:                    25bd5ed4-e438-4d68-8dac-6771fd7cb68f\n  Kernel Version:             5.19.0-50-generic\n  OS Image:                   Ubuntu 22.04.2 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  cri-o://1.25.3\n  Kubelet Version:            v1.25.12\n  Kube-Proxy Version:         v1.25.12\nPodCIDR:                      10.233.64.0/24\nPodCIDRs:                     10.233.64.0/24\nNon-terminated Pods:          (9 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 cilium-gfkpl                                               100m (6%)     0 (0%)      100Mi (3%)       0 (0%)         55m\n  kube-system                 cilium-node-init-n8876                                     100m (6%)     0 (0%)      100Mi (3%)       0 (0%)         55m\n  kube-system                 coredns-565d847f94-xm9ff                                   100m (6%)     0 (0%)      70Mi (2%)        170Mi (5%)     44m\n  kube-system                 kube-addon-manager-wa4quivohpee-1                          5m (0%)       0 (0%)      50Mi (1%)        0 (0%)         55m\n  kube-system                 kube-apiserver-wa4quivohpee-1                              250m (15%)    0 (0%)      0 (0%)           0 (0%)         64m\n  kube-system                 kube-controller-manager-wa4quivohpee-1                     200m (12%)    0 (0%)      0 (0%)           0 (0%)         64m\n  kube-system                 kube-proxy-gdffl                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         64m\n  kube-system                 kube-scheduler-wa4quivohpee-1                              100m (6%)     0 (0%)      0 (0%)           0 (0%)         64m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-b6f0dae2c3174900-mtck5    0 (0%)        0 (0%)      0 (0%)           0 (0%)         54m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                855m (53%)  0 (0%)\n  memory             320Mi (9%)  170Mi (5%)\n  ephemeral-storage  0 (0%)      0 (0%)\n  hugepages-1Gi      0 (0%)      0 (0%)\n  hugepages-2Mi      0 (0%)      0 (0%)\nEvents:              <none>\n"
    Jul 29 16:18:54.320: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-2536 describe namespace kubectl-2536'
    Jul 29 16:18:54.492: INFO: stderr: ""
    Jul 29 16:18:54.492: INFO: stdout: "Name:         kubectl-2536\nLabels:       e2e-framework=kubectl\n              e2e-run=fdedc122-77f5-414b-b179-f28fd2dcbd5c\n              kubernetes.io/metadata.name=kubectl-2536\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jul 29 16:18:54.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-2536" for this suite. 07/29/23 16:18:54.501
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Security Context When creating a pod with privileged
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:527
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:18:54.514
Jul 29 16:18:54.514: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename security-context-test 07/29/23 16:18:54.516
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:18:54.538
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:18:54.544
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:527
Jul 29 16:18:54.562: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-0504525c-c473-4af5-b341-cea215133153" in namespace "security-context-test-1586" to be "Succeeded or Failed"
Jul 29 16:18:54.569: INFO: Pod "busybox-privileged-false-0504525c-c473-4af5-b341-cea215133153": Phase="Pending", Reason="", readiness=false. Elapsed: 6.963517ms
Jul 29 16:18:56.579: INFO: Pod "busybox-privileged-false-0504525c-c473-4af5-b341-cea215133153": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016024908s
Jul 29 16:18:58.587: INFO: Pod "busybox-privileged-false-0504525c-c473-4af5-b341-cea215133153": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02405497s
Jul 29 16:18:58.587: INFO: Pod "busybox-privileged-false-0504525c-c473-4af5-b341-cea215133153" satisfied condition "Succeeded or Failed"
Jul 29 16:18:58.620: INFO: Got logs for pod "busybox-privileged-false-0504525c-c473-4af5-b341-cea215133153": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Jul 29 16:18:58.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-1586" for this suite. 07/29/23 16:18:58.629
{"msg":"PASSED [sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","completed":197,"skipped":3617,"failed":0}
------------------------------
â€¢ [4.131 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with privileged
  test/e2e/common/node/security_context.go:490
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:527

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:18:54.514
    Jul 29 16:18:54.514: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename security-context-test 07/29/23 16:18:54.516
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:18:54.538
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:18:54.544
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:527
    Jul 29 16:18:54.562: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-0504525c-c473-4af5-b341-cea215133153" in namespace "security-context-test-1586" to be "Succeeded or Failed"
    Jul 29 16:18:54.569: INFO: Pod "busybox-privileged-false-0504525c-c473-4af5-b341-cea215133153": Phase="Pending", Reason="", readiness=false. Elapsed: 6.963517ms
    Jul 29 16:18:56.579: INFO: Pod "busybox-privileged-false-0504525c-c473-4af5-b341-cea215133153": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016024908s
    Jul 29 16:18:58.587: INFO: Pod "busybox-privileged-false-0504525c-c473-4af5-b341-cea215133153": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02405497s
    Jul 29 16:18:58.587: INFO: Pod "busybox-privileged-false-0504525c-c473-4af5-b341-cea215133153" satisfied condition "Succeeded or Failed"
    Jul 29 16:18:58.620: INFO: Got logs for pod "busybox-privileged-false-0504525c-c473-4af5-b341-cea215133153": "ip: RTNETLINK answers: Operation not permitted\n"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Jul 29 16:18:58.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-1586" for this suite. 07/29/23 16:18:58.629
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIStorageCapacity
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
[BeforeEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:18:58.661
Jul 29 16:18:58.661: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename csistoragecapacity 07/29/23 16:18:58.663
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:18:58.715
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:18:58.719
[It]  should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
STEP: getting /apis 07/29/23 16:18:58.723
STEP: getting /apis/storage.k8s.io 07/29/23 16:18:58.727
STEP: getting /apis/storage.k8s.io/v1 07/29/23 16:18:58.729
STEP: creating 07/29/23 16:18:58.731
STEP: watching 07/29/23 16:18:58.763
Jul 29 16:18:58.763: INFO: starting watch
STEP: getting 07/29/23 16:18:58.775
STEP: listing in namespace 07/29/23 16:18:58.78
STEP: listing across namespaces 07/29/23 16:18:58.789
STEP: patching 07/29/23 16:18:58.795
STEP: updating 07/29/23 16:18:58.808
Jul 29 16:18:58.824: INFO: waiting for watch events with expected annotations in namespace
Jul 29 16:18:58.824: INFO: waiting for watch events with expected annotations across namespace
STEP: deleting 07/29/23 16:18:58.825
STEP: deleting a collection 07/29/23 16:18:58.847
[AfterEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/framework.go:187
Jul 29 16:18:58.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "csistoragecapacity-3999" for this suite. 07/29/23 16:18:58.884
{"msg":"PASSED [sig-storage] CSIStorageCapacity  should support CSIStorageCapacities API operations [Conformance]","completed":198,"skipped":3712,"failed":0}
------------------------------
â€¢ [0.235 seconds]
[sig-storage] CSIStorageCapacity
test/e2e/storage/utils/framework.go:23
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:18:58.661
    Jul 29 16:18:58.661: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename csistoragecapacity 07/29/23 16:18:58.663
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:18:58.715
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:18:58.719
    [It]  should support CSIStorageCapacities API operations [Conformance]
      test/e2e/storage/csistoragecapacity.go:49
    STEP: getting /apis 07/29/23 16:18:58.723
    STEP: getting /apis/storage.k8s.io 07/29/23 16:18:58.727
    STEP: getting /apis/storage.k8s.io/v1 07/29/23 16:18:58.729
    STEP: creating 07/29/23 16:18:58.731
    STEP: watching 07/29/23 16:18:58.763
    Jul 29 16:18:58.763: INFO: starting watch
    STEP: getting 07/29/23 16:18:58.775
    STEP: listing in namespace 07/29/23 16:18:58.78
    STEP: listing across namespaces 07/29/23 16:18:58.789
    STEP: patching 07/29/23 16:18:58.795
    STEP: updating 07/29/23 16:18:58.808
    Jul 29 16:18:58.824: INFO: waiting for watch events with expected annotations in namespace
    Jul 29 16:18:58.824: INFO: waiting for watch events with expected annotations across namespace
    STEP: deleting 07/29/23 16:18:58.825
    STEP: deleting a collection 07/29/23 16:18:58.847
    [AfterEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/framework.go:187
    Jul 29 16:18:58.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "csistoragecapacity-3999" for this suite. 07/29/23 16:18:58.884
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:18:58.903
Jul 29 16:18:58.903: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename endpointslice 07/29/23 16:18:58.905
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:18:58.941
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:18:58.949
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65
Jul 29 16:18:58.966: INFO: Endpoints addresses: [192.168.121.206 192.168.121.28] , ports: [6443]
Jul 29 16:18:58.966: INFO: EndpointSlices addresses: [192.168.121.206 192.168.121.28] , ports: [6443]
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Jul 29 16:18:58.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-583" for this suite. 07/29/23 16:18:58.971
{"msg":"PASSED [sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]","completed":199,"skipped":3727,"failed":0}
------------------------------
â€¢ [0.083 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:18:58.903
    Jul 29 16:18:58.903: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename endpointslice 07/29/23 16:18:58.905
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:18:58.941
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:18:58.949
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
      test/e2e/network/endpointslice.go:65
    Jul 29 16:18:58.966: INFO: Endpoints addresses: [192.168.121.206 192.168.121.28] , ports: [6443]
    Jul 29 16:18:58.966: INFO: EndpointSlices addresses: [192.168.121.206 192.168.121.28] , ports: [6443]
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Jul 29 16:18:58.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-583" for this suite. 07/29/23 16:18:58.971
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:18:58.989
Jul 29 16:18:58.989: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename projected 07/29/23 16:18:58.992
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:18:59.019
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:18:59.025
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248
STEP: Creating a pod to test downward API volume plugin 07/29/23 16:18:59.028
Jul 29 16:18:59.043: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9fc86974-fb95-4e39-8065-0e3f236c5c32" in namespace "projected-8060" to be "Succeeded or Failed"
Jul 29 16:18:59.051: INFO: Pod "downwardapi-volume-9fc86974-fb95-4e39-8065-0e3f236c5c32": Phase="Pending", Reason="", readiness=false. Elapsed: 7.63229ms
Jul 29 16:19:01.058: INFO: Pod "downwardapi-volume-9fc86974-fb95-4e39-8065-0e3f236c5c32": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015110756s
Jul 29 16:19:03.060: INFO: Pod "downwardapi-volume-9fc86974-fb95-4e39-8065-0e3f236c5c32": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016629884s
Jul 29 16:19:05.061: INFO: Pod "downwardapi-volume-9fc86974-fb95-4e39-8065-0e3f236c5c32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.018266467s
STEP: Saw pod success 07/29/23 16:19:05.062
Jul 29 16:19:05.063: INFO: Pod "downwardapi-volume-9fc86974-fb95-4e39-8065-0e3f236c5c32" satisfied condition "Succeeded or Failed"
Jul 29 16:19:05.069: INFO: Trying to get logs from node wa4quivohpee-3 pod downwardapi-volume-9fc86974-fb95-4e39-8065-0e3f236c5c32 container client-container: <nil>
STEP: delete the pod 07/29/23 16:19:05.089
Jul 29 16:19:05.106: INFO: Waiting for pod downwardapi-volume-9fc86974-fb95-4e39-8065-0e3f236c5c32 to disappear
Jul 29 16:19:05.112: INFO: Pod downwardapi-volume-9fc86974-fb95-4e39-8065-0e3f236c5c32 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jul 29 16:19:05.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8060" for this suite. 07/29/23 16:19:05.119
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","completed":200,"skipped":3728,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.171 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:18:58.989
    Jul 29 16:18:58.989: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename projected 07/29/23 16:18:58.992
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:18:59.019
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:18:59.025
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:248
    STEP: Creating a pod to test downward API volume plugin 07/29/23 16:18:59.028
    Jul 29 16:18:59.043: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9fc86974-fb95-4e39-8065-0e3f236c5c32" in namespace "projected-8060" to be "Succeeded or Failed"
    Jul 29 16:18:59.051: INFO: Pod "downwardapi-volume-9fc86974-fb95-4e39-8065-0e3f236c5c32": Phase="Pending", Reason="", readiness=false. Elapsed: 7.63229ms
    Jul 29 16:19:01.058: INFO: Pod "downwardapi-volume-9fc86974-fb95-4e39-8065-0e3f236c5c32": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015110756s
    Jul 29 16:19:03.060: INFO: Pod "downwardapi-volume-9fc86974-fb95-4e39-8065-0e3f236c5c32": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016629884s
    Jul 29 16:19:05.061: INFO: Pod "downwardapi-volume-9fc86974-fb95-4e39-8065-0e3f236c5c32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.018266467s
    STEP: Saw pod success 07/29/23 16:19:05.062
    Jul 29 16:19:05.063: INFO: Pod "downwardapi-volume-9fc86974-fb95-4e39-8065-0e3f236c5c32" satisfied condition "Succeeded or Failed"
    Jul 29 16:19:05.069: INFO: Trying to get logs from node wa4quivohpee-3 pod downwardapi-volume-9fc86974-fb95-4e39-8065-0e3f236c5c32 container client-container: <nil>
    STEP: delete the pod 07/29/23 16:19:05.089
    Jul 29 16:19:05.106: INFO: Waiting for pod downwardapi-volume-9fc86974-fb95-4e39-8065-0e3f236c5c32 to disappear
    Jul 29 16:19:05.112: INFO: Pod downwardapi-volume-9fc86974-fb95-4e39-8065-0e3f236c5c32 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jul 29 16:19:05.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-8060" for this suite. 07/29/23 16:19:05.119
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:231
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:19:05.17
Jul 29 16:19:05.171: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename container-runtime 07/29/23 16:19:05.172
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:19:05.198
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:19:05.204
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:231
STEP: create the container 07/29/23 16:19:05.208
STEP: wait for the container to reach Succeeded 07/29/23 16:19:05.222
STEP: get the container status 07/29/23 16:19:08.259
STEP: the container should be terminated 07/29/23 16:19:08.268
STEP: the termination message should be set 07/29/23 16:19:08.268
Jul 29 16:19:08.268: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container 07/29/23 16:19:08.268
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Jul 29 16:19:08.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2788" for this suite. 07/29/23 16:19:08.32
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":201,"skipped":3752,"failed":0}
------------------------------
â€¢ [3.166 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:231

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:19:05.17
    Jul 29 16:19:05.171: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename container-runtime 07/29/23 16:19:05.172
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:19:05.198
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:19:05.204
    [It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:231
    STEP: create the container 07/29/23 16:19:05.208
    STEP: wait for the container to reach Succeeded 07/29/23 16:19:05.222
    STEP: get the container status 07/29/23 16:19:08.259
    STEP: the container should be terminated 07/29/23 16:19:08.268
    STEP: the termination message should be set 07/29/23 16:19:08.268
    Jul 29 16:19:08.268: INFO: Expected: &{} to match Container's Termination Message:  --
    STEP: delete the container 07/29/23 16:19:08.268
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Jul 29 16:19:08.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-2788" for this suite. 07/29/23 16:19:08.32
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:585
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:19:08.343
Jul 29 16:19:08.344: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename statefulset 07/29/23 16:19:08.346
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:19:08.393
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:19:08.397
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-6745 07/29/23 16:19:08.402
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:585
STEP: Initializing watcher for selector baz=blah,foo=bar 07/29/23 16:19:08.415
STEP: Creating stateful set ss in namespace statefulset-6745 07/29/23 16:19:08.423
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-6745 07/29/23 16:19:08.436
Jul 29 16:19:08.442: INFO: Found 0 stateful pods, waiting for 1
Jul 29 16:19:18.452: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 07/29/23 16:19:18.452
Jul 29 16:19:18.459: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=statefulset-6745 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 29 16:19:18.842: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 29 16:19:18.842: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 29 16:19:18.842: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul 29 16:19:18.850: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jul 29 16:19:28.858: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jul 29 16:19:28.858: INFO: Waiting for statefulset status.replicas updated to 0
Jul 29 16:19:28.887: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999458s
Jul 29 16:19:29.899: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.990653456s
Jul 29 16:19:30.910: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.978565913s
Jul 29 16:19:31.918: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.968117666s
Jul 29 16:19:32.926: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.959732011s
Jul 29 16:19:33.936: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.950822631s
Jul 29 16:19:34.946: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.941018691s
Jul 29 16:19:35.954: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.931593485s
Jul 29 16:19:36.962: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.923906017s
Jul 29 16:19:37.970: INFO: Verifying statefulset ss doesn't scale past 1 for another 915.192877ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-6745 07/29/23 16:19:38.971
Jul 29 16:19:38.984: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=statefulset-6745 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 29 16:19:39.251: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jul 29 16:19:39.251: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul 29 16:19:39.251: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul 29 16:19:39.262: INFO: Found 1 stateful pods, waiting for 3
Jul 29 16:19:49.271: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 29 16:19:49.271: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 29 16:19:49.271: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order 07/29/23 16:19:49.271
STEP: Scale down will halt with unhealthy stateful pod 07/29/23 16:19:49.271
Jul 29 16:19:49.285: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=statefulset-6745 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 29 16:19:49.543: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 29 16:19:49.543: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 29 16:19:49.543: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul 29 16:19:49.543: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=statefulset-6745 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 29 16:19:49.817: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 29 16:19:49.817: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 29 16:19:49.817: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul 29 16:19:49.817: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=statefulset-6745 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 29 16:19:50.137: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 29 16:19:50.137: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 29 16:19:50.137: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul 29 16:19:50.137: INFO: Waiting for statefulset status.replicas updated to 0
Jul 29 16:19:50.144: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Jul 29 16:20:00.173: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jul 29 16:20:00.173: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jul 29 16:20:00.173: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jul 29 16:20:00.203: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999263s
Jul 29 16:20:01.221: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.991561609s
Jul 29 16:20:02.229: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.973464087s
Jul 29 16:20:03.239: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.965480981s
Jul 29 16:20:04.248: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.954797433s
Jul 29 16:20:05.257: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.946093594s
Jul 29 16:20:06.267: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.936636863s
Jul 29 16:20:07.276: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.926761916s
Jul 29 16:20:08.285: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.917970174s
Jul 29 16:20:09.295: INFO: Verifying statefulset ss doesn't scale past 3 for another 908.832464ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-6745 07/29/23 16:20:10.295
Jul 29 16:20:10.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=statefulset-6745 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 29 16:20:10.588: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jul 29 16:20:10.588: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul 29 16:20:10.588: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul 29 16:20:10.589: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=statefulset-6745 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 29 16:20:10.897: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jul 29 16:20:10.897: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul 29 16:20:10.897: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul 29 16:20:10.897: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=statefulset-6745 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 29 16:20:11.130: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jul 29 16:20:11.130: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul 29 16:20:11.130: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul 29 16:20:11.130: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order 07/29/23 16:20:21.174
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jul 29 16:20:21.175: INFO: Deleting all statefulset in ns statefulset-6745
Jul 29 16:20:21.183: INFO: Scaling statefulset ss to 0
Jul 29 16:20:21.214: INFO: Waiting for statefulset status.replicas updated to 0
Jul 29 16:20:21.219: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jul 29 16:20:21.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6745" for this suite. 07/29/23 16:20:21.263
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","completed":202,"skipped":3757,"failed":0}
------------------------------
â€¢ [SLOW TEST] [72.935 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    test/e2e/apps/statefulset.go:585

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:19:08.343
    Jul 29 16:19:08.344: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename statefulset 07/29/23 16:19:08.346
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:19:08.393
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:19:08.397
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-6745 07/29/23 16:19:08.402
    [It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
      test/e2e/apps/statefulset.go:585
    STEP: Initializing watcher for selector baz=blah,foo=bar 07/29/23 16:19:08.415
    STEP: Creating stateful set ss in namespace statefulset-6745 07/29/23 16:19:08.423
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-6745 07/29/23 16:19:08.436
    Jul 29 16:19:08.442: INFO: Found 0 stateful pods, waiting for 1
    Jul 29 16:19:18.452: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 07/29/23 16:19:18.452
    Jul 29 16:19:18.459: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=statefulset-6745 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jul 29 16:19:18.842: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jul 29 16:19:18.842: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jul 29 16:19:18.842: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jul 29 16:19:18.850: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Jul 29 16:19:28.858: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jul 29 16:19:28.858: INFO: Waiting for statefulset status.replicas updated to 0
    Jul 29 16:19:28.887: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999458s
    Jul 29 16:19:29.899: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.990653456s
    Jul 29 16:19:30.910: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.978565913s
    Jul 29 16:19:31.918: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.968117666s
    Jul 29 16:19:32.926: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.959732011s
    Jul 29 16:19:33.936: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.950822631s
    Jul 29 16:19:34.946: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.941018691s
    Jul 29 16:19:35.954: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.931593485s
    Jul 29 16:19:36.962: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.923906017s
    Jul 29 16:19:37.970: INFO: Verifying statefulset ss doesn't scale past 1 for another 915.192877ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-6745 07/29/23 16:19:38.971
    Jul 29 16:19:38.984: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=statefulset-6745 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jul 29 16:19:39.251: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jul 29 16:19:39.251: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jul 29 16:19:39.251: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jul 29 16:19:39.262: INFO: Found 1 stateful pods, waiting for 3
    Jul 29 16:19:49.271: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Jul 29 16:19:49.271: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Jul 29 16:19:49.271: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Verifying that stateful set ss was scaled up in order 07/29/23 16:19:49.271
    STEP: Scale down will halt with unhealthy stateful pod 07/29/23 16:19:49.271
    Jul 29 16:19:49.285: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=statefulset-6745 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jul 29 16:19:49.543: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jul 29 16:19:49.543: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jul 29 16:19:49.543: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jul 29 16:19:49.543: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=statefulset-6745 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jul 29 16:19:49.817: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jul 29 16:19:49.817: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jul 29 16:19:49.817: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jul 29 16:19:49.817: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=statefulset-6745 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jul 29 16:19:50.137: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jul 29 16:19:50.137: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jul 29 16:19:50.137: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jul 29 16:19:50.137: INFO: Waiting for statefulset status.replicas updated to 0
    Jul 29 16:19:50.144: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
    Jul 29 16:20:00.173: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jul 29 16:20:00.173: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Jul 29 16:20:00.173: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Jul 29 16:20:00.203: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999263s
    Jul 29 16:20:01.221: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.991561609s
    Jul 29 16:20:02.229: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.973464087s
    Jul 29 16:20:03.239: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.965480981s
    Jul 29 16:20:04.248: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.954797433s
    Jul 29 16:20:05.257: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.946093594s
    Jul 29 16:20:06.267: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.936636863s
    Jul 29 16:20:07.276: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.926761916s
    Jul 29 16:20:08.285: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.917970174s
    Jul 29 16:20:09.295: INFO: Verifying statefulset ss doesn't scale past 3 for another 908.832464ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-6745 07/29/23 16:20:10.295
    Jul 29 16:20:10.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=statefulset-6745 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jul 29 16:20:10.588: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jul 29 16:20:10.588: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jul 29 16:20:10.588: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jul 29 16:20:10.589: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=statefulset-6745 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jul 29 16:20:10.897: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jul 29 16:20:10.897: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jul 29 16:20:10.897: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jul 29 16:20:10.897: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=statefulset-6745 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jul 29 16:20:11.130: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jul 29 16:20:11.130: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jul 29 16:20:11.130: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jul 29 16:20:11.130: INFO: Scaling statefulset ss to 0
    STEP: Verifying that stateful set ss was scaled down in reverse order 07/29/23 16:20:21.174
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jul 29 16:20:21.175: INFO: Deleting all statefulset in ns statefulset-6745
    Jul 29 16:20:21.183: INFO: Scaling statefulset ss to 0
    Jul 29 16:20:21.214: INFO: Waiting for statefulset status.replicas updated to 0
    Jul 29 16:20:21.219: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jul 29 16:20:21.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-6745" for this suite. 07/29/23 16:20:21.263
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] Job
  should delete a job [Conformance]
  test/e2e/apps/job.go:309
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:20:21.279
Jul 29 16:20:21.279: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename job 07/29/23 16:20:21.283
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:20:21.325
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:20:21.33
[It] should delete a job [Conformance]
  test/e2e/apps/job.go:309
STEP: Creating a job 07/29/23 16:20:21.368
STEP: Ensuring active pods == parallelism 07/29/23 16:20:21.379
STEP: delete a job 07/29/23 16:20:25.388
STEP: deleting Job.batch foo in namespace job-1243, will wait for the garbage collector to delete the pods 07/29/23 16:20:25.388
Jul 29 16:20:25.457: INFO: Deleting Job.batch foo took: 11.568666ms
Jul 29 16:20:25.557: INFO: Terminating Job.batch foo pods took: 100.36511ms
STEP: Ensuring job was deleted 07/29/23 16:20:56.758
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Jul 29 16:20:56.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-1243" for this suite. 07/29/23 16:20:56.773
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","completed":203,"skipped":3762,"failed":0}
------------------------------
â€¢ [SLOW TEST] [35.508 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  test/e2e/apps/job.go:309

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:20:21.279
    Jul 29 16:20:21.279: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename job 07/29/23 16:20:21.283
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:20:21.325
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:20:21.33
    [It] should delete a job [Conformance]
      test/e2e/apps/job.go:309
    STEP: Creating a job 07/29/23 16:20:21.368
    STEP: Ensuring active pods == parallelism 07/29/23 16:20:21.379
    STEP: delete a job 07/29/23 16:20:25.388
    STEP: deleting Job.batch foo in namespace job-1243, will wait for the garbage collector to delete the pods 07/29/23 16:20:25.388
    Jul 29 16:20:25.457: INFO: Deleting Job.batch foo took: 11.568666ms
    Jul 29 16:20:25.557: INFO: Terminating Job.batch foo pods took: 100.36511ms
    STEP: Ensuring job was deleted 07/29/23 16:20:56.758
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Jul 29 16:20:56.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-1243" for this suite. 07/29/23 16:20:56.773
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:86
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:20:56.791
Jul 29 16:20:56.792: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename disruption 07/29/23 16:20:56.794
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:20:56.829
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:20:56.834
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:20:56.838
Jul 29 16:20:56.838: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename disruption-2 07/29/23 16:20:56.84
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:20:56.866
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:20:56.87
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:86
STEP: Waiting for the pdb to be processed 07/29/23 16:20:56.883
STEP: Waiting for the pdb to be processed 07/29/23 16:20:58.902
STEP: Waiting for the pdb to be processed 07/29/23 16:21:00.93
STEP: listing a collection of PDBs across all namespaces 07/29/23 16:21:00.948
STEP: listing a collection of PDBs in namespace disruption-2931 07/29/23 16:21:00.96
STEP: deleting a collection of PDBs 07/29/23 16:21:00.967
STEP: Waiting for the PDB collection to be deleted 07/29/23 16:21:00.991
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/framework.go:187
Jul 29 16:21:00.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2-4756" for this suite. 07/29/23 16:21:01.003
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Jul 29 16:21:01.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2931" for this suite. 07/29/23 16:21:01.025
{"msg":"PASSED [sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]","completed":204,"skipped":3781,"failed":0}
------------------------------
â€¢ [4.248 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  test/e2e/apps/disruption.go:77
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    test/e2e/apps/disruption.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:20:56.791
    Jul 29 16:20:56.792: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename disruption 07/29/23 16:20:56.794
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:20:56.829
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:20:56.834
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:20:56.838
    Jul 29 16:20:56.838: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename disruption-2 07/29/23 16:20:56.84
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:20:56.866
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:20:56.87
    [It] should list and delete a collection of PodDisruptionBudgets [Conformance]
      test/e2e/apps/disruption.go:86
    STEP: Waiting for the pdb to be processed 07/29/23 16:20:56.883
    STEP: Waiting for the pdb to be processed 07/29/23 16:20:58.902
    STEP: Waiting for the pdb to be processed 07/29/23 16:21:00.93
    STEP: listing a collection of PDBs across all namespaces 07/29/23 16:21:00.948
    STEP: listing a collection of PDBs in namespace disruption-2931 07/29/23 16:21:00.96
    STEP: deleting a collection of PDBs 07/29/23 16:21:00.967
    STEP: Waiting for the PDB collection to be deleted 07/29/23 16:21:00.991
    [AfterEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/framework.go:187
    Jul 29 16:21:00.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-2-4756" for this suite. 07/29/23 16:21:01.003
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Jul 29 16:21:01.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-2931" for this suite. 07/29/23 16:21:01.025
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:21:01.044
Jul 29 16:21:01.044: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename emptydir 07/29/23 16:21:01.046
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:21:01.08
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:21:01.086
[It] pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226
STEP: Creating Pod 07/29/23 16:21:01.095
Jul 29 16:21:01.110: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-ec013207-cd77-455a-b5fb-a3429be32cbe" in namespace "emptydir-4325" to be "running"
Jul 29 16:21:01.116: INFO: Pod "pod-sharedvolume-ec013207-cd77-455a-b5fb-a3429be32cbe": Phase="Pending", Reason="", readiness=false. Elapsed: 6.358514ms
Jul 29 16:21:03.123: INFO: Pod "pod-sharedvolume-ec013207-cd77-455a-b5fb-a3429be32cbe": Phase="Running", Reason="", readiness=false. Elapsed: 2.013614927s
Jul 29 16:21:03.124: INFO: Pod "pod-sharedvolume-ec013207-cd77-455a-b5fb-a3429be32cbe" satisfied condition "running"
STEP: Reading file content from the nginx-container 07/29/23 16:21:03.124
Jul 29 16:21:03.124: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-4325 PodName:pod-sharedvolume-ec013207-cd77-455a-b5fb-a3429be32cbe ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 29 16:21:03.124: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
Jul 29 16:21:03.126: INFO: ExecWithOptions: Clientset creation
Jul 29 16:21:03.126: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/emptydir-4325/pods/pod-sharedvolume-ec013207-cd77-455a-b5fb-a3429be32cbe/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
Jul 29 16:21:03.225: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jul 29 16:21:03.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4325" for this suite. 07/29/23 16:21:03.234
{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","completed":205,"skipped":3797,"failed":0}
------------------------------
â€¢ [2.204 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:21:01.044
    Jul 29 16:21:01.044: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename emptydir 07/29/23 16:21:01.046
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:21:01.08
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:21:01.086
    [It] pod should support shared volumes between containers [Conformance]
      test/e2e/common/storage/empty_dir.go:226
    STEP: Creating Pod 07/29/23 16:21:01.095
    Jul 29 16:21:01.110: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-ec013207-cd77-455a-b5fb-a3429be32cbe" in namespace "emptydir-4325" to be "running"
    Jul 29 16:21:01.116: INFO: Pod "pod-sharedvolume-ec013207-cd77-455a-b5fb-a3429be32cbe": Phase="Pending", Reason="", readiness=false. Elapsed: 6.358514ms
    Jul 29 16:21:03.123: INFO: Pod "pod-sharedvolume-ec013207-cd77-455a-b5fb-a3429be32cbe": Phase="Running", Reason="", readiness=false. Elapsed: 2.013614927s
    Jul 29 16:21:03.124: INFO: Pod "pod-sharedvolume-ec013207-cd77-455a-b5fb-a3429be32cbe" satisfied condition "running"
    STEP: Reading file content from the nginx-container 07/29/23 16:21:03.124
    Jul 29 16:21:03.124: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-4325 PodName:pod-sharedvolume-ec013207-cd77-455a-b5fb-a3429be32cbe ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jul 29 16:21:03.124: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    Jul 29 16:21:03.126: INFO: ExecWithOptions: Clientset creation
    Jul 29 16:21:03.126: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/emptydir-4325/pods/pod-sharedvolume-ec013207-cd77-455a-b5fb-a3429be32cbe/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
    Jul 29 16:21:03.225: INFO: Exec stderr: ""
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jul 29 16:21:03.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-4325" for this suite. 07/29/23 16:21:03.234
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:21:03.251
Jul 29 16:21:03.251: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename configmap 07/29/23 16:21:03.253
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:21:03.278
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:21:03.283
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239
STEP: Creating configMap with name cm-test-opt-del-2b6eae9d-9a8c-4ac8-8b71-096bef64c71e 07/29/23 16:21:03.294
STEP: Creating configMap with name cm-test-opt-upd-40da854b-5099-4bca-ae76-90d277d6c90f 07/29/23 16:21:03.303
STEP: Creating the pod 07/29/23 16:21:03.312
Jul 29 16:21:03.328: INFO: Waiting up to 5m0s for pod "pod-configmaps-78188676-a8de-4a5e-aff5-86b75aca5410" in namespace "configmap-4638" to be "running and ready"
Jul 29 16:21:03.343: INFO: Pod "pod-configmaps-78188676-a8de-4a5e-aff5-86b75aca5410": Phase="Pending", Reason="", readiness=false. Elapsed: 15.1818ms
Jul 29 16:21:03.344: INFO: The phase of Pod pod-configmaps-78188676-a8de-4a5e-aff5-86b75aca5410 is Pending, waiting for it to be Running (with Ready = true)
Jul 29 16:21:05.354: INFO: Pod "pod-configmaps-78188676-a8de-4a5e-aff5-86b75aca5410": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026190432s
Jul 29 16:21:05.355: INFO: The phase of Pod pod-configmaps-78188676-a8de-4a5e-aff5-86b75aca5410 is Pending, waiting for it to be Running (with Ready = true)
Jul 29 16:21:07.350: INFO: Pod "pod-configmaps-78188676-a8de-4a5e-aff5-86b75aca5410": Phase="Running", Reason="", readiness=true. Elapsed: 4.021880159s
Jul 29 16:21:07.350: INFO: The phase of Pod pod-configmaps-78188676-a8de-4a5e-aff5-86b75aca5410 is Running (Ready = true)
Jul 29 16:21:07.350: INFO: Pod "pod-configmaps-78188676-a8de-4a5e-aff5-86b75aca5410" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-2b6eae9d-9a8c-4ac8-8b71-096bef64c71e 07/29/23 16:21:07.408
STEP: Updating configmap cm-test-opt-upd-40da854b-5099-4bca-ae76-90d277d6c90f 07/29/23 16:21:07.422
STEP: Creating configMap with name cm-test-opt-create-fd8c4220-f3cf-4188-8ada-2aa10723d602 07/29/23 16:21:07.431
STEP: waiting to observe update in volume 07/29/23 16:21:07.44
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jul 29 16:22:32.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4638" for this suite. 07/29/23 16:22:32.262
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":206,"skipped":3821,"failed":0}
------------------------------
â€¢ [SLOW TEST] [89.024 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:21:03.251
    Jul 29 16:21:03.251: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename configmap 07/29/23 16:21:03.253
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:21:03.278
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:21:03.283
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:239
    STEP: Creating configMap with name cm-test-opt-del-2b6eae9d-9a8c-4ac8-8b71-096bef64c71e 07/29/23 16:21:03.294
    STEP: Creating configMap with name cm-test-opt-upd-40da854b-5099-4bca-ae76-90d277d6c90f 07/29/23 16:21:03.303
    STEP: Creating the pod 07/29/23 16:21:03.312
    Jul 29 16:21:03.328: INFO: Waiting up to 5m0s for pod "pod-configmaps-78188676-a8de-4a5e-aff5-86b75aca5410" in namespace "configmap-4638" to be "running and ready"
    Jul 29 16:21:03.343: INFO: Pod "pod-configmaps-78188676-a8de-4a5e-aff5-86b75aca5410": Phase="Pending", Reason="", readiness=false. Elapsed: 15.1818ms
    Jul 29 16:21:03.344: INFO: The phase of Pod pod-configmaps-78188676-a8de-4a5e-aff5-86b75aca5410 is Pending, waiting for it to be Running (with Ready = true)
    Jul 29 16:21:05.354: INFO: Pod "pod-configmaps-78188676-a8de-4a5e-aff5-86b75aca5410": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026190432s
    Jul 29 16:21:05.355: INFO: The phase of Pod pod-configmaps-78188676-a8de-4a5e-aff5-86b75aca5410 is Pending, waiting for it to be Running (with Ready = true)
    Jul 29 16:21:07.350: INFO: Pod "pod-configmaps-78188676-a8de-4a5e-aff5-86b75aca5410": Phase="Running", Reason="", readiness=true. Elapsed: 4.021880159s
    Jul 29 16:21:07.350: INFO: The phase of Pod pod-configmaps-78188676-a8de-4a5e-aff5-86b75aca5410 is Running (Ready = true)
    Jul 29 16:21:07.350: INFO: Pod "pod-configmaps-78188676-a8de-4a5e-aff5-86b75aca5410" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-2b6eae9d-9a8c-4ac8-8b71-096bef64c71e 07/29/23 16:21:07.408
    STEP: Updating configmap cm-test-opt-upd-40da854b-5099-4bca-ae76-90d277d6c90f 07/29/23 16:21:07.422
    STEP: Creating configMap with name cm-test-opt-create-fd8c4220-f3cf-4188-8ada-2aa10723d602 07/29/23 16:21:07.431
    STEP: waiting to observe update in volume 07/29/23 16:21:07.44
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jul 29 16:22:32.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-4638" for this suite. 07/29/23 16:22:32.262
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod
  should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:22:32.278
Jul 29 16:22:32.278: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename kubelet-test 07/29/23 16:22:32.281
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:22:32.308
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:22:32.311
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
Jul 29 16:22:32.329: INFO: Waiting up to 5m0s for pod "busybox-scheduling-23398761-c03a-40d0-941d-85b8062d6212" in namespace "kubelet-test-719" to be "running and ready"
Jul 29 16:22:32.334: INFO: Pod "busybox-scheduling-23398761-c03a-40d0-941d-85b8062d6212": Phase="Pending", Reason="", readiness=false. Elapsed: 4.077434ms
Jul 29 16:22:32.334: INFO: The phase of Pod busybox-scheduling-23398761-c03a-40d0-941d-85b8062d6212 is Pending, waiting for it to be Running (with Ready = true)
Jul 29 16:22:34.341: INFO: Pod "busybox-scheduling-23398761-c03a-40d0-941d-85b8062d6212": Phase="Running", Reason="", readiness=true. Elapsed: 2.011659392s
Jul 29 16:22:34.341: INFO: The phase of Pod busybox-scheduling-23398761-c03a-40d0-941d-85b8062d6212 is Running (Ready = true)
Jul 29 16:22:34.341: INFO: Pod "busybox-scheduling-23398761-c03a-40d0-941d-85b8062d6212" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Jul 29 16:22:34.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-719" for this suite. 07/29/23 16:22:34.374
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","completed":207,"skipped":3830,"failed":0}
------------------------------
â€¢ [2.109 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command in a pod
  test/e2e/common/node/kubelet.go:44
    should print the output to logs [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:22:32.278
    Jul 29 16:22:32.278: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename kubelet-test 07/29/23 16:22:32.281
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:22:32.308
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:22:32.311
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should print the output to logs [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:52
    Jul 29 16:22:32.329: INFO: Waiting up to 5m0s for pod "busybox-scheduling-23398761-c03a-40d0-941d-85b8062d6212" in namespace "kubelet-test-719" to be "running and ready"
    Jul 29 16:22:32.334: INFO: Pod "busybox-scheduling-23398761-c03a-40d0-941d-85b8062d6212": Phase="Pending", Reason="", readiness=false. Elapsed: 4.077434ms
    Jul 29 16:22:32.334: INFO: The phase of Pod busybox-scheduling-23398761-c03a-40d0-941d-85b8062d6212 is Pending, waiting for it to be Running (with Ready = true)
    Jul 29 16:22:34.341: INFO: Pod "busybox-scheduling-23398761-c03a-40d0-941d-85b8062d6212": Phase="Running", Reason="", readiness=true. Elapsed: 2.011659392s
    Jul 29 16:22:34.341: INFO: The phase of Pod busybox-scheduling-23398761-c03a-40d0-941d-85b8062d6212 is Running (Ready = true)
    Jul 29 16:22:34.341: INFO: Pod "busybox-scheduling-23398761-c03a-40d0-941d-85b8062d6212" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Jul 29 16:22:34.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-719" for this suite. 07/29/23 16:22:34.374
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:22:34.389
Jul 29 16:22:34.390: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename webhook 07/29/23 16:22:34.393
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:22:34.432
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:22:34.437
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 07/29/23 16:22:34.465
STEP: Create role binding to let webhook read extension-apiserver-authentication 07/29/23 16:22:35.269
STEP: Deploying the webhook pod 07/29/23 16:22:35.292
STEP: Wait for the deployment to be ready 07/29/23 16:22:35.316
Jul 29 16:22:35.341: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 07/29/23 16:22:37.37
STEP: Verifying the service has paired with the endpoint 07/29/23 16:22:37.399
Jul 29 16:22:38.403: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307
STEP: Registering the crd webhook via the AdmissionRegistration API 07/29/23 16:22:38.41
STEP: Creating a custom resource definition that should be denied by the webhook 07/29/23 16:22:38.441
Jul 29 16:22:38.441: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jul 29 16:22:38.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3648" for this suite. 07/29/23 16:22:38.478
STEP: Destroying namespace "webhook-3648-markers" for this suite. 07/29/23 16:22:38.49
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","completed":208,"skipped":3834,"failed":0}
------------------------------
â€¢ [4.170 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:22:34.389
    Jul 29 16:22:34.390: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename webhook 07/29/23 16:22:34.393
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:22:34.432
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:22:34.437
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 07/29/23 16:22:34.465
    STEP: Create role binding to let webhook read extension-apiserver-authentication 07/29/23 16:22:35.269
    STEP: Deploying the webhook pod 07/29/23 16:22:35.292
    STEP: Wait for the deployment to be ready 07/29/23 16:22:35.316
    Jul 29 16:22:35.341: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 07/29/23 16:22:37.37
    STEP: Verifying the service has paired with the endpoint 07/29/23 16:22:37.399
    Jul 29 16:22:38.403: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should deny crd creation [Conformance]
      test/e2e/apimachinery/webhook.go:307
    STEP: Registering the crd webhook via the AdmissionRegistration API 07/29/23 16:22:38.41
    STEP: Creating a custom resource definition that should be denied by the webhook 07/29/23 16:22:38.441
    Jul 29 16:22:38.441: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jul 29 16:22:38.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-3648" for this suite. 07/29/23 16:22:38.478
    STEP: Destroying namespace "webhook-3648-markers" for this suite. 07/29/23 16:22:38.49
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:22:38.563
Jul 29 16:22:38.563: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename replicaset 07/29/23 16:22:38.566
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:22:38.61
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:22:38.62
[It] should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
STEP: Create a Replicaset 07/29/23 16:22:38.641
STEP: Verify that the required pods have come up. 07/29/23 16:22:38.656
Jul 29 16:22:38.663: INFO: Pod name sample-pod: Found 0 pods out of 1
Jul 29 16:22:43.671: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 07/29/23 16:22:43.671
STEP: Getting /status 07/29/23 16:22:43.672
Jul 29 16:22:43.680: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status 07/29/23 16:22:43.68
Jul 29 16:22:43.699: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated 07/29/23 16:22:43.699
Jul 29 16:22:43.703: INFO: Observed &ReplicaSet event: ADDED
Jul 29 16:22:43.703: INFO: Observed &ReplicaSet event: MODIFIED
Jul 29 16:22:43.704: INFO: Observed &ReplicaSet event: MODIFIED
Jul 29 16:22:43.704: INFO: Observed &ReplicaSet event: MODIFIED
Jul 29 16:22:43.704: INFO: Found replicaset test-rs in namespace replicaset-6765 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jul 29 16:22:43.704: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status 07/29/23 16:22:43.704
Jul 29 16:22:43.704: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jul 29 16:22:43.715: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched 07/29/23 16:22:43.715
Jul 29 16:22:43.723: INFO: Observed &ReplicaSet event: ADDED
Jul 29 16:22:43.726: INFO: Observed &ReplicaSet event: MODIFIED
Jul 29 16:22:43.726: INFO: Observed &ReplicaSet event: MODIFIED
Jul 29 16:22:43.727: INFO: Observed &ReplicaSet event: MODIFIED
Jul 29 16:22:43.727: INFO: Observed replicaset test-rs in namespace replicaset-6765 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jul 29 16:22:43.727: INFO: Observed &ReplicaSet event: MODIFIED
Jul 29 16:22:43.729: INFO: Found replicaset test-rs in namespace replicaset-6765 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Jul 29 16:22:43.729: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Jul 29 16:22:43.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-6765" for this suite. 07/29/23 16:22:43.738
{"msg":"PASSED [sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]","completed":209,"skipped":3843,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.191 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:22:38.563
    Jul 29 16:22:38.563: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename replicaset 07/29/23 16:22:38.566
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:22:38.61
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:22:38.62
    [It] should validate Replicaset Status endpoints [Conformance]
      test/e2e/apps/replica_set.go:176
    STEP: Create a Replicaset 07/29/23 16:22:38.641
    STEP: Verify that the required pods have come up. 07/29/23 16:22:38.656
    Jul 29 16:22:38.663: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jul 29 16:22:43.671: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 07/29/23 16:22:43.671
    STEP: Getting /status 07/29/23 16:22:43.672
    Jul 29 16:22:43.680: INFO: Replicaset test-rs has Conditions: []
    STEP: updating the Replicaset Status 07/29/23 16:22:43.68
    Jul 29 16:22:43.699: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the ReplicaSet status to be updated 07/29/23 16:22:43.699
    Jul 29 16:22:43.703: INFO: Observed &ReplicaSet event: ADDED
    Jul 29 16:22:43.703: INFO: Observed &ReplicaSet event: MODIFIED
    Jul 29 16:22:43.704: INFO: Observed &ReplicaSet event: MODIFIED
    Jul 29 16:22:43.704: INFO: Observed &ReplicaSet event: MODIFIED
    Jul 29 16:22:43.704: INFO: Found replicaset test-rs in namespace replicaset-6765 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jul 29 16:22:43.704: INFO: Replicaset test-rs has an updated status
    STEP: patching the Replicaset Status 07/29/23 16:22:43.704
    Jul 29 16:22:43.704: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Jul 29 16:22:43.715: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Replicaset status to be patched 07/29/23 16:22:43.715
    Jul 29 16:22:43.723: INFO: Observed &ReplicaSet event: ADDED
    Jul 29 16:22:43.726: INFO: Observed &ReplicaSet event: MODIFIED
    Jul 29 16:22:43.726: INFO: Observed &ReplicaSet event: MODIFIED
    Jul 29 16:22:43.727: INFO: Observed &ReplicaSet event: MODIFIED
    Jul 29 16:22:43.727: INFO: Observed replicaset test-rs in namespace replicaset-6765 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jul 29 16:22:43.727: INFO: Observed &ReplicaSet event: MODIFIED
    Jul 29 16:22:43.729: INFO: Found replicaset test-rs in namespace replicaset-6765 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
    Jul 29 16:22:43.729: INFO: Replicaset test-rs has a patched status
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Jul 29 16:22:43.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-6765" for this suite. 07/29/23 16:22:43.738
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:22:43.76
Jul 29 16:22:43.760: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename init-container 07/29/23 16:22:43.764
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:22:43.8
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:22:43.805
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457
STEP: creating the pod 07/29/23 16:22:43.813
Jul 29 16:22:43.813: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Jul 29 16:22:48.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9145" for this suite. 07/29/23 16:22:48.058
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","completed":210,"skipped":3866,"failed":0}
------------------------------
â€¢ [4.311 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:22:43.76
    Jul 29 16:22:43.760: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename init-container 07/29/23 16:22:43.764
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:22:43.8
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:22:43.805
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:457
    STEP: creating the pod 07/29/23 16:22:43.813
    Jul 29 16:22:43.813: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Jul 29 16:22:48.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-9145" for this suite. 07/29/23 16:22:48.058
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:22:48.072
Jul 29 16:22:48.072: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename resourcequota 07/29/23 16:22:48.075
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:22:48.142
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:22:48.147
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382
STEP: Counting existing ResourceQuota 07/29/23 16:22:48.152
STEP: Creating a ResourceQuota 07/29/23 16:22:53.158
STEP: Ensuring resource quota status is calculated 07/29/23 16:22:53.172
STEP: Creating a ReplicationController 07/29/23 16:22:55.181
STEP: Ensuring resource quota status captures replication controller creation 07/29/23 16:22:55.201
STEP: Deleting a ReplicationController 07/29/23 16:22:57.211
STEP: Ensuring resource quota status released usage 07/29/23 16:22:57.223
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jul 29 16:22:59.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3170" for this suite. 07/29/23 16:22:59.241
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","completed":211,"skipped":3868,"failed":0}
------------------------------
â€¢ [SLOW TEST] [11.180 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:22:48.072
    Jul 29 16:22:48.072: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename resourcequota 07/29/23 16:22:48.075
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:22:48.142
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:22:48.147
    [It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
      test/e2e/apimachinery/resource_quota.go:382
    STEP: Counting existing ResourceQuota 07/29/23 16:22:48.152
    STEP: Creating a ResourceQuota 07/29/23 16:22:53.158
    STEP: Ensuring resource quota status is calculated 07/29/23 16:22:53.172
    STEP: Creating a ReplicationController 07/29/23 16:22:55.181
    STEP: Ensuring resource quota status captures replication controller creation 07/29/23 16:22:55.201
    STEP: Deleting a ReplicationController 07/29/23 16:22:57.211
    STEP: Ensuring resource quota status released usage 07/29/23 16:22:57.223
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jul 29 16:22:59.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-3170" for this suite. 07/29/23 16:22:59.241
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose
  should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1413
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:22:59.261
Jul 29 16:22:59.261: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename kubectl 07/29/23 16:22:59.263
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:22:59.293
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:22:59.299
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1413
STEP: creating Agnhost RC 07/29/23 16:22:59.305
Jul 29 16:22:59.305: INFO: namespace kubectl-2213
Jul 29 16:22:59.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-2213 create -f -'
Jul 29 16:22:59.928: INFO: stderr: ""
Jul 29 16:22:59.928: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 07/29/23 16:22:59.928
Jul 29 16:23:00.936: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 29 16:23:00.936: INFO: Found 0 / 1
Jul 29 16:23:01.940: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 29 16:23:01.940: INFO: Found 1 / 1
Jul 29 16:23:01.940: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jul 29 16:23:01.949: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 29 16:23:01.949: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jul 29 16:23:01.949: INFO: wait on agnhost-primary startup in kubectl-2213 
Jul 29 16:23:01.950: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-2213 logs agnhost-primary-ghgk4 agnhost-primary'
Jul 29 16:23:02.125: INFO: stderr: ""
Jul 29 16:23:02.125: INFO: stdout: "Paused\n"
STEP: exposing RC 07/29/23 16:23:02.125
Jul 29 16:23:02.126: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-2213 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Jul 29 16:23:02.302: INFO: stderr: ""
Jul 29 16:23:02.302: INFO: stdout: "service/rm2 exposed\n"
Jul 29 16:23:02.310: INFO: Service rm2 in namespace kubectl-2213 found.
STEP: exposing service 07/29/23 16:23:04.324
Jul 29 16:23:04.325: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-2213 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Jul 29 16:23:04.494: INFO: stderr: ""
Jul 29 16:23:04.494: INFO: stdout: "service/rm3 exposed\n"
Jul 29 16:23:04.504: INFO: Service rm3 in namespace kubectl-2213 found.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jul 29 16:23:06.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2213" for this suite. 07/29/23 16:23:06.525
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","completed":212,"skipped":3886,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.277 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl expose
  test/e2e/kubectl/kubectl.go:1407
    should create services for rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1413

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:22:59.261
    Jul 29 16:22:59.261: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename kubectl 07/29/23 16:22:59.263
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:22:59.293
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:22:59.299
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should create services for rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1413
    STEP: creating Agnhost RC 07/29/23 16:22:59.305
    Jul 29 16:22:59.305: INFO: namespace kubectl-2213
    Jul 29 16:22:59.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-2213 create -f -'
    Jul 29 16:22:59.928: INFO: stderr: ""
    Jul 29 16:22:59.928: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 07/29/23 16:22:59.928
    Jul 29 16:23:00.936: INFO: Selector matched 1 pods for map[app:agnhost]
    Jul 29 16:23:00.936: INFO: Found 0 / 1
    Jul 29 16:23:01.940: INFO: Selector matched 1 pods for map[app:agnhost]
    Jul 29 16:23:01.940: INFO: Found 1 / 1
    Jul 29 16:23:01.940: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Jul 29 16:23:01.949: INFO: Selector matched 1 pods for map[app:agnhost]
    Jul 29 16:23:01.949: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Jul 29 16:23:01.949: INFO: wait on agnhost-primary startup in kubectl-2213 
    Jul 29 16:23:01.950: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-2213 logs agnhost-primary-ghgk4 agnhost-primary'
    Jul 29 16:23:02.125: INFO: stderr: ""
    Jul 29 16:23:02.125: INFO: stdout: "Paused\n"
    STEP: exposing RC 07/29/23 16:23:02.125
    Jul 29 16:23:02.126: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-2213 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
    Jul 29 16:23:02.302: INFO: stderr: ""
    Jul 29 16:23:02.302: INFO: stdout: "service/rm2 exposed\n"
    Jul 29 16:23:02.310: INFO: Service rm2 in namespace kubectl-2213 found.
    STEP: exposing service 07/29/23 16:23:04.324
    Jul 29 16:23:04.325: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-2213 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
    Jul 29 16:23:04.494: INFO: stderr: ""
    Jul 29 16:23:04.494: INFO: stdout: "service/rm3 exposed\n"
    Jul 29 16:23:04.504: INFO: Service rm3 in namespace kubectl-2213 found.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jul 29 16:23:06.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-2213" for this suite. 07/29/23 16:23:06.525
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:23:06.545
Jul 29 16:23:06.545: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename container-probe 07/29/23 16:23:06.547
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:23:06.577
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:23:06.583
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jul 29 16:24:06.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-641" for this suite. 07/29/23 16:24:06.637
{"msg":"PASSED [sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","completed":213,"skipped":3897,"failed":0}
------------------------------
â€¢ [SLOW TEST] [60.126 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:23:06.545
    Jul 29 16:23:06.545: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename container-probe 07/29/23 16:23:06.547
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:23:06.577
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:23:06.583
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:104
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jul 29 16:24:06.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-641" for this suite. 07/29/23 16:24:06.637
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:24:06.673
Jul 29 16:24:06.673: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename emptydir 07/29/23 16:24:06.676
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:24:06.73
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:24:06.735
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156
STEP: Creating a pod to test emptydir volume type on node default medium 07/29/23 16:24:06.74
Jul 29 16:24:06.777: INFO: Waiting up to 5m0s for pod "pod-a7c3c23f-bfba-4caf-a7cd-18f99707e7fc" in namespace "emptydir-5182" to be "Succeeded or Failed"
Jul 29 16:24:06.790: INFO: Pod "pod-a7c3c23f-bfba-4caf-a7cd-18f99707e7fc": Phase="Pending", Reason="", readiness=false. Elapsed: 11.967768ms
Jul 29 16:24:08.800: INFO: Pod "pod-a7c3c23f-bfba-4caf-a7cd-18f99707e7fc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022042883s
Jul 29 16:24:10.801: INFO: Pod "pod-a7c3c23f-bfba-4caf-a7cd-18f99707e7fc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023336844s
STEP: Saw pod success 07/29/23 16:24:10.801
Jul 29 16:24:10.802: INFO: Pod "pod-a7c3c23f-bfba-4caf-a7cd-18f99707e7fc" satisfied condition "Succeeded or Failed"
Jul 29 16:24:10.810: INFO: Trying to get logs from node wa4quivohpee-3 pod pod-a7c3c23f-bfba-4caf-a7cd-18f99707e7fc container test-container: <nil>
STEP: delete the pod 07/29/23 16:24:10.847
Jul 29 16:24:10.881: INFO: Waiting for pod pod-a7c3c23f-bfba-4caf-a7cd-18f99707e7fc to disappear
Jul 29 16:24:10.887: INFO: Pod pod-a7c3c23f-bfba-4caf-a7cd-18f99707e7fc no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jul 29 16:24:10.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5182" for this suite. 07/29/23 16:24:10.897
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","completed":214,"skipped":3914,"failed":0}
------------------------------
â€¢ [4.238 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:24:06.673
    Jul 29 16:24:06.673: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename emptydir 07/29/23 16:24:06.676
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:24:06.73
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:24:06.735
    [It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:156
    STEP: Creating a pod to test emptydir volume type on node default medium 07/29/23 16:24:06.74
    Jul 29 16:24:06.777: INFO: Waiting up to 5m0s for pod "pod-a7c3c23f-bfba-4caf-a7cd-18f99707e7fc" in namespace "emptydir-5182" to be "Succeeded or Failed"
    Jul 29 16:24:06.790: INFO: Pod "pod-a7c3c23f-bfba-4caf-a7cd-18f99707e7fc": Phase="Pending", Reason="", readiness=false. Elapsed: 11.967768ms
    Jul 29 16:24:08.800: INFO: Pod "pod-a7c3c23f-bfba-4caf-a7cd-18f99707e7fc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022042883s
    Jul 29 16:24:10.801: INFO: Pod "pod-a7c3c23f-bfba-4caf-a7cd-18f99707e7fc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023336844s
    STEP: Saw pod success 07/29/23 16:24:10.801
    Jul 29 16:24:10.802: INFO: Pod "pod-a7c3c23f-bfba-4caf-a7cd-18f99707e7fc" satisfied condition "Succeeded or Failed"
    Jul 29 16:24:10.810: INFO: Trying to get logs from node wa4quivohpee-3 pod pod-a7c3c23f-bfba-4caf-a7cd-18f99707e7fc container test-container: <nil>
    STEP: delete the pod 07/29/23 16:24:10.847
    Jul 29 16:24:10.881: INFO: Waiting for pod pod-a7c3c23f-bfba-4caf-a7cd-18f99707e7fc to disappear
    Jul 29 16:24:10.887: INFO: Pod pod-a7c3c23f-bfba-4caf-a7cd-18f99707e7fc no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jul 29 16:24:10.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-5182" for this suite. 07/29/23 16:24:10.897
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:24:10.921
Jul 29 16:24:10.921: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename emptydir 07/29/23 16:24:10.928
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:24:10.961
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:24:10.966
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96
STEP: Creating a pod to test emptydir 0644 on tmpfs 07/29/23 16:24:10.971
Jul 29 16:24:10.989: INFO: Waiting up to 5m0s for pod "pod-06ca25f9-da94-40fd-8207-4cb2745e54e4" in namespace "emptydir-4713" to be "Succeeded or Failed"
Jul 29 16:24:10.994: INFO: Pod "pod-06ca25f9-da94-40fd-8207-4cb2745e54e4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.686948ms
Jul 29 16:24:13.001: INFO: Pod "pod-06ca25f9-da94-40fd-8207-4cb2745e54e4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012099959s
Jul 29 16:24:15.001: INFO: Pod "pod-06ca25f9-da94-40fd-8207-4cb2745e54e4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011812191s
STEP: Saw pod success 07/29/23 16:24:15.001
Jul 29 16:24:15.001: INFO: Pod "pod-06ca25f9-da94-40fd-8207-4cb2745e54e4" satisfied condition "Succeeded or Failed"
Jul 29 16:24:15.007: INFO: Trying to get logs from node wa4quivohpee-3 pod pod-06ca25f9-da94-40fd-8207-4cb2745e54e4 container test-container: <nil>
STEP: delete the pod 07/29/23 16:24:15.021
Jul 29 16:24:15.052: INFO: Waiting for pod pod-06ca25f9-da94-40fd-8207-4cb2745e54e4 to disappear
Jul 29 16:24:15.062: INFO: Pod pod-06ca25f9-da94-40fd-8207-4cb2745e54e4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jul 29 16:24:15.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4713" for this suite. 07/29/23 16:24:15.071
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":215,"skipped":3952,"failed":0}
------------------------------
â€¢ [4.159 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:24:10.921
    Jul 29 16:24:10.921: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename emptydir 07/29/23 16:24:10.928
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:24:10.961
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:24:10.966
    [It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:96
    STEP: Creating a pod to test emptydir 0644 on tmpfs 07/29/23 16:24:10.971
    Jul 29 16:24:10.989: INFO: Waiting up to 5m0s for pod "pod-06ca25f9-da94-40fd-8207-4cb2745e54e4" in namespace "emptydir-4713" to be "Succeeded or Failed"
    Jul 29 16:24:10.994: INFO: Pod "pod-06ca25f9-da94-40fd-8207-4cb2745e54e4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.686948ms
    Jul 29 16:24:13.001: INFO: Pod "pod-06ca25f9-da94-40fd-8207-4cb2745e54e4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012099959s
    Jul 29 16:24:15.001: INFO: Pod "pod-06ca25f9-da94-40fd-8207-4cb2745e54e4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011812191s
    STEP: Saw pod success 07/29/23 16:24:15.001
    Jul 29 16:24:15.001: INFO: Pod "pod-06ca25f9-da94-40fd-8207-4cb2745e54e4" satisfied condition "Succeeded or Failed"
    Jul 29 16:24:15.007: INFO: Trying to get logs from node wa4quivohpee-3 pod pod-06ca25f9-da94-40fd-8207-4cb2745e54e4 container test-container: <nil>
    STEP: delete the pod 07/29/23 16:24:15.021
    Jul 29 16:24:15.052: INFO: Waiting for pod pod-06ca25f9-da94-40fd-8207-4cb2745e54e4 to disappear
    Jul 29 16:24:15.062: INFO: Pod pod-06ca25f9-da94-40fd-8207-4cb2745e54e4 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jul 29 16:24:15.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-4713" for this suite. 07/29/23 16:24:15.071
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] CronJob
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:24:15.087
Jul 29 16:24:15.087: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename cronjob 07/29/23 16:24:15.09
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:24:15.123
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:24:15.128
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
STEP: Creating a ForbidConcurrent cronjob 07/29/23 16:24:15.133
STEP: Ensuring a job is scheduled 07/29/23 16:24:15.143
STEP: Ensuring exactly one is scheduled 07/29/23 16:25:01.164
STEP: Ensuring exactly one running job exists by listing jobs explicitly 07/29/23 16:25:01.178
STEP: Ensuring no more jobs are scheduled 07/29/23 16:25:01.186
STEP: Removing cronjob 07/29/23 16:30:01.201
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Jul 29 16:30:01.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-427" for this suite. 07/29/23 16:30:01.227
{"msg":"PASSED [sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]","completed":216,"skipped":3955,"failed":0}
------------------------------
â€¢ [SLOW TEST] [346.152 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:24:15.087
    Jul 29 16:24:15.087: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename cronjob 07/29/23 16:24:15.09
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:24:15.123
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:24:15.128
    [It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
      test/e2e/apps/cronjob.go:124
    STEP: Creating a ForbidConcurrent cronjob 07/29/23 16:24:15.133
    STEP: Ensuring a job is scheduled 07/29/23 16:24:15.143
    STEP: Ensuring exactly one is scheduled 07/29/23 16:25:01.164
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 07/29/23 16:25:01.178
    STEP: Ensuring no more jobs are scheduled 07/29/23 16:25:01.186
    STEP: Removing cronjob 07/29/23 16:30:01.201
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Jul 29 16:30:01.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-427" for this suite. 07/29/23 16:30:01.227
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:30:01.245
Jul 29 16:30:01.245: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename runtimeclass 07/29/23 16:30:01.249
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:30:01.282
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:30:01.291
[It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Jul 29 16:30:01.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-3349" for this suite. 07/29/23 16:30:01.342
{"msg":"PASSED [sig-node] RuntimeClass should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]","completed":217,"skipped":3973,"failed":0}
------------------------------
â€¢ [0.106 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:30:01.245
    Jul 29 16:30:01.245: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename runtimeclass 07/29/23 16:30:01.249
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:30:01.282
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:30:01.291
    [It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:55
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Jul 29 16:30:01.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-3349" for this suite. 07/29/23 16:30:01.342
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:30:01.365
Jul 29 16:30:01.365: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename svcaccounts 07/29/23 16:30:01.37
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:30:01.416
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:30:01.422
[It] should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158
Jul 29 16:30:01.464: INFO: created pod pod-service-account-defaultsa
Jul 29 16:30:01.464: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Jul 29 16:30:01.475: INFO: created pod pod-service-account-mountsa
Jul 29 16:30:01.476: INFO: pod pod-service-account-mountsa service account token volume mount: true
Jul 29 16:30:01.494: INFO: created pod pod-service-account-nomountsa
Jul 29 16:30:01.494: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Jul 29 16:30:01.507: INFO: created pod pod-service-account-defaultsa-mountspec
Jul 29 16:30:01.507: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Jul 29 16:30:01.517: INFO: created pod pod-service-account-mountsa-mountspec
Jul 29 16:30:01.517: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Jul 29 16:30:01.527: INFO: created pod pod-service-account-nomountsa-mountspec
Jul 29 16:30:01.527: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Jul 29 16:30:01.567: INFO: created pod pod-service-account-defaultsa-nomountspec
Jul 29 16:30:01.567: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Jul 29 16:30:01.585: INFO: created pod pod-service-account-mountsa-nomountspec
Jul 29 16:30:01.585: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Jul 29 16:30:01.606: INFO: created pod pod-service-account-nomountsa-nomountspec
Jul 29 16:30:01.606: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Jul 29 16:30:01.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-1147" for this suite. 07/29/23 16:30:01.626
{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","completed":218,"skipped":3985,"failed":0}
------------------------------
â€¢ [0.298 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:30:01.365
    Jul 29 16:30:01.365: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename svcaccounts 07/29/23 16:30:01.37
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:30:01.416
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:30:01.422
    [It] should allow opting out of API token automount  [Conformance]
      test/e2e/auth/service_accounts.go:158
    Jul 29 16:30:01.464: INFO: created pod pod-service-account-defaultsa
    Jul 29 16:30:01.464: INFO: pod pod-service-account-defaultsa service account token volume mount: true
    Jul 29 16:30:01.475: INFO: created pod pod-service-account-mountsa
    Jul 29 16:30:01.476: INFO: pod pod-service-account-mountsa service account token volume mount: true
    Jul 29 16:30:01.494: INFO: created pod pod-service-account-nomountsa
    Jul 29 16:30:01.494: INFO: pod pod-service-account-nomountsa service account token volume mount: false
    Jul 29 16:30:01.507: INFO: created pod pod-service-account-defaultsa-mountspec
    Jul 29 16:30:01.507: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
    Jul 29 16:30:01.517: INFO: created pod pod-service-account-mountsa-mountspec
    Jul 29 16:30:01.517: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
    Jul 29 16:30:01.527: INFO: created pod pod-service-account-nomountsa-mountspec
    Jul 29 16:30:01.527: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
    Jul 29 16:30:01.567: INFO: created pod pod-service-account-defaultsa-nomountspec
    Jul 29 16:30:01.567: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
    Jul 29 16:30:01.585: INFO: created pod pod-service-account-mountsa-nomountspec
    Jul 29 16:30:01.585: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
    Jul 29 16:30:01.606: INFO: created pod pod-service-account-nomountsa-nomountspec
    Jul 29 16:30:01.606: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Jul 29 16:30:01.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-1147" for this suite. 07/29/23 16:30:01.626
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] Deployment
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:30:01.663
Jul 29 16:30:01.664: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename deployment 07/29/23 16:30:01.667
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:30:01.835
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:30:01.841
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
STEP: creating a Deployment 07/29/23 16:30:01.853
Jul 29 16:30:01.853: INFO: Creating simple deployment test-deployment-k7zjk
Jul 29 16:30:01.913: INFO: deployment "test-deployment-k7zjk" doesn't have the required revision set
Jul 29 16:30:03.942: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 16, 30, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 16, 30, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 16, 30, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 16, 30, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-k7zjk-777898ffcc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 29 16:30:05.951: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 16, 30, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 16, 30, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 16, 30, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 16, 30, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-k7zjk-777898ffcc\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Getting /status 07/29/23 16:30:07.97
Jul 29 16:30:07.984: INFO: Deployment test-deployment-k7zjk has Conditions: [{Available True 2023-07-29 16:30:06 +0000 UTC 2023-07-29 16:30:06 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-07-29 16:30:06 +0000 UTC 2023-07-29 16:30:01 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-k7zjk-777898ffcc" has successfully progressed.}]
STEP: updating Deployment Status 07/29/23 16:30:07.984
Jul 29 16:30:08.012: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 16, 30, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 16, 30, 6, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 16, 30, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 16, 30, 1, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-k7zjk-777898ffcc\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated 07/29/23 16:30:08.012
Jul 29 16:30:08.016: INFO: Observed &Deployment event: ADDED
Jul 29 16:30:08.017: INFO: Observed Deployment test-deployment-k7zjk in namespace deployment-2415 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-07-29 16:30:01 +0000 UTC 2023-07-29 16:30:01 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-k7zjk-777898ffcc"}
Jul 29 16:30:08.017: INFO: Observed &Deployment event: MODIFIED
Jul 29 16:30:08.017: INFO: Observed Deployment test-deployment-k7zjk in namespace deployment-2415 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-07-29 16:30:01 +0000 UTC 2023-07-29 16:30:01 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-k7zjk-777898ffcc"}
Jul 29 16:30:08.017: INFO: Observed Deployment test-deployment-k7zjk in namespace deployment-2415 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-07-29 16:30:02 +0000 UTC 2023-07-29 16:30:02 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jul 29 16:30:08.018: INFO: Observed &Deployment event: MODIFIED
Jul 29 16:30:08.018: INFO: Observed Deployment test-deployment-k7zjk in namespace deployment-2415 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-07-29 16:30:02 +0000 UTC 2023-07-29 16:30:02 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jul 29 16:30:08.018: INFO: Observed Deployment test-deployment-k7zjk in namespace deployment-2415 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-07-29 16:30:02 +0000 UTC 2023-07-29 16:30:01 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-k7zjk-777898ffcc" is progressing.}
Jul 29 16:30:08.019: INFO: Observed &Deployment event: MODIFIED
Jul 29 16:30:08.019: INFO: Observed Deployment test-deployment-k7zjk in namespace deployment-2415 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-07-29 16:30:06 +0000 UTC 2023-07-29 16:30:06 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jul 29 16:30:08.019: INFO: Observed Deployment test-deployment-k7zjk in namespace deployment-2415 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-07-29 16:30:06 +0000 UTC 2023-07-29 16:30:01 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-k7zjk-777898ffcc" has successfully progressed.}
Jul 29 16:30:08.019: INFO: Observed &Deployment event: MODIFIED
Jul 29 16:30:08.019: INFO: Observed Deployment test-deployment-k7zjk in namespace deployment-2415 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-07-29 16:30:06 +0000 UTC 2023-07-29 16:30:06 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jul 29 16:30:08.019: INFO: Observed Deployment test-deployment-k7zjk in namespace deployment-2415 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-07-29 16:30:06 +0000 UTC 2023-07-29 16:30:01 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-k7zjk-777898ffcc" has successfully progressed.}
Jul 29 16:30:08.020: INFO: Found Deployment test-deployment-k7zjk in namespace deployment-2415 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jul 29 16:30:08.020: INFO: Deployment test-deployment-k7zjk has an updated status
STEP: patching the Statefulset Status 07/29/23 16:30:08.02
Jul 29 16:30:08.020: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jul 29 16:30:08.033: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched 07/29/23 16:30:08.033
Jul 29 16:30:08.036: INFO: Observed &Deployment event: ADDED
Jul 29 16:30:08.036: INFO: Observed deployment test-deployment-k7zjk in namespace deployment-2415 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-07-29 16:30:01 +0000 UTC 2023-07-29 16:30:01 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-k7zjk-777898ffcc"}
Jul 29 16:30:08.036: INFO: Observed &Deployment event: MODIFIED
Jul 29 16:30:08.036: INFO: Observed deployment test-deployment-k7zjk in namespace deployment-2415 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-07-29 16:30:01 +0000 UTC 2023-07-29 16:30:01 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-k7zjk-777898ffcc"}
Jul 29 16:30:08.036: INFO: Observed deployment test-deployment-k7zjk in namespace deployment-2415 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-07-29 16:30:02 +0000 UTC 2023-07-29 16:30:02 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jul 29 16:30:08.036: INFO: Observed &Deployment event: MODIFIED
Jul 29 16:30:08.036: INFO: Observed deployment test-deployment-k7zjk in namespace deployment-2415 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-07-29 16:30:02 +0000 UTC 2023-07-29 16:30:02 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jul 29 16:30:08.036: INFO: Observed deployment test-deployment-k7zjk in namespace deployment-2415 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-07-29 16:30:02 +0000 UTC 2023-07-29 16:30:01 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-k7zjk-777898ffcc" is progressing.}
Jul 29 16:30:08.037: INFO: Observed &Deployment event: MODIFIED
Jul 29 16:30:08.038: INFO: Observed deployment test-deployment-k7zjk in namespace deployment-2415 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-07-29 16:30:06 +0000 UTC 2023-07-29 16:30:06 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jul 29 16:30:08.038: INFO: Observed deployment test-deployment-k7zjk in namespace deployment-2415 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-07-29 16:30:06 +0000 UTC 2023-07-29 16:30:01 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-k7zjk-777898ffcc" has successfully progressed.}
Jul 29 16:30:08.038: INFO: Observed &Deployment event: MODIFIED
Jul 29 16:30:08.039: INFO: Observed deployment test-deployment-k7zjk in namespace deployment-2415 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-07-29 16:30:06 +0000 UTC 2023-07-29 16:30:06 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jul 29 16:30:08.039: INFO: Observed deployment test-deployment-k7zjk in namespace deployment-2415 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-07-29 16:30:06 +0000 UTC 2023-07-29 16:30:01 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-k7zjk-777898ffcc" has successfully progressed.}
Jul 29 16:30:08.039: INFO: Observed deployment test-deployment-k7zjk in namespace deployment-2415 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jul 29 16:30:08.039: INFO: Observed &Deployment event: MODIFIED
Jul 29 16:30:08.039: INFO: Found deployment test-deployment-k7zjk in namespace deployment-2415 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Jul 29 16:30:08.039: INFO: Deployment test-deployment-k7zjk has a patched status
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jul 29 16:30:08.049: INFO: Deployment "test-deployment-k7zjk":
&Deployment{ObjectMeta:{test-deployment-k7zjk  deployment-2415  94255766-e3a7-4331-802f-6a4645da46fb 23831 1 2023-07-29 16:30:01 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-07-29 16:30:01 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-29 16:30:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status} {e2e.test Update apps/v1 2023-07-29 16:30:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003e7f458 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jul 29 16:30:08.057: INFO: New ReplicaSet "test-deployment-k7zjk-777898ffcc" of Deployment "test-deployment-k7zjk":
&ReplicaSet{ObjectMeta:{test-deployment-k7zjk-777898ffcc  deployment-2415  4d8e76dc-a724-4925-bd67-bd0f1787108b 23782 1 2023-07-29 16:30:01 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-k7zjk 94255766-e3a7-4331-802f-6a4645da46fb 0xc006e34ec7 0xc006e34ec8}] [] [{kube-controller-manager Update apps/v1 2023-07-29 16:30:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"94255766-e3a7-4331-802f-6a4645da46fb\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-29 16:30:06 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 777898ffcc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006e34f78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jul 29 16:30:08.063: INFO: Pod "test-deployment-k7zjk-777898ffcc-m6xq6" is available:
&Pod{ObjectMeta:{test-deployment-k7zjk-777898ffcc-m6xq6 test-deployment-k7zjk-777898ffcc- deployment-2415  f398941a-15a1-42b8-974e-e55ef345d8aa 23781 0 2023-07-29 16:30:01 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[] [{apps/v1 ReplicaSet test-deployment-k7zjk-777898ffcc 4d8e76dc-a724-4925-bd67-bd0f1787108b 0xc006e35320 0xc006e35321}] [] [{kube-controller-manager Update v1 2023-07-29 16:30:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4d8e76dc-a724-4925-bd67-bd0f1787108b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 16:30:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.65.249\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7dhw5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7dhw5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wa4quivohpee-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:30:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:30:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:30:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:30:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.234,PodIP:10.233.65.249,StartTime:2023-07-29 16:30:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-29 16:30:03 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://6e2ee6c999a14ffff698a2db8fbab336bb55da908aca516c21faa0e20e064ca6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.65.249,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jul 29 16:30:08.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2415" for this suite. 07/29/23 16:30:08.071
{"msg":"PASSED [sig-apps] Deployment should validate Deployment Status endpoints [Conformance]","completed":219,"skipped":3986,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.418 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:30:01.663
    Jul 29 16:30:01.664: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename deployment 07/29/23 16:30:01.667
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:30:01.835
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:30:01.841
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should validate Deployment Status endpoints [Conformance]
      test/e2e/apps/deployment.go:479
    STEP: creating a Deployment 07/29/23 16:30:01.853
    Jul 29 16:30:01.853: INFO: Creating simple deployment test-deployment-k7zjk
    Jul 29 16:30:01.913: INFO: deployment "test-deployment-k7zjk" doesn't have the required revision set
    Jul 29 16:30:03.942: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 16, 30, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 16, 30, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 16, 30, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 16, 30, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-k7zjk-777898ffcc\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jul 29 16:30:05.951: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 16, 30, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 16, 30, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 16, 30, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 16, 30, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-k7zjk-777898ffcc\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Getting /status 07/29/23 16:30:07.97
    Jul 29 16:30:07.984: INFO: Deployment test-deployment-k7zjk has Conditions: [{Available True 2023-07-29 16:30:06 +0000 UTC 2023-07-29 16:30:06 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-07-29 16:30:06 +0000 UTC 2023-07-29 16:30:01 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-k7zjk-777898ffcc" has successfully progressed.}]
    STEP: updating Deployment Status 07/29/23 16:30:07.984
    Jul 29 16:30:08.012: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 16, 30, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 16, 30, 6, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 16, 30, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 16, 30, 1, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-k7zjk-777898ffcc\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Deployment status to be updated 07/29/23 16:30:08.012
    Jul 29 16:30:08.016: INFO: Observed &Deployment event: ADDED
    Jul 29 16:30:08.017: INFO: Observed Deployment test-deployment-k7zjk in namespace deployment-2415 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-07-29 16:30:01 +0000 UTC 2023-07-29 16:30:01 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-k7zjk-777898ffcc"}
    Jul 29 16:30:08.017: INFO: Observed &Deployment event: MODIFIED
    Jul 29 16:30:08.017: INFO: Observed Deployment test-deployment-k7zjk in namespace deployment-2415 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-07-29 16:30:01 +0000 UTC 2023-07-29 16:30:01 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-k7zjk-777898ffcc"}
    Jul 29 16:30:08.017: INFO: Observed Deployment test-deployment-k7zjk in namespace deployment-2415 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-07-29 16:30:02 +0000 UTC 2023-07-29 16:30:02 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jul 29 16:30:08.018: INFO: Observed &Deployment event: MODIFIED
    Jul 29 16:30:08.018: INFO: Observed Deployment test-deployment-k7zjk in namespace deployment-2415 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-07-29 16:30:02 +0000 UTC 2023-07-29 16:30:02 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jul 29 16:30:08.018: INFO: Observed Deployment test-deployment-k7zjk in namespace deployment-2415 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-07-29 16:30:02 +0000 UTC 2023-07-29 16:30:01 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-k7zjk-777898ffcc" is progressing.}
    Jul 29 16:30:08.019: INFO: Observed &Deployment event: MODIFIED
    Jul 29 16:30:08.019: INFO: Observed Deployment test-deployment-k7zjk in namespace deployment-2415 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-07-29 16:30:06 +0000 UTC 2023-07-29 16:30:06 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jul 29 16:30:08.019: INFO: Observed Deployment test-deployment-k7zjk in namespace deployment-2415 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-07-29 16:30:06 +0000 UTC 2023-07-29 16:30:01 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-k7zjk-777898ffcc" has successfully progressed.}
    Jul 29 16:30:08.019: INFO: Observed &Deployment event: MODIFIED
    Jul 29 16:30:08.019: INFO: Observed Deployment test-deployment-k7zjk in namespace deployment-2415 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-07-29 16:30:06 +0000 UTC 2023-07-29 16:30:06 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jul 29 16:30:08.019: INFO: Observed Deployment test-deployment-k7zjk in namespace deployment-2415 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-07-29 16:30:06 +0000 UTC 2023-07-29 16:30:01 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-k7zjk-777898ffcc" has successfully progressed.}
    Jul 29 16:30:08.020: INFO: Found Deployment test-deployment-k7zjk in namespace deployment-2415 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jul 29 16:30:08.020: INFO: Deployment test-deployment-k7zjk has an updated status
    STEP: patching the Statefulset Status 07/29/23 16:30:08.02
    Jul 29 16:30:08.020: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Jul 29 16:30:08.033: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Deployment status to be patched 07/29/23 16:30:08.033
    Jul 29 16:30:08.036: INFO: Observed &Deployment event: ADDED
    Jul 29 16:30:08.036: INFO: Observed deployment test-deployment-k7zjk in namespace deployment-2415 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-07-29 16:30:01 +0000 UTC 2023-07-29 16:30:01 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-k7zjk-777898ffcc"}
    Jul 29 16:30:08.036: INFO: Observed &Deployment event: MODIFIED
    Jul 29 16:30:08.036: INFO: Observed deployment test-deployment-k7zjk in namespace deployment-2415 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-07-29 16:30:01 +0000 UTC 2023-07-29 16:30:01 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-k7zjk-777898ffcc"}
    Jul 29 16:30:08.036: INFO: Observed deployment test-deployment-k7zjk in namespace deployment-2415 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-07-29 16:30:02 +0000 UTC 2023-07-29 16:30:02 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jul 29 16:30:08.036: INFO: Observed &Deployment event: MODIFIED
    Jul 29 16:30:08.036: INFO: Observed deployment test-deployment-k7zjk in namespace deployment-2415 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-07-29 16:30:02 +0000 UTC 2023-07-29 16:30:02 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jul 29 16:30:08.036: INFO: Observed deployment test-deployment-k7zjk in namespace deployment-2415 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-07-29 16:30:02 +0000 UTC 2023-07-29 16:30:01 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-k7zjk-777898ffcc" is progressing.}
    Jul 29 16:30:08.037: INFO: Observed &Deployment event: MODIFIED
    Jul 29 16:30:08.038: INFO: Observed deployment test-deployment-k7zjk in namespace deployment-2415 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-07-29 16:30:06 +0000 UTC 2023-07-29 16:30:06 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jul 29 16:30:08.038: INFO: Observed deployment test-deployment-k7zjk in namespace deployment-2415 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-07-29 16:30:06 +0000 UTC 2023-07-29 16:30:01 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-k7zjk-777898ffcc" has successfully progressed.}
    Jul 29 16:30:08.038: INFO: Observed &Deployment event: MODIFIED
    Jul 29 16:30:08.039: INFO: Observed deployment test-deployment-k7zjk in namespace deployment-2415 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-07-29 16:30:06 +0000 UTC 2023-07-29 16:30:06 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jul 29 16:30:08.039: INFO: Observed deployment test-deployment-k7zjk in namespace deployment-2415 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-07-29 16:30:06 +0000 UTC 2023-07-29 16:30:01 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-k7zjk-777898ffcc" has successfully progressed.}
    Jul 29 16:30:08.039: INFO: Observed deployment test-deployment-k7zjk in namespace deployment-2415 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jul 29 16:30:08.039: INFO: Observed &Deployment event: MODIFIED
    Jul 29 16:30:08.039: INFO: Found deployment test-deployment-k7zjk in namespace deployment-2415 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
    Jul 29 16:30:08.039: INFO: Deployment test-deployment-k7zjk has a patched status
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jul 29 16:30:08.049: INFO: Deployment "test-deployment-k7zjk":
    &Deployment{ObjectMeta:{test-deployment-k7zjk  deployment-2415  94255766-e3a7-4331-802f-6a4645da46fb 23831 1 2023-07-29 16:30:01 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-07-29 16:30:01 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-29 16:30:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status} {e2e.test Update apps/v1 2023-07-29 16:30:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003e7f458 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jul 29 16:30:08.057: INFO: New ReplicaSet "test-deployment-k7zjk-777898ffcc" of Deployment "test-deployment-k7zjk":
    &ReplicaSet{ObjectMeta:{test-deployment-k7zjk-777898ffcc  deployment-2415  4d8e76dc-a724-4925-bd67-bd0f1787108b 23782 1 2023-07-29 16:30:01 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-k7zjk 94255766-e3a7-4331-802f-6a4645da46fb 0xc006e34ec7 0xc006e34ec8}] [] [{kube-controller-manager Update apps/v1 2023-07-29 16:30:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"94255766-e3a7-4331-802f-6a4645da46fb\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-29 16:30:06 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 777898ffcc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006e34f78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jul 29 16:30:08.063: INFO: Pod "test-deployment-k7zjk-777898ffcc-m6xq6" is available:
    &Pod{ObjectMeta:{test-deployment-k7zjk-777898ffcc-m6xq6 test-deployment-k7zjk-777898ffcc- deployment-2415  f398941a-15a1-42b8-974e-e55ef345d8aa 23781 0 2023-07-29 16:30:01 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[] [{apps/v1 ReplicaSet test-deployment-k7zjk-777898ffcc 4d8e76dc-a724-4925-bd67-bd0f1787108b 0xc006e35320 0xc006e35321}] [] [{kube-controller-manager Update v1 2023-07-29 16:30:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4d8e76dc-a724-4925-bd67-bd0f1787108b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 16:30:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.65.249\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7dhw5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7dhw5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wa4quivohpee-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:30:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:30:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:30:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:30:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.234,PodIP:10.233.65.249,StartTime:2023-07-29 16:30:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-29 16:30:03 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://6e2ee6c999a14ffff698a2db8fbab336bb55da908aca516c21faa0e20e064ca6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.65.249,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jul 29 16:30:08.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-2415" for this suite. 07/29/23 16:30:08.071
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:30:08.084
Jul 29 16:30:08.084: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename security-context 07/29/23 16:30:08.086
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:30:08.113
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:30:08.117
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 07/29/23 16:30:08.12
Jul 29 16:30:08.135: INFO: Waiting up to 5m0s for pod "security-context-e281aff2-2411-4e9d-848a-ebdebf11c819" in namespace "security-context-3210" to be "Succeeded or Failed"
Jul 29 16:30:08.141: INFO: Pod "security-context-e281aff2-2411-4e9d-848a-ebdebf11c819": Phase="Pending", Reason="", readiness=false. Elapsed: 6.015886ms
Jul 29 16:30:10.150: INFO: Pod "security-context-e281aff2-2411-4e9d-848a-ebdebf11c819": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014388187s
Jul 29 16:30:12.150: INFO: Pod "security-context-e281aff2-2411-4e9d-848a-ebdebf11c819": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014283456s
Jul 29 16:30:14.147: INFO: Pod "security-context-e281aff2-2411-4e9d-848a-ebdebf11c819": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012111263s
STEP: Saw pod success 07/29/23 16:30:14.148
Jul 29 16:30:14.148: INFO: Pod "security-context-e281aff2-2411-4e9d-848a-ebdebf11c819" satisfied condition "Succeeded or Failed"
Jul 29 16:30:14.153: INFO: Trying to get logs from node wa4quivohpee-3 pod security-context-e281aff2-2411-4e9d-848a-ebdebf11c819 container test-container: <nil>
STEP: delete the pod 07/29/23 16:30:14.189
Jul 29 16:30:14.215: INFO: Waiting for pod security-context-e281aff2-2411-4e9d-848a-ebdebf11c819 to disappear
Jul 29 16:30:14.219: INFO: Pod security-context-e281aff2-2411-4e9d-848a-ebdebf11c819 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Jul 29 16:30:14.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-3210" for this suite. 07/29/23 16:30:14.23
{"msg":"PASSED [sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","completed":220,"skipped":3998,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.158 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:30:08.084
    Jul 29 16:30:08.084: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename security-context 07/29/23 16:30:08.086
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:30:08.113
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:30:08.117
    [It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:132
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 07/29/23 16:30:08.12
    Jul 29 16:30:08.135: INFO: Waiting up to 5m0s for pod "security-context-e281aff2-2411-4e9d-848a-ebdebf11c819" in namespace "security-context-3210" to be "Succeeded or Failed"
    Jul 29 16:30:08.141: INFO: Pod "security-context-e281aff2-2411-4e9d-848a-ebdebf11c819": Phase="Pending", Reason="", readiness=false. Elapsed: 6.015886ms
    Jul 29 16:30:10.150: INFO: Pod "security-context-e281aff2-2411-4e9d-848a-ebdebf11c819": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014388187s
    Jul 29 16:30:12.150: INFO: Pod "security-context-e281aff2-2411-4e9d-848a-ebdebf11c819": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014283456s
    Jul 29 16:30:14.147: INFO: Pod "security-context-e281aff2-2411-4e9d-848a-ebdebf11c819": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012111263s
    STEP: Saw pod success 07/29/23 16:30:14.148
    Jul 29 16:30:14.148: INFO: Pod "security-context-e281aff2-2411-4e9d-848a-ebdebf11c819" satisfied condition "Succeeded or Failed"
    Jul 29 16:30:14.153: INFO: Trying to get logs from node wa4quivohpee-3 pod security-context-e281aff2-2411-4e9d-848a-ebdebf11c819 container test-container: <nil>
    STEP: delete the pod 07/29/23 16:30:14.189
    Jul 29 16:30:14.215: INFO: Waiting for pod security-context-e281aff2-2411-4e9d-848a-ebdebf11c819 to disappear
    Jul 29 16:30:14.219: INFO: Pod security-context-e281aff2-2411-4e9d-848a-ebdebf11c819 no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Jul 29 16:30:14.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-3210" for this suite. 07/29/23 16:30:14.23
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:30:14.246
Jul 29 16:30:14.246: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename configmap 07/29/23 16:30:14.248
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:30:14.277
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:30:14.281
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46
STEP: Creating configMap with name configmap-test-volume-92659483-d1c5-4aa8-bbf6-f8f04d54ffc4 07/29/23 16:30:14.285
STEP: Creating a pod to test consume configMaps 07/29/23 16:30:14.295
Jul 29 16:30:14.314: INFO: Waiting up to 5m0s for pod "pod-configmaps-3dcaeab0-92d8-47cc-8886-87eeeb9713ff" in namespace "configmap-1879" to be "Succeeded or Failed"
Jul 29 16:30:14.327: INFO: Pod "pod-configmaps-3dcaeab0-92d8-47cc-8886-87eeeb9713ff": Phase="Pending", Reason="", readiness=false. Elapsed: 12.295584ms
Jul 29 16:30:16.335: INFO: Pod "pod-configmaps-3dcaeab0-92d8-47cc-8886-87eeeb9713ff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021009964s
Jul 29 16:30:18.336: INFO: Pod "pod-configmaps-3dcaeab0-92d8-47cc-8886-87eeeb9713ff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021463961s
STEP: Saw pod success 07/29/23 16:30:18.336
Jul 29 16:30:18.336: INFO: Pod "pod-configmaps-3dcaeab0-92d8-47cc-8886-87eeeb9713ff" satisfied condition "Succeeded or Failed"
Jul 29 16:30:18.341: INFO: Trying to get logs from node wa4quivohpee-3 pod pod-configmaps-3dcaeab0-92d8-47cc-8886-87eeeb9713ff container agnhost-container: <nil>
STEP: delete the pod 07/29/23 16:30:18.355
Jul 29 16:30:18.376: INFO: Waiting for pod pod-configmaps-3dcaeab0-92d8-47cc-8886-87eeeb9713ff to disappear
Jul 29 16:30:18.381: INFO: Pod pod-configmaps-3dcaeab0-92d8-47cc-8886-87eeeb9713ff no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jul 29 16:30:18.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1879" for this suite. 07/29/23 16:30:18.389
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","completed":221,"skipped":4004,"failed":0}
------------------------------
â€¢ [4.154 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:30:14.246
    Jul 29 16:30:14.246: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename configmap 07/29/23 16:30:14.248
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:30:14.277
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:30:14.281
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:46
    STEP: Creating configMap with name configmap-test-volume-92659483-d1c5-4aa8-bbf6-f8f04d54ffc4 07/29/23 16:30:14.285
    STEP: Creating a pod to test consume configMaps 07/29/23 16:30:14.295
    Jul 29 16:30:14.314: INFO: Waiting up to 5m0s for pod "pod-configmaps-3dcaeab0-92d8-47cc-8886-87eeeb9713ff" in namespace "configmap-1879" to be "Succeeded or Failed"
    Jul 29 16:30:14.327: INFO: Pod "pod-configmaps-3dcaeab0-92d8-47cc-8886-87eeeb9713ff": Phase="Pending", Reason="", readiness=false. Elapsed: 12.295584ms
    Jul 29 16:30:16.335: INFO: Pod "pod-configmaps-3dcaeab0-92d8-47cc-8886-87eeeb9713ff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021009964s
    Jul 29 16:30:18.336: INFO: Pod "pod-configmaps-3dcaeab0-92d8-47cc-8886-87eeeb9713ff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021463961s
    STEP: Saw pod success 07/29/23 16:30:18.336
    Jul 29 16:30:18.336: INFO: Pod "pod-configmaps-3dcaeab0-92d8-47cc-8886-87eeeb9713ff" satisfied condition "Succeeded or Failed"
    Jul 29 16:30:18.341: INFO: Trying to get logs from node wa4quivohpee-3 pod pod-configmaps-3dcaeab0-92d8-47cc-8886-87eeeb9713ff container agnhost-container: <nil>
    STEP: delete the pod 07/29/23 16:30:18.355
    Jul 29 16:30:18.376: INFO: Waiting for pod pod-configmaps-3dcaeab0-92d8-47cc-8886-87eeeb9713ff to disappear
    Jul 29 16:30:18.381: INFO: Pod pod-configmaps-3dcaeab0-92d8-47cc-8886-87eeeb9713ff no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jul 29 16:30:18.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-1879" for this suite. 07/29/23 16:30:18.389
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:30:18.408
Jul 29 16:30:18.409: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename sched-preemption 07/29/23 16:30:18.411
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:30:18.444
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:30:18.45
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Jul 29 16:30:18.476: INFO: Waiting up to 1m0s for all nodes to be ready
Jul 29 16:31:18.546: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218
STEP: Create pods that use 4/5 of node resources. 07/29/23 16:31:18.552
Jul 29 16:31:18.596: INFO: Created pod: pod0-0-sched-preemption-low-priority
Jul 29 16:31:18.608: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Jul 29 16:31:18.644: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Jul 29 16:31:18.666: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Jul 29 16:31:18.696: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Jul 29 16:31:18.705: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 07/29/23 16:31:18.705
Jul 29 16:31:18.706: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-5108" to be "running"
Jul 29 16:31:18.715: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 8.712757ms
Jul 29 16:31:20.724: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017967614s
Jul 29 16:31:22.731: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025102051s
Jul 29 16:31:24.724: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.018295669s
Jul 29 16:31:26.723: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 8.017179603s
Jul 29 16:31:28.745: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 10.038747799s
Jul 29 16:31:30.727: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 12.021585696s
Jul 29 16:31:30.728: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Jul 29 16:31:30.728: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-5108" to be "running"
Jul 29 16:31:30.734: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.113107ms
Jul 29 16:31:30.734: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Jul 29 16:31:30.734: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-5108" to be "running"
Jul 29 16:31:30.738: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.551527ms
Jul 29 16:31:30.739: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Jul 29 16:31:30.739: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-5108" to be "running"
Jul 29 16:31:30.744: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.601906ms
Jul 29 16:31:30.745: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
Jul 29 16:31:30.745: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-5108" to be "running"
Jul 29 16:31:30.750: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.333105ms
Jul 29 16:31:30.750: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
Jul 29 16:31:30.751: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-5108" to be "running"
Jul 29 16:31:30.755: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.386932ms
Jul 29 16:31:30.755: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a critical pod that use same resources as that of a lower priority pod 07/29/23 16:31:30.755
Jul 29 16:31:30.773: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
Jul 29 16:31:30.779: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.035141ms
Jul 29 16:31:32.823: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.049981786s
Jul 29 16:31:34.791: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017597406s
Jul 29 16:31:36.787: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.014232465s
Jul 29 16:31:36.787: INFO: Pod "critical-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Jul 29 16:31:36.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-5108" for this suite. 07/29/23 16:31:36.865
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","completed":222,"skipped":4016,"failed":0}
------------------------------
â€¢ [SLOW TEST] [78.538 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:30:18.408
    Jul 29 16:30:18.409: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename sched-preemption 07/29/23 16:30:18.411
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:30:18.444
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:30:18.45
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Jul 29 16:30:18.476: INFO: Waiting up to 1m0s for all nodes to be ready
    Jul 29 16:31:18.546: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates lower priority pod preemption by critical pod [Conformance]
      test/e2e/scheduling/preemption.go:218
    STEP: Create pods that use 4/5 of node resources. 07/29/23 16:31:18.552
    Jul 29 16:31:18.596: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Jul 29 16:31:18.608: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Jul 29 16:31:18.644: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Jul 29 16:31:18.666: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    Jul 29 16:31:18.696: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    Jul 29 16:31:18.705: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 07/29/23 16:31:18.705
    Jul 29 16:31:18.706: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-5108" to be "running"
    Jul 29 16:31:18.715: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 8.712757ms
    Jul 29 16:31:20.724: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017967614s
    Jul 29 16:31:22.731: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025102051s
    Jul 29 16:31:24.724: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.018295669s
    Jul 29 16:31:26.723: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 8.017179603s
    Jul 29 16:31:28.745: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 10.038747799s
    Jul 29 16:31:30.727: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 12.021585696s
    Jul 29 16:31:30.728: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Jul 29 16:31:30.728: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-5108" to be "running"
    Jul 29 16:31:30.734: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.113107ms
    Jul 29 16:31:30.734: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Jul 29 16:31:30.734: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-5108" to be "running"
    Jul 29 16:31:30.738: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.551527ms
    Jul 29 16:31:30.739: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Jul 29 16:31:30.739: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-5108" to be "running"
    Jul 29 16:31:30.744: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.601906ms
    Jul 29 16:31:30.745: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    Jul 29 16:31:30.745: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-5108" to be "running"
    Jul 29 16:31:30.750: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.333105ms
    Jul 29 16:31:30.750: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    Jul 29 16:31:30.751: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-5108" to be "running"
    Jul 29 16:31:30.755: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.386932ms
    Jul 29 16:31:30.755: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a critical pod that use same resources as that of a lower priority pod 07/29/23 16:31:30.755
    Jul 29 16:31:30.773: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
    Jul 29 16:31:30.779: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.035141ms
    Jul 29 16:31:32.823: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.049981786s
    Jul 29 16:31:34.791: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017597406s
    Jul 29 16:31:36.787: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.014232465s
    Jul 29 16:31:36.787: INFO: Pod "critical-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Jul 29 16:31:36.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-5108" for this suite. 07/29/23 16:31:36.865
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:31:36.953
Jul 29 16:31:36.953: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 07/29/23 16:31:36.956
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:31:36.996
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:31:37.001
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
STEP: Setting up the test 07/29/23 16:31:37.006
STEP: Creating hostNetwork=false pod 07/29/23 16:31:37.006
Jul 29 16:31:37.021: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-1683" to be "running and ready"
Jul 29 16:31:37.029: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 8.02865ms
Jul 29 16:31:37.029: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Jul 29 16:31:39.039: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.01849462s
Jul 29 16:31:39.039: INFO: The phase of Pod test-pod is Running (Ready = true)
Jul 29 16:31:39.039: INFO: Pod "test-pod" satisfied condition "running and ready"
STEP: Creating hostNetwork=true pod 07/29/23 16:31:39.044
Jul 29 16:31:39.056: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-1683" to be "running and ready"
Jul 29 16:31:39.064: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.821654ms
Jul 29 16:31:39.064: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Jul 29 16:31:41.071: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.014491172s
Jul 29 16:31:41.071: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
Jul 29 16:31:41.071: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
STEP: Running the test 07/29/23 16:31:41.076
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 07/29/23 16:31:41.077
Jul 29 16:31:41.077: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1683 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 29 16:31:41.077: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
Jul 29 16:31:41.081: INFO: ExecWithOptions: Clientset creation
Jul 29 16:31:41.081: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1683/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jul 29 16:31:41.195: INFO: Exec stderr: ""
Jul 29 16:31:41.195: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1683 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 29 16:31:41.195: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
Jul 29 16:31:41.197: INFO: ExecWithOptions: Clientset creation
Jul 29 16:31:41.198: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1683/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jul 29 16:31:41.289: INFO: Exec stderr: ""
Jul 29 16:31:41.289: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1683 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 29 16:31:41.289: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
Jul 29 16:31:41.291: INFO: ExecWithOptions: Clientset creation
Jul 29 16:31:41.292: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1683/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jul 29 16:31:41.417: INFO: Exec stderr: ""
Jul 29 16:31:41.417: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1683 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 29 16:31:41.417: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
Jul 29 16:31:41.421: INFO: ExecWithOptions: Clientset creation
Jul 29 16:31:41.421: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1683/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jul 29 16:31:41.512: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 07/29/23 16:31:41.513
Jul 29 16:31:41.523: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1683 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 29 16:31:41.523: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
Jul 29 16:31:41.527: INFO: ExecWithOptions: Clientset creation
Jul 29 16:31:41.527: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1683/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Jul 29 16:31:41.636: INFO: Exec stderr: ""
Jul 29 16:31:41.637: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1683 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 29 16:31:41.637: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
Jul 29 16:31:41.639: INFO: ExecWithOptions: Clientset creation
Jul 29 16:31:41.639: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1683/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Jul 29 16:31:41.743: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 07/29/23 16:31:41.744
Jul 29 16:31:41.744: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1683 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 29 16:31:41.745: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
Jul 29 16:31:41.749: INFO: ExecWithOptions: Clientset creation
Jul 29 16:31:41.749: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1683/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jul 29 16:31:41.878: INFO: Exec stderr: ""
Jul 29 16:31:41.878: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1683 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 29 16:31:41.878: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
Jul 29 16:31:41.880: INFO: ExecWithOptions: Clientset creation
Jul 29 16:31:41.881: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1683/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jul 29 16:31:42.025: INFO: Exec stderr: ""
Jul 29 16:31:42.025: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1683 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 29 16:31:42.026: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
Jul 29 16:31:42.028: INFO: ExecWithOptions: Clientset creation
Jul 29 16:31:42.028: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1683/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jul 29 16:31:42.150: INFO: Exec stderr: ""
Jul 29 16:31:42.150: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1683 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 29 16:31:42.150: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
Jul 29 16:31:42.153: INFO: ExecWithOptions: Clientset creation
Jul 29 16:31:42.153: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1683/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jul 29 16:31:42.267: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/framework.go:187
Jul 29 16:31:42.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-1683" for this suite. 07/29/23 16:31:42.277
{"msg":"PASSED [sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","completed":223,"skipped":4028,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.338 seconds]
[sig-node] KubeletManagedEtcHosts
test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:31:36.953
    Jul 29 16:31:36.953: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 07/29/23 16:31:36.956
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:31:36.996
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:31:37.001
    [It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet_etc_hosts.go:63
    STEP: Setting up the test 07/29/23 16:31:37.006
    STEP: Creating hostNetwork=false pod 07/29/23 16:31:37.006
    Jul 29 16:31:37.021: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-1683" to be "running and ready"
    Jul 29 16:31:37.029: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 8.02865ms
    Jul 29 16:31:37.029: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Jul 29 16:31:39.039: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.01849462s
    Jul 29 16:31:39.039: INFO: The phase of Pod test-pod is Running (Ready = true)
    Jul 29 16:31:39.039: INFO: Pod "test-pod" satisfied condition "running and ready"
    STEP: Creating hostNetwork=true pod 07/29/23 16:31:39.044
    Jul 29 16:31:39.056: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-1683" to be "running and ready"
    Jul 29 16:31:39.064: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.821654ms
    Jul 29 16:31:39.064: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
    Jul 29 16:31:41.071: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.014491172s
    Jul 29 16:31:41.071: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
    Jul 29 16:31:41.071: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
    STEP: Running the test 07/29/23 16:31:41.076
    STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 07/29/23 16:31:41.077
    Jul 29 16:31:41.077: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1683 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jul 29 16:31:41.077: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    Jul 29 16:31:41.081: INFO: ExecWithOptions: Clientset creation
    Jul 29 16:31:41.081: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1683/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jul 29 16:31:41.195: INFO: Exec stderr: ""
    Jul 29 16:31:41.195: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1683 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jul 29 16:31:41.195: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    Jul 29 16:31:41.197: INFO: ExecWithOptions: Clientset creation
    Jul 29 16:31:41.198: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1683/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jul 29 16:31:41.289: INFO: Exec stderr: ""
    Jul 29 16:31:41.289: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1683 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jul 29 16:31:41.289: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    Jul 29 16:31:41.291: INFO: ExecWithOptions: Clientset creation
    Jul 29 16:31:41.292: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1683/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jul 29 16:31:41.417: INFO: Exec stderr: ""
    Jul 29 16:31:41.417: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1683 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jul 29 16:31:41.417: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    Jul 29 16:31:41.421: INFO: ExecWithOptions: Clientset creation
    Jul 29 16:31:41.421: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1683/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jul 29 16:31:41.512: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 07/29/23 16:31:41.513
    Jul 29 16:31:41.523: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1683 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jul 29 16:31:41.523: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    Jul 29 16:31:41.527: INFO: ExecWithOptions: Clientset creation
    Jul 29 16:31:41.527: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1683/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Jul 29 16:31:41.636: INFO: Exec stderr: ""
    Jul 29 16:31:41.637: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1683 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jul 29 16:31:41.637: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    Jul 29 16:31:41.639: INFO: ExecWithOptions: Clientset creation
    Jul 29 16:31:41.639: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1683/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Jul 29 16:31:41.743: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 07/29/23 16:31:41.744
    Jul 29 16:31:41.744: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1683 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jul 29 16:31:41.745: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    Jul 29 16:31:41.749: INFO: ExecWithOptions: Clientset creation
    Jul 29 16:31:41.749: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1683/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jul 29 16:31:41.878: INFO: Exec stderr: ""
    Jul 29 16:31:41.878: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1683 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jul 29 16:31:41.878: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    Jul 29 16:31:41.880: INFO: ExecWithOptions: Clientset creation
    Jul 29 16:31:41.881: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1683/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jul 29 16:31:42.025: INFO: Exec stderr: ""
    Jul 29 16:31:42.025: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1683 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jul 29 16:31:42.026: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    Jul 29 16:31:42.028: INFO: ExecWithOptions: Clientset creation
    Jul 29 16:31:42.028: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1683/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jul 29 16:31:42.150: INFO: Exec stderr: ""
    Jul 29 16:31:42.150: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1683 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jul 29 16:31:42.150: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    Jul 29 16:31:42.153: INFO: ExecWithOptions: Clientset creation
    Jul 29 16:31:42.153: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1683/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jul 29 16:31:42.267: INFO: Exec stderr: ""
    [AfterEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/framework.go:187
    Jul 29 16:31:42.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "e2e-kubelet-etc-hosts-1683" for this suite. 07/29/23 16:31:42.277
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:31:42.294
Jul 29 16:31:42.294: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename daemonsets 07/29/23 16:31:42.297
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:31:42.329
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:31:42.334
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193
Jul 29 16:31:42.377: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes. 07/29/23 16:31:42.393
Jul 29 16:31:42.401: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jul 29 16:31:42.402: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched. 07/29/23 16:31:42.402
Jul 29 16:31:42.446: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jul 29 16:31:42.446: INFO: Node wa4quivohpee-1 is running 0 daemon pod, expected 1
Jul 29 16:31:43.461: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jul 29 16:31:43.461: INFO: Node wa4quivohpee-1 is running 0 daemon pod, expected 1
Jul 29 16:31:44.461: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jul 29 16:31:44.462: INFO: Node wa4quivohpee-1 is running 0 daemon pod, expected 1
Jul 29 16:31:45.453: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jul 29 16:31:45.453: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled 07/29/23 16:31:45.458
Jul 29 16:31:45.486: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jul 29 16:31:45.486: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
Jul 29 16:31:46.494: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jul 29 16:31:46.494: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 07/29/23 16:31:46.494
Jul 29 16:31:46.525: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jul 29 16:31:46.525: INFO: Node wa4quivohpee-1 is running 0 daemon pod, expected 1
Jul 29 16:31:47.534: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jul 29 16:31:47.534: INFO: Node wa4quivohpee-1 is running 0 daemon pod, expected 1
Jul 29 16:31:48.545: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jul 29 16:31:48.545: INFO: Node wa4quivohpee-1 is running 0 daemon pod, expected 1
Jul 29 16:31:49.533: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jul 29 16:31:49.533: INFO: Node wa4quivohpee-1 is running 0 daemon pod, expected 1
Jul 29 16:31:50.535: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jul 29 16:31:50.535: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 07/29/23 16:31:50.546
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4532, will wait for the garbage collector to delete the pods 07/29/23 16:31:50.546
Jul 29 16:31:50.623: INFO: Deleting DaemonSet.extensions daemon-set took: 20.650916ms
Jul 29 16:31:50.724: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.000761ms
Jul 29 16:31:53.152: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jul 29 16:31:53.152: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jul 29 16:31:53.156: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"24417"},"items":null}

Jul 29 16:31:53.162: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"24417"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Jul 29 16:31:53.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4532" for this suite. 07/29/23 16:31:53.22
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","completed":224,"skipped":4056,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.936 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:31:42.294
    Jul 29 16:31:42.294: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename daemonsets 07/29/23 16:31:42.297
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:31:42.329
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:31:42.334
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should run and stop complex daemon [Conformance]
      test/e2e/apps/daemon_set.go:193
    Jul 29 16:31:42.377: INFO: Creating daemon "daemon-set" with a node selector
    STEP: Initially, daemon pods should not be running on any nodes. 07/29/23 16:31:42.393
    Jul 29 16:31:42.401: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jul 29 16:31:42.402: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Change node label to blue, check that daemon pod is launched. 07/29/23 16:31:42.402
    Jul 29 16:31:42.446: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jul 29 16:31:42.446: INFO: Node wa4quivohpee-1 is running 0 daemon pod, expected 1
    Jul 29 16:31:43.461: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jul 29 16:31:43.461: INFO: Node wa4quivohpee-1 is running 0 daemon pod, expected 1
    Jul 29 16:31:44.461: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jul 29 16:31:44.462: INFO: Node wa4quivohpee-1 is running 0 daemon pod, expected 1
    Jul 29 16:31:45.453: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jul 29 16:31:45.453: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    STEP: Update the node label to green, and wait for daemons to be unscheduled 07/29/23 16:31:45.458
    Jul 29 16:31:45.486: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jul 29 16:31:45.486: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
    Jul 29 16:31:46.494: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jul 29 16:31:46.494: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 07/29/23 16:31:46.494
    Jul 29 16:31:46.525: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jul 29 16:31:46.525: INFO: Node wa4quivohpee-1 is running 0 daemon pod, expected 1
    Jul 29 16:31:47.534: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jul 29 16:31:47.534: INFO: Node wa4quivohpee-1 is running 0 daemon pod, expected 1
    Jul 29 16:31:48.545: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jul 29 16:31:48.545: INFO: Node wa4quivohpee-1 is running 0 daemon pod, expected 1
    Jul 29 16:31:49.533: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jul 29 16:31:49.533: INFO: Node wa4quivohpee-1 is running 0 daemon pod, expected 1
    Jul 29 16:31:50.535: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jul 29 16:31:50.535: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 07/29/23 16:31:50.546
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4532, will wait for the garbage collector to delete the pods 07/29/23 16:31:50.546
    Jul 29 16:31:50.623: INFO: Deleting DaemonSet.extensions daemon-set took: 20.650916ms
    Jul 29 16:31:50.724: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.000761ms
    Jul 29 16:31:53.152: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jul 29 16:31:53.152: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jul 29 16:31:53.156: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"24417"},"items":null}

    Jul 29 16:31:53.162: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"24417"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Jul 29 16:31:53.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-4532" for this suite. 07/29/23 16:31:53.22
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:31:53.231
Jul 29 16:31:53.232: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename downward-api 07/29/23 16:31:53.234
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:31:53.267
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:31:53.273
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260
STEP: Creating a pod to test downward API volume plugin 07/29/23 16:31:53.277
Jul 29 16:31:53.293: INFO: Waiting up to 5m0s for pod "downwardapi-volume-edecc8d6-cdb1-4eb3-8f09-d4b7cb759118" in namespace "downward-api-6632" to be "Succeeded or Failed"
Jul 29 16:31:53.299: INFO: Pod "downwardapi-volume-edecc8d6-cdb1-4eb3-8f09-d4b7cb759118": Phase="Pending", Reason="", readiness=false. Elapsed: 5.49424ms
Jul 29 16:31:55.307: INFO: Pod "downwardapi-volume-edecc8d6-cdb1-4eb3-8f09-d4b7cb759118": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01316186s
Jul 29 16:31:57.309: INFO: Pod "downwardapi-volume-edecc8d6-cdb1-4eb3-8f09-d4b7cb759118": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015416197s
STEP: Saw pod success 07/29/23 16:31:57.309
Jul 29 16:31:57.310: INFO: Pod "downwardapi-volume-edecc8d6-cdb1-4eb3-8f09-d4b7cb759118" satisfied condition "Succeeded or Failed"
Jul 29 16:31:57.314: INFO: Trying to get logs from node wa4quivohpee-3 pod downwardapi-volume-edecc8d6-cdb1-4eb3-8f09-d4b7cb759118 container client-container: <nil>
STEP: delete the pod 07/29/23 16:31:57.344
Jul 29 16:31:57.363: INFO: Waiting for pod downwardapi-volume-edecc8d6-cdb1-4eb3-8f09-d4b7cb759118 to disappear
Jul 29 16:31:57.369: INFO: Pod downwardapi-volume-edecc8d6-cdb1-4eb3-8f09-d4b7cb759118 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jul 29 16:31:57.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6632" for this suite. 07/29/23 16:31:57.381
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","completed":225,"skipped":4062,"failed":0}
------------------------------
â€¢ [4.164 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:31:53.231
    Jul 29 16:31:53.232: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename downward-api 07/29/23 16:31:53.234
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:31:53.267
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:31:53.273
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:260
    STEP: Creating a pod to test downward API volume plugin 07/29/23 16:31:53.277
    Jul 29 16:31:53.293: INFO: Waiting up to 5m0s for pod "downwardapi-volume-edecc8d6-cdb1-4eb3-8f09-d4b7cb759118" in namespace "downward-api-6632" to be "Succeeded or Failed"
    Jul 29 16:31:53.299: INFO: Pod "downwardapi-volume-edecc8d6-cdb1-4eb3-8f09-d4b7cb759118": Phase="Pending", Reason="", readiness=false. Elapsed: 5.49424ms
    Jul 29 16:31:55.307: INFO: Pod "downwardapi-volume-edecc8d6-cdb1-4eb3-8f09-d4b7cb759118": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01316186s
    Jul 29 16:31:57.309: INFO: Pod "downwardapi-volume-edecc8d6-cdb1-4eb3-8f09-d4b7cb759118": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015416197s
    STEP: Saw pod success 07/29/23 16:31:57.309
    Jul 29 16:31:57.310: INFO: Pod "downwardapi-volume-edecc8d6-cdb1-4eb3-8f09-d4b7cb759118" satisfied condition "Succeeded or Failed"
    Jul 29 16:31:57.314: INFO: Trying to get logs from node wa4quivohpee-3 pod downwardapi-volume-edecc8d6-cdb1-4eb3-8f09-d4b7cb759118 container client-container: <nil>
    STEP: delete the pod 07/29/23 16:31:57.344
    Jul 29 16:31:57.363: INFO: Waiting for pod downwardapi-volume-edecc8d6-cdb1-4eb3-8f09-d4b7cb759118 to disappear
    Jul 29 16:31:57.369: INFO: Pod downwardapi-volume-edecc8d6-cdb1-4eb3-8f09-d4b7cb759118 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jul 29 16:31:57.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-6632" for this suite. 07/29/23 16:31:57.381
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:31:57.401
Jul 29 16:31:57.401: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename custom-resource-definition 07/29/23 16:31:57.403
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:31:57.435
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:31:57.44
[It] should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
STEP: fetching the /apis discovery document 07/29/23 16:31:57.445
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 07/29/23 16:31:57.446
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 07/29/23 16:31:57.447
STEP: fetching the /apis/apiextensions.k8s.io discovery document 07/29/23 16:31:57.447
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 07/29/23 16:31:57.449
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 07/29/23 16:31:57.449
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 07/29/23 16:31:57.452
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jul 29 16:31:57.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-9212" for this suite. 07/29/23 16:31:57.46
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","completed":226,"skipped":4071,"failed":0}
------------------------------
â€¢ [0.071 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:31:57.401
    Jul 29 16:31:57.401: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename custom-resource-definition 07/29/23 16:31:57.403
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:31:57.435
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:31:57.44
    [It] should include custom resource definition resources in discovery documents [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:198
    STEP: fetching the /apis discovery document 07/29/23 16:31:57.445
    STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 07/29/23 16:31:57.446
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 07/29/23 16:31:57.447
    STEP: fetching the /apis/apiextensions.k8s.io discovery document 07/29/23 16:31:57.447
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 07/29/23 16:31:57.449
    STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 07/29/23 16:31:57.449
    STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 07/29/23 16:31:57.452
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jul 29 16:31:57.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-9212" for this suite. 07/29/23 16:31:57.46
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:31:57.472
Jul 29 16:31:57.473: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename configmap 07/29/23 16:31:57.476
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:31:57.503
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:31:57.508
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88
STEP: Creating configMap with name configmap-test-volume-map-05528f27-d0cf-4335-88a0-c917db02b71c 07/29/23 16:31:57.512
STEP: Creating a pod to test consume configMaps 07/29/23 16:31:57.521
Jul 29 16:31:57.533: INFO: Waiting up to 5m0s for pod "pod-configmaps-59efdd07-7b86-4694-93de-7ed4c463b60e" in namespace "configmap-2513" to be "Succeeded or Failed"
Jul 29 16:31:57.539: INFO: Pod "pod-configmaps-59efdd07-7b86-4694-93de-7ed4c463b60e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.116201ms
Jul 29 16:31:59.547: INFO: Pod "pod-configmaps-59efdd07-7b86-4694-93de-7ed4c463b60e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01374605s
Jul 29 16:32:01.549: INFO: Pod "pod-configmaps-59efdd07-7b86-4694-93de-7ed4c463b60e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01585392s
STEP: Saw pod success 07/29/23 16:32:01.549
Jul 29 16:32:01.549: INFO: Pod "pod-configmaps-59efdd07-7b86-4694-93de-7ed4c463b60e" satisfied condition "Succeeded or Failed"
Jul 29 16:32:01.553: INFO: Trying to get logs from node wa4quivohpee-3 pod pod-configmaps-59efdd07-7b86-4694-93de-7ed4c463b60e container agnhost-container: <nil>
STEP: delete the pod 07/29/23 16:32:01.565
Jul 29 16:32:01.587: INFO: Waiting for pod pod-configmaps-59efdd07-7b86-4694-93de-7ed4c463b60e to disappear
Jul 29 16:32:01.591: INFO: Pod pod-configmaps-59efdd07-7b86-4694-93de-7ed4c463b60e no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jul 29 16:32:01.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2513" for this suite. 07/29/23 16:32:01.597
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":227,"skipped":4076,"failed":0}
------------------------------
â€¢ [4.133 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:31:57.472
    Jul 29 16:31:57.473: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename configmap 07/29/23 16:31:57.476
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:31:57.503
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:31:57.508
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:88
    STEP: Creating configMap with name configmap-test-volume-map-05528f27-d0cf-4335-88a0-c917db02b71c 07/29/23 16:31:57.512
    STEP: Creating a pod to test consume configMaps 07/29/23 16:31:57.521
    Jul 29 16:31:57.533: INFO: Waiting up to 5m0s for pod "pod-configmaps-59efdd07-7b86-4694-93de-7ed4c463b60e" in namespace "configmap-2513" to be "Succeeded or Failed"
    Jul 29 16:31:57.539: INFO: Pod "pod-configmaps-59efdd07-7b86-4694-93de-7ed4c463b60e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.116201ms
    Jul 29 16:31:59.547: INFO: Pod "pod-configmaps-59efdd07-7b86-4694-93de-7ed4c463b60e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01374605s
    Jul 29 16:32:01.549: INFO: Pod "pod-configmaps-59efdd07-7b86-4694-93de-7ed4c463b60e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01585392s
    STEP: Saw pod success 07/29/23 16:32:01.549
    Jul 29 16:32:01.549: INFO: Pod "pod-configmaps-59efdd07-7b86-4694-93de-7ed4c463b60e" satisfied condition "Succeeded or Failed"
    Jul 29 16:32:01.553: INFO: Trying to get logs from node wa4quivohpee-3 pod pod-configmaps-59efdd07-7b86-4694-93de-7ed4c463b60e container agnhost-container: <nil>
    STEP: delete the pod 07/29/23 16:32:01.565
    Jul 29 16:32:01.587: INFO: Waiting for pod pod-configmaps-59efdd07-7b86-4694-93de-7ed4c463b60e to disappear
    Jul 29 16:32:01.591: INFO: Pod pod-configmaps-59efdd07-7b86-4694-93de-7ed4c463b60e no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jul 29 16:32:01.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-2513" for this suite. 07/29/23 16:32:01.597
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:32:01.608
Jul 29 16:32:01.608: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename emptydir-wrapper 07/29/23 16:32:01.61
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:32:01.638
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:32:01.643
[It] should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
Jul 29 16:32:01.680: INFO: Waiting up to 5m0s for pod "pod-secrets-47582a46-26fe-4f79-a83a-a39ba1d48a64" in namespace "emptydir-wrapper-9586" to be "running and ready"
Jul 29 16:32:01.688: INFO: Pod "pod-secrets-47582a46-26fe-4f79-a83a-a39ba1d48a64": Phase="Pending", Reason="", readiness=false. Elapsed: 8.207115ms
Jul 29 16:32:01.688: INFO: The phase of Pod pod-secrets-47582a46-26fe-4f79-a83a-a39ba1d48a64 is Pending, waiting for it to be Running (with Ready = true)
Jul 29 16:32:03.695: INFO: Pod "pod-secrets-47582a46-26fe-4f79-a83a-a39ba1d48a64": Phase="Running", Reason="", readiness=true. Elapsed: 2.015738484s
Jul 29 16:32:03.696: INFO: The phase of Pod pod-secrets-47582a46-26fe-4f79-a83a-a39ba1d48a64 is Running (Ready = true)
Jul 29 16:32:03.696: INFO: Pod "pod-secrets-47582a46-26fe-4f79-a83a-a39ba1d48a64" satisfied condition "running and ready"
STEP: Cleaning up the secret 07/29/23 16:32:03.702
STEP: Cleaning up the configmap 07/29/23 16:32:03.713
STEP: Cleaning up the pod 07/29/23 16:32:03.727
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:187
Jul 29 16:32:03.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-9586" for this suite. 07/29/23 16:32:03.759
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","completed":228,"skipped":4078,"failed":0}
------------------------------
â€¢ [2.161 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:32:01.608
    Jul 29 16:32:01.608: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename emptydir-wrapper 07/29/23 16:32:01.61
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:32:01.638
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:32:01.643
    [It] should not conflict [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:67
    Jul 29 16:32:01.680: INFO: Waiting up to 5m0s for pod "pod-secrets-47582a46-26fe-4f79-a83a-a39ba1d48a64" in namespace "emptydir-wrapper-9586" to be "running and ready"
    Jul 29 16:32:01.688: INFO: Pod "pod-secrets-47582a46-26fe-4f79-a83a-a39ba1d48a64": Phase="Pending", Reason="", readiness=false. Elapsed: 8.207115ms
    Jul 29 16:32:01.688: INFO: The phase of Pod pod-secrets-47582a46-26fe-4f79-a83a-a39ba1d48a64 is Pending, waiting for it to be Running (with Ready = true)
    Jul 29 16:32:03.695: INFO: Pod "pod-secrets-47582a46-26fe-4f79-a83a-a39ba1d48a64": Phase="Running", Reason="", readiness=true. Elapsed: 2.015738484s
    Jul 29 16:32:03.696: INFO: The phase of Pod pod-secrets-47582a46-26fe-4f79-a83a-a39ba1d48a64 is Running (Ready = true)
    Jul 29 16:32:03.696: INFO: Pod "pod-secrets-47582a46-26fe-4f79-a83a-a39ba1d48a64" satisfied condition "running and ready"
    STEP: Cleaning up the secret 07/29/23 16:32:03.702
    STEP: Cleaning up the configmap 07/29/23 16:32:03.713
    STEP: Cleaning up the pod 07/29/23 16:32:03.727
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:187
    Jul 29 16:32:03.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-wrapper-9586" for this suite. 07/29/23 16:32:03.759
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] PodTemplates
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:32:03.773
Jul 29 16:32:03.773: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename podtemplate 07/29/23 16:32:03.777
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:32:03.81
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:32:03.817
[It] should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
STEP: Create a pod template 07/29/23 16:32:03.823
STEP: Replace a pod template 07/29/23 16:32:03.831
Jul 29 16:32:03.847: INFO: Found updated podtemplate annotation: "true"

[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
Jul 29 16:32:03.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-3055" for this suite. 07/29/23 16:32:03.854
{"msg":"PASSED [sig-node] PodTemplates should replace a pod template [Conformance]","completed":229,"skipped":4081,"failed":0}
------------------------------
â€¢ [0.090 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:32:03.773
    Jul 29 16:32:03.773: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename podtemplate 07/29/23 16:32:03.777
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:32:03.81
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:32:03.817
    [It] should replace a pod template [Conformance]
      test/e2e/common/node/podtemplates.go:176
    STEP: Create a pod template 07/29/23 16:32:03.823
    STEP: Replace a pod template 07/29/23 16:32:03.831
    Jul 29 16:32:03.847: INFO: Found updated podtemplate annotation: "true"

    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    Jul 29 16:32:03.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-3055" for this suite. 07/29/23 16:32:03.854
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:32:03.865
Jul 29 16:32:03.865: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename resourcequota 07/29/23 16:32:03.868
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:32:03.891
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:32:03.897
[It] should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793
STEP: Creating a ResourceQuota with best effort scope 07/29/23 16:32:03.901
STEP: Ensuring ResourceQuota status is calculated 07/29/23 16:32:03.91
STEP: Creating a ResourceQuota with not best effort scope 07/29/23 16:32:05.918
STEP: Ensuring ResourceQuota status is calculated 07/29/23 16:32:05.927
STEP: Creating a best-effort pod 07/29/23 16:32:07.936
STEP: Ensuring resource quota with best effort scope captures the pod usage 07/29/23 16:32:07.959
STEP: Ensuring resource quota with not best effort ignored the pod usage 07/29/23 16:32:09.966
STEP: Deleting the pod 07/29/23 16:32:11.973
STEP: Ensuring resource quota status released the pod usage 07/29/23 16:32:12
STEP: Creating a not best-effort pod 07/29/23 16:32:14.01
STEP: Ensuring resource quota with not best effort scope captures the pod usage 07/29/23 16:32:14.028
STEP: Ensuring resource quota with best effort scope ignored the pod usage 07/29/23 16:32:16.036
STEP: Deleting the pod 07/29/23 16:32:18.044
STEP: Ensuring resource quota status released the pod usage 07/29/23 16:32:18.084
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jul 29 16:32:20.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5318" for this suite. 07/29/23 16:32:20.111
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","completed":230,"skipped":4081,"failed":0}
------------------------------
â€¢ [SLOW TEST] [16.262 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:32:03.865
    Jul 29 16:32:03.865: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename resourcequota 07/29/23 16:32:03.868
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:32:03.891
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:32:03.897
    [It] should verify ResourceQuota with best effort scope. [Conformance]
      test/e2e/apimachinery/resource_quota.go:793
    STEP: Creating a ResourceQuota with best effort scope 07/29/23 16:32:03.901
    STEP: Ensuring ResourceQuota status is calculated 07/29/23 16:32:03.91
    STEP: Creating a ResourceQuota with not best effort scope 07/29/23 16:32:05.918
    STEP: Ensuring ResourceQuota status is calculated 07/29/23 16:32:05.927
    STEP: Creating a best-effort pod 07/29/23 16:32:07.936
    STEP: Ensuring resource quota with best effort scope captures the pod usage 07/29/23 16:32:07.959
    STEP: Ensuring resource quota with not best effort ignored the pod usage 07/29/23 16:32:09.966
    STEP: Deleting the pod 07/29/23 16:32:11.973
    STEP: Ensuring resource quota status released the pod usage 07/29/23 16:32:12
    STEP: Creating a not best-effort pod 07/29/23 16:32:14.01
    STEP: Ensuring resource quota with not best effort scope captures the pod usage 07/29/23 16:32:14.028
    STEP: Ensuring resource quota with best effort scope ignored the pod usage 07/29/23 16:32:16.036
    STEP: Deleting the pod 07/29/23 16:32:18.044
    STEP: Ensuring resource quota status released the pod usage 07/29/23 16:32:18.084
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jul 29 16:32:20.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-5318" for this suite. 07/29/23 16:32:20.111
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] Garbage collector
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:32:20.134
Jul 29 16:32:20.134: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename gc 07/29/23 16:32:20.136
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:32:20.166
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:32:20.173
[It] should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
Jul 29 16:32:20.235: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"58061bc5-1aad-4383-8fa7-2e1040ebad5f", Controller:(*bool)(0xc00347bace), BlockOwnerDeletion:(*bool)(0xc00347bacf)}}
Jul 29 16:32:20.257: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"81e5cf6f-1ebd-4a40-9c8d-e56ff42affdc", Controller:(*bool)(0xc00347bcda), BlockOwnerDeletion:(*bool)(0xc00347bcdb)}}
Jul 29 16:32:20.267: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"17a77542-15f6-4e78-9d2b-aaec64d7821e", Controller:(*bool)(0xc0032947c6), BlockOwnerDeletion:(*bool)(0xc0032947c7)}}
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Jul 29 16:32:25.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5835" for this suite. 07/29/23 16:32:25.3
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","completed":231,"skipped":4084,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.178 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:32:20.134
    Jul 29 16:32:20.134: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename gc 07/29/23 16:32:20.136
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:32:20.166
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:32:20.173
    [It] should not be blocked by dependency circle [Conformance]
      test/e2e/apimachinery/garbage_collector.go:849
    Jul 29 16:32:20.235: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"58061bc5-1aad-4383-8fa7-2e1040ebad5f", Controller:(*bool)(0xc00347bace), BlockOwnerDeletion:(*bool)(0xc00347bacf)}}
    Jul 29 16:32:20.257: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"81e5cf6f-1ebd-4a40-9c8d-e56ff42affdc", Controller:(*bool)(0xc00347bcda), BlockOwnerDeletion:(*bool)(0xc00347bcdb)}}
    Jul 29 16:32:20.267: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"17a77542-15f6-4e78-9d2b-aaec64d7821e", Controller:(*bool)(0xc0032947c6), BlockOwnerDeletion:(*bool)(0xc0032947c7)}}
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Jul 29 16:32:25.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-5835" for this suite. 07/29/23 16:32:25.3
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:32:25.318
Jul 29 16:32:25.319: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename runtimeclass 07/29/23 16:32:25.322
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:32:25.403
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:32:25.408
[It]  should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
STEP: getting /apis 07/29/23 16:32:25.413
STEP: getting /apis/node.k8s.io 07/29/23 16:32:25.417
STEP: getting /apis/node.k8s.io/v1 07/29/23 16:32:25.419
STEP: creating 07/29/23 16:32:25.42
STEP: watching 07/29/23 16:32:25.446
Jul 29 16:32:25.446: INFO: starting watch
STEP: getting 07/29/23 16:32:25.456
STEP: listing 07/29/23 16:32:25.462
STEP: patching 07/29/23 16:32:25.468
STEP: updating 07/29/23 16:32:25.481
Jul 29 16:32:25.491: INFO: waiting for watch events with expected annotations
STEP: deleting 07/29/23 16:32:25.491
STEP: deleting a collection 07/29/23 16:32:25.514
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Jul 29 16:32:25.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-3290" for this suite. 07/29/23 16:32:25.574
{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","completed":232,"skipped":4108,"failed":0}
------------------------------
â€¢ [0.273 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:32:25.318
    Jul 29 16:32:25.319: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename runtimeclass 07/29/23 16:32:25.322
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:32:25.403
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:32:25.408
    [It]  should support RuntimeClasses API operations [Conformance]
      test/e2e/common/node/runtimeclass.go:189
    STEP: getting /apis 07/29/23 16:32:25.413
    STEP: getting /apis/node.k8s.io 07/29/23 16:32:25.417
    STEP: getting /apis/node.k8s.io/v1 07/29/23 16:32:25.419
    STEP: creating 07/29/23 16:32:25.42
    STEP: watching 07/29/23 16:32:25.446
    Jul 29 16:32:25.446: INFO: starting watch
    STEP: getting 07/29/23 16:32:25.456
    STEP: listing 07/29/23 16:32:25.462
    STEP: patching 07/29/23 16:32:25.468
    STEP: updating 07/29/23 16:32:25.481
    Jul 29 16:32:25.491: INFO: waiting for watch events with expected annotations
    STEP: deleting 07/29/23 16:32:25.491
    STEP: deleting a collection 07/29/23 16:32:25.514
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Jul 29 16:32:25.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-3290" for this suite. 07/29/23 16:32:25.574
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:32:25.594
Jul 29 16:32:25.594: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename custom-resource-definition 07/29/23 16:32:25.596
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:32:25.633
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:32:25.639
[It] listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
Jul 29 16:32:25.643: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jul 29 16:32:32.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7872" for this suite. 07/29/23 16:32:32.453
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","completed":233,"skipped":4141,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.885 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    listing custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:85

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:32:25.594
    Jul 29 16:32:25.594: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename custom-resource-definition 07/29/23 16:32:25.596
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:32:25.633
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:32:25.639
    [It] listing custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:85
    Jul 29 16:32:25.643: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jul 29 16:32:32.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-7872" for this suite. 07/29/23 16:32:32.453
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:32:32.48
Jul 29 16:32:32.480: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename secrets 07/29/23 16:32:32.488
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:32:32.572
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:32:32.577
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67
STEP: Creating secret with name secret-test-765b4fe7-9402-4e75-a4ee-0d5e4ebe3651 07/29/23 16:32:32.582
STEP: Creating a pod to test consume secrets 07/29/23 16:32:32.596
Jul 29 16:32:32.610: INFO: Waiting up to 5m0s for pod "pod-secrets-862af861-b39d-43ed-9e2b-e3427c8c19b3" in namespace "secrets-3364" to be "Succeeded or Failed"
Jul 29 16:32:32.618: INFO: Pod "pod-secrets-862af861-b39d-43ed-9e2b-e3427c8c19b3": Phase="Pending", Reason="", readiness=false. Elapsed: 7.987861ms
Jul 29 16:32:34.628: INFO: Pod "pod-secrets-862af861-b39d-43ed-9e2b-e3427c8c19b3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017245733s
Jul 29 16:32:36.639: INFO: Pod "pod-secrets-862af861-b39d-43ed-9e2b-e3427c8c19b3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028364427s
STEP: Saw pod success 07/29/23 16:32:36.639
Jul 29 16:32:36.639: INFO: Pod "pod-secrets-862af861-b39d-43ed-9e2b-e3427c8c19b3" satisfied condition "Succeeded or Failed"
Jul 29 16:32:36.645: INFO: Trying to get logs from node wa4quivohpee-3 pod pod-secrets-862af861-b39d-43ed-9e2b-e3427c8c19b3 container secret-volume-test: <nil>
STEP: delete the pod 07/29/23 16:32:36.655
Jul 29 16:32:36.683: INFO: Waiting for pod pod-secrets-862af861-b39d-43ed-9e2b-e3427c8c19b3 to disappear
Jul 29 16:32:36.690: INFO: Pod pod-secrets-862af861-b39d-43ed-9e2b-e3427c8c19b3 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jul 29 16:32:36.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3364" for this suite. 07/29/23 16:32:36.7
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","completed":234,"skipped":4146,"failed":0}
------------------------------
â€¢ [4.247 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:32:32.48
    Jul 29 16:32:32.480: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename secrets 07/29/23 16:32:32.488
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:32:32.572
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:32:32.577
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:67
    STEP: Creating secret with name secret-test-765b4fe7-9402-4e75-a4ee-0d5e4ebe3651 07/29/23 16:32:32.582
    STEP: Creating a pod to test consume secrets 07/29/23 16:32:32.596
    Jul 29 16:32:32.610: INFO: Waiting up to 5m0s for pod "pod-secrets-862af861-b39d-43ed-9e2b-e3427c8c19b3" in namespace "secrets-3364" to be "Succeeded or Failed"
    Jul 29 16:32:32.618: INFO: Pod "pod-secrets-862af861-b39d-43ed-9e2b-e3427c8c19b3": Phase="Pending", Reason="", readiness=false. Elapsed: 7.987861ms
    Jul 29 16:32:34.628: INFO: Pod "pod-secrets-862af861-b39d-43ed-9e2b-e3427c8c19b3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017245733s
    Jul 29 16:32:36.639: INFO: Pod "pod-secrets-862af861-b39d-43ed-9e2b-e3427c8c19b3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028364427s
    STEP: Saw pod success 07/29/23 16:32:36.639
    Jul 29 16:32:36.639: INFO: Pod "pod-secrets-862af861-b39d-43ed-9e2b-e3427c8c19b3" satisfied condition "Succeeded or Failed"
    Jul 29 16:32:36.645: INFO: Trying to get logs from node wa4quivohpee-3 pod pod-secrets-862af861-b39d-43ed-9e2b-e3427c8c19b3 container secret-volume-test: <nil>
    STEP: delete the pod 07/29/23 16:32:36.655
    Jul 29 16:32:36.683: INFO: Waiting for pod pod-secrets-862af861-b39d-43ed-9e2b-e3427c8c19b3 to disappear
    Jul 29 16:32:36.690: INFO: Pod pod-secrets-862af861-b39d-43ed-9e2b-e3427c8c19b3 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jul 29 16:32:36.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-3364" for this suite. 07/29/23 16:32:36.7
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:32:36.732
Jul 29 16:32:36.732: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename webhook 07/29/23 16:32:36.735
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:32:36.793
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:32:36.799
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 07/29/23 16:32:36.83
STEP: Create role binding to let webhook read extension-apiserver-authentication 07/29/23 16:32:37.264
STEP: Deploying the webhook pod 07/29/23 16:32:37.279
STEP: Wait for the deployment to be ready 07/29/23 16:32:37.307
Jul 29 16:32:37.338: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 07/29/23 16:32:39.372
STEP: Verifying the service has paired with the endpoint 07/29/23 16:32:39.391
Jul 29 16:32:40.392: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263
STEP: Registering the mutating pod webhook via the AdmissionRegistration API 07/29/23 16:32:40.398
STEP: create a pod that should be updated by the webhook 07/29/23 16:32:40.423
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jul 29 16:32:40.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4343" for this suite. 07/29/23 16:32:40.484
STEP: Destroying namespace "webhook-4343-markers" for this suite. 07/29/23 16:32:40.496
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","completed":235,"skipped":4149,"failed":0}
------------------------------
â€¢ [3.886 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:32:36.732
    Jul 29 16:32:36.732: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename webhook 07/29/23 16:32:36.735
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:32:36.793
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:32:36.799
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 07/29/23 16:32:36.83
    STEP: Create role binding to let webhook read extension-apiserver-authentication 07/29/23 16:32:37.264
    STEP: Deploying the webhook pod 07/29/23 16:32:37.279
    STEP: Wait for the deployment to be ready 07/29/23 16:32:37.307
    Jul 29 16:32:37.338: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 07/29/23 16:32:39.372
    STEP: Verifying the service has paired with the endpoint 07/29/23 16:32:39.391
    Jul 29 16:32:40.392: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate pod and apply defaults after mutation [Conformance]
      test/e2e/apimachinery/webhook.go:263
    STEP: Registering the mutating pod webhook via the AdmissionRegistration API 07/29/23 16:32:40.398
    STEP: create a pod that should be updated by the webhook 07/29/23 16:32:40.423
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jul 29 16:32:40.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-4343" for this suite. 07/29/23 16:32:40.484
    STEP: Destroying namespace "webhook-4343-markers" for this suite. 07/29/23 16:32:40.496
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:32:40.621
Jul 29 16:32:40.621: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename var-expansion 07/29/23 16:32:40.627
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:32:40.663
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:32:40.669
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185
Jul 29 16:32:40.691: INFO: Waiting up to 2m0s for pod "var-expansion-f8eec1f3-00fc-40d0-ac84-b66aa0a8a840" in namespace "var-expansion-1351" to be "container 0 failed with reason CreateContainerConfigError"
Jul 29 16:32:40.714: INFO: Pod "var-expansion-f8eec1f3-00fc-40d0-ac84-b66aa0a8a840": Phase="Pending", Reason="", readiness=false. Elapsed: 23.592127ms
Jul 29 16:32:42.723: INFO: Pod "var-expansion-f8eec1f3-00fc-40d0-ac84-b66aa0a8a840": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032200801s
Jul 29 16:32:42.723: INFO: Pod "var-expansion-f8eec1f3-00fc-40d0-ac84-b66aa0a8a840" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Jul 29 16:32:42.723: INFO: Deleting pod "var-expansion-f8eec1f3-00fc-40d0-ac84-b66aa0a8a840" in namespace "var-expansion-1351"
Jul 29 16:32:42.737: INFO: Wait up to 5m0s for pod "var-expansion-f8eec1f3-00fc-40d0-ac84-b66aa0a8a840" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jul 29 16:32:44.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1351" for this suite. 07/29/23 16:32:44.757
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]","completed":236,"skipped":4190,"failed":0}
------------------------------
â€¢ [4.147 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:32:40.621
    Jul 29 16:32:40.621: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename var-expansion 07/29/23 16:32:40.627
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:32:40.663
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:32:40.669
    [It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
      test/e2e/common/node/expansion.go:185
    Jul 29 16:32:40.691: INFO: Waiting up to 2m0s for pod "var-expansion-f8eec1f3-00fc-40d0-ac84-b66aa0a8a840" in namespace "var-expansion-1351" to be "container 0 failed with reason CreateContainerConfigError"
    Jul 29 16:32:40.714: INFO: Pod "var-expansion-f8eec1f3-00fc-40d0-ac84-b66aa0a8a840": Phase="Pending", Reason="", readiness=false. Elapsed: 23.592127ms
    Jul 29 16:32:42.723: INFO: Pod "var-expansion-f8eec1f3-00fc-40d0-ac84-b66aa0a8a840": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032200801s
    Jul 29 16:32:42.723: INFO: Pod "var-expansion-f8eec1f3-00fc-40d0-ac84-b66aa0a8a840" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Jul 29 16:32:42.723: INFO: Deleting pod "var-expansion-f8eec1f3-00fc-40d0-ac84-b66aa0a8a840" in namespace "var-expansion-1351"
    Jul 29 16:32:42.737: INFO: Wait up to 5m0s for pod "var-expansion-f8eec1f3-00fc-40d0-ac84-b66aa0a8a840" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jul 29 16:32:44.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-1351" for this suite. 07/29/23 16:32:44.757
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:608
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:32:44.77
Jul 29 16:32:44.770: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename security-context-test 07/29/23 16:32:44.772
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:32:44.802
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:32:44.81
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:608
Jul 29 16:32:44.835: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-0bdffe5a-163e-4749-bd65-c5ecf40e52c8" in namespace "security-context-test-437" to be "Succeeded or Failed"
Jul 29 16:32:44.846: INFO: Pod "alpine-nnp-false-0bdffe5a-163e-4749-bd65-c5ecf40e52c8": Phase="Pending", Reason="", readiness=false. Elapsed: 11.101351ms
Jul 29 16:32:46.855: INFO: Pod "alpine-nnp-false-0bdffe5a-163e-4749-bd65-c5ecf40e52c8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019850644s
Jul 29 16:32:48.856: INFO: Pod "alpine-nnp-false-0bdffe5a-163e-4749-bd65-c5ecf40e52c8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021262188s
Jul 29 16:32:50.856: INFO: Pod "alpine-nnp-false-0bdffe5a-163e-4749-bd65-c5ecf40e52c8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.020372158s
Jul 29 16:32:52.855: INFO: Pod "alpine-nnp-false-0bdffe5a-163e-4749-bd65-c5ecf40e52c8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.019979248s
Jul 29 16:32:52.855: INFO: Pod "alpine-nnp-false-0bdffe5a-163e-4749-bd65-c5ecf40e52c8" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Jul 29 16:32:52.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-437" for this suite. 07/29/23 16:32:52.878
{"msg":"PASSED [sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","completed":237,"skipped":4216,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.118 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  test/e2e/common/node/security_context.go:554
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:608

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:32:44.77
    Jul 29 16:32:44.770: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename security-context-test 07/29/23 16:32:44.772
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:32:44.802
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:32:44.81
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:608
    Jul 29 16:32:44.835: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-0bdffe5a-163e-4749-bd65-c5ecf40e52c8" in namespace "security-context-test-437" to be "Succeeded or Failed"
    Jul 29 16:32:44.846: INFO: Pod "alpine-nnp-false-0bdffe5a-163e-4749-bd65-c5ecf40e52c8": Phase="Pending", Reason="", readiness=false. Elapsed: 11.101351ms
    Jul 29 16:32:46.855: INFO: Pod "alpine-nnp-false-0bdffe5a-163e-4749-bd65-c5ecf40e52c8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019850644s
    Jul 29 16:32:48.856: INFO: Pod "alpine-nnp-false-0bdffe5a-163e-4749-bd65-c5ecf40e52c8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021262188s
    Jul 29 16:32:50.856: INFO: Pod "alpine-nnp-false-0bdffe5a-163e-4749-bd65-c5ecf40e52c8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.020372158s
    Jul 29 16:32:52.855: INFO: Pod "alpine-nnp-false-0bdffe5a-163e-4749-bd65-c5ecf40e52c8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.019979248s
    Jul 29 16:32:52.855: INFO: Pod "alpine-nnp-false-0bdffe5a-163e-4749-bd65-c5ecf40e52c8" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Jul 29 16:32:52.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-437" for this suite. 07/29/23 16:32:52.878
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace
  should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1745
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:32:52.896
Jul 29 16:32:52.896: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename kubectl 07/29/23 16:32:52.898
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:32:52.941
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:32:52.946
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1732
[It] should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1745
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 07/29/23 16:32:52.95
Jul 29 16:32:52.950: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-8592 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Jul 29 16:32:53.106: INFO: stderr: ""
Jul 29 16:32:53.106: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running 07/29/23 16:32:53.106
STEP: verifying the pod e2e-test-httpd-pod was created 07/29/23 16:32:58.159
Jul 29 16:32:58.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-8592 get pod e2e-test-httpd-pod -o json'
Jul 29 16:32:58.348: INFO: stderr: ""
Jul 29 16:32:58.348: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2023-07-29T16:32:53Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-8592\",\n        \"resourceVersion\": \"24996\",\n        \"uid\": \"8411d171-111f-4ac1-b75d-156f44f4033c\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-752k2\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"wa4quivohpee-3\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-752k2\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-07-29T16:32:53Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-07-29T16:32:54Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-07-29T16:32:54Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-07-29T16:32:53Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"cri-o://35ebd74eacfbf8d747a2031dc542adf4ffd057eea826688790d1312bc9f555a0\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-07-29T16:32:53Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.121.234\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.233.65.200\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.233.65.200\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-07-29T16:32:53Z\"\n    }\n}\n"
STEP: replace the image in the pod 07/29/23 16:32:58.348
Jul 29 16:32:58.349: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-8592 replace -f -'
Jul 29 16:33:00.357: INFO: stderr: ""
Jul 29 16:33:00.362: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-2 07/29/23 16:33:00.362
[AfterEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1736
Jul 29 16:33:00.370: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-8592 delete pods e2e-test-httpd-pod'
Jul 29 16:33:02.418: INFO: stderr: ""
Jul 29 16:33:02.418: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jul 29 16:33:02.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8592" for this suite. 07/29/23 16:33:02.427
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","completed":238,"skipped":4254,"failed":0}
------------------------------
â€¢ [SLOW TEST] [9.542 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl replace
  test/e2e/kubectl/kubectl.go:1729
    should update a single-container pod's image  [Conformance]
    test/e2e/kubectl/kubectl.go:1745

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:32:52.896
    Jul 29 16:32:52.896: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename kubectl 07/29/23 16:32:52.898
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:32:52.941
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:32:52.946
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1732
    [It] should update a single-container pod's image  [Conformance]
      test/e2e/kubectl/kubectl.go:1745
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 07/29/23 16:32:52.95
    Jul 29 16:32:52.950: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-8592 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Jul 29 16:32:53.106: INFO: stderr: ""
    Jul 29 16:32:53.106: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod is running 07/29/23 16:32:53.106
    STEP: verifying the pod e2e-test-httpd-pod was created 07/29/23 16:32:58.159
    Jul 29 16:32:58.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-8592 get pod e2e-test-httpd-pod -o json'
    Jul 29 16:32:58.348: INFO: stderr: ""
    Jul 29 16:32:58.348: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2023-07-29T16:32:53Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-8592\",\n        \"resourceVersion\": \"24996\",\n        \"uid\": \"8411d171-111f-4ac1-b75d-156f44f4033c\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-752k2\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"wa4quivohpee-3\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-752k2\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-07-29T16:32:53Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-07-29T16:32:54Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-07-29T16:32:54Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-07-29T16:32:53Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"cri-o://35ebd74eacfbf8d747a2031dc542adf4ffd057eea826688790d1312bc9f555a0\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-07-29T16:32:53Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.121.234\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.233.65.200\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.233.65.200\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-07-29T16:32:53Z\"\n    }\n}\n"
    STEP: replace the image in the pod 07/29/23 16:32:58.348
    Jul 29 16:32:58.349: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-8592 replace -f -'
    Jul 29 16:33:00.357: INFO: stderr: ""
    Jul 29 16:33:00.362: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-2 07/29/23 16:33:00.362
    [AfterEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1736
    Jul 29 16:33:00.370: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-8592 delete pods e2e-test-httpd-pod'
    Jul 29 16:33:02.418: INFO: stderr: ""
    Jul 29 16:33:02.418: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jul 29 16:33:02.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-8592" for this suite. 07/29/23 16:33:02.427
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:33:02.439
Jul 29 16:33:02.439: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename configmap 07/29/23 16:33:02.456
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:33:02.51
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:33:02.518
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44
STEP: Creating configMap configmap-4063/configmap-test-1286a098-27a2-456b-bb64-3ef8778a1134 07/29/23 16:33:02.526
STEP: Creating a pod to test consume configMaps 07/29/23 16:33:02.534
Jul 29 16:33:02.549: INFO: Waiting up to 5m0s for pod "pod-configmaps-ef75133b-3658-49ec-9b61-f370561b6f71" in namespace "configmap-4063" to be "Succeeded or Failed"
Jul 29 16:33:02.556: INFO: Pod "pod-configmaps-ef75133b-3658-49ec-9b61-f370561b6f71": Phase="Pending", Reason="", readiness=false. Elapsed: 6.649425ms
Jul 29 16:33:04.567: INFO: Pod "pod-configmaps-ef75133b-3658-49ec-9b61-f370561b6f71": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017288282s
Jul 29 16:33:06.565: INFO: Pod "pod-configmaps-ef75133b-3658-49ec-9b61-f370561b6f71": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015760813s
STEP: Saw pod success 07/29/23 16:33:06.566
Jul 29 16:33:06.566: INFO: Pod "pod-configmaps-ef75133b-3658-49ec-9b61-f370561b6f71" satisfied condition "Succeeded or Failed"
Jul 29 16:33:06.572: INFO: Trying to get logs from node wa4quivohpee-3 pod pod-configmaps-ef75133b-3658-49ec-9b61-f370561b6f71 container env-test: <nil>
STEP: delete the pod 07/29/23 16:33:06.586
Jul 29 16:33:06.625: INFO: Waiting for pod pod-configmaps-ef75133b-3658-49ec-9b61-f370561b6f71 to disappear
Jul 29 16:33:06.632: INFO: Pod pod-configmaps-ef75133b-3658-49ec-9b61-f370561b6f71 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Jul 29 16:33:06.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4063" for this suite. 07/29/23 16:33:06.645
{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","completed":239,"skipped":4271,"failed":0}
------------------------------
â€¢ [4.218 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:33:02.439
    Jul 29 16:33:02.439: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename configmap 07/29/23 16:33:02.456
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:33:02.51
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:33:02.518
    [It] should be consumable via environment variable [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:44
    STEP: Creating configMap configmap-4063/configmap-test-1286a098-27a2-456b-bb64-3ef8778a1134 07/29/23 16:33:02.526
    STEP: Creating a pod to test consume configMaps 07/29/23 16:33:02.534
    Jul 29 16:33:02.549: INFO: Waiting up to 5m0s for pod "pod-configmaps-ef75133b-3658-49ec-9b61-f370561b6f71" in namespace "configmap-4063" to be "Succeeded or Failed"
    Jul 29 16:33:02.556: INFO: Pod "pod-configmaps-ef75133b-3658-49ec-9b61-f370561b6f71": Phase="Pending", Reason="", readiness=false. Elapsed: 6.649425ms
    Jul 29 16:33:04.567: INFO: Pod "pod-configmaps-ef75133b-3658-49ec-9b61-f370561b6f71": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017288282s
    Jul 29 16:33:06.565: INFO: Pod "pod-configmaps-ef75133b-3658-49ec-9b61-f370561b6f71": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015760813s
    STEP: Saw pod success 07/29/23 16:33:06.566
    Jul 29 16:33:06.566: INFO: Pod "pod-configmaps-ef75133b-3658-49ec-9b61-f370561b6f71" satisfied condition "Succeeded or Failed"
    Jul 29 16:33:06.572: INFO: Trying to get logs from node wa4quivohpee-3 pod pod-configmaps-ef75133b-3658-49ec-9b61-f370561b6f71 container env-test: <nil>
    STEP: delete the pod 07/29/23 16:33:06.586
    Jul 29 16:33:06.625: INFO: Waiting for pod pod-configmaps-ef75133b-3658-49ec-9b61-f370561b6f71 to disappear
    Jul 29 16:33:06.632: INFO: Pod pod-configmaps-ef75133b-3658-49ec-9b61-f370561b6f71 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Jul 29 16:33:06.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-4063" for this suite. 07/29/23 16:33:06.645
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] PreStop
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
[BeforeEach] [sig-node] PreStop
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:33:06.66
Jul 29 16:33:06.660: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename prestop 07/29/23 16:33:06.663
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:33:06.734
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:33:06.739
[BeforeEach] [sig-node] PreStop
  test/e2e/node/pre_stop.go:159
[It] should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
STEP: Creating server pod server in namespace prestop-6050 07/29/23 16:33:06.742
STEP: Waiting for pods to come up. 07/29/23 16:33:06.755
Jul 29 16:33:06.755: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-6050" to be "running"
Jul 29 16:33:06.764: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 9.560495ms
Jul 29 16:33:08.774: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.019294329s
Jul 29 16:33:08.774: INFO: Pod "server" satisfied condition "running"
STEP: Creating tester pod tester in namespace prestop-6050 07/29/23 16:33:08.78
Jul 29 16:33:08.788: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-6050" to be "running"
Jul 29 16:33:08.796: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 7.758469ms
Jul 29 16:33:10.802: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.014493283s
Jul 29 16:33:10.802: INFO: Pod "tester" satisfied condition "running"
STEP: Deleting pre-stop pod 07/29/23 16:33:10.802
Jul 29 16:33:15.831: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod 07/29/23 16:33:15.831
[AfterEach] [sig-node] PreStop
  test/e2e/framework/framework.go:187
Jul 29 16:33:15.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-6050" for this suite. 07/29/23 16:33:15.877
{"msg":"PASSED [sig-node] PreStop should call prestop when killing a pod  [Conformance]","completed":240,"skipped":4272,"failed":0}
------------------------------
â€¢ [SLOW TEST] [9.233 seconds]
[sig-node] PreStop
test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PreStop
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:33:06.66
    Jul 29 16:33:06.660: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename prestop 07/29/23 16:33:06.663
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:33:06.734
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:33:06.739
    [BeforeEach] [sig-node] PreStop
      test/e2e/node/pre_stop.go:159
    [It] should call prestop when killing a pod  [Conformance]
      test/e2e/node/pre_stop.go:168
    STEP: Creating server pod server in namespace prestop-6050 07/29/23 16:33:06.742
    STEP: Waiting for pods to come up. 07/29/23 16:33:06.755
    Jul 29 16:33:06.755: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-6050" to be "running"
    Jul 29 16:33:06.764: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 9.560495ms
    Jul 29 16:33:08.774: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.019294329s
    Jul 29 16:33:08.774: INFO: Pod "server" satisfied condition "running"
    STEP: Creating tester pod tester in namespace prestop-6050 07/29/23 16:33:08.78
    Jul 29 16:33:08.788: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-6050" to be "running"
    Jul 29 16:33:08.796: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 7.758469ms
    Jul 29 16:33:10.802: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.014493283s
    Jul 29 16:33:10.802: INFO: Pod "tester" satisfied condition "running"
    STEP: Deleting pre-stop pod 07/29/23 16:33:10.802
    Jul 29 16:33:15.831: INFO: Saw: {
    	"Hostname": "server",
    	"Sent": null,
    	"Received": {
    		"prestop": 1
    	},
    	"Errors": null,
    	"Log": [
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
    	],
    	"StillContactingPeers": true
    }
    STEP: Deleting the server pod 07/29/23 16:33:15.831
    [AfterEach] [sig-node] PreStop
      test/e2e/framework/framework.go:187
    Jul 29 16:33:15.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "prestop-6050" for this suite. 07/29/23 16:33:15.877
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:33:15.895
Jul 29 16:33:15.896: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename events 07/29/23 16:33:15.901
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:33:15.934
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:33:15.939
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
STEP: Create set of events 07/29/23 16:33:15.943
STEP: get a list of Events with a label in the current namespace 07/29/23 16:33:15.975
STEP: delete a list of events 07/29/23 16:33:15.982
Jul 29 16:33:15.983: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 07/29/23 16:33:16.05
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:187
Jul 29 16:33:16.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-8675" for this suite. 07/29/23 16:33:16.075
{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","completed":241,"skipped":4283,"failed":0}
------------------------------
â€¢ [0.196 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:33:15.895
    Jul 29 16:33:15.896: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename events 07/29/23 16:33:15.901
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:33:15.934
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:33:15.939
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/events.go:207
    STEP: Create set of events 07/29/23 16:33:15.943
    STEP: get a list of Events with a label in the current namespace 07/29/23 16:33:15.975
    STEP: delete a list of events 07/29/23 16:33:15.982
    Jul 29 16:33:15.983: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 07/29/23 16:33:16.05
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:187
    Jul 29 16:33:16.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-8675" for this suite. 07/29/23 16:33:16.075
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:33:16.1
Jul 29 16:33:16.101: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename disruption 07/29/23 16:33:16.102
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:33:16.134
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:33:16.139
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163
STEP: Waiting for the pdb to be processed 07/29/23 16:33:16.153
STEP: Updating PodDisruptionBudget status 07/29/23 16:33:18.171
STEP: Waiting for all pods to be running 07/29/23 16:33:18.194
Jul 29 16:33:18.207: INFO: running pods: 0 < 1
STEP: locating a running pod 07/29/23 16:33:20.215
STEP: Waiting for the pdb to be processed 07/29/23 16:33:20.236
STEP: Patching PodDisruptionBudget status 07/29/23 16:33:20.251
STEP: Waiting for the pdb to be processed 07/29/23 16:33:20.269
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Jul 29 16:33:20.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-8723" for this suite. 07/29/23 16:33:20.284
{"msg":"PASSED [sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]","completed":242,"skipped":4337,"failed":0}
------------------------------
â€¢ [4.194 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:33:16.1
    Jul 29 16:33:16.101: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename disruption 07/29/23 16:33:16.102
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:33:16.134
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:33:16.139
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should update/patch PodDisruptionBudget status [Conformance]
      test/e2e/apps/disruption.go:163
    STEP: Waiting for the pdb to be processed 07/29/23 16:33:16.153
    STEP: Updating PodDisruptionBudget status 07/29/23 16:33:18.171
    STEP: Waiting for all pods to be running 07/29/23 16:33:18.194
    Jul 29 16:33:18.207: INFO: running pods: 0 < 1
    STEP: locating a running pod 07/29/23 16:33:20.215
    STEP: Waiting for the pdb to be processed 07/29/23 16:33:20.236
    STEP: Patching PodDisruptionBudget status 07/29/23 16:33:20.251
    STEP: Waiting for the pdb to be processed 07/29/23 16:33:20.269
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Jul 29 16:33:20.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-8723" for this suite. 07/29/23 16:33:20.284
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Pods
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:33:20.296
Jul 29 16:33:20.296: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename pods 07/29/23 16:33:20.299
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:33:20.344
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:33:20.352
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895
STEP: creating a Pod with a static label 07/29/23 16:33:20.379
STEP: watching for Pod to be ready 07/29/23 16:33:20.394
Jul 29 16:33:20.397: INFO: observed Pod pod-test in namespace pods-4949 in phase Pending with labels: map[test-pod-static:true] & conditions []
Jul 29 16:33:20.413: INFO: observed Pod pod-test in namespace pods-4949 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 16:33:20 +0000 UTC  }]
Jul 29 16:33:20.432: INFO: observed Pod pod-test in namespace pods-4949 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 16:33:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-07-29 16:33:20 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-07-29 16:33:20 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 16:33:20 +0000 UTC  }]
Jul 29 16:33:21.474: INFO: Found Pod pod-test in namespace pods-4949 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 16:33:20 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 16:33:21 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 16:33:21 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 16:33:20 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data 07/29/23 16:33:21.479
STEP: getting the Pod and ensuring that it's patched 07/29/23 16:33:21.499
STEP: replacing the Pod's status Ready condition to False 07/29/23 16:33:21.507
STEP: check the Pod again to ensure its Ready conditions are False 07/29/23 16:33:21.535
STEP: deleting the Pod via a Collection with a LabelSelector 07/29/23 16:33:21.536
STEP: watching for the Pod to be deleted 07/29/23 16:33:21.559
Jul 29 16:33:21.564: INFO: observed event type MODIFIED
Jul 29 16:33:23.480: INFO: observed event type MODIFIED
Jul 29 16:33:24.496: INFO: observed event type MODIFIED
Jul 29 16:33:24.522: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jul 29 16:33:24.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4949" for this suite. 07/29/23 16:33:24.575
{"msg":"PASSED [sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","completed":243,"skipped":4338,"failed":0}
------------------------------
â€¢ [4.296 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:33:20.296
    Jul 29 16:33:20.296: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename pods 07/29/23 16:33:20.299
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:33:20.344
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:33:20.352
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should run through the lifecycle of Pods and PodStatus [Conformance]
      test/e2e/common/node/pods.go:895
    STEP: creating a Pod with a static label 07/29/23 16:33:20.379
    STEP: watching for Pod to be ready 07/29/23 16:33:20.394
    Jul 29 16:33:20.397: INFO: observed Pod pod-test in namespace pods-4949 in phase Pending with labels: map[test-pod-static:true] & conditions []
    Jul 29 16:33:20.413: INFO: observed Pod pod-test in namespace pods-4949 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 16:33:20 +0000 UTC  }]
    Jul 29 16:33:20.432: INFO: observed Pod pod-test in namespace pods-4949 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 16:33:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-07-29 16:33:20 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-07-29 16:33:20 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 16:33:20 +0000 UTC  }]
    Jul 29 16:33:21.474: INFO: Found Pod pod-test in namespace pods-4949 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 16:33:20 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 16:33:21 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 16:33:21 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 16:33:20 +0000 UTC  }]
    STEP: patching the Pod with a new Label and updated data 07/29/23 16:33:21.479
    STEP: getting the Pod and ensuring that it's patched 07/29/23 16:33:21.499
    STEP: replacing the Pod's status Ready condition to False 07/29/23 16:33:21.507
    STEP: check the Pod again to ensure its Ready conditions are False 07/29/23 16:33:21.535
    STEP: deleting the Pod via a Collection with a LabelSelector 07/29/23 16:33:21.536
    STEP: watching for the Pod to be deleted 07/29/23 16:33:21.559
    Jul 29 16:33:21.564: INFO: observed event type MODIFIED
    Jul 29 16:33:23.480: INFO: observed event type MODIFIED
    Jul 29 16:33:24.496: INFO: observed event type MODIFIED
    Jul 29 16:33:24.522: INFO: observed event type MODIFIED
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jul 29 16:33:24.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-4949" for this suite. 07/29/23 16:33:24.575
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:33:24.602
Jul 29 16:33:24.602: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename endpointslicemirroring 07/29/23 16:33:24.604
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:33:24.642
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:33:24.647
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/network/endpointslicemirroring.go:41
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
STEP: mirroring a new custom Endpoint 07/29/23 16:33:24.67
Jul 29 16:33:24.687: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint 07/29/23 16:33:26.695
Jul 29 16:33:26.707: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
STEP: mirroring deletion of a custom Endpoint 07/29/23 16:33:28.716
Jul 29 16:33:28.737: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/framework.go:187
Jul 29 16:33:30.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslicemirroring-3784" for this suite. 07/29/23 16:33:30.751
{"msg":"PASSED [sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]","completed":244,"skipped":4370,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.162 seconds]
[sig-network] EndpointSliceMirroring
test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:33:24.602
    Jul 29 16:33:24.602: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename endpointslicemirroring 07/29/23 16:33:24.604
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:33:24.642
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:33:24.647
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/network/endpointslicemirroring.go:41
    [It] should mirror a custom Endpoints resource through create update and delete [Conformance]
      test/e2e/network/endpointslicemirroring.go:53
    STEP: mirroring a new custom Endpoint 07/29/23 16:33:24.67
    Jul 29 16:33:24.687: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
    STEP: mirroring an update to a custom Endpoint 07/29/23 16:33:26.695
    Jul 29 16:33:26.707: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
    STEP: mirroring deletion of a custom Endpoint 07/29/23 16:33:28.716
    Jul 29 16:33:28.737: INFO: Waiting for 0 EndpointSlices to exist, got 1
    [AfterEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/framework.go:187
    Jul 29 16:33:30.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslicemirroring-3784" for this suite. 07/29/23 16:33:30.751
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:33:30.765
Jul 29 16:33:30.765: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename projected 07/29/23 16:33:30.77
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:33:30.801
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:33:30.807
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118
STEP: Creating secret with name projected-secret-test-17e94624-5c8e-4f1d-b181-3d414d8b620c 07/29/23 16:33:30.811
STEP: Creating a pod to test consume secrets 07/29/23 16:33:30.82
Jul 29 16:33:30.834: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-9c2cddcc-e950-435a-b202-26c159c577d5" in namespace "projected-3646" to be "Succeeded or Failed"
Jul 29 16:33:30.839: INFO: Pod "pod-projected-secrets-9c2cddcc-e950-435a-b202-26c159c577d5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.625275ms
Jul 29 16:33:32.846: INFO: Pod "pod-projected-secrets-9c2cddcc-e950-435a-b202-26c159c577d5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012037547s
Jul 29 16:33:34.848: INFO: Pod "pod-projected-secrets-9c2cddcc-e950-435a-b202-26c159c577d5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013692875s
STEP: Saw pod success 07/29/23 16:33:34.848
Jul 29 16:33:34.849: INFO: Pod "pod-projected-secrets-9c2cddcc-e950-435a-b202-26c159c577d5" satisfied condition "Succeeded or Failed"
Jul 29 16:33:34.855: INFO: Trying to get logs from node wa4quivohpee-3 pod pod-projected-secrets-9c2cddcc-e950-435a-b202-26c159c577d5 container secret-volume-test: <nil>
STEP: delete the pod 07/29/23 16:33:34.87
Jul 29 16:33:34.895: INFO: Waiting for pod pod-projected-secrets-9c2cddcc-e950-435a-b202-26c159c577d5 to disappear
Jul 29 16:33:34.901: INFO: Pod pod-projected-secrets-9c2cddcc-e950-435a-b202-26c159c577d5 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Jul 29 16:33:34.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3646" for this suite. 07/29/23 16:33:34.909
{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","completed":245,"skipped":4375,"failed":0}
------------------------------
â€¢ [4.154 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:33:30.765
    Jul 29 16:33:30.765: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename projected 07/29/23 16:33:30.77
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:33:30.801
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:33:30.807
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:118
    STEP: Creating secret with name projected-secret-test-17e94624-5c8e-4f1d-b181-3d414d8b620c 07/29/23 16:33:30.811
    STEP: Creating a pod to test consume secrets 07/29/23 16:33:30.82
    Jul 29 16:33:30.834: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-9c2cddcc-e950-435a-b202-26c159c577d5" in namespace "projected-3646" to be "Succeeded or Failed"
    Jul 29 16:33:30.839: INFO: Pod "pod-projected-secrets-9c2cddcc-e950-435a-b202-26c159c577d5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.625275ms
    Jul 29 16:33:32.846: INFO: Pod "pod-projected-secrets-9c2cddcc-e950-435a-b202-26c159c577d5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012037547s
    Jul 29 16:33:34.848: INFO: Pod "pod-projected-secrets-9c2cddcc-e950-435a-b202-26c159c577d5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013692875s
    STEP: Saw pod success 07/29/23 16:33:34.848
    Jul 29 16:33:34.849: INFO: Pod "pod-projected-secrets-9c2cddcc-e950-435a-b202-26c159c577d5" satisfied condition "Succeeded or Failed"
    Jul 29 16:33:34.855: INFO: Trying to get logs from node wa4quivohpee-3 pod pod-projected-secrets-9c2cddcc-e950-435a-b202-26c159c577d5 container secret-volume-test: <nil>
    STEP: delete the pod 07/29/23 16:33:34.87
    Jul 29 16:33:34.895: INFO: Waiting for pod pod-projected-secrets-9c2cddcc-e950-435a-b202-26c159c577d5 to disappear
    Jul 29 16:33:34.901: INFO: Pod pod-projected-secrets-9c2cddcc-e950-435a-b202-26c159c577d5 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Jul 29 16:33:34.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3646" for this suite. 07/29/23 16:33:34.909
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:33:34.923
Jul 29 16:33:34.924: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename daemonsets 07/29/23 16:33:34.928
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:33:34.958
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:33:34.962
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861
STEP: Creating simple DaemonSet "daemon-set" 07/29/23 16:33:35.022
STEP: Check that daemon pods launch on every node of the cluster. 07/29/23 16:33:35.032
Jul 29 16:33:35.048: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jul 29 16:33:35.048: INFO: Node wa4quivohpee-1 is running 0 daemon pod, expected 1
Jul 29 16:33:36.072: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jul 29 16:33:36.072: INFO: Node wa4quivohpee-1 is running 0 daemon pod, expected 1
Jul 29 16:33:37.064: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jul 29 16:33:37.064: INFO: Node wa4quivohpee-1 is running 0 daemon pod, expected 1
Jul 29 16:33:38.064: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jul 29 16:33:38.064: INFO: Node wa4quivohpee-1 is running 0 daemon pod, expected 1
Jul 29 16:33:39.066: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jul 29 16:33:39.066: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Getting /status 07/29/23 16:33:39.072
Jul 29 16:33:39.079: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status 07/29/23 16:33:39.079
Jul 29 16:33:39.100: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated 07/29/23 16:33:39.1
Jul 29 16:33:39.108: INFO: Observed &DaemonSet event: ADDED
Jul 29 16:33:39.108: INFO: Observed &DaemonSet event: MODIFIED
Jul 29 16:33:39.109: INFO: Observed &DaemonSet event: MODIFIED
Jul 29 16:33:39.109: INFO: Observed &DaemonSet event: MODIFIED
Jul 29 16:33:39.110: INFO: Observed &DaemonSet event: MODIFIED
Jul 29 16:33:39.110: INFO: Observed &DaemonSet event: MODIFIED
Jul 29 16:33:39.110: INFO: Found daemon set daemon-set in namespace daemonsets-7679 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jul 29 16:33:39.111: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status 07/29/23 16:33:39.111
STEP: watching for the daemon set status to be patched 07/29/23 16:33:39.125
Jul 29 16:33:39.129: INFO: Observed &DaemonSet event: ADDED
Jul 29 16:33:39.130: INFO: Observed &DaemonSet event: MODIFIED
Jul 29 16:33:39.130: INFO: Observed &DaemonSet event: MODIFIED
Jul 29 16:33:39.131: INFO: Observed &DaemonSet event: MODIFIED
Jul 29 16:33:39.133: INFO: Observed &DaemonSet event: MODIFIED
Jul 29 16:33:39.134: INFO: Observed &DaemonSet event: MODIFIED
Jul 29 16:33:39.134: INFO: Observed daemon set daemon-set in namespace daemonsets-7679 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jul 29 16:33:39.136: INFO: Observed &DaemonSet event: MODIFIED
Jul 29 16:33:39.137: INFO: Found daemon set daemon-set in namespace daemonsets-7679 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Jul 29 16:33:39.137: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 07/29/23 16:33:39.142
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7679, will wait for the garbage collector to delete the pods 07/29/23 16:33:39.143
Jul 29 16:33:39.217: INFO: Deleting DaemonSet.extensions daemon-set took: 18.646136ms
Jul 29 16:33:39.318: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.691977ms
Jul 29 16:33:41.527: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jul 29 16:33:41.527: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jul 29 16:33:41.532: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"25404"},"items":null}

Jul 29 16:33:41.539: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"25404"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Jul 29 16:33:41.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7679" for this suite. 07/29/23 16:33:41.575
{"msg":"PASSED [sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]","completed":246,"skipped":4379,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.663 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:33:34.923
    Jul 29 16:33:34.924: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename daemonsets 07/29/23 16:33:34.928
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:33:34.958
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:33:34.962
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should verify changes to a daemon set status [Conformance]
      test/e2e/apps/daemon_set.go:861
    STEP: Creating simple DaemonSet "daemon-set" 07/29/23 16:33:35.022
    STEP: Check that daemon pods launch on every node of the cluster. 07/29/23 16:33:35.032
    Jul 29 16:33:35.048: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jul 29 16:33:35.048: INFO: Node wa4quivohpee-1 is running 0 daemon pod, expected 1
    Jul 29 16:33:36.072: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jul 29 16:33:36.072: INFO: Node wa4quivohpee-1 is running 0 daemon pod, expected 1
    Jul 29 16:33:37.064: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jul 29 16:33:37.064: INFO: Node wa4quivohpee-1 is running 0 daemon pod, expected 1
    Jul 29 16:33:38.064: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jul 29 16:33:38.064: INFO: Node wa4quivohpee-1 is running 0 daemon pod, expected 1
    Jul 29 16:33:39.066: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jul 29 16:33:39.066: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Getting /status 07/29/23 16:33:39.072
    Jul 29 16:33:39.079: INFO: Daemon Set daemon-set has Conditions: []
    STEP: updating the DaemonSet Status 07/29/23 16:33:39.079
    Jul 29 16:33:39.100: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the daemon set status to be updated 07/29/23 16:33:39.1
    Jul 29 16:33:39.108: INFO: Observed &DaemonSet event: ADDED
    Jul 29 16:33:39.108: INFO: Observed &DaemonSet event: MODIFIED
    Jul 29 16:33:39.109: INFO: Observed &DaemonSet event: MODIFIED
    Jul 29 16:33:39.109: INFO: Observed &DaemonSet event: MODIFIED
    Jul 29 16:33:39.110: INFO: Observed &DaemonSet event: MODIFIED
    Jul 29 16:33:39.110: INFO: Observed &DaemonSet event: MODIFIED
    Jul 29 16:33:39.110: INFO: Found daemon set daemon-set in namespace daemonsets-7679 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jul 29 16:33:39.111: INFO: Daemon set daemon-set has an updated status
    STEP: patching the DaemonSet Status 07/29/23 16:33:39.111
    STEP: watching for the daemon set status to be patched 07/29/23 16:33:39.125
    Jul 29 16:33:39.129: INFO: Observed &DaemonSet event: ADDED
    Jul 29 16:33:39.130: INFO: Observed &DaemonSet event: MODIFIED
    Jul 29 16:33:39.130: INFO: Observed &DaemonSet event: MODIFIED
    Jul 29 16:33:39.131: INFO: Observed &DaemonSet event: MODIFIED
    Jul 29 16:33:39.133: INFO: Observed &DaemonSet event: MODIFIED
    Jul 29 16:33:39.134: INFO: Observed &DaemonSet event: MODIFIED
    Jul 29 16:33:39.134: INFO: Observed daemon set daemon-set in namespace daemonsets-7679 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jul 29 16:33:39.136: INFO: Observed &DaemonSet event: MODIFIED
    Jul 29 16:33:39.137: INFO: Found daemon set daemon-set in namespace daemonsets-7679 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
    Jul 29 16:33:39.137: INFO: Daemon set daemon-set has a patched status
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 07/29/23 16:33:39.142
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7679, will wait for the garbage collector to delete the pods 07/29/23 16:33:39.143
    Jul 29 16:33:39.217: INFO: Deleting DaemonSet.extensions daemon-set took: 18.646136ms
    Jul 29 16:33:39.318: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.691977ms
    Jul 29 16:33:41.527: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jul 29 16:33:41.527: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jul 29 16:33:41.532: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"25404"},"items":null}

    Jul 29 16:33:41.539: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"25404"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Jul 29 16:33:41.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-7679" for this suite. 07/29/23 16:33:41.575
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:33:41.593
Jul 29 16:33:41.593: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename pods 07/29/23 16:33:41.596
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:33:41.635
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:33:41.641
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443
Jul 29 16:33:41.664: INFO: Waiting up to 5m0s for pod "server-envvars-eda5383b-330b-47fc-bc1d-1704f668f7ee" in namespace "pods-809" to be "running and ready"
Jul 29 16:33:41.671: INFO: Pod "server-envvars-eda5383b-330b-47fc-bc1d-1704f668f7ee": Phase="Pending", Reason="", readiness=false. Elapsed: 6.548099ms
Jul 29 16:33:41.671: INFO: The phase of Pod server-envvars-eda5383b-330b-47fc-bc1d-1704f668f7ee is Pending, waiting for it to be Running (with Ready = true)
Jul 29 16:33:43.679: INFO: Pod "server-envvars-eda5383b-330b-47fc-bc1d-1704f668f7ee": Phase="Running", Reason="", readiness=true. Elapsed: 2.014382344s
Jul 29 16:33:43.679: INFO: The phase of Pod server-envvars-eda5383b-330b-47fc-bc1d-1704f668f7ee is Running (Ready = true)
Jul 29 16:33:43.679: INFO: Pod "server-envvars-eda5383b-330b-47fc-bc1d-1704f668f7ee" satisfied condition "running and ready"
Jul 29 16:33:43.730: INFO: Waiting up to 5m0s for pod "client-envvars-c765b66c-6bd9-4b18-a6f4-c252ed89bef6" in namespace "pods-809" to be "Succeeded or Failed"
Jul 29 16:33:43.742: INFO: Pod "client-envvars-c765b66c-6bd9-4b18-a6f4-c252ed89bef6": Phase="Pending", Reason="", readiness=false. Elapsed: 11.718052ms
Jul 29 16:33:45.752: INFO: Pod "client-envvars-c765b66c-6bd9-4b18-a6f4-c252ed89bef6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021714194s
Jul 29 16:33:47.750: INFO: Pod "client-envvars-c765b66c-6bd9-4b18-a6f4-c252ed89bef6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019764321s
STEP: Saw pod success 07/29/23 16:33:47.75
Jul 29 16:33:47.750: INFO: Pod "client-envvars-c765b66c-6bd9-4b18-a6f4-c252ed89bef6" satisfied condition "Succeeded or Failed"
Jul 29 16:33:47.756: INFO: Trying to get logs from node wa4quivohpee-3 pod client-envvars-c765b66c-6bd9-4b18-a6f4-c252ed89bef6 container env3cont: <nil>
STEP: delete the pod 07/29/23 16:33:47.769
Jul 29 16:33:47.800: INFO: Waiting for pod client-envvars-c765b66c-6bd9-4b18-a6f4-c252ed89bef6 to disappear
Jul 29 16:33:47.806: INFO: Pod client-envvars-c765b66c-6bd9-4b18-a6f4-c252ed89bef6 no longer exists
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jul 29 16:33:47.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-809" for this suite. 07/29/23 16:33:47.814
{"msg":"PASSED [sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]","completed":247,"skipped":4403,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.243 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:33:41.593
    Jul 29 16:33:41.593: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename pods 07/29/23 16:33:41.596
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:33:41.635
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:33:41.641
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should contain environment variables for services [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:443
    Jul 29 16:33:41.664: INFO: Waiting up to 5m0s for pod "server-envvars-eda5383b-330b-47fc-bc1d-1704f668f7ee" in namespace "pods-809" to be "running and ready"
    Jul 29 16:33:41.671: INFO: Pod "server-envvars-eda5383b-330b-47fc-bc1d-1704f668f7ee": Phase="Pending", Reason="", readiness=false. Elapsed: 6.548099ms
    Jul 29 16:33:41.671: INFO: The phase of Pod server-envvars-eda5383b-330b-47fc-bc1d-1704f668f7ee is Pending, waiting for it to be Running (with Ready = true)
    Jul 29 16:33:43.679: INFO: Pod "server-envvars-eda5383b-330b-47fc-bc1d-1704f668f7ee": Phase="Running", Reason="", readiness=true. Elapsed: 2.014382344s
    Jul 29 16:33:43.679: INFO: The phase of Pod server-envvars-eda5383b-330b-47fc-bc1d-1704f668f7ee is Running (Ready = true)
    Jul 29 16:33:43.679: INFO: Pod "server-envvars-eda5383b-330b-47fc-bc1d-1704f668f7ee" satisfied condition "running and ready"
    Jul 29 16:33:43.730: INFO: Waiting up to 5m0s for pod "client-envvars-c765b66c-6bd9-4b18-a6f4-c252ed89bef6" in namespace "pods-809" to be "Succeeded or Failed"
    Jul 29 16:33:43.742: INFO: Pod "client-envvars-c765b66c-6bd9-4b18-a6f4-c252ed89bef6": Phase="Pending", Reason="", readiness=false. Elapsed: 11.718052ms
    Jul 29 16:33:45.752: INFO: Pod "client-envvars-c765b66c-6bd9-4b18-a6f4-c252ed89bef6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021714194s
    Jul 29 16:33:47.750: INFO: Pod "client-envvars-c765b66c-6bd9-4b18-a6f4-c252ed89bef6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019764321s
    STEP: Saw pod success 07/29/23 16:33:47.75
    Jul 29 16:33:47.750: INFO: Pod "client-envvars-c765b66c-6bd9-4b18-a6f4-c252ed89bef6" satisfied condition "Succeeded or Failed"
    Jul 29 16:33:47.756: INFO: Trying to get logs from node wa4quivohpee-3 pod client-envvars-c765b66c-6bd9-4b18-a6f4-c252ed89bef6 container env3cont: <nil>
    STEP: delete the pod 07/29/23 16:33:47.769
    Jul 29 16:33:47.800: INFO: Waiting for pod client-envvars-c765b66c-6bd9-4b18-a6f4-c252ed89bef6 to disappear
    Jul 29 16:33:47.806: INFO: Pod client-envvars-c765b66c-6bd9-4b18-a6f4-c252ed89bef6 no longer exists
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jul 29 16:33:47.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-809" for this suite. 07/29/23 16:33:47.814
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:33:47.842
Jul 29 16:33:47.842: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename replication-controller 07/29/23 16:33:47.844
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:33:47.871
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:33:47.875
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91
STEP: Given a Pod with a 'name' label pod-adoption is created 07/29/23 16:33:47.879
Jul 29 16:33:47.895: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-9622" to be "running and ready"
Jul 29 16:33:47.917: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 21.988981ms
Jul 29 16:33:47.917: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Jul 29 16:33:49.928: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.033293518s
Jul 29 16:33:49.928: INFO: The phase of Pod pod-adoption is Running (Ready = true)
Jul 29 16:33:49.929: INFO: Pod "pod-adoption" satisfied condition "running and ready"
STEP: When a replication controller with a matching selector is created 07/29/23 16:33:49.938
STEP: Then the orphan pod is adopted 07/29/23 16:33:49.952
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Jul 29 16:33:50.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9622" for this suite. 07/29/23 16:33:50.977
{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","completed":248,"skipped":4455,"failed":0}
------------------------------
â€¢ [3.148 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:33:47.842
    Jul 29 16:33:47.842: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename replication-controller 07/29/23 16:33:47.844
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:33:47.871
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:33:47.875
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should adopt matching pods on creation [Conformance]
      test/e2e/apps/rc.go:91
    STEP: Given a Pod with a 'name' label pod-adoption is created 07/29/23 16:33:47.879
    Jul 29 16:33:47.895: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-9622" to be "running and ready"
    Jul 29 16:33:47.917: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 21.988981ms
    Jul 29 16:33:47.917: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    Jul 29 16:33:49.928: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.033293518s
    Jul 29 16:33:49.928: INFO: The phase of Pod pod-adoption is Running (Ready = true)
    Jul 29 16:33:49.929: INFO: Pod "pod-adoption" satisfied condition "running and ready"
    STEP: When a replication controller with a matching selector is created 07/29/23 16:33:49.938
    STEP: Then the orphan pod is adopted 07/29/23 16:33:49.952
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Jul 29 16:33:50.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-9622" for this suite. 07/29/23 16:33:50.977
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Services
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:33:50.993
Jul 29 16:33:50.993: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename services 07/29/23 16:33:50.999
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:33:51.026
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:33:51.034
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157
STEP: creating service in namespace services-2578 07/29/23 16:33:51.039
STEP: creating service affinity-clusterip in namespace services-2578 07/29/23 16:33:51.04
STEP: creating replication controller affinity-clusterip in namespace services-2578 07/29/23 16:33:51.08
I0729 16:33:51.099561      13 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-2578, replica count: 3
I0729 16:33:54.151494      13 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 29 16:33:54.165: INFO: Creating new exec pod
Jul 29 16:33:54.183: INFO: Waiting up to 5m0s for pod "execpod-affinityx227c" in namespace "services-2578" to be "running"
Jul 29 16:33:54.190: INFO: Pod "execpod-affinityx227c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.943381ms
Jul 29 16:33:56.199: INFO: Pod "execpod-affinityx227c": Phase="Running", Reason="", readiness=true. Elapsed: 2.016459443s
Jul 29 16:33:56.199: INFO: Pod "execpod-affinityx227c" satisfied condition "running"
Jul 29 16:33:57.200: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=services-2578 exec execpod-affinityx227c -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
Jul 29 16:33:57.460: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Jul 29 16:33:57.460: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul 29 16:33:57.460: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=services-2578 exec execpod-affinityx227c -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.8.179 80'
Jul 29 16:33:57.694: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.8.179 80\nConnection to 10.233.8.179 80 port [tcp/http] succeeded!\n"
Jul 29 16:33:57.694: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul 29 16:33:57.694: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=services-2578 exec execpod-affinityx227c -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.8.179:80/ ; done'
Jul 29 16:33:58.107: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.179:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.179:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.179:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.179:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.179:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.179:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.179:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.179:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.179:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.179:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.179:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.179:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.179:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.179:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.179:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.179:80/\n"
Jul 29 16:33:58.107: INFO: stdout: "\naffinity-clusterip-8c9t7\naffinity-clusterip-8c9t7\naffinity-clusterip-8c9t7\naffinity-clusterip-8c9t7\naffinity-clusterip-8c9t7\naffinity-clusterip-8c9t7\naffinity-clusterip-8c9t7\naffinity-clusterip-8c9t7\naffinity-clusterip-8c9t7\naffinity-clusterip-8c9t7\naffinity-clusterip-8c9t7\naffinity-clusterip-8c9t7\naffinity-clusterip-8c9t7\naffinity-clusterip-8c9t7\naffinity-clusterip-8c9t7\naffinity-clusterip-8c9t7"
Jul 29 16:33:58.107: INFO: Received response from host: affinity-clusterip-8c9t7
Jul 29 16:33:58.107: INFO: Received response from host: affinity-clusterip-8c9t7
Jul 29 16:33:58.107: INFO: Received response from host: affinity-clusterip-8c9t7
Jul 29 16:33:58.107: INFO: Received response from host: affinity-clusterip-8c9t7
Jul 29 16:33:58.107: INFO: Received response from host: affinity-clusterip-8c9t7
Jul 29 16:33:58.107: INFO: Received response from host: affinity-clusterip-8c9t7
Jul 29 16:33:58.107: INFO: Received response from host: affinity-clusterip-8c9t7
Jul 29 16:33:58.107: INFO: Received response from host: affinity-clusterip-8c9t7
Jul 29 16:33:58.107: INFO: Received response from host: affinity-clusterip-8c9t7
Jul 29 16:33:58.107: INFO: Received response from host: affinity-clusterip-8c9t7
Jul 29 16:33:58.107: INFO: Received response from host: affinity-clusterip-8c9t7
Jul 29 16:33:58.107: INFO: Received response from host: affinity-clusterip-8c9t7
Jul 29 16:33:58.107: INFO: Received response from host: affinity-clusterip-8c9t7
Jul 29 16:33:58.107: INFO: Received response from host: affinity-clusterip-8c9t7
Jul 29 16:33:58.107: INFO: Received response from host: affinity-clusterip-8c9t7
Jul 29 16:33:58.107: INFO: Received response from host: affinity-clusterip-8c9t7
Jul 29 16:33:58.107: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-2578, will wait for the garbage collector to delete the pods 07/29/23 16:33:58.13
Jul 29 16:33:58.203: INFO: Deleting ReplicationController affinity-clusterip took: 13.864847ms
Jul 29 16:33:58.306: INFO: Terminating ReplicationController affinity-clusterip pods took: 103.346665ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jul 29 16:34:00.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2578" for this suite. 07/29/23 16:34:00.659
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","completed":249,"skipped":4458,"failed":0}
------------------------------
â€¢ [SLOW TEST] [9.680 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:33:50.993
    Jul 29 16:33:50.993: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename services 07/29/23 16:33:50.999
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:33:51.026
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:33:51.034
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2157
    STEP: creating service in namespace services-2578 07/29/23 16:33:51.039
    STEP: creating service affinity-clusterip in namespace services-2578 07/29/23 16:33:51.04
    STEP: creating replication controller affinity-clusterip in namespace services-2578 07/29/23 16:33:51.08
    I0729 16:33:51.099561      13 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-2578, replica count: 3
    I0729 16:33:54.151494      13 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jul 29 16:33:54.165: INFO: Creating new exec pod
    Jul 29 16:33:54.183: INFO: Waiting up to 5m0s for pod "execpod-affinityx227c" in namespace "services-2578" to be "running"
    Jul 29 16:33:54.190: INFO: Pod "execpod-affinityx227c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.943381ms
    Jul 29 16:33:56.199: INFO: Pod "execpod-affinityx227c": Phase="Running", Reason="", readiness=true. Elapsed: 2.016459443s
    Jul 29 16:33:56.199: INFO: Pod "execpod-affinityx227c" satisfied condition "running"
    Jul 29 16:33:57.200: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=services-2578 exec execpod-affinityx227c -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
    Jul 29 16:33:57.460: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
    Jul 29 16:33:57.460: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jul 29 16:33:57.460: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=services-2578 exec execpod-affinityx227c -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.8.179 80'
    Jul 29 16:33:57.694: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.8.179 80\nConnection to 10.233.8.179 80 port [tcp/http] succeeded!\n"
    Jul 29 16:33:57.694: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jul 29 16:33:57.694: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=services-2578 exec execpod-affinityx227c -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.8.179:80/ ; done'
    Jul 29 16:33:58.107: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.179:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.179:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.179:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.179:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.179:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.179:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.179:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.179:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.179:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.179:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.179:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.179:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.179:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.179:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.179:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.179:80/\n"
    Jul 29 16:33:58.107: INFO: stdout: "\naffinity-clusterip-8c9t7\naffinity-clusterip-8c9t7\naffinity-clusterip-8c9t7\naffinity-clusterip-8c9t7\naffinity-clusterip-8c9t7\naffinity-clusterip-8c9t7\naffinity-clusterip-8c9t7\naffinity-clusterip-8c9t7\naffinity-clusterip-8c9t7\naffinity-clusterip-8c9t7\naffinity-clusterip-8c9t7\naffinity-clusterip-8c9t7\naffinity-clusterip-8c9t7\naffinity-clusterip-8c9t7\naffinity-clusterip-8c9t7\naffinity-clusterip-8c9t7"
    Jul 29 16:33:58.107: INFO: Received response from host: affinity-clusterip-8c9t7
    Jul 29 16:33:58.107: INFO: Received response from host: affinity-clusterip-8c9t7
    Jul 29 16:33:58.107: INFO: Received response from host: affinity-clusterip-8c9t7
    Jul 29 16:33:58.107: INFO: Received response from host: affinity-clusterip-8c9t7
    Jul 29 16:33:58.107: INFO: Received response from host: affinity-clusterip-8c9t7
    Jul 29 16:33:58.107: INFO: Received response from host: affinity-clusterip-8c9t7
    Jul 29 16:33:58.107: INFO: Received response from host: affinity-clusterip-8c9t7
    Jul 29 16:33:58.107: INFO: Received response from host: affinity-clusterip-8c9t7
    Jul 29 16:33:58.107: INFO: Received response from host: affinity-clusterip-8c9t7
    Jul 29 16:33:58.107: INFO: Received response from host: affinity-clusterip-8c9t7
    Jul 29 16:33:58.107: INFO: Received response from host: affinity-clusterip-8c9t7
    Jul 29 16:33:58.107: INFO: Received response from host: affinity-clusterip-8c9t7
    Jul 29 16:33:58.107: INFO: Received response from host: affinity-clusterip-8c9t7
    Jul 29 16:33:58.107: INFO: Received response from host: affinity-clusterip-8c9t7
    Jul 29 16:33:58.107: INFO: Received response from host: affinity-clusterip-8c9t7
    Jul 29 16:33:58.107: INFO: Received response from host: affinity-clusterip-8c9t7
    Jul 29 16:33:58.107: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip in namespace services-2578, will wait for the garbage collector to delete the pods 07/29/23 16:33:58.13
    Jul 29 16:33:58.203: INFO: Deleting ReplicationController affinity-clusterip took: 13.864847ms
    Jul 29 16:33:58.306: INFO: Terminating ReplicationController affinity-clusterip pods took: 103.346665ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jul 29 16:34:00.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-2578" for this suite. 07/29/23 16:34:00.659
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:34:00.675
Jul 29 16:34:00.676: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename projected 07/29/23 16:34:00.679
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:34:00.726
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:34:00.735
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52
STEP: Creating a pod to test downward API volume plugin 07/29/23 16:34:00.74
Jul 29 16:34:00.756: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4e1d1dfa-40c7-4988-9be8-674076226964" in namespace "projected-4450" to be "Succeeded or Failed"
Jul 29 16:34:00.763: INFO: Pod "downwardapi-volume-4e1d1dfa-40c7-4988-9be8-674076226964": Phase="Pending", Reason="", readiness=false. Elapsed: 6.558215ms
Jul 29 16:34:02.779: INFO: Pod "downwardapi-volume-4e1d1dfa-40c7-4988-9be8-674076226964": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023263162s
Jul 29 16:34:04.772: INFO: Pod "downwardapi-volume-4e1d1dfa-40c7-4988-9be8-674076226964": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015515035s
STEP: Saw pod success 07/29/23 16:34:04.772
Jul 29 16:34:04.772: INFO: Pod "downwardapi-volume-4e1d1dfa-40c7-4988-9be8-674076226964" satisfied condition "Succeeded or Failed"
Jul 29 16:34:04.779: INFO: Trying to get logs from node wa4quivohpee-3 pod downwardapi-volume-4e1d1dfa-40c7-4988-9be8-674076226964 container client-container: <nil>
STEP: delete the pod 07/29/23 16:34:04.79
Jul 29 16:34:04.810: INFO: Waiting for pod downwardapi-volume-4e1d1dfa-40c7-4988-9be8-674076226964 to disappear
Jul 29 16:34:04.818: INFO: Pod downwardapi-volume-4e1d1dfa-40c7-4988-9be8-674076226964 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jul 29 16:34:04.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4450" for this suite. 07/29/23 16:34:04.827
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","completed":250,"skipped":4459,"failed":0}
------------------------------
â€¢ [4.165 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:34:00.675
    Jul 29 16:34:00.676: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename projected 07/29/23 16:34:00.679
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:34:00.726
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:34:00.735
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:52
    STEP: Creating a pod to test downward API volume plugin 07/29/23 16:34:00.74
    Jul 29 16:34:00.756: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4e1d1dfa-40c7-4988-9be8-674076226964" in namespace "projected-4450" to be "Succeeded or Failed"
    Jul 29 16:34:00.763: INFO: Pod "downwardapi-volume-4e1d1dfa-40c7-4988-9be8-674076226964": Phase="Pending", Reason="", readiness=false. Elapsed: 6.558215ms
    Jul 29 16:34:02.779: INFO: Pod "downwardapi-volume-4e1d1dfa-40c7-4988-9be8-674076226964": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023263162s
    Jul 29 16:34:04.772: INFO: Pod "downwardapi-volume-4e1d1dfa-40c7-4988-9be8-674076226964": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015515035s
    STEP: Saw pod success 07/29/23 16:34:04.772
    Jul 29 16:34:04.772: INFO: Pod "downwardapi-volume-4e1d1dfa-40c7-4988-9be8-674076226964" satisfied condition "Succeeded or Failed"
    Jul 29 16:34:04.779: INFO: Trying to get logs from node wa4quivohpee-3 pod downwardapi-volume-4e1d1dfa-40c7-4988-9be8-674076226964 container client-container: <nil>
    STEP: delete the pod 07/29/23 16:34:04.79
    Jul 29 16:34:04.810: INFO: Waiting for pod downwardapi-volume-4e1d1dfa-40c7-4988-9be8-674076226964 to disappear
    Jul 29 16:34:04.818: INFO: Pod downwardapi-volume-4e1d1dfa-40c7-4988-9be8-674076226964 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jul 29 16:34:04.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4450" for this suite. 07/29/23 16:34:04.827
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:34:04.84
Jul 29 16:34:04.840: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename subpath 07/29/23 16:34:04.847
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:34:04.878
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:34:04.884
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 07/29/23 16:34:04.891
[It] should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
STEP: Creating pod pod-subpath-test-secret-nt9j 07/29/23 16:34:04.915
STEP: Creating a pod to test atomic-volume-subpath 07/29/23 16:34:04.915
Jul 29 16:34:04.940: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-nt9j" in namespace "subpath-7390" to be "Succeeded or Failed"
Jul 29 16:34:04.961: INFO: Pod "pod-subpath-test-secret-nt9j": Phase="Pending", Reason="", readiness=false. Elapsed: 20.451743ms
Jul 29 16:34:06.983: INFO: Pod "pod-subpath-test-secret-nt9j": Phase="Running", Reason="", readiness=true. Elapsed: 2.04254105s
Jul 29 16:34:08.982: INFO: Pod "pod-subpath-test-secret-nt9j": Phase="Running", Reason="", readiness=true. Elapsed: 4.041375077s
Jul 29 16:34:10.982: INFO: Pod "pod-subpath-test-secret-nt9j": Phase="Running", Reason="", readiness=true. Elapsed: 6.041198476s
Jul 29 16:34:12.982: INFO: Pod "pod-subpath-test-secret-nt9j": Phase="Running", Reason="", readiness=true. Elapsed: 8.041967694s
Jul 29 16:34:14.982: INFO: Pod "pod-subpath-test-secret-nt9j": Phase="Running", Reason="", readiness=true. Elapsed: 10.041492306s
Jul 29 16:34:16.980: INFO: Pod "pod-subpath-test-secret-nt9j": Phase="Running", Reason="", readiness=true. Elapsed: 12.039782849s
Jul 29 16:34:18.980: INFO: Pod "pod-subpath-test-secret-nt9j": Phase="Running", Reason="", readiness=true. Elapsed: 14.039546161s
Jul 29 16:34:20.982: INFO: Pod "pod-subpath-test-secret-nt9j": Phase="Running", Reason="", readiness=true. Elapsed: 16.041787039s
Jul 29 16:34:22.982: INFO: Pod "pod-subpath-test-secret-nt9j": Phase="Running", Reason="", readiness=true. Elapsed: 18.041159276s
Jul 29 16:34:24.983: INFO: Pod "pod-subpath-test-secret-nt9j": Phase="Running", Reason="", readiness=true. Elapsed: 20.04274482s
Jul 29 16:34:26.985: INFO: Pod "pod-subpath-test-secret-nt9j": Phase="Running", Reason="", readiness=false. Elapsed: 22.044766338s
Jul 29 16:34:28.982: INFO: Pod "pod-subpath-test-secret-nt9j": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.041732281s
STEP: Saw pod success 07/29/23 16:34:28.982
Jul 29 16:34:28.983: INFO: Pod "pod-subpath-test-secret-nt9j" satisfied condition "Succeeded or Failed"
Jul 29 16:34:28.990: INFO: Trying to get logs from node wa4quivohpee-3 pod pod-subpath-test-secret-nt9j container test-container-subpath-secret-nt9j: <nil>
STEP: delete the pod 07/29/23 16:34:29.009
Jul 29 16:34:29.030: INFO: Waiting for pod pod-subpath-test-secret-nt9j to disappear
Jul 29 16:34:29.043: INFO: Pod pod-subpath-test-secret-nt9j no longer exists
STEP: Deleting pod pod-subpath-test-secret-nt9j 07/29/23 16:34:29.043
Jul 29 16:34:29.043: INFO: Deleting pod "pod-subpath-test-secret-nt9j" in namespace "subpath-7390"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Jul 29 16:34:29.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7390" for this suite. 07/29/23 16:34:29.06
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [Conformance]","completed":251,"skipped":4467,"failed":0}
------------------------------
â€¢ [SLOW TEST] [24.235 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with secret pod [Conformance]
    test/e2e/storage/subpath.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:34:04.84
    Jul 29 16:34:04.840: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename subpath 07/29/23 16:34:04.847
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:34:04.878
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:34:04.884
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 07/29/23 16:34:04.891
    [It] should support subpaths with secret pod [Conformance]
      test/e2e/storage/subpath.go:60
    STEP: Creating pod pod-subpath-test-secret-nt9j 07/29/23 16:34:04.915
    STEP: Creating a pod to test atomic-volume-subpath 07/29/23 16:34:04.915
    Jul 29 16:34:04.940: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-nt9j" in namespace "subpath-7390" to be "Succeeded or Failed"
    Jul 29 16:34:04.961: INFO: Pod "pod-subpath-test-secret-nt9j": Phase="Pending", Reason="", readiness=false. Elapsed: 20.451743ms
    Jul 29 16:34:06.983: INFO: Pod "pod-subpath-test-secret-nt9j": Phase="Running", Reason="", readiness=true. Elapsed: 2.04254105s
    Jul 29 16:34:08.982: INFO: Pod "pod-subpath-test-secret-nt9j": Phase="Running", Reason="", readiness=true. Elapsed: 4.041375077s
    Jul 29 16:34:10.982: INFO: Pod "pod-subpath-test-secret-nt9j": Phase="Running", Reason="", readiness=true. Elapsed: 6.041198476s
    Jul 29 16:34:12.982: INFO: Pod "pod-subpath-test-secret-nt9j": Phase="Running", Reason="", readiness=true. Elapsed: 8.041967694s
    Jul 29 16:34:14.982: INFO: Pod "pod-subpath-test-secret-nt9j": Phase="Running", Reason="", readiness=true. Elapsed: 10.041492306s
    Jul 29 16:34:16.980: INFO: Pod "pod-subpath-test-secret-nt9j": Phase="Running", Reason="", readiness=true. Elapsed: 12.039782849s
    Jul 29 16:34:18.980: INFO: Pod "pod-subpath-test-secret-nt9j": Phase="Running", Reason="", readiness=true. Elapsed: 14.039546161s
    Jul 29 16:34:20.982: INFO: Pod "pod-subpath-test-secret-nt9j": Phase="Running", Reason="", readiness=true. Elapsed: 16.041787039s
    Jul 29 16:34:22.982: INFO: Pod "pod-subpath-test-secret-nt9j": Phase="Running", Reason="", readiness=true. Elapsed: 18.041159276s
    Jul 29 16:34:24.983: INFO: Pod "pod-subpath-test-secret-nt9j": Phase="Running", Reason="", readiness=true. Elapsed: 20.04274482s
    Jul 29 16:34:26.985: INFO: Pod "pod-subpath-test-secret-nt9j": Phase="Running", Reason="", readiness=false. Elapsed: 22.044766338s
    Jul 29 16:34:28.982: INFO: Pod "pod-subpath-test-secret-nt9j": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.041732281s
    STEP: Saw pod success 07/29/23 16:34:28.982
    Jul 29 16:34:28.983: INFO: Pod "pod-subpath-test-secret-nt9j" satisfied condition "Succeeded or Failed"
    Jul 29 16:34:28.990: INFO: Trying to get logs from node wa4quivohpee-3 pod pod-subpath-test-secret-nt9j container test-container-subpath-secret-nt9j: <nil>
    STEP: delete the pod 07/29/23 16:34:29.009
    Jul 29 16:34:29.030: INFO: Waiting for pod pod-subpath-test-secret-nt9j to disappear
    Jul 29 16:34:29.043: INFO: Pod pod-subpath-test-secret-nt9j no longer exists
    STEP: Deleting pod pod-subpath-test-secret-nt9j 07/29/23 16:34:29.043
    Jul 29 16:34:29.043: INFO: Deleting pod "pod-subpath-test-secret-nt9j" in namespace "subpath-7390"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Jul 29 16:34:29.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-7390" for this suite. 07/29/23 16:34:29.06
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:34:29.106
Jul 29 16:34:29.106: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename downward-api 07/29/23 16:34:29.108
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:34:29.181
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:34:29.186
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206
STEP: Creating a pod to test downward API volume plugin 07/29/23 16:34:29.192
Jul 29 16:34:29.219: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0e61b655-82b2-4e81-a2a0-7ce25b289d5e" in namespace "downward-api-2960" to be "Succeeded or Failed"
Jul 29 16:34:29.227: INFO: Pod "downwardapi-volume-0e61b655-82b2-4e81-a2a0-7ce25b289d5e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.078671ms
Jul 29 16:34:31.236: INFO: Pod "downwardapi-volume-0e61b655-82b2-4e81-a2a0-7ce25b289d5e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016741763s
Jul 29 16:34:33.236: INFO: Pod "downwardapi-volume-0e61b655-82b2-4e81-a2a0-7ce25b289d5e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016324538s
STEP: Saw pod success 07/29/23 16:34:33.236
Jul 29 16:34:33.236: INFO: Pod "downwardapi-volume-0e61b655-82b2-4e81-a2a0-7ce25b289d5e" satisfied condition "Succeeded or Failed"
Jul 29 16:34:33.242: INFO: Trying to get logs from node wa4quivohpee-3 pod downwardapi-volume-0e61b655-82b2-4e81-a2a0-7ce25b289d5e container client-container: <nil>
STEP: delete the pod 07/29/23 16:34:33.258
Jul 29 16:34:33.277: INFO: Waiting for pod downwardapi-volume-0e61b655-82b2-4e81-a2a0-7ce25b289d5e to disappear
Jul 29 16:34:33.283: INFO: Pod downwardapi-volume-0e61b655-82b2-4e81-a2a0-7ce25b289d5e no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jul 29 16:34:33.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2960" for this suite. 07/29/23 16:34:33.293
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","completed":252,"skipped":4512,"failed":0}
------------------------------
â€¢ [4.199 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:34:29.106
    Jul 29 16:34:29.106: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename downward-api 07/29/23 16:34:29.108
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:34:29.181
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:34:29.186
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:206
    STEP: Creating a pod to test downward API volume plugin 07/29/23 16:34:29.192
    Jul 29 16:34:29.219: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0e61b655-82b2-4e81-a2a0-7ce25b289d5e" in namespace "downward-api-2960" to be "Succeeded or Failed"
    Jul 29 16:34:29.227: INFO: Pod "downwardapi-volume-0e61b655-82b2-4e81-a2a0-7ce25b289d5e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.078671ms
    Jul 29 16:34:31.236: INFO: Pod "downwardapi-volume-0e61b655-82b2-4e81-a2a0-7ce25b289d5e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016741763s
    Jul 29 16:34:33.236: INFO: Pod "downwardapi-volume-0e61b655-82b2-4e81-a2a0-7ce25b289d5e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016324538s
    STEP: Saw pod success 07/29/23 16:34:33.236
    Jul 29 16:34:33.236: INFO: Pod "downwardapi-volume-0e61b655-82b2-4e81-a2a0-7ce25b289d5e" satisfied condition "Succeeded or Failed"
    Jul 29 16:34:33.242: INFO: Trying to get logs from node wa4quivohpee-3 pod downwardapi-volume-0e61b655-82b2-4e81-a2a0-7ce25b289d5e container client-container: <nil>
    STEP: delete the pod 07/29/23 16:34:33.258
    Jul 29 16:34:33.277: INFO: Waiting for pod downwardapi-volume-0e61b655-82b2-4e81-a2a0-7ce25b289d5e to disappear
    Jul 29 16:34:33.283: INFO: Pod downwardapi-volume-0e61b655-82b2-4e81-a2a0-7ce25b289d5e no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jul 29 16:34:33.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-2960" for this suite. 07/29/23 16:34:33.293
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:34:33.323
Jul 29 16:34:33.323: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename dns 07/29/23 16:34:33.325
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:34:33.358
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:34:33.363
[It] should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 07/29/23 16:34:33.368
Jul 29 16:34:33.383: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-482  32ea80a3-4e62-4d2c-89db-7fabfe6c70ce 25824 0 2023-07-29 16:34:33 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-07-29 16:34:33 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l4s2p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l4s2p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 29 16:34:33.385: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-482" to be "running and ready"
Jul 29 16:34:33.393: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 7.385747ms
Jul 29 16:34:33.393: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Jul 29 16:34:35.410: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.024989561s
Jul 29 16:34:35.411: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
Jul 29 16:34:35.411: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
STEP: Verifying customized DNS suffix list is configured on pod... 07/29/23 16:34:35.411
Jul 29 16:34:35.411: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-482 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 29 16:34:35.411: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
Jul 29 16:34:35.413: INFO: ExecWithOptions: Clientset creation
Jul 29 16:34:35.413: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/dns-482/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
STEP: Verifying customized DNS server is configured on pod... 07/29/23 16:34:35.529
Jul 29 16:34:35.529: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-482 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 29 16:34:35.529: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
Jul 29 16:34:35.532: INFO: ExecWithOptions: Clientset creation
Jul 29 16:34:35.532: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/dns-482/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jul 29 16:34:35.659: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jul 29 16:34:35.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-482" for this suite. 07/29/23 16:34:35.691
{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","completed":253,"skipped":4555,"failed":0}
------------------------------
â€¢ [2.381 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:34:33.323
    Jul 29 16:34:33.323: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename dns 07/29/23 16:34:33.325
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:34:33.358
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:34:33.363
    [It] should support configurable pod DNS nameservers [Conformance]
      test/e2e/network/dns.go:411
    STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 07/29/23 16:34:33.368
    Jul 29 16:34:33.383: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-482  32ea80a3-4e62-4d2c-89db-7fabfe6c70ce 25824 0 2023-07-29 16:34:33 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-07-29 16:34:33 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l4s2p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l4s2p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jul 29 16:34:33.385: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-482" to be "running and ready"
    Jul 29 16:34:33.393: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 7.385747ms
    Jul 29 16:34:33.393: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
    Jul 29 16:34:35.410: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.024989561s
    Jul 29 16:34:35.411: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
    Jul 29 16:34:35.411: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
    STEP: Verifying customized DNS suffix list is configured on pod... 07/29/23 16:34:35.411
    Jul 29 16:34:35.411: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-482 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jul 29 16:34:35.411: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    Jul 29 16:34:35.413: INFO: ExecWithOptions: Clientset creation
    Jul 29 16:34:35.413: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/dns-482/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    STEP: Verifying customized DNS server is configured on pod... 07/29/23 16:34:35.529
    Jul 29 16:34:35.529: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-482 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jul 29 16:34:35.529: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    Jul 29 16:34:35.532: INFO: ExecWithOptions: Clientset creation
    Jul 29 16:34:35.532: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/dns-482/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jul 29 16:34:35.659: INFO: Deleting pod test-dns-nameservers...
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jul 29 16:34:35.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-482" for this suite. 07/29/23 16:34:35.691
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:34:35.71
Jul 29 16:34:35.711: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename crd-publish-openapi 07/29/23 16:34:35.714
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:34:35.749
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:34:35.754
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 07/29/23 16:34:35.76
Jul 29 16:34:35.761: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
Jul 29 16:34:39.908: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jul 29 16:35:00.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6605" for this suite. 07/29/23 16:35:00.208
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","completed":254,"skipped":4581,"failed":0}
------------------------------
â€¢ [SLOW TEST] [24.511 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:34:35.71
    Jul 29 16:34:35.711: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename crd-publish-openapi 07/29/23 16:34:35.714
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:34:35.749
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:34:35.754
    [It] works for multiple CRDs of same group and version but different kinds [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:356
    STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 07/29/23 16:34:35.76
    Jul 29 16:34:35.761: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    Jul 29 16:34:39.908: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jul 29 16:35:00.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-6605" for this suite. 07/29/23 16:35:00.208
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:35:00.246
Jul 29 16:35:00.247: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename configmap 07/29/23 16:35:00.25
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:35:00.285
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:35:00.289
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422
STEP: Creating configMap with name configmap-test-volume-e2a9c0e2-1c09-4ee9-96b3-5d5e52de6ca4 07/29/23 16:35:00.294
STEP: Creating a pod to test consume configMaps 07/29/23 16:35:00.303
Jul 29 16:35:00.322: INFO: Waiting up to 5m0s for pod "pod-configmaps-ee29ac99-99f6-48fe-80ec-d12efbaed205" in namespace "configmap-6058" to be "Succeeded or Failed"
Jul 29 16:35:00.331: INFO: Pod "pod-configmaps-ee29ac99-99f6-48fe-80ec-d12efbaed205": Phase="Pending", Reason="", readiness=false. Elapsed: 8.392344ms
Jul 29 16:35:02.339: INFO: Pod "pod-configmaps-ee29ac99-99f6-48fe-80ec-d12efbaed205": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016568503s
Jul 29 16:35:04.338: INFO: Pod "pod-configmaps-ee29ac99-99f6-48fe-80ec-d12efbaed205": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014846792s
STEP: Saw pod success 07/29/23 16:35:04.338
Jul 29 16:35:04.338: INFO: Pod "pod-configmaps-ee29ac99-99f6-48fe-80ec-d12efbaed205" satisfied condition "Succeeded or Failed"
Jul 29 16:35:04.345: INFO: Trying to get logs from node wa4quivohpee-3 pod pod-configmaps-ee29ac99-99f6-48fe-80ec-d12efbaed205 container configmap-volume-test: <nil>
STEP: delete the pod 07/29/23 16:35:04.379
Jul 29 16:35:04.403: INFO: Waiting for pod pod-configmaps-ee29ac99-99f6-48fe-80ec-d12efbaed205 to disappear
Jul 29 16:35:04.410: INFO: Pod pod-configmaps-ee29ac99-99f6-48fe-80ec-d12efbaed205 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jul 29 16:35:04.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6058" for this suite. 07/29/23 16:35:04.418
{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","completed":255,"skipped":4670,"failed":0}
------------------------------
â€¢ [4.183 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:35:00.246
    Jul 29 16:35:00.247: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename configmap 07/29/23 16:35:00.25
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:35:00.285
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:35:00.289
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:422
    STEP: Creating configMap with name configmap-test-volume-e2a9c0e2-1c09-4ee9-96b3-5d5e52de6ca4 07/29/23 16:35:00.294
    STEP: Creating a pod to test consume configMaps 07/29/23 16:35:00.303
    Jul 29 16:35:00.322: INFO: Waiting up to 5m0s for pod "pod-configmaps-ee29ac99-99f6-48fe-80ec-d12efbaed205" in namespace "configmap-6058" to be "Succeeded or Failed"
    Jul 29 16:35:00.331: INFO: Pod "pod-configmaps-ee29ac99-99f6-48fe-80ec-d12efbaed205": Phase="Pending", Reason="", readiness=false. Elapsed: 8.392344ms
    Jul 29 16:35:02.339: INFO: Pod "pod-configmaps-ee29ac99-99f6-48fe-80ec-d12efbaed205": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016568503s
    Jul 29 16:35:04.338: INFO: Pod "pod-configmaps-ee29ac99-99f6-48fe-80ec-d12efbaed205": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014846792s
    STEP: Saw pod success 07/29/23 16:35:04.338
    Jul 29 16:35:04.338: INFO: Pod "pod-configmaps-ee29ac99-99f6-48fe-80ec-d12efbaed205" satisfied condition "Succeeded or Failed"
    Jul 29 16:35:04.345: INFO: Trying to get logs from node wa4quivohpee-3 pod pod-configmaps-ee29ac99-99f6-48fe-80ec-d12efbaed205 container configmap-volume-test: <nil>
    STEP: delete the pod 07/29/23 16:35:04.379
    Jul 29 16:35:04.403: INFO: Waiting for pod pod-configmaps-ee29ac99-99f6-48fe-80ec-d12efbaed205 to disappear
    Jul 29 16:35:04.410: INFO: Pod pod-configmaps-ee29ac99-99f6-48fe-80ec-d12efbaed205 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jul 29 16:35:04.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-6058" for this suite. 07/29/23 16:35:04.418
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:35:04.435
Jul 29 16:35:04.435: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename var-expansion 07/29/23 16:35:04.442
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:35:04.482
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:35:04.489
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224
STEP: creating the pod with failed condition 07/29/23 16:35:04.493
Jul 29 16:35:04.536: INFO: Waiting up to 2m0s for pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3" in namespace "var-expansion-8098" to be "running"
Jul 29 16:35:04.544: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 8.114994ms
Jul 29 16:35:06.552: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016723131s
Jul 29 16:35:08.552: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01591018s
Jul 29 16:35:10.551: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.015448171s
Jul 29 16:35:12.552: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 8.016751248s
Jul 29 16:35:14.551: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 10.015728646s
Jul 29 16:35:16.553: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 12.017595365s
Jul 29 16:35:18.552: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 14.016177758s
Jul 29 16:35:20.553: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 16.017531647s
Jul 29 16:35:22.553: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 18.016923999s
Jul 29 16:35:24.552: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 20.016216399s
Jul 29 16:35:26.550: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 22.013852859s
Jul 29 16:35:28.556: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 24.020376386s
Jul 29 16:35:30.555: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 26.01910889s
Jul 29 16:35:32.554: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 28.018321231s
Jul 29 16:35:34.555: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 30.019045773s
Jul 29 16:35:36.554: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 32.018116042s
Jul 29 16:35:38.560: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 34.023798489s
Jul 29 16:35:40.552: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 36.01636714s
Jul 29 16:35:42.552: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 38.016608577s
Jul 29 16:35:44.559: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 40.022810884s
Jul 29 16:35:46.550: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 42.014306086s
Jul 29 16:35:48.552: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 44.016204976s
Jul 29 16:35:50.553: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 46.017053987s
Jul 29 16:35:52.553: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 48.017345743s
Jul 29 16:35:54.552: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 50.01649518s
Jul 29 16:35:56.555: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 52.019086112s
Jul 29 16:35:58.553: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 54.017661271s
Jul 29 16:36:00.558: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 56.02205483s
Jul 29 16:36:02.554: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 58.018688917s
Jul 29 16:36:04.551: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.015367181s
Jul 29 16:36:06.551: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.014813646s
Jul 29 16:36:08.552: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.016617681s
Jul 29 16:36:10.552: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.016616859s
Jul 29 16:36:12.551: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.014997626s
Jul 29 16:36:14.552: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.016031958s
Jul 29 16:36:16.550: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.014559299s
Jul 29 16:36:18.551: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.015514756s
Jul 29 16:36:20.553: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.016788258s
Jul 29 16:36:22.553: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.017614283s
Jul 29 16:36:24.554: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.018378771s
Jul 29 16:36:26.553: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.017331245s
Jul 29 16:36:28.552: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.016078261s
Jul 29 16:36:30.551: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.015470866s
Jul 29 16:36:32.552: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.016455347s
Jul 29 16:36:34.553: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.016839794s
Jul 29 16:36:36.553: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.017608439s
Jul 29 16:36:38.551: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.015262116s
Jul 29 16:36:40.554: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.017965814s
Jul 29 16:36:42.554: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.017861231s
Jul 29 16:36:44.552: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.016658529s
Jul 29 16:36:46.549: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.013173304s
Jul 29 16:36:48.552: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.016258046s
Jul 29 16:36:50.552: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.016490798s
Jul 29 16:36:52.552: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.016436757s
Jul 29 16:36:54.554: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.018013118s
Jul 29 16:36:56.552: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.016500382s
Jul 29 16:36:58.553: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.016860363s
Jul 29 16:37:00.553: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.016977008s
Jul 29 16:37:02.551: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.015563515s
Jul 29 16:37:04.553: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.016838155s
Jul 29 16:37:04.561: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.024853941s
STEP: updating the pod 07/29/23 16:37:04.561
Jul 29 16:37:05.083: INFO: Successfully updated pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3"
STEP: waiting for pod running 07/29/23 16:37:05.083
Jul 29 16:37:05.099: INFO: Waiting up to 2m0s for pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3" in namespace "var-expansion-8098" to be "running"
Jul 29 16:37:05.113: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 14.175715ms
Jul 29 16:37:07.122: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Running", Reason="", readiness=true. Elapsed: 2.023013696s
Jul 29 16:37:07.122: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3" satisfied condition "running"
STEP: deleting the pod gracefully 07/29/23 16:37:07.122
Jul 29 16:37:07.123: INFO: Deleting pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3" in namespace "var-expansion-8098"
Jul 29 16:37:07.135: INFO: Wait up to 5m0s for pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jul 29 16:37:39.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8098" for this suite. 07/29/23 16:37:39.198
{"msg":"PASSED [sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]","completed":256,"skipped":4747,"failed":0}
------------------------------
â€¢ [SLOW TEST] [154.782 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:35:04.435
    Jul 29 16:35:04.435: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename var-expansion 07/29/23 16:35:04.442
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:35:04.482
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:35:04.489
    [It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:224
    STEP: creating the pod with failed condition 07/29/23 16:35:04.493
    Jul 29 16:35:04.536: INFO: Waiting up to 2m0s for pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3" in namespace "var-expansion-8098" to be "running"
    Jul 29 16:35:04.544: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 8.114994ms
    Jul 29 16:35:06.552: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016723131s
    Jul 29 16:35:08.552: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01591018s
    Jul 29 16:35:10.551: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.015448171s
    Jul 29 16:35:12.552: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 8.016751248s
    Jul 29 16:35:14.551: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 10.015728646s
    Jul 29 16:35:16.553: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 12.017595365s
    Jul 29 16:35:18.552: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 14.016177758s
    Jul 29 16:35:20.553: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 16.017531647s
    Jul 29 16:35:22.553: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 18.016923999s
    Jul 29 16:35:24.552: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 20.016216399s
    Jul 29 16:35:26.550: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 22.013852859s
    Jul 29 16:35:28.556: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 24.020376386s
    Jul 29 16:35:30.555: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 26.01910889s
    Jul 29 16:35:32.554: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 28.018321231s
    Jul 29 16:35:34.555: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 30.019045773s
    Jul 29 16:35:36.554: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 32.018116042s
    Jul 29 16:35:38.560: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 34.023798489s
    Jul 29 16:35:40.552: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 36.01636714s
    Jul 29 16:35:42.552: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 38.016608577s
    Jul 29 16:35:44.559: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 40.022810884s
    Jul 29 16:35:46.550: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 42.014306086s
    Jul 29 16:35:48.552: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 44.016204976s
    Jul 29 16:35:50.553: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 46.017053987s
    Jul 29 16:35:52.553: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 48.017345743s
    Jul 29 16:35:54.552: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 50.01649518s
    Jul 29 16:35:56.555: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 52.019086112s
    Jul 29 16:35:58.553: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 54.017661271s
    Jul 29 16:36:00.558: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 56.02205483s
    Jul 29 16:36:02.554: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 58.018688917s
    Jul 29 16:36:04.551: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.015367181s
    Jul 29 16:36:06.551: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.014813646s
    Jul 29 16:36:08.552: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.016617681s
    Jul 29 16:36:10.552: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.016616859s
    Jul 29 16:36:12.551: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.014997626s
    Jul 29 16:36:14.552: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.016031958s
    Jul 29 16:36:16.550: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.014559299s
    Jul 29 16:36:18.551: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.015514756s
    Jul 29 16:36:20.553: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.016788258s
    Jul 29 16:36:22.553: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.017614283s
    Jul 29 16:36:24.554: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.018378771s
    Jul 29 16:36:26.553: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.017331245s
    Jul 29 16:36:28.552: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.016078261s
    Jul 29 16:36:30.551: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.015470866s
    Jul 29 16:36:32.552: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.016455347s
    Jul 29 16:36:34.553: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.016839794s
    Jul 29 16:36:36.553: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.017608439s
    Jul 29 16:36:38.551: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.015262116s
    Jul 29 16:36:40.554: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.017965814s
    Jul 29 16:36:42.554: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.017861231s
    Jul 29 16:36:44.552: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.016658529s
    Jul 29 16:36:46.549: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.013173304s
    Jul 29 16:36:48.552: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.016258046s
    Jul 29 16:36:50.552: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.016490798s
    Jul 29 16:36:52.552: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.016436757s
    Jul 29 16:36:54.554: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.018013118s
    Jul 29 16:36:56.552: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.016500382s
    Jul 29 16:36:58.553: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.016860363s
    Jul 29 16:37:00.553: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.016977008s
    Jul 29 16:37:02.551: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.015563515s
    Jul 29 16:37:04.553: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.016838155s
    Jul 29 16:37:04.561: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.024853941s
    STEP: updating the pod 07/29/23 16:37:04.561
    Jul 29 16:37:05.083: INFO: Successfully updated pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3"
    STEP: waiting for pod running 07/29/23 16:37:05.083
    Jul 29 16:37:05.099: INFO: Waiting up to 2m0s for pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3" in namespace "var-expansion-8098" to be "running"
    Jul 29 16:37:05.113: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Pending", Reason="", readiness=false. Elapsed: 14.175715ms
    Jul 29 16:37:07.122: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3": Phase="Running", Reason="", readiness=true. Elapsed: 2.023013696s
    Jul 29 16:37:07.122: INFO: Pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3" satisfied condition "running"
    STEP: deleting the pod gracefully 07/29/23 16:37:07.122
    Jul 29 16:37:07.123: INFO: Deleting pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3" in namespace "var-expansion-8098"
    Jul 29 16:37:07.135: INFO: Wait up to 5m0s for pod "var-expansion-6dd30002-329c-4b8b-9e5a-57c61a038be3" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jul 29 16:37:39.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-8098" for this suite. 07/29/23 16:37:39.198
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:37:39.22
Jul 29 16:37:39.221: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename deployment 07/29/23 16:37:39.223
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:37:39.272
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:37:39.279
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
Jul 29 16:37:39.296: INFO: Creating simple deployment test-new-deployment
Jul 29 16:37:39.344: INFO: deployment "test-new-deployment" doesn't have the required revision set
STEP: getting scale subresource 07/29/23 16:37:41.373
STEP: updating a scale subresource 07/29/23 16:37:41.38
STEP: verifying the deployment Spec.Replicas was modified 07/29/23 16:37:41.393
STEP: Patch a scale subresource 07/29/23 16:37:41.399
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jul 29 16:37:41.440: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-8700  27d55018-bd97-473c-a6c0-6afa6b4e6513 26363 3 2023-07-29 16:37:39 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-07-29 16:37:39 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-29 16:37:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003b506c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-07-29 16:37:40 +0000 UTC,LastTransitionTime:2023-07-29 16:37:40 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-845c8977d9" has successfully progressed.,LastUpdateTime:2023-07-29 16:37:40 +0000 UTC,LastTransitionTime:2023-07-29 16:37:39 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jul 29 16:37:41.476: INFO: New ReplicaSet "test-new-deployment-845c8977d9" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-845c8977d9  deployment-8700  628bd5e9-be93-49fa-98b6-d6fa950b281e 26366 2 2023-07-29 16:37:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 27d55018-bd97-473c-a6c0-6afa6b4e6513 0xc003b50b17 0xc003b50b18}] [] [{kube-controller-manager Update apps/v1 2023-07-29 16:37:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"27d55018-bd97-473c-a6c0-6afa6b4e6513\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-29 16:37:41 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003b50ba8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jul 29 16:37:41.493: INFO: Pod "test-new-deployment-845c8977d9-pnpwc" is not available:
&Pod{ObjectMeta:{test-new-deployment-845c8977d9-pnpwc test-new-deployment-845c8977d9- deployment-8700  90688dda-9077-4441-bbae-177baaf7a9a7 26371 0 2023-07-29 16:37:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 628bd5e9-be93-49fa-98b6-d6fa950b281e 0xc003865777 0xc003865778}] [] [{kube-controller-manager Update v1 2023-07-29 16:37:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"628bd5e9-be93-49fa-98b6-d6fa950b281e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 16:37:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6rjx7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6rjx7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wa4quivohpee-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:37:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:37:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:37:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:37:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.28,PodIP:,StartTime:2023-07-29 16:37:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 29 16:37:41.494: INFO: Pod "test-new-deployment-845c8977d9-wtpfs" is available:
&Pod{ObjectMeta:{test-new-deployment-845c8977d9-wtpfs test-new-deployment-845c8977d9- deployment-8700  1388df5f-1b5c-4ded-a7a2-92115445b9a8 26355 0 2023-07-29 16:37:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 628bd5e9-be93-49fa-98b6-d6fa950b281e 0xc003865947 0xc003865948}] [] [{kube-controller-manager Update v1 2023-07-29 16:37:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"628bd5e9-be93-49fa-98b6-d6fa950b281e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 16:37:40 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.65.77\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zvchb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zvchb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wa4quivohpee-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:37:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:37:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:37:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:37:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.234,PodIP:10.233.65.77,StartTime:2023-07-29 16:37:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-29 16:37:40 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://1e9878e7980981346e1b2fb2bfda9eeec1be0b6318e67228e7fa1fcf8101afd1,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.65.77,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jul 29 16:37:41.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8700" for this suite. 07/29/23 16:37:41.505
{"msg":"PASSED [sig-apps] Deployment Deployment should have a working scale subresource [Conformance]","completed":257,"skipped":4763,"failed":0}
------------------------------
â€¢ [2.310 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:37:39.22
    Jul 29 16:37:39.221: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename deployment 07/29/23 16:37:39.223
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:37:39.272
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:37:39.279
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] Deployment should have a working scale subresource [Conformance]
      test/e2e/apps/deployment.go:150
    Jul 29 16:37:39.296: INFO: Creating simple deployment test-new-deployment
    Jul 29 16:37:39.344: INFO: deployment "test-new-deployment" doesn't have the required revision set
    STEP: getting scale subresource 07/29/23 16:37:41.373
    STEP: updating a scale subresource 07/29/23 16:37:41.38
    STEP: verifying the deployment Spec.Replicas was modified 07/29/23 16:37:41.393
    STEP: Patch a scale subresource 07/29/23 16:37:41.399
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jul 29 16:37:41.440: INFO: Deployment "test-new-deployment":
    &Deployment{ObjectMeta:{test-new-deployment  deployment-8700  27d55018-bd97-473c-a6c0-6afa6b4e6513 26363 3 2023-07-29 16:37:39 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-07-29 16:37:39 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-29 16:37:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003b506c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-07-29 16:37:40 +0000 UTC,LastTransitionTime:2023-07-29 16:37:40 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-845c8977d9" has successfully progressed.,LastUpdateTime:2023-07-29 16:37:40 +0000 UTC,LastTransitionTime:2023-07-29 16:37:39 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jul 29 16:37:41.476: INFO: New ReplicaSet "test-new-deployment-845c8977d9" of Deployment "test-new-deployment":
    &ReplicaSet{ObjectMeta:{test-new-deployment-845c8977d9  deployment-8700  628bd5e9-be93-49fa-98b6-d6fa950b281e 26366 2 2023-07-29 16:37:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 27d55018-bd97-473c-a6c0-6afa6b4e6513 0xc003b50b17 0xc003b50b18}] [] [{kube-controller-manager Update apps/v1 2023-07-29 16:37:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"27d55018-bd97-473c-a6c0-6afa6b4e6513\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-29 16:37:41 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003b50ba8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jul 29 16:37:41.493: INFO: Pod "test-new-deployment-845c8977d9-pnpwc" is not available:
    &Pod{ObjectMeta:{test-new-deployment-845c8977d9-pnpwc test-new-deployment-845c8977d9- deployment-8700  90688dda-9077-4441-bbae-177baaf7a9a7 26371 0 2023-07-29 16:37:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 628bd5e9-be93-49fa-98b6-d6fa950b281e 0xc003865777 0xc003865778}] [] [{kube-controller-manager Update v1 2023-07-29 16:37:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"628bd5e9-be93-49fa-98b6-d6fa950b281e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 16:37:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6rjx7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6rjx7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wa4quivohpee-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:37:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:37:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:37:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:37:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.28,PodIP:,StartTime:2023-07-29 16:37:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jul 29 16:37:41.494: INFO: Pod "test-new-deployment-845c8977d9-wtpfs" is available:
    &Pod{ObjectMeta:{test-new-deployment-845c8977d9-wtpfs test-new-deployment-845c8977d9- deployment-8700  1388df5f-1b5c-4ded-a7a2-92115445b9a8 26355 0 2023-07-29 16:37:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 628bd5e9-be93-49fa-98b6-d6fa950b281e 0xc003865947 0xc003865948}] [] [{kube-controller-manager Update v1 2023-07-29 16:37:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"628bd5e9-be93-49fa-98b6-d6fa950b281e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 16:37:40 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.65.77\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zvchb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zvchb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wa4quivohpee-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:37:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:37:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:37:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:37:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.234,PodIP:10.233.65.77,StartTime:2023-07-29 16:37:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-29 16:37:40 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://1e9878e7980981346e1b2fb2bfda9eeec1be0b6318e67228e7fa1fcf8101afd1,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.65.77,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jul 29 16:37:41.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-8700" for this suite. 07/29/23 16:37:41.505
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:37:41.547
Jul 29 16:37:41.548: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename replicaset 07/29/23 16:37:41.549
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:37:41.578
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:37:41.582
[It] Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
Jul 29 16:37:41.614: INFO: Pod name sample-pod: Found 0 pods out of 1
Jul 29 16:37:46.634: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 07/29/23 16:37:46.634
STEP: Scaling up "test-rs" replicaset  07/29/23 16:37:46.635
Jul 29 16:37:46.659: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet 07/29/23 16:37:46.659
W0729 16:37:46.678120      13 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Jul 29 16:37:46.684: INFO: observed ReplicaSet test-rs in namespace replicaset-3548 with ReadyReplicas 1, AvailableReplicas 1
Jul 29 16:37:46.779: INFO: observed ReplicaSet test-rs in namespace replicaset-3548 with ReadyReplicas 1, AvailableReplicas 1
Jul 29 16:37:46.933: INFO: observed ReplicaSet test-rs in namespace replicaset-3548 with ReadyReplicas 1, AvailableReplicas 1
Jul 29 16:37:46.980: INFO: observed ReplicaSet test-rs in namespace replicaset-3548 with ReadyReplicas 1, AvailableReplicas 1
Jul 29 16:37:47.828: INFO: observed ReplicaSet test-rs in namespace replicaset-3548 with ReadyReplicas 2, AvailableReplicas 2
Jul 29 16:37:49.293: INFO: observed Replicaset test-rs in namespace replicaset-3548 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Jul 29 16:37:49.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-3548" for this suite. 07/29/23 16:37:49.305
{"msg":"PASSED [sig-apps] ReplicaSet Replace and Patch tests [Conformance]","completed":258,"skipped":4806,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.768 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:37:41.547
    Jul 29 16:37:41.548: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename replicaset 07/29/23 16:37:41.549
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:37:41.578
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:37:41.582
    [It] Replace and Patch tests [Conformance]
      test/e2e/apps/replica_set.go:154
    Jul 29 16:37:41.614: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jul 29 16:37:46.634: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 07/29/23 16:37:46.634
    STEP: Scaling up "test-rs" replicaset  07/29/23 16:37:46.635
    Jul 29 16:37:46.659: INFO: Updating replica set "test-rs"
    STEP: patching the ReplicaSet 07/29/23 16:37:46.659
    W0729 16:37:46.678120      13 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Jul 29 16:37:46.684: INFO: observed ReplicaSet test-rs in namespace replicaset-3548 with ReadyReplicas 1, AvailableReplicas 1
    Jul 29 16:37:46.779: INFO: observed ReplicaSet test-rs in namespace replicaset-3548 with ReadyReplicas 1, AvailableReplicas 1
    Jul 29 16:37:46.933: INFO: observed ReplicaSet test-rs in namespace replicaset-3548 with ReadyReplicas 1, AvailableReplicas 1
    Jul 29 16:37:46.980: INFO: observed ReplicaSet test-rs in namespace replicaset-3548 with ReadyReplicas 1, AvailableReplicas 1
    Jul 29 16:37:47.828: INFO: observed ReplicaSet test-rs in namespace replicaset-3548 with ReadyReplicas 2, AvailableReplicas 2
    Jul 29 16:37:49.293: INFO: observed Replicaset test-rs in namespace replicaset-3548 with ReadyReplicas 3 found true
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Jul 29 16:37:49.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-3548" for this suite. 07/29/23 16:37:49.305
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:37:49.336
Jul 29 16:37:49.336: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename resourcequota 07/29/23 16:37:49.347
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:37:49.389
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:37:49.394
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438
STEP: Counting existing ResourceQuota 07/29/23 16:37:49.399
STEP: Creating a ResourceQuota 07/29/23 16:37:54.425
STEP: Ensuring resource quota status is calculated 07/29/23 16:37:54.45
STEP: Creating a ReplicaSet 07/29/23 16:37:56.46
STEP: Ensuring resource quota status captures replicaset creation 07/29/23 16:37:56.495
STEP: Deleting a ReplicaSet 07/29/23 16:37:58.502
STEP: Ensuring resource quota status released usage 07/29/23 16:37:58.519
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jul 29 16:38:00.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5397" for this suite. 07/29/23 16:38:00.536
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","completed":259,"skipped":4836,"failed":0}
------------------------------
â€¢ [SLOW TEST] [11.214 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:37:49.336
    Jul 29 16:37:49.336: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename resourcequota 07/29/23 16:37:49.347
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:37:49.389
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:37:49.394
    [It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
      test/e2e/apimachinery/resource_quota.go:438
    STEP: Counting existing ResourceQuota 07/29/23 16:37:49.399
    STEP: Creating a ResourceQuota 07/29/23 16:37:54.425
    STEP: Ensuring resource quota status is calculated 07/29/23 16:37:54.45
    STEP: Creating a ReplicaSet 07/29/23 16:37:56.46
    STEP: Ensuring resource quota status captures replicaset creation 07/29/23 16:37:56.495
    STEP: Deleting a ReplicaSet 07/29/23 16:37:58.502
    STEP: Ensuring resource quota status released usage 07/29/23 16:37:58.519
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jul 29 16:38:00.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-5397" for this suite. 07/29/23 16:38:00.536
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:38:00.556
Jul 29 16:38:00.556: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename var-expansion 07/29/23 16:38:00.559
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:38:00.591
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:38:00.596
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151
Jul 29 16:38:00.619: INFO: Waiting up to 2m0s for pod "var-expansion-c0e78bef-6129-4aad-b8a7-23aa2028e7c6" in namespace "var-expansion-4778" to be "container 0 failed with reason CreateContainerConfigError"
Jul 29 16:38:00.635: INFO: Pod "var-expansion-c0e78bef-6129-4aad-b8a7-23aa2028e7c6": Phase="Pending", Reason="", readiness=false. Elapsed: 15.306251ms
Jul 29 16:38:02.654: INFO: Pod "var-expansion-c0e78bef-6129-4aad-b8a7-23aa2028e7c6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034643801s
Jul 29 16:38:02.655: INFO: Pod "var-expansion-c0e78bef-6129-4aad-b8a7-23aa2028e7c6" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Jul 29 16:38:02.655: INFO: Deleting pod "var-expansion-c0e78bef-6129-4aad-b8a7-23aa2028e7c6" in namespace "var-expansion-4778"
Jul 29 16:38:02.672: INFO: Wait up to 5m0s for pod "var-expansion-c0e78bef-6129-4aad-b8a7-23aa2028e7c6" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jul 29 16:38:04.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4778" for this suite. 07/29/23 16:38:04.718
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]","completed":260,"skipped":4839,"failed":0}
------------------------------
â€¢ [4.176 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:38:00.556
    Jul 29 16:38:00.556: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename var-expansion 07/29/23 16:38:00.559
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:38:00.591
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:38:00.596
    [It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
      test/e2e/common/node/expansion.go:151
    Jul 29 16:38:00.619: INFO: Waiting up to 2m0s for pod "var-expansion-c0e78bef-6129-4aad-b8a7-23aa2028e7c6" in namespace "var-expansion-4778" to be "container 0 failed with reason CreateContainerConfigError"
    Jul 29 16:38:00.635: INFO: Pod "var-expansion-c0e78bef-6129-4aad-b8a7-23aa2028e7c6": Phase="Pending", Reason="", readiness=false. Elapsed: 15.306251ms
    Jul 29 16:38:02.654: INFO: Pod "var-expansion-c0e78bef-6129-4aad-b8a7-23aa2028e7c6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034643801s
    Jul 29 16:38:02.655: INFO: Pod "var-expansion-c0e78bef-6129-4aad-b8a7-23aa2028e7c6" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Jul 29 16:38:02.655: INFO: Deleting pod "var-expansion-c0e78bef-6129-4aad-b8a7-23aa2028e7c6" in namespace "var-expansion-4778"
    Jul 29 16:38:02.672: INFO: Wait up to 5m0s for pod "var-expansion-c0e78bef-6129-4aad-b8a7-23aa2028e7c6" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jul 29 16:38:04.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-4778" for this suite. 07/29/23 16:38:04.718
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-architecture] Conformance Tests
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
[BeforeEach] [sig-architecture] Conformance Tests
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:38:04.733
Jul 29 16:38:04.733: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename conformance-tests 07/29/23 16:38:04.735
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:38:04.762
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:38:04.768
[It] should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
STEP: Getting node addresses 07/29/23 16:38:04.772
Jul 29 16:38:04.772: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
[AfterEach] [sig-architecture] Conformance Tests
  test/e2e/framework/framework.go:187
Jul 29 16:38:04.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "conformance-tests-6709" for this suite. 07/29/23 16:38:04.791
{"msg":"PASSED [sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]","completed":261,"skipped":4842,"failed":0}
------------------------------
â€¢ [0.069 seconds]
[sig-architecture] Conformance Tests
test/e2e/architecture/framework.go:23
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-architecture] Conformance Tests
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:38:04.733
    Jul 29 16:38:04.733: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename conformance-tests 07/29/23 16:38:04.735
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:38:04.762
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:38:04.768
    [It] should have at least two untainted nodes [Conformance]
      test/e2e/architecture/conformance.go:38
    STEP: Getting node addresses 07/29/23 16:38:04.772
    Jul 29 16:38:04.772: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    [AfterEach] [sig-architecture] Conformance Tests
      test/e2e/framework/framework.go:187
    Jul 29 16:38:04.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "conformance-tests-6709" for this suite. 07/29/23 16:38:04.791
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:38:04.804
Jul 29 16:38:04.805: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename emptydir 07/29/23 16:38:04.808
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:38:04.833
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:38:04.842
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166
STEP: Creating a pod to test emptydir 0644 on node default medium 07/29/23 16:38:04.847
Jul 29 16:38:04.861: INFO: Waiting up to 5m0s for pod "pod-2e1ff5ab-8b16-41d8-8b5f-62db1bfe955c" in namespace "emptydir-9643" to be "Succeeded or Failed"
Jul 29 16:38:04.868: INFO: Pod "pod-2e1ff5ab-8b16-41d8-8b5f-62db1bfe955c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.957815ms
Jul 29 16:38:06.873: INFO: Pod "pod-2e1ff5ab-8b16-41d8-8b5f-62db1bfe955c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012576268s
Jul 29 16:38:08.885: INFO: Pod "pod-2e1ff5ab-8b16-41d8-8b5f-62db1bfe955c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024415872s
STEP: Saw pod success 07/29/23 16:38:08.886
Jul 29 16:38:08.886: INFO: Pod "pod-2e1ff5ab-8b16-41d8-8b5f-62db1bfe955c" satisfied condition "Succeeded or Failed"
Jul 29 16:38:08.891: INFO: Trying to get logs from node wa4quivohpee-3 pod pod-2e1ff5ab-8b16-41d8-8b5f-62db1bfe955c container test-container: <nil>
STEP: delete the pod 07/29/23 16:38:08.924
Jul 29 16:38:08.941: INFO: Waiting for pod pod-2e1ff5ab-8b16-41d8-8b5f-62db1bfe955c to disappear
Jul 29 16:38:08.946: INFO: Pod pod-2e1ff5ab-8b16-41d8-8b5f-62db1bfe955c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jul 29 16:38:08.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9643" for this suite. 07/29/23 16:38:08.956
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":262,"skipped":4847,"failed":0}
------------------------------
â€¢ [4.162 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:38:04.804
    Jul 29 16:38:04.805: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename emptydir 07/29/23 16:38:04.808
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:38:04.833
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:38:04.842
    [It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:166
    STEP: Creating a pod to test emptydir 0644 on node default medium 07/29/23 16:38:04.847
    Jul 29 16:38:04.861: INFO: Waiting up to 5m0s for pod "pod-2e1ff5ab-8b16-41d8-8b5f-62db1bfe955c" in namespace "emptydir-9643" to be "Succeeded or Failed"
    Jul 29 16:38:04.868: INFO: Pod "pod-2e1ff5ab-8b16-41d8-8b5f-62db1bfe955c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.957815ms
    Jul 29 16:38:06.873: INFO: Pod "pod-2e1ff5ab-8b16-41d8-8b5f-62db1bfe955c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012576268s
    Jul 29 16:38:08.885: INFO: Pod "pod-2e1ff5ab-8b16-41d8-8b5f-62db1bfe955c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024415872s
    STEP: Saw pod success 07/29/23 16:38:08.886
    Jul 29 16:38:08.886: INFO: Pod "pod-2e1ff5ab-8b16-41d8-8b5f-62db1bfe955c" satisfied condition "Succeeded or Failed"
    Jul 29 16:38:08.891: INFO: Trying to get logs from node wa4quivohpee-3 pod pod-2e1ff5ab-8b16-41d8-8b5f-62db1bfe955c container test-container: <nil>
    STEP: delete the pod 07/29/23 16:38:08.924
    Jul 29 16:38:08.941: INFO: Waiting for pod pod-2e1ff5ab-8b16-41d8-8b5f-62db1bfe955c to disappear
    Jul 29 16:38:08.946: INFO: Pod pod-2e1ff5ab-8b16-41d8-8b5f-62db1bfe955c no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jul 29 16:38:08.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-9643" for this suite. 07/29/23 16:38:08.956
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:38:08.987
Jul 29 16:38:08.987: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename replicaset 07/29/23 16:38:08.989
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:38:09.021
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:38:09.027
[It] Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 07/29/23 16:38:09.032
Jul 29 16:38:09.055: INFO: Pod name sample-pod: Found 0 pods out of 1
Jul 29 16:38:14.064: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 07/29/23 16:38:14.064
STEP: getting scale subresource 07/29/23 16:38:14.065
STEP: updating a scale subresource 07/29/23 16:38:14.072
STEP: verifying the replicaset Spec.Replicas was modified 07/29/23 16:38:14.082
STEP: Patch a scale subresource 07/29/23 16:38:14.093
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Jul 29 16:38:14.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-4176" for this suite. 07/29/23 16:38:14.15
{"msg":"PASSED [sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]","completed":263,"skipped":4917,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.192 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:38:08.987
    Jul 29 16:38:08.987: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename replicaset 07/29/23 16:38:08.989
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:38:09.021
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:38:09.027
    [It] Replicaset should have a working scale subresource [Conformance]
      test/e2e/apps/replica_set.go:143
    STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 07/29/23 16:38:09.032
    Jul 29 16:38:09.055: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jul 29 16:38:14.064: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 07/29/23 16:38:14.064
    STEP: getting scale subresource 07/29/23 16:38:14.065
    STEP: updating a scale subresource 07/29/23 16:38:14.072
    STEP: verifying the replicaset Spec.Replicas was modified 07/29/23 16:38:14.082
    STEP: Patch a scale subresource 07/29/23 16:38:14.093
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Jul 29 16:38:14.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-4176" for this suite. 07/29/23 16:38:14.15
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] Job
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:38:14.18
Jul 29 16:38:14.181: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename job 07/29/23 16:38:14.187
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:38:14.221
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:38:14.228
[It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194
STEP: Creating Indexed job 07/29/23 16:38:14.232
STEP: Ensuring job reaches completions 07/29/23 16:38:14.241
STEP: Ensuring pods with index for job exist 07/29/23 16:38:24.249
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Jul 29 16:38:24.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-7888" for this suite. 07/29/23 16:38:24.271
{"msg":"PASSED [sig-apps] Job should create pods for an Indexed job with completion indexes and specified hostname [Conformance]","completed":264,"skipped":4924,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.103 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:38:14.18
    Jul 29 16:38:14.181: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename job 07/29/23 16:38:14.187
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:38:14.221
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:38:14.228
    [It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
      test/e2e/apps/job.go:194
    STEP: Creating Indexed job 07/29/23 16:38:14.232
    STEP: Ensuring job reaches completions 07/29/23 16:38:14.241
    STEP: Ensuring pods with index for job exist 07/29/23 16:38:24.249
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Jul 29 16:38:24.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-7888" for this suite. 07/29/23 16:38:24.271
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:38:24.295
Jul 29 16:38:24.295: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename emptydir 07/29/23 16:38:24.297
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:38:24.332
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:38:24.336
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146
STEP: Creating a pod to test emptydir 0777 on tmpfs 07/29/23 16:38:24.341
Jul 29 16:38:24.361: INFO: Waiting up to 5m0s for pod "pod-5e36209a-ffd6-460f-9ee9-9f03b20f0058" in namespace "emptydir-9697" to be "Succeeded or Failed"
Jul 29 16:38:24.376: INFO: Pod "pod-5e36209a-ffd6-460f-9ee9-9f03b20f0058": Phase="Pending", Reason="", readiness=false. Elapsed: 15.117054ms
Jul 29 16:38:26.389: INFO: Pod "pod-5e36209a-ffd6-460f-9ee9-9f03b20f0058": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027951914s
Jul 29 16:38:28.383: INFO: Pod "pod-5e36209a-ffd6-460f-9ee9-9f03b20f0058": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021367171s
STEP: Saw pod success 07/29/23 16:38:28.383
Jul 29 16:38:28.383: INFO: Pod "pod-5e36209a-ffd6-460f-9ee9-9f03b20f0058" satisfied condition "Succeeded or Failed"
Jul 29 16:38:28.395: INFO: Trying to get logs from node wa4quivohpee-3 pod pod-5e36209a-ffd6-460f-9ee9-9f03b20f0058 container test-container: <nil>
STEP: delete the pod 07/29/23 16:38:28.407
Jul 29 16:38:28.431: INFO: Waiting for pod pod-5e36209a-ffd6-460f-9ee9-9f03b20f0058 to disappear
Jul 29 16:38:28.440: INFO: Pod pod-5e36209a-ffd6-460f-9ee9-9f03b20f0058 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jul 29 16:38:28.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9697" for this suite. 07/29/23 16:38:28.448
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":265,"skipped":4983,"failed":0}
------------------------------
â€¢ [4.173 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:38:24.295
    Jul 29 16:38:24.295: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename emptydir 07/29/23 16:38:24.297
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:38:24.332
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:38:24.336
    [It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:146
    STEP: Creating a pod to test emptydir 0777 on tmpfs 07/29/23 16:38:24.341
    Jul 29 16:38:24.361: INFO: Waiting up to 5m0s for pod "pod-5e36209a-ffd6-460f-9ee9-9f03b20f0058" in namespace "emptydir-9697" to be "Succeeded or Failed"
    Jul 29 16:38:24.376: INFO: Pod "pod-5e36209a-ffd6-460f-9ee9-9f03b20f0058": Phase="Pending", Reason="", readiness=false. Elapsed: 15.117054ms
    Jul 29 16:38:26.389: INFO: Pod "pod-5e36209a-ffd6-460f-9ee9-9f03b20f0058": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027951914s
    Jul 29 16:38:28.383: INFO: Pod "pod-5e36209a-ffd6-460f-9ee9-9f03b20f0058": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021367171s
    STEP: Saw pod success 07/29/23 16:38:28.383
    Jul 29 16:38:28.383: INFO: Pod "pod-5e36209a-ffd6-460f-9ee9-9f03b20f0058" satisfied condition "Succeeded or Failed"
    Jul 29 16:38:28.395: INFO: Trying to get logs from node wa4quivohpee-3 pod pod-5e36209a-ffd6-460f-9ee9-9f03b20f0058 container test-container: <nil>
    STEP: delete the pod 07/29/23 16:38:28.407
    Jul 29 16:38:28.431: INFO: Waiting for pod pod-5e36209a-ffd6-460f-9ee9-9f03b20f0058 to disappear
    Jul 29 16:38:28.440: INFO: Pod pod-5e36209a-ffd6-460f-9ee9-9f03b20f0058 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jul 29 16:38:28.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-9697" for this suite. 07/29/23 16:38:28.448
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:38:28.474
Jul 29 16:38:28.475: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename namespaces 07/29/23 16:38:28.477
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:38:28.511
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:38:28.521
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242
STEP: Creating a test namespace 07/29/23 16:38:28.557
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:38:28.585
STEP: Creating a pod in the namespace 07/29/23 16:38:28.594
STEP: Waiting for the pod to have running status 07/29/23 16:38:28.613
Jul 29 16:38:28.613: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-1820" to be "running"
Jul 29 16:38:28.628: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 14.536347ms
Jul 29 16:38:30.634: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.020794219s
Jul 29 16:38:30.634: INFO: Pod "test-pod" satisfied condition "running"
STEP: Deleting the namespace 07/29/23 16:38:30.635
STEP: Waiting for the namespace to be removed. 07/29/23 16:38:30.649
STEP: Recreating the namespace 07/29/23 16:38:41.657
STEP: Verifying there are no pods in the namespace 07/29/23 16:38:41.691
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Jul 29 16:38:41.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-9508" for this suite. 07/29/23 16:38:41.717
STEP: Destroying namespace "nsdeletetest-1820" for this suite. 07/29/23 16:38:41.738
Jul 29 16:38:41.745: INFO: Namespace nsdeletetest-1820 was already deleted
STEP: Destroying namespace "nsdeletetest-2131" for this suite. 07/29/23 16:38:41.746
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","completed":266,"skipped":4996,"failed":0}
------------------------------
â€¢ [SLOW TEST] [13.287 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:38:28.474
    Jul 29 16:38:28.475: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename namespaces 07/29/23 16:38:28.477
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:38:28.511
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:38:28.521
    [It] should ensure that all pods are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:242
    STEP: Creating a test namespace 07/29/23 16:38:28.557
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:38:28.585
    STEP: Creating a pod in the namespace 07/29/23 16:38:28.594
    STEP: Waiting for the pod to have running status 07/29/23 16:38:28.613
    Jul 29 16:38:28.613: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-1820" to be "running"
    Jul 29 16:38:28.628: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 14.536347ms
    Jul 29 16:38:30.634: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.020794219s
    Jul 29 16:38:30.634: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Deleting the namespace 07/29/23 16:38:30.635
    STEP: Waiting for the namespace to be removed. 07/29/23 16:38:30.649
    STEP: Recreating the namespace 07/29/23 16:38:41.657
    STEP: Verifying there are no pods in the namespace 07/29/23 16:38:41.691
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Jul 29 16:38:41.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-9508" for this suite. 07/29/23 16:38:41.717
    STEP: Destroying namespace "nsdeletetest-1820" for this suite. 07/29/23 16:38:41.738
    Jul 29 16:38:41.745: INFO: Namespace nsdeletetest-1820 was already deleted
    STEP: Destroying namespace "nsdeletetest-2131" for this suite. 07/29/23 16:38:41.746
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:38:41.762
Jul 29 16:38:41.762: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename services 07/29/23 16:38:41.766
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:38:41.788
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:38:41.795
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443
STEP: creating a service externalname-service with the type=ExternalName in namespace services-9018 07/29/23 16:38:41.801
STEP: changing the ExternalName service to type=NodePort 07/29/23 16:38:41.819
STEP: creating replication controller externalname-service in namespace services-9018 07/29/23 16:38:41.88
I0729 16:38:41.888729      13 runners.go:193] Created replication controller with name: externalname-service, namespace: services-9018, replica count: 2
I0729 16:38:44.947357      13 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 29 16:38:44.947: INFO: Creating new exec pod
Jul 29 16:38:44.958: INFO: Waiting up to 5m0s for pod "execpodcv94k" in namespace "services-9018" to be "running"
Jul 29 16:38:44.964: INFO: Pod "execpodcv94k": Phase="Pending", Reason="", readiness=false. Elapsed: 5.570706ms
Jul 29 16:38:46.974: INFO: Pod "execpodcv94k": Phase="Running", Reason="", readiness=true. Elapsed: 2.015611113s
Jul 29 16:38:46.974: INFO: Pod "execpodcv94k" satisfied condition "running"
Jul 29 16:38:47.992: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=services-9018 exec execpodcv94k -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jul 29 16:38:48.289: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jul 29 16:38:48.289: INFO: stdout: ""
Jul 29 16:38:49.290: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=services-9018 exec execpodcv94k -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jul 29 16:38:49.540: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jul 29 16:38:49.540: INFO: stdout: "externalname-service-x9lvv"
Jul 29 16:38:49.541: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=services-9018 exec execpodcv94k -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.23.110 80'
Jul 29 16:38:49.762: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.23.110 80\nConnection to 10.233.23.110 80 port [tcp/http] succeeded!\n"
Jul 29 16:38:49.762: INFO: stdout: ""
Jul 29 16:38:50.762: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=services-9018 exec execpodcv94k -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.23.110 80'
Jul 29 16:38:50.997: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.23.110 80\nConnection to 10.233.23.110 80 port [tcp/http] succeeded!\n"
Jul 29 16:38:50.997: INFO: stdout: "externalname-service-22kpn"
Jul 29 16:38:50.997: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=services-9018 exec execpodcv94k -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.234 30415'
Jul 29 16:38:51.207: INFO: stderr: "+ + nc -v -t -w 2 192.168.121.234 30415\necho hostName\nConnection to 192.168.121.234 30415 port [tcp/*] succeeded!\n"
Jul 29 16:38:51.207: INFO: stdout: "externalname-service-22kpn"
Jul 29 16:38:51.208: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=services-9018 exec execpodcv94k -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.28 30415'
Jul 29 16:38:51.464: INFO: stderr: "+ nc -v -t -w 2 192.168.121.28 30415\n+ echo hostName\nConnection to 192.168.121.28 30415 port [tcp/*] succeeded!\n"
Jul 29 16:38:51.464: INFO: stdout: "externalname-service-x9lvv"
Jul 29 16:38:51.464: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jul 29 16:38:51.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9018" for this suite. 07/29/23 16:38:51.548
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","completed":267,"skipped":5002,"failed":0}
------------------------------
â€¢ [SLOW TEST] [9.800 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:38:41.762
    Jul 29 16:38:41.762: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename services 07/29/23 16:38:41.766
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:38:41.788
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:38:41.795
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ExternalName to NodePort [Conformance]
      test/e2e/network/service.go:1443
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-9018 07/29/23 16:38:41.801
    STEP: changing the ExternalName service to type=NodePort 07/29/23 16:38:41.819
    STEP: creating replication controller externalname-service in namespace services-9018 07/29/23 16:38:41.88
    I0729 16:38:41.888729      13 runners.go:193] Created replication controller with name: externalname-service, namespace: services-9018, replica count: 2
    I0729 16:38:44.947357      13 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jul 29 16:38:44.947: INFO: Creating new exec pod
    Jul 29 16:38:44.958: INFO: Waiting up to 5m0s for pod "execpodcv94k" in namespace "services-9018" to be "running"
    Jul 29 16:38:44.964: INFO: Pod "execpodcv94k": Phase="Pending", Reason="", readiness=false. Elapsed: 5.570706ms
    Jul 29 16:38:46.974: INFO: Pod "execpodcv94k": Phase="Running", Reason="", readiness=true. Elapsed: 2.015611113s
    Jul 29 16:38:46.974: INFO: Pod "execpodcv94k" satisfied condition "running"
    Jul 29 16:38:47.992: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=services-9018 exec execpodcv94k -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Jul 29 16:38:48.289: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Jul 29 16:38:48.289: INFO: stdout: ""
    Jul 29 16:38:49.290: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=services-9018 exec execpodcv94k -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Jul 29 16:38:49.540: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Jul 29 16:38:49.540: INFO: stdout: "externalname-service-x9lvv"
    Jul 29 16:38:49.541: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=services-9018 exec execpodcv94k -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.23.110 80'
    Jul 29 16:38:49.762: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.23.110 80\nConnection to 10.233.23.110 80 port [tcp/http] succeeded!\n"
    Jul 29 16:38:49.762: INFO: stdout: ""
    Jul 29 16:38:50.762: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=services-9018 exec execpodcv94k -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.23.110 80'
    Jul 29 16:38:50.997: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.23.110 80\nConnection to 10.233.23.110 80 port [tcp/http] succeeded!\n"
    Jul 29 16:38:50.997: INFO: stdout: "externalname-service-22kpn"
    Jul 29 16:38:50.997: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=services-9018 exec execpodcv94k -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.234 30415'
    Jul 29 16:38:51.207: INFO: stderr: "+ + nc -v -t -w 2 192.168.121.234 30415\necho hostName\nConnection to 192.168.121.234 30415 port [tcp/*] succeeded!\n"
    Jul 29 16:38:51.207: INFO: stdout: "externalname-service-22kpn"
    Jul 29 16:38:51.208: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=services-9018 exec execpodcv94k -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.28 30415'
    Jul 29 16:38:51.464: INFO: stderr: "+ nc -v -t -w 2 192.168.121.28 30415\n+ echo hostName\nConnection to 192.168.121.28 30415 port [tcp/*] succeeded!\n"
    Jul 29 16:38:51.464: INFO: stdout: "externalname-service-x9lvv"
    Jul 29 16:38:51.464: INFO: Cleaning up the ExternalName to NodePort test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jul 29 16:38:51.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-9018" for this suite. 07/29/23 16:38:51.548
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:38:51.565
Jul 29 16:38:51.566: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename sched-preemption 07/29/23 16:38:51.568
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:38:51.601
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:38:51.605
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Jul 29 16:38:51.663: INFO: Waiting up to 1m0s for all nodes to be ready
Jul 29 16:39:51.728: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125
STEP: Create pods that use 4/5 of node resources. 07/29/23 16:39:51.734
Jul 29 16:39:51.766: INFO: Created pod: pod0-0-sched-preemption-low-priority
Jul 29 16:39:51.777: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Jul 29 16:39:51.833: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Jul 29 16:39:51.854: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Jul 29 16:39:51.896: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Jul 29 16:39:51.927: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 07/29/23 16:39:51.927
Jul 29 16:39:51.928: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-7638" to be "running"
Jul 29 16:39:51.941: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 12.159353ms
Jul 29 16:39:53.951: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.022551867s
Jul 29 16:39:53.951: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Jul 29 16:39:53.951: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-7638" to be "running"
Jul 29 16:39:53.956: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.094291ms
Jul 29 16:39:53.956: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Jul 29 16:39:53.956: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-7638" to be "running"
Jul 29 16:39:53.964: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 7.940346ms
Jul 29 16:39:55.969: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.013699942s
Jul 29 16:39:55.969: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Jul 29 16:39:55.969: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-7638" to be "running"
Jul 29 16:39:55.976: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.32626ms
Jul 29 16:39:55.976: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
Jul 29 16:39:55.976: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-7638" to be "running"
Jul 29 16:39:55.982: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.07383ms
Jul 29 16:39:55.983: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
Jul 29 16:39:55.983: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-7638" to be "running"
Jul 29 16:39:55.987: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.865911ms
Jul 29 16:39:55.987: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a high priority pod that has same requirements as that of lower priority pod 07/29/23 16:39:55.987
Jul 29 16:39:56.002: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-7638" to be "running"
Jul 29 16:39:56.014: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 11.478748ms
Jul 29 16:39:58.022: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019512835s
Jul 29 16:40:00.028: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.025568423s
Jul 29 16:40:00.028: INFO: Pod "preemptor-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Jul 29 16:40:00.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-7638" for this suite. 07/29/23 16:40:00.069
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","completed":268,"skipped":5005,"failed":0}
------------------------------
â€¢ [SLOW TEST] [68.612 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:38:51.565
    Jul 29 16:38:51.566: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename sched-preemption 07/29/23 16:38:51.568
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:38:51.601
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:38:51.605
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Jul 29 16:38:51.663: INFO: Waiting up to 1m0s for all nodes to be ready
    Jul 29 16:39:51.728: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates basic preemption works [Conformance]
      test/e2e/scheduling/preemption.go:125
    STEP: Create pods that use 4/5 of node resources. 07/29/23 16:39:51.734
    Jul 29 16:39:51.766: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Jul 29 16:39:51.777: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Jul 29 16:39:51.833: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Jul 29 16:39:51.854: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    Jul 29 16:39:51.896: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    Jul 29 16:39:51.927: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 07/29/23 16:39:51.927
    Jul 29 16:39:51.928: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-7638" to be "running"
    Jul 29 16:39:51.941: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 12.159353ms
    Jul 29 16:39:53.951: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.022551867s
    Jul 29 16:39:53.951: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Jul 29 16:39:53.951: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-7638" to be "running"
    Jul 29 16:39:53.956: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.094291ms
    Jul 29 16:39:53.956: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Jul 29 16:39:53.956: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-7638" to be "running"
    Jul 29 16:39:53.964: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 7.940346ms
    Jul 29 16:39:55.969: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.013699942s
    Jul 29 16:39:55.969: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Jul 29 16:39:55.969: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-7638" to be "running"
    Jul 29 16:39:55.976: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.32626ms
    Jul 29 16:39:55.976: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    Jul 29 16:39:55.976: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-7638" to be "running"
    Jul 29 16:39:55.982: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.07383ms
    Jul 29 16:39:55.983: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    Jul 29 16:39:55.983: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-7638" to be "running"
    Jul 29 16:39:55.987: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.865911ms
    Jul 29 16:39:55.987: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a high priority pod that has same requirements as that of lower priority pod 07/29/23 16:39:55.987
    Jul 29 16:39:56.002: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-7638" to be "running"
    Jul 29 16:39:56.014: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 11.478748ms
    Jul 29 16:39:58.022: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019512835s
    Jul 29 16:40:00.028: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.025568423s
    Jul 29 16:40:00.028: INFO: Pod "preemptor-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Jul 29 16:40:00.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-7638" for this suite. 07/29/23 16:40:00.069
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:40:00.183
Jul 29 16:40:00.183: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename svcaccounts 07/29/23 16:40:00.186
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:40:00.212
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:40:00.215
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739
Jul 29 16:40:00.226: INFO: Got root ca configmap in namespace "svcaccounts-4602"
Jul 29 16:40:00.239: INFO: Deleted root ca configmap in namespace "svcaccounts-4602"
STEP: waiting for a new root ca configmap created 07/29/23 16:40:00.739
Jul 29 16:40:00.745: INFO: Recreated root ca configmap in namespace "svcaccounts-4602"
Jul 29 16:40:00.753: INFO: Updated root ca configmap in namespace "svcaccounts-4602"
STEP: waiting for the root ca configmap reconciled 07/29/23 16:40:01.255
Jul 29 16:40:01.268: INFO: Reconciled root ca configmap in namespace "svcaccounts-4602"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Jul 29 16:40:01.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-4602" for this suite. 07/29/23 16:40:01.278
{"msg":"PASSED [sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]","completed":269,"skipped":5014,"failed":0}
------------------------------
â€¢ [1.111 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:40:00.183
    Jul 29 16:40:00.183: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename svcaccounts 07/29/23 16:40:00.186
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:40:00.212
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:40:00.215
    [It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
      test/e2e/auth/service_accounts.go:739
    Jul 29 16:40:00.226: INFO: Got root ca configmap in namespace "svcaccounts-4602"
    Jul 29 16:40:00.239: INFO: Deleted root ca configmap in namespace "svcaccounts-4602"
    STEP: waiting for a new root ca configmap created 07/29/23 16:40:00.739
    Jul 29 16:40:00.745: INFO: Recreated root ca configmap in namespace "svcaccounts-4602"
    Jul 29 16:40:00.753: INFO: Updated root ca configmap in namespace "svcaccounts-4602"
    STEP: waiting for the root ca configmap reconciled 07/29/23 16:40:01.255
    Jul 29 16:40:01.268: INFO: Reconciled root ca configmap in namespace "svcaccounts-4602"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Jul 29 16:40:01.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-4602" for this suite. 07/29/23 16:40:01.278
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:40:01.296
Jul 29 16:40:01.297: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename crd-publish-openapi 07/29/23 16:40:01.299
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:40:01.322
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:40:01.332
[It] updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390
STEP: set up a multi version CRD 07/29/23 16:40:01.337
Jul 29 16:40:01.343: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: rename a version 07/29/23 16:40:14.734
STEP: check the new version name is served 07/29/23 16:40:14.766
STEP: check the old version name is removed 07/29/23 16:40:19.559
STEP: check the other version is not changed 07/29/23 16:40:21.862
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jul 29 16:40:32.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-262" for this suite. 07/29/23 16:40:32.329
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","completed":270,"skipped":5015,"failed":0}
------------------------------
â€¢ [SLOW TEST] [31.044 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:40:01.296
    Jul 29 16:40:01.297: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename crd-publish-openapi 07/29/23 16:40:01.299
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:40:01.322
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:40:01.332
    [It] updates the published spec when one version gets renamed [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:390
    STEP: set up a multi version CRD 07/29/23 16:40:01.337
    Jul 29 16:40:01.343: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: rename a version 07/29/23 16:40:14.734
    STEP: check the new version name is served 07/29/23 16:40:14.766
    STEP: check the old version name is removed 07/29/23 16:40:19.559
    STEP: check the other version is not changed 07/29/23 16:40:21.862
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jul 29 16:40:32.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-262" for this suite. 07/29/23 16:40:32.329
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:40:32.358
Jul 29 16:40:32.358: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename var-expansion 07/29/23 16:40:32.363
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:40:32.396
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:40:32.4
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91
STEP: Creating a pod to test substitution in container's args 07/29/23 16:40:32.404
Jul 29 16:40:32.419: INFO: Waiting up to 5m0s for pod "var-expansion-39ef21e9-a243-495e-9a32-8dcd1b1fa59d" in namespace "var-expansion-295" to be "Succeeded or Failed"
Jul 29 16:40:32.426: INFO: Pod "var-expansion-39ef21e9-a243-495e-9a32-8dcd1b1fa59d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.757824ms
Jul 29 16:40:34.433: INFO: Pod "var-expansion-39ef21e9-a243-495e-9a32-8dcd1b1fa59d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013626991s
Jul 29 16:40:36.433: INFO: Pod "var-expansion-39ef21e9-a243-495e-9a32-8dcd1b1fa59d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013111905s
STEP: Saw pod success 07/29/23 16:40:36.433
Jul 29 16:40:36.433: INFO: Pod "var-expansion-39ef21e9-a243-495e-9a32-8dcd1b1fa59d" satisfied condition "Succeeded or Failed"
Jul 29 16:40:36.451: INFO: Trying to get logs from node wa4quivohpee-3 pod var-expansion-39ef21e9-a243-495e-9a32-8dcd1b1fa59d container dapi-container: <nil>
STEP: delete the pod 07/29/23 16:40:36.478
Jul 29 16:40:36.532: INFO: Waiting for pod var-expansion-39ef21e9-a243-495e-9a32-8dcd1b1fa59d to disappear
Jul 29 16:40:36.539: INFO: Pod var-expansion-39ef21e9-a243-495e-9a32-8dcd1b1fa59d no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jul 29 16:40:36.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-295" for this suite. 07/29/23 16:40:36.549
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","completed":271,"skipped":5048,"failed":0}
------------------------------
â€¢ [4.202 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:40:32.358
    Jul 29 16:40:32.358: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename var-expansion 07/29/23 16:40:32.363
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:40:32.396
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:40:32.4
    [It] should allow substituting values in a container's args [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:91
    STEP: Creating a pod to test substitution in container's args 07/29/23 16:40:32.404
    Jul 29 16:40:32.419: INFO: Waiting up to 5m0s for pod "var-expansion-39ef21e9-a243-495e-9a32-8dcd1b1fa59d" in namespace "var-expansion-295" to be "Succeeded or Failed"
    Jul 29 16:40:32.426: INFO: Pod "var-expansion-39ef21e9-a243-495e-9a32-8dcd1b1fa59d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.757824ms
    Jul 29 16:40:34.433: INFO: Pod "var-expansion-39ef21e9-a243-495e-9a32-8dcd1b1fa59d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013626991s
    Jul 29 16:40:36.433: INFO: Pod "var-expansion-39ef21e9-a243-495e-9a32-8dcd1b1fa59d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013111905s
    STEP: Saw pod success 07/29/23 16:40:36.433
    Jul 29 16:40:36.433: INFO: Pod "var-expansion-39ef21e9-a243-495e-9a32-8dcd1b1fa59d" satisfied condition "Succeeded or Failed"
    Jul 29 16:40:36.451: INFO: Trying to get logs from node wa4quivohpee-3 pod var-expansion-39ef21e9-a243-495e-9a32-8dcd1b1fa59d container dapi-container: <nil>
    STEP: delete the pod 07/29/23 16:40:36.478
    Jul 29 16:40:36.532: INFO: Waiting for pod var-expansion-39ef21e9-a243-495e-9a32-8dcd1b1fa59d to disappear
    Jul 29 16:40:36.539: INFO: Pod var-expansion-39ef21e9-a243-495e-9a32-8dcd1b1fa59d no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jul 29 16:40:36.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-295" for this suite. 07/29/23 16:40:36.549
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:906
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:40:36.563
Jul 29 16:40:36.564: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename statefulset 07/29/23 16:40:36.566
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:40:36.594
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:40:36.598
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-2640 07/29/23 16:40:36.602
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:906
Jul 29 16:40:36.644: INFO: Found 0 stateful pods, waiting for 1
Jul 29 16:40:46.654: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet 07/29/23 16:40:46.663
W0729 16:40:46.688998      13 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Jul 29 16:40:46.704: INFO: Found 1 stateful pods, waiting for 2
Jul 29 16:40:56.724: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 29 16:40:56.724: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets 07/29/23 16:40:56.736
STEP: Delete all of the StatefulSets 07/29/23 16:40:56.742
STEP: Verify that StatefulSets have been deleted 07/29/23 16:40:56.756
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jul 29 16:40:56.765: INFO: Deleting all statefulset in ns statefulset-2640
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jul 29 16:40:56.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2640" for this suite. 07/29/23 16:40:56.787
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]","completed":272,"skipped":5051,"failed":0}
------------------------------
â€¢ [SLOW TEST] [20.242 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should list, patch and delete a collection of StatefulSets [Conformance]
    test/e2e/apps/statefulset.go:906

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:40:36.563
    Jul 29 16:40:36.564: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename statefulset 07/29/23 16:40:36.566
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:40:36.594
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:40:36.598
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-2640 07/29/23 16:40:36.602
    [It] should list, patch and delete a collection of StatefulSets [Conformance]
      test/e2e/apps/statefulset.go:906
    Jul 29 16:40:36.644: INFO: Found 0 stateful pods, waiting for 1
    Jul 29 16:40:46.654: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: patching the StatefulSet 07/29/23 16:40:46.663
    W0729 16:40:46.688998      13 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Jul 29 16:40:46.704: INFO: Found 1 stateful pods, waiting for 2
    Jul 29 16:40:56.724: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Jul 29 16:40:56.724: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Listing all StatefulSets 07/29/23 16:40:56.736
    STEP: Delete all of the StatefulSets 07/29/23 16:40:56.742
    STEP: Verify that StatefulSets have been deleted 07/29/23 16:40:56.756
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jul 29 16:40:56.765: INFO: Deleting all statefulset in ns statefulset-2640
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jul 29 16:40:56.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-2640" for this suite. 07/29/23 16:40:56.787
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:40:56.811
Jul 29 16:40:56.812: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename resourcequota 07/29/23 16:40:56.814
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:40:56.897
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:40:56.904
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90
STEP: Counting existing ResourceQuota 07/29/23 16:40:56.916
STEP: Creating a ResourceQuota 07/29/23 16:41:01.924
STEP: Ensuring resource quota status is calculated 07/29/23 16:41:01.936
STEP: Creating a Service 07/29/23 16:41:03.944
STEP: Creating a NodePort Service 07/29/23 16:41:03.974
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 07/29/23 16:41:04.025
STEP: Ensuring resource quota status captures service creation 07/29/23 16:41:04.065
STEP: Deleting Services 07/29/23 16:41:06.07
STEP: Ensuring resource quota status released usage 07/29/23 16:41:06.139
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jul 29 16:41:08.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1619" for this suite. 07/29/23 16:41:08.157
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","completed":273,"skipped":5056,"failed":0}
------------------------------
â€¢ [SLOW TEST] [11.362 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:40:56.811
    Jul 29 16:40:56.812: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename resourcequota 07/29/23 16:40:56.814
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:40:56.897
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:40:56.904
    [It] should create a ResourceQuota and capture the life of a service. [Conformance]
      test/e2e/apimachinery/resource_quota.go:90
    STEP: Counting existing ResourceQuota 07/29/23 16:40:56.916
    STEP: Creating a ResourceQuota 07/29/23 16:41:01.924
    STEP: Ensuring resource quota status is calculated 07/29/23 16:41:01.936
    STEP: Creating a Service 07/29/23 16:41:03.944
    STEP: Creating a NodePort Service 07/29/23 16:41:03.974
    STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 07/29/23 16:41:04.025
    STEP: Ensuring resource quota status captures service creation 07/29/23 16:41:04.065
    STEP: Deleting Services 07/29/23 16:41:06.07
    STEP: Ensuring resource quota status released usage 07/29/23 16:41:06.139
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jul 29 16:41:08.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-1619" for this suite. 07/29/23 16:41:08.157
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] Services
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:41:08.177
Jul 29 16:41:08.178: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename services 07/29/23 16:41:08.18
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:41:08.221
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:41:08.228
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852
STEP: creating service multi-endpoint-test in namespace services-1906 07/29/23 16:41:08.234
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1906 to expose endpoints map[] 07/29/23 16:41:08.254
Jul 29 16:41:08.263: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Jul 29 16:41:09.279: INFO: successfully validated that service multi-endpoint-test in namespace services-1906 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-1906 07/29/23 16:41:09.279
Jul 29 16:41:09.298: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-1906" to be "running and ready"
Jul 29 16:41:09.307: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.951286ms
Jul 29 16:41:09.307: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jul 29 16:41:11.316: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.01844495s
Jul 29 16:41:11.316: INFO: The phase of Pod pod1 is Running (Ready = true)
Jul 29 16:41:11.316: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1906 to expose endpoints map[pod1:[100]] 07/29/23 16:41:11.324
Jul 29 16:41:11.344: INFO: successfully validated that service multi-endpoint-test in namespace services-1906 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-1906 07/29/23 16:41:11.344
Jul 29 16:41:11.358: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-1906" to be "running and ready"
Jul 29 16:41:11.374: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 15.764551ms
Jul 29 16:41:11.375: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jul 29 16:41:13.382: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.023160899s
Jul 29 16:41:13.382: INFO: The phase of Pod pod2 is Running (Ready = true)
Jul 29 16:41:13.382: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1906 to expose endpoints map[pod1:[100] pod2:[101]] 07/29/23 16:41:13.397
Jul 29 16:41:13.421: INFO: successfully validated that service multi-endpoint-test in namespace services-1906 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods 07/29/23 16:41:13.421
Jul 29 16:41:13.422: INFO: Creating new exec pod
Jul 29 16:41:13.457: INFO: Waiting up to 5m0s for pod "execpodczlpn" in namespace "services-1906" to be "running"
Jul 29 16:41:13.467: INFO: Pod "execpodczlpn": Phase="Pending", Reason="", readiness=false. Elapsed: 9.69855ms
Jul 29 16:41:15.478: INFO: Pod "execpodczlpn": Phase="Running", Reason="", readiness=true. Elapsed: 2.020031622s
Jul 29 16:41:15.478: INFO: Pod "execpodczlpn" satisfied condition "running"
Jul 29 16:41:16.479: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=services-1906 exec execpodczlpn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
Jul 29 16:41:16.862: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Jul 29 16:41:16.863: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul 29 16:41:16.863: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=services-1906 exec execpodczlpn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.47.9 80'
Jul 29 16:41:17.103: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.47.9 80\nConnection to 10.233.47.9 80 port [tcp/http] succeeded!\n"
Jul 29 16:41:17.103: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul 29 16:41:17.104: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=services-1906 exec execpodczlpn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
Jul 29 16:41:17.405: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Jul 29 16:41:17.405: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul 29 16:41:17.406: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=services-1906 exec execpodczlpn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.47.9 81'
Jul 29 16:41:17.630: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.47.9 81\nConnection to 10.233.47.9 81 port [tcp/*] succeeded!\n"
Jul 29 16:41:17.630: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-1906 07/29/23 16:41:17.631
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1906 to expose endpoints map[pod2:[101]] 07/29/23 16:41:17.658
Jul 29 16:41:18.711: INFO: successfully validated that service multi-endpoint-test in namespace services-1906 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-1906 07/29/23 16:41:18.711
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1906 to expose endpoints map[] 07/29/23 16:41:18.788
Jul 29 16:41:18.807: INFO: successfully validated that service multi-endpoint-test in namespace services-1906 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jul 29 16:41:18.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1906" for this suite. 07/29/23 16:41:18.879
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","completed":274,"skipped":5064,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.714 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:41:08.177
    Jul 29 16:41:08.178: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename services 07/29/23 16:41:08.18
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:41:08.221
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:41:08.228
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should serve multiport endpoints from pods  [Conformance]
      test/e2e/network/service.go:852
    STEP: creating service multi-endpoint-test in namespace services-1906 07/29/23 16:41:08.234
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1906 to expose endpoints map[] 07/29/23 16:41:08.254
    Jul 29 16:41:08.263: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
    Jul 29 16:41:09.279: INFO: successfully validated that service multi-endpoint-test in namespace services-1906 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-1906 07/29/23 16:41:09.279
    Jul 29 16:41:09.298: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-1906" to be "running and ready"
    Jul 29 16:41:09.307: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.951286ms
    Jul 29 16:41:09.307: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jul 29 16:41:11.316: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.01844495s
    Jul 29 16:41:11.316: INFO: The phase of Pod pod1 is Running (Ready = true)
    Jul 29 16:41:11.316: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1906 to expose endpoints map[pod1:[100]] 07/29/23 16:41:11.324
    Jul 29 16:41:11.344: INFO: successfully validated that service multi-endpoint-test in namespace services-1906 exposes endpoints map[pod1:[100]]
    STEP: Creating pod pod2 in namespace services-1906 07/29/23 16:41:11.344
    Jul 29 16:41:11.358: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-1906" to be "running and ready"
    Jul 29 16:41:11.374: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 15.764551ms
    Jul 29 16:41:11.375: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jul 29 16:41:13.382: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.023160899s
    Jul 29 16:41:13.382: INFO: The phase of Pod pod2 is Running (Ready = true)
    Jul 29 16:41:13.382: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1906 to expose endpoints map[pod1:[100] pod2:[101]] 07/29/23 16:41:13.397
    Jul 29 16:41:13.421: INFO: successfully validated that service multi-endpoint-test in namespace services-1906 exposes endpoints map[pod1:[100] pod2:[101]]
    STEP: Checking if the Service forwards traffic to pods 07/29/23 16:41:13.421
    Jul 29 16:41:13.422: INFO: Creating new exec pod
    Jul 29 16:41:13.457: INFO: Waiting up to 5m0s for pod "execpodczlpn" in namespace "services-1906" to be "running"
    Jul 29 16:41:13.467: INFO: Pod "execpodczlpn": Phase="Pending", Reason="", readiness=false. Elapsed: 9.69855ms
    Jul 29 16:41:15.478: INFO: Pod "execpodczlpn": Phase="Running", Reason="", readiness=true. Elapsed: 2.020031622s
    Jul 29 16:41:15.478: INFO: Pod "execpodczlpn" satisfied condition "running"
    Jul 29 16:41:16.479: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=services-1906 exec execpodczlpn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
    Jul 29 16:41:16.862: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
    Jul 29 16:41:16.863: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jul 29 16:41:16.863: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=services-1906 exec execpodczlpn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.47.9 80'
    Jul 29 16:41:17.103: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.47.9 80\nConnection to 10.233.47.9 80 port [tcp/http] succeeded!\n"
    Jul 29 16:41:17.103: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jul 29 16:41:17.104: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=services-1906 exec execpodczlpn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
    Jul 29 16:41:17.405: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
    Jul 29 16:41:17.405: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jul 29 16:41:17.406: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=services-1906 exec execpodczlpn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.47.9 81'
    Jul 29 16:41:17.630: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.47.9 81\nConnection to 10.233.47.9 81 port [tcp/*] succeeded!\n"
    Jul 29 16:41:17.630: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod1 in namespace services-1906 07/29/23 16:41:17.631
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1906 to expose endpoints map[pod2:[101]] 07/29/23 16:41:17.658
    Jul 29 16:41:18.711: INFO: successfully validated that service multi-endpoint-test in namespace services-1906 exposes endpoints map[pod2:[101]]
    STEP: Deleting pod pod2 in namespace services-1906 07/29/23 16:41:18.711
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1906 to expose endpoints map[] 07/29/23 16:41:18.788
    Jul 29 16:41:18.807: INFO: successfully validated that service multi-endpoint-test in namespace services-1906 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jul 29 16:41:18.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-1906" for this suite. 07/29/23 16:41:18.879
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:41:18.896
Jul 29 16:41:18.896: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename webhook 07/29/23 16:41:18.9
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:41:18.956
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:41:18.961
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 07/29/23 16:41:18.991
STEP: Create role binding to let webhook read extension-apiserver-authentication 07/29/23 16:41:19.67
STEP: Deploying the webhook pod 07/29/23 16:41:19.686
STEP: Wait for the deployment to be ready 07/29/23 16:41:19.708
Jul 29 16:41:19.794: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 07/29/23 16:41:21.819
STEP: Verifying the service has paired with the endpoint 07/29/23 16:41:21.852
Jul 29 16:41:22.853: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 07/29/23 16:41:22.862
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 07/29/23 16:41:22.894
STEP: Creating a dummy validating-webhook-configuration object 07/29/23 16:41:22.923
STEP: Deleting the validating-webhook-configuration, which should be possible to remove 07/29/23 16:41:22.941
STEP: Creating a dummy mutating-webhook-configuration object 07/29/23 16:41:22.954
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 07/29/23 16:41:22.971
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jul 29 16:41:23.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9824" for this suite. 07/29/23 16:41:23.015
STEP: Destroying namespace "webhook-9824-markers" for this suite. 07/29/23 16:41:23.028
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","completed":275,"skipped":5072,"failed":0}
------------------------------
â€¢ [4.225 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:41:18.896
    Jul 29 16:41:18.896: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename webhook 07/29/23 16:41:18.9
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:41:18.956
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:41:18.961
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 07/29/23 16:41:18.991
    STEP: Create role binding to let webhook read extension-apiserver-authentication 07/29/23 16:41:19.67
    STEP: Deploying the webhook pod 07/29/23 16:41:19.686
    STEP: Wait for the deployment to be ready 07/29/23 16:41:19.708
    Jul 29 16:41:19.794: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 07/29/23 16:41:21.819
    STEP: Verifying the service has paired with the endpoint 07/29/23 16:41:21.852
    Jul 29 16:41:22.853: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
      test/e2e/apimachinery/webhook.go:276
    STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 07/29/23 16:41:22.862
    STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 07/29/23 16:41:22.894
    STEP: Creating a dummy validating-webhook-configuration object 07/29/23 16:41:22.923
    STEP: Deleting the validating-webhook-configuration, which should be possible to remove 07/29/23 16:41:22.941
    STEP: Creating a dummy mutating-webhook-configuration object 07/29/23 16:41:22.954
    STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 07/29/23 16:41:22.971
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jul 29 16:41:23.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-9824" for this suite. 07/29/23 16:41:23.015
    STEP: Destroying namespace "webhook-9824-markers" for this suite. 07/29/23 16:41:23.028
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] PodTemplates
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:41:23.129
Jul 29 16:41:23.130: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename podtemplate 07/29/23 16:41:23.139
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:41:23.197
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:41:23.205
[It] should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
STEP: Create set of pod templates 07/29/23 16:41:23.215
Jul 29 16:41:23.231: INFO: created test-podtemplate-1
Jul 29 16:41:23.244: INFO: created test-podtemplate-2
Jul 29 16:41:23.254: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace 07/29/23 16:41:23.254
STEP: delete collection of pod templates 07/29/23 16:41:23.26
Jul 29 16:41:23.261: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity 07/29/23 16:41:23.295
Jul 29 16:41:23.296: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
Jul 29 16:41:23.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-579" for this suite. 07/29/23 16:41:23.308
{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","completed":276,"skipped":5072,"failed":0}
------------------------------
â€¢ [0.206 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:41:23.129
    Jul 29 16:41:23.130: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename podtemplate 07/29/23 16:41:23.139
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:41:23.197
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:41:23.205
    [It] should delete a collection of pod templates [Conformance]
      test/e2e/common/node/podtemplates.go:122
    STEP: Create set of pod templates 07/29/23 16:41:23.215
    Jul 29 16:41:23.231: INFO: created test-podtemplate-1
    Jul 29 16:41:23.244: INFO: created test-podtemplate-2
    Jul 29 16:41:23.254: INFO: created test-podtemplate-3
    STEP: get a list of pod templates with a label in the current namespace 07/29/23 16:41:23.254
    STEP: delete collection of pod templates 07/29/23 16:41:23.26
    Jul 29 16:41:23.261: INFO: requesting DeleteCollection of pod templates
    STEP: check that the list of pod templates matches the requested quantity 07/29/23 16:41:23.295
    Jul 29 16:41:23.296: INFO: requesting list of pod templates to confirm quantity
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    Jul 29 16:41:23.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-579" for this suite. 07/29/23 16:41:23.308
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:41:23.348
Jul 29 16:41:23.349: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename gc 07/29/23 16:41:23.352
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:41:23.383
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:41:23.391
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
STEP: create the rc 07/29/23 16:41:23.407
STEP: delete the rc 07/29/23 16:41:28.595
STEP: wait for the rc to be deleted 07/29/23 16:41:28.791
Jul 29 16:41:30.237: INFO: 91 pods remaining
Jul 29 16:41:30.237: INFO: 81 pods has nil DeletionTimestamp
Jul 29 16:41:30.237: INFO: 
Jul 29 16:41:31.591: INFO: 85 pods remaining
Jul 29 16:41:31.591: INFO: 72 pods has nil DeletionTimestamp
Jul 29 16:41:31.591: INFO: 
Jul 29 16:41:32.377: INFO: 78 pods remaining
Jul 29 16:41:32.378: INFO: 60 pods has nil DeletionTimestamp
Jul 29 16:41:32.378: INFO: 
Jul 29 16:41:34.073: INFO: 60 pods remaining
Jul 29 16:41:34.073: INFO: 37 pods has nil DeletionTimestamp
Jul 29 16:41:34.073: INFO: 
Jul 29 16:41:35.162: INFO: 56 pods remaining
Jul 29 16:41:35.163: INFO: 20 pods has nil DeletionTimestamp
Jul 29 16:41:35.163: INFO: 
Jul 29 16:41:36.293: INFO: 53 pods remaining
Jul 29 16:41:36.293: INFO: 5 pods has nil DeletionTimestamp
Jul 29 16:41:36.293: INFO: 
Jul 29 16:41:37.438: INFO: 46 pods remaining
Jul 29 16:41:37.438: INFO: 0 pods has nil DeletionTimestamp
Jul 29 16:41:37.439: INFO: 
Jul 29 16:41:37.966: INFO: 40 pods remaining
Jul 29 16:41:37.966: INFO: 0 pods has nil DeletionTimestamp
Jul 29 16:41:37.967: INFO: 
Jul 29 16:41:39.613: INFO: 30 pods remaining
Jul 29 16:41:39.613: INFO: 0 pods has nil DeletionTimestamp
Jul 29 16:41:39.613: INFO: 
Jul 29 16:41:40.226: INFO: 25 pods remaining
Jul 29 16:41:40.233: INFO: 0 pods has nil DeletionTimestamp
Jul 29 16:41:40.238: INFO: 
Jul 29 16:41:41.897: INFO: 16 pods remaining
Jul 29 16:41:41.897: INFO: 0 pods has nil DeletionTimestamp
Jul 29 16:41:41.897: INFO: 
Jul 29 16:41:42.957: INFO: 8 pods remaining
Jul 29 16:41:42.958: INFO: 0 pods has nil DeletionTimestamp
Jul 29 16:41:42.958: INFO: 
Jul 29 16:41:43.896: INFO: 1 pods remaining
Jul 29 16:41:43.897: INFO: 0 pods has nil DeletionTimestamp
Jul 29 16:41:43.897: INFO: 
STEP: Gathering metrics 07/29/23 16:41:44.825
Jul 29 16:41:44.910: INFO: Waiting up to 5m0s for pod "kube-controller-manager-wa4quivohpee-2" in namespace "kube-system" to be "running and ready"
Jul 29 16:41:44.917: INFO: Pod "kube-controller-manager-wa4quivohpee-2": Phase="Running", Reason="", readiness=true. Elapsed: 6.669396ms
Jul 29 16:41:44.917: INFO: The phase of Pod kube-controller-manager-wa4quivohpee-2 is Running (Ready = true)
Jul 29 16:41:44.917: INFO: Pod "kube-controller-manager-wa4quivohpee-2" satisfied condition "running and ready"
Jul 29 16:41:45.173: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Jul 29 16:41:45.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8142" for this suite. 07/29/23 16:41:45.227
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","completed":277,"skipped":5090,"failed":0}
------------------------------
â€¢ [SLOW TEST] [21.929 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:41:23.348
    Jul 29 16:41:23.349: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename gc 07/29/23 16:41:23.352
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:41:23.383
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:41:23.391
    [It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:650
    STEP: create the rc 07/29/23 16:41:23.407
    STEP: delete the rc 07/29/23 16:41:28.595
    STEP: wait for the rc to be deleted 07/29/23 16:41:28.791
    Jul 29 16:41:30.237: INFO: 91 pods remaining
    Jul 29 16:41:30.237: INFO: 81 pods has nil DeletionTimestamp
    Jul 29 16:41:30.237: INFO: 
    Jul 29 16:41:31.591: INFO: 85 pods remaining
    Jul 29 16:41:31.591: INFO: 72 pods has nil DeletionTimestamp
    Jul 29 16:41:31.591: INFO: 
    Jul 29 16:41:32.377: INFO: 78 pods remaining
    Jul 29 16:41:32.378: INFO: 60 pods has nil DeletionTimestamp
    Jul 29 16:41:32.378: INFO: 
    Jul 29 16:41:34.073: INFO: 60 pods remaining
    Jul 29 16:41:34.073: INFO: 37 pods has nil DeletionTimestamp
    Jul 29 16:41:34.073: INFO: 
    Jul 29 16:41:35.162: INFO: 56 pods remaining
    Jul 29 16:41:35.163: INFO: 20 pods has nil DeletionTimestamp
    Jul 29 16:41:35.163: INFO: 
    Jul 29 16:41:36.293: INFO: 53 pods remaining
    Jul 29 16:41:36.293: INFO: 5 pods has nil DeletionTimestamp
    Jul 29 16:41:36.293: INFO: 
    Jul 29 16:41:37.438: INFO: 46 pods remaining
    Jul 29 16:41:37.438: INFO: 0 pods has nil DeletionTimestamp
    Jul 29 16:41:37.439: INFO: 
    Jul 29 16:41:37.966: INFO: 40 pods remaining
    Jul 29 16:41:37.966: INFO: 0 pods has nil DeletionTimestamp
    Jul 29 16:41:37.967: INFO: 
    Jul 29 16:41:39.613: INFO: 30 pods remaining
    Jul 29 16:41:39.613: INFO: 0 pods has nil DeletionTimestamp
    Jul 29 16:41:39.613: INFO: 
    Jul 29 16:41:40.226: INFO: 25 pods remaining
    Jul 29 16:41:40.233: INFO: 0 pods has nil DeletionTimestamp
    Jul 29 16:41:40.238: INFO: 
    Jul 29 16:41:41.897: INFO: 16 pods remaining
    Jul 29 16:41:41.897: INFO: 0 pods has nil DeletionTimestamp
    Jul 29 16:41:41.897: INFO: 
    Jul 29 16:41:42.957: INFO: 8 pods remaining
    Jul 29 16:41:42.958: INFO: 0 pods has nil DeletionTimestamp
    Jul 29 16:41:42.958: INFO: 
    Jul 29 16:41:43.896: INFO: 1 pods remaining
    Jul 29 16:41:43.897: INFO: 0 pods has nil DeletionTimestamp
    Jul 29 16:41:43.897: INFO: 
    STEP: Gathering metrics 07/29/23 16:41:44.825
    Jul 29 16:41:44.910: INFO: Waiting up to 5m0s for pod "kube-controller-manager-wa4quivohpee-2" in namespace "kube-system" to be "running and ready"
    Jul 29 16:41:44.917: INFO: Pod "kube-controller-manager-wa4quivohpee-2": Phase="Running", Reason="", readiness=true. Elapsed: 6.669396ms
    Jul 29 16:41:44.917: INFO: The phase of Pod kube-controller-manager-wa4quivohpee-2 is Running (Ready = true)
    Jul 29 16:41:44.917: INFO: Pod "kube-controller-manager-wa4quivohpee-2" satisfied condition "running and ready"
    Jul 29 16:41:45.173: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Jul 29 16:41:45.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-8142" for this suite. 07/29/23 16:41:45.227
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:41:45.286
Jul 29 16:41:45.287: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename projected 07/29/23 16:41:45.311
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:41:45.913
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:41:45.979
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87
STEP: Creating projection with secret that has name projected-secret-test-map-80461999-d2d4-4ced-b882-87cbe944d85a 07/29/23 16:41:46.082
STEP: Creating a pod to test consume secrets 07/29/23 16:41:46.179
Jul 29 16:41:46.289: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-03151bb6-627b-4fb6-b43d-5e6592361fba" in namespace "projected-2867" to be "Succeeded or Failed"
Jul 29 16:41:46.354: INFO: Pod "pod-projected-secrets-03151bb6-627b-4fb6-b43d-5e6592361fba": Phase="Pending", Reason="", readiness=false. Elapsed: 64.836295ms
Jul 29 16:41:48.383: INFO: Pod "pod-projected-secrets-03151bb6-627b-4fb6-b43d-5e6592361fba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.093781973s
Jul 29 16:41:50.367: INFO: Pod "pod-projected-secrets-03151bb6-627b-4fb6-b43d-5e6592361fba": Phase="Pending", Reason="", readiness=false. Elapsed: 4.077728308s
Jul 29 16:41:52.372: INFO: Pod "pod-projected-secrets-03151bb6-627b-4fb6-b43d-5e6592361fba": Phase="Pending", Reason="", readiness=false. Elapsed: 6.083018365s
Jul 29 16:41:54.367: INFO: Pod "pod-projected-secrets-03151bb6-627b-4fb6-b43d-5e6592361fba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.077497339s
STEP: Saw pod success 07/29/23 16:41:54.367
Jul 29 16:41:54.367: INFO: Pod "pod-projected-secrets-03151bb6-627b-4fb6-b43d-5e6592361fba" satisfied condition "Succeeded or Failed"
Jul 29 16:41:54.377: INFO: Trying to get logs from node wa4quivohpee-3 pod pod-projected-secrets-03151bb6-627b-4fb6-b43d-5e6592361fba container projected-secret-volume-test: <nil>
STEP: delete the pod 07/29/23 16:41:54.391
Jul 29 16:41:54.444: INFO: Waiting for pod pod-projected-secrets-03151bb6-627b-4fb6-b43d-5e6592361fba to disappear
Jul 29 16:41:54.452: INFO: Pod pod-projected-secrets-03151bb6-627b-4fb6-b43d-5e6592361fba no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Jul 29 16:41:54.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2867" for this suite. 07/29/23 16:41:54.468
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":278,"skipped":5094,"failed":0}
------------------------------
â€¢ [SLOW TEST] [9.209 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:41:45.286
    Jul 29 16:41:45.287: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename projected 07/29/23 16:41:45.311
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:41:45.913
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:41:45.979
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:87
    STEP: Creating projection with secret that has name projected-secret-test-map-80461999-d2d4-4ced-b882-87cbe944d85a 07/29/23 16:41:46.082
    STEP: Creating a pod to test consume secrets 07/29/23 16:41:46.179
    Jul 29 16:41:46.289: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-03151bb6-627b-4fb6-b43d-5e6592361fba" in namespace "projected-2867" to be "Succeeded or Failed"
    Jul 29 16:41:46.354: INFO: Pod "pod-projected-secrets-03151bb6-627b-4fb6-b43d-5e6592361fba": Phase="Pending", Reason="", readiness=false. Elapsed: 64.836295ms
    Jul 29 16:41:48.383: INFO: Pod "pod-projected-secrets-03151bb6-627b-4fb6-b43d-5e6592361fba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.093781973s
    Jul 29 16:41:50.367: INFO: Pod "pod-projected-secrets-03151bb6-627b-4fb6-b43d-5e6592361fba": Phase="Pending", Reason="", readiness=false. Elapsed: 4.077728308s
    Jul 29 16:41:52.372: INFO: Pod "pod-projected-secrets-03151bb6-627b-4fb6-b43d-5e6592361fba": Phase="Pending", Reason="", readiness=false. Elapsed: 6.083018365s
    Jul 29 16:41:54.367: INFO: Pod "pod-projected-secrets-03151bb6-627b-4fb6-b43d-5e6592361fba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.077497339s
    STEP: Saw pod success 07/29/23 16:41:54.367
    Jul 29 16:41:54.367: INFO: Pod "pod-projected-secrets-03151bb6-627b-4fb6-b43d-5e6592361fba" satisfied condition "Succeeded or Failed"
    Jul 29 16:41:54.377: INFO: Trying to get logs from node wa4quivohpee-3 pod pod-projected-secrets-03151bb6-627b-4fb6-b43d-5e6592361fba container projected-secret-volume-test: <nil>
    STEP: delete the pod 07/29/23 16:41:54.391
    Jul 29 16:41:54.444: INFO: Waiting for pod pod-projected-secrets-03151bb6-627b-4fb6-b43d-5e6592361fba to disappear
    Jul 29 16:41:54.452: INFO: Pod pod-projected-secrets-03151bb6-627b-4fb6-b43d-5e6592361fba no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Jul 29 16:41:54.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-2867" for this suite. 07/29/23 16:41:54.468
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Secrets
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:41:54.497
Jul 29 16:41:54.498: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename secrets 07/29/23 16:41:54.504
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:41:54.543
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:41:54.549
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204
STEP: Creating secret with name s-test-opt-del-624dd539-a13b-4024-9159-caea0e6b62d6 07/29/23 16:41:54.566
STEP: Creating secret with name s-test-opt-upd-2a5355db-5036-4820-ac7a-905ee280481c 07/29/23 16:41:54.577
STEP: Creating the pod 07/29/23 16:41:54.599
Jul 29 16:41:54.642: INFO: Waiting up to 5m0s for pod "pod-secrets-b4293870-4244-4974-9c91-131fd415e266" in namespace "secrets-5556" to be "running and ready"
Jul 29 16:41:54.657: INFO: Pod "pod-secrets-b4293870-4244-4974-9c91-131fd415e266": Phase="Pending", Reason="", readiness=false. Elapsed: 14.180716ms
Jul 29 16:41:54.657: INFO: The phase of Pod pod-secrets-b4293870-4244-4974-9c91-131fd415e266 is Pending, waiting for it to be Running (with Ready = true)
Jul 29 16:41:56.691: INFO: Pod "pod-secrets-b4293870-4244-4974-9c91-131fd415e266": Phase="Pending", Reason="", readiness=false. Elapsed: 2.048346222s
Jul 29 16:41:56.691: INFO: The phase of Pod pod-secrets-b4293870-4244-4974-9c91-131fd415e266 is Pending, waiting for it to be Running (with Ready = true)
Jul 29 16:41:58.688: INFO: Pod "pod-secrets-b4293870-4244-4974-9c91-131fd415e266": Phase="Running", Reason="", readiness=true. Elapsed: 4.045968895s
Jul 29 16:41:58.688: INFO: The phase of Pod pod-secrets-b4293870-4244-4974-9c91-131fd415e266 is Running (Ready = true)
Jul 29 16:41:58.688: INFO: Pod "pod-secrets-b4293870-4244-4974-9c91-131fd415e266" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-624dd539-a13b-4024-9159-caea0e6b62d6 07/29/23 16:41:58.799
STEP: Updating secret s-test-opt-upd-2a5355db-5036-4820-ac7a-905ee280481c 07/29/23 16:41:58.822
STEP: Creating secret with name s-test-opt-create-6f8b1a6b-41e3-4d85-8dad-8af34b66eb36 07/29/23 16:41:58.853
STEP: waiting to observe update in volume 07/29/23 16:41:58.913
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jul 29 16:42:01.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5556" for this suite. 07/29/23 16:42:01.068
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":279,"skipped":5099,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.582 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:41:54.497
    Jul 29 16:41:54.498: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename secrets 07/29/23 16:41:54.504
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:41:54.543
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:41:54.549
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:204
    STEP: Creating secret with name s-test-opt-del-624dd539-a13b-4024-9159-caea0e6b62d6 07/29/23 16:41:54.566
    STEP: Creating secret with name s-test-opt-upd-2a5355db-5036-4820-ac7a-905ee280481c 07/29/23 16:41:54.577
    STEP: Creating the pod 07/29/23 16:41:54.599
    Jul 29 16:41:54.642: INFO: Waiting up to 5m0s for pod "pod-secrets-b4293870-4244-4974-9c91-131fd415e266" in namespace "secrets-5556" to be "running and ready"
    Jul 29 16:41:54.657: INFO: Pod "pod-secrets-b4293870-4244-4974-9c91-131fd415e266": Phase="Pending", Reason="", readiness=false. Elapsed: 14.180716ms
    Jul 29 16:41:54.657: INFO: The phase of Pod pod-secrets-b4293870-4244-4974-9c91-131fd415e266 is Pending, waiting for it to be Running (with Ready = true)
    Jul 29 16:41:56.691: INFO: Pod "pod-secrets-b4293870-4244-4974-9c91-131fd415e266": Phase="Pending", Reason="", readiness=false. Elapsed: 2.048346222s
    Jul 29 16:41:56.691: INFO: The phase of Pod pod-secrets-b4293870-4244-4974-9c91-131fd415e266 is Pending, waiting for it to be Running (with Ready = true)
    Jul 29 16:41:58.688: INFO: Pod "pod-secrets-b4293870-4244-4974-9c91-131fd415e266": Phase="Running", Reason="", readiness=true. Elapsed: 4.045968895s
    Jul 29 16:41:58.688: INFO: The phase of Pod pod-secrets-b4293870-4244-4974-9c91-131fd415e266 is Running (Ready = true)
    Jul 29 16:41:58.688: INFO: Pod "pod-secrets-b4293870-4244-4974-9c91-131fd415e266" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-624dd539-a13b-4024-9159-caea0e6b62d6 07/29/23 16:41:58.799
    STEP: Updating secret s-test-opt-upd-2a5355db-5036-4820-ac7a-905ee280481c 07/29/23 16:41:58.822
    STEP: Creating secret with name s-test-opt-create-6f8b1a6b-41e3-4d85-8dad-8af34b66eb36 07/29/23 16:41:58.853
    STEP: waiting to observe update in volume 07/29/23 16:41:58.913
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jul 29 16:42:01.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-5556" for this suite. 07/29/23 16:42:01.068
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run
  should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:960
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:42:01.084
Jul 29 16:42:01.084: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename kubectl 07/29/23 16:42:01.091
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:42:01.131
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:42:01.138
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:960
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 07/29/23 16:42:01.143
Jul 29 16:42:01.144: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-9496 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Jul 29 16:42:01.348: INFO: stderr: ""
Jul 29 16:42:01.348: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run 07/29/23 16:42:01.348
Jul 29 16:42:01.359: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-9496 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-2"}]}} --dry-run=server'
Jul 29 16:42:02.150: INFO: stderr: ""
Jul 29 16:42:02.150: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 07/29/23 16:42:02.15
Jul 29 16:42:02.162: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-9496 delete pods e2e-test-httpd-pod'
Jul 29 16:42:04.021: INFO: stderr: ""
Jul 29 16:42:04.021: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jul 29 16:42:04.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9496" for this suite. 07/29/23 16:42:04.032
{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","completed":280,"skipped":5117,"failed":0}
------------------------------
â€¢ [2.961 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  test/e2e/kubectl/kubectl.go:954
    should check if kubectl can dry-run update Pods [Conformance]
    test/e2e/kubectl/kubectl.go:960

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:42:01.084
    Jul 29 16:42:01.084: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename kubectl 07/29/23 16:42:01.091
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:42:01.131
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:42:01.138
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl can dry-run update Pods [Conformance]
      test/e2e/kubectl/kubectl.go:960
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 07/29/23 16:42:01.143
    Jul 29 16:42:01.144: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-9496 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Jul 29 16:42:01.348: INFO: stderr: ""
    Jul 29 16:42:01.348: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: replace the image in the pod with server-side dry-run 07/29/23 16:42:01.348
    Jul 29 16:42:01.359: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-9496 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-2"}]}} --dry-run=server'
    Jul 29 16:42:02.150: INFO: stderr: ""
    Jul 29 16:42:02.150: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 07/29/23 16:42:02.15
    Jul 29 16:42:02.162: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-9496 delete pods e2e-test-httpd-pod'
    Jul 29 16:42:04.021: INFO: stderr: ""
    Jul 29 16:42:04.021: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jul 29 16:42:04.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-9496" for this suite. 07/29/23 16:42:04.032
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:42:04.049
Jul 29 16:42:04.049: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename daemonsets 07/29/23 16:42:04.052
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:42:04.12
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:42:04.127
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165
STEP: Creating simple DaemonSet "daemon-set" 07/29/23 16:42:04.171
STEP: Check that daemon pods launch on every node of the cluster. 07/29/23 16:42:04.184
Jul 29 16:42:04.201: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jul 29 16:42:04.202: INFO: Node wa4quivohpee-1 is running 0 daemon pod, expected 1
Jul 29 16:42:05.241: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jul 29 16:42:05.242: INFO: Node wa4quivohpee-1 is running 0 daemon pod, expected 1
Jul 29 16:42:06.216: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jul 29 16:42:06.217: INFO: Node wa4quivohpee-2 is running 0 daemon pod, expected 1
Jul 29 16:42:07.221: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jul 29 16:42:07.221: INFO: Node wa4quivohpee-2 is running 0 daemon pod, expected 1
Jul 29 16:42:08.227: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jul 29 16:42:08.227: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived. 07/29/23 16:42:08.233
Jul 29 16:42:08.305: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jul 29 16:42:08.305: INFO: Node wa4quivohpee-1 is running 0 daemon pod, expected 1
Jul 29 16:42:09.366: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jul 29 16:42:09.366: INFO: Node wa4quivohpee-1 is running 0 daemon pod, expected 1
Jul 29 16:42:10.329: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jul 29 16:42:10.330: INFO: Node wa4quivohpee-1 is running 0 daemon pod, expected 1
Jul 29 16:42:11.326: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jul 29 16:42:11.326: INFO: Node wa4quivohpee-1 is running 0 daemon pod, expected 1
Jul 29 16:42:12.321: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jul 29 16:42:12.321: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 07/29/23 16:42:12.327
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4763, will wait for the garbage collector to delete the pods 07/29/23 16:42:12.327
Jul 29 16:42:12.399: INFO: Deleting DaemonSet.extensions daemon-set took: 15.093668ms
Jul 29 16:42:12.499: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.450804ms
Jul 29 16:42:14.407: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jul 29 16:42:14.407: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jul 29 16:42:14.413: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"29577"},"items":null}

Jul 29 16:42:14.421: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"29577"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Jul 29 16:42:14.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4763" for this suite. 07/29/23 16:42:14.458
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","completed":281,"skipped":5146,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.421 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:42:04.049
    Jul 29 16:42:04.049: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename daemonsets 07/29/23 16:42:04.052
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:42:04.12
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:42:04.127
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should run and stop simple daemon [Conformance]
      test/e2e/apps/daemon_set.go:165
    STEP: Creating simple DaemonSet "daemon-set" 07/29/23 16:42:04.171
    STEP: Check that daemon pods launch on every node of the cluster. 07/29/23 16:42:04.184
    Jul 29 16:42:04.201: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jul 29 16:42:04.202: INFO: Node wa4quivohpee-1 is running 0 daemon pod, expected 1
    Jul 29 16:42:05.241: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jul 29 16:42:05.242: INFO: Node wa4quivohpee-1 is running 0 daemon pod, expected 1
    Jul 29 16:42:06.216: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jul 29 16:42:06.217: INFO: Node wa4quivohpee-2 is running 0 daemon pod, expected 1
    Jul 29 16:42:07.221: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jul 29 16:42:07.221: INFO: Node wa4quivohpee-2 is running 0 daemon pod, expected 1
    Jul 29 16:42:08.227: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jul 29 16:42:08.227: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Stop a daemon pod, check that the daemon pod is revived. 07/29/23 16:42:08.233
    Jul 29 16:42:08.305: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jul 29 16:42:08.305: INFO: Node wa4quivohpee-1 is running 0 daemon pod, expected 1
    Jul 29 16:42:09.366: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jul 29 16:42:09.366: INFO: Node wa4quivohpee-1 is running 0 daemon pod, expected 1
    Jul 29 16:42:10.329: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jul 29 16:42:10.330: INFO: Node wa4quivohpee-1 is running 0 daemon pod, expected 1
    Jul 29 16:42:11.326: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jul 29 16:42:11.326: INFO: Node wa4quivohpee-1 is running 0 daemon pod, expected 1
    Jul 29 16:42:12.321: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jul 29 16:42:12.321: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 07/29/23 16:42:12.327
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4763, will wait for the garbage collector to delete the pods 07/29/23 16:42:12.327
    Jul 29 16:42:12.399: INFO: Deleting DaemonSet.extensions daemon-set took: 15.093668ms
    Jul 29 16:42:12.499: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.450804ms
    Jul 29 16:42:14.407: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jul 29 16:42:14.407: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jul 29 16:42:14.413: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"29577"},"items":null}

    Jul 29 16:42:14.421: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"29577"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Jul 29 16:42:14.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-4763" for this suite. 07/29/23 16:42:14.458
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] ReplicationController
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:42:14.472
Jul 29 16:42:14.472: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename replication-controller 07/29/23 16:42:14.474
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:42:14.505
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:42:14.51
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82
Jul 29 16:42:14.514: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 07/29/23 16:42:15.547
STEP: Checking rc "condition-test" has the desired failure condition set 07/29/23 16:42:15.558
STEP: Scaling down rc "condition-test" to satisfy pod quota 07/29/23 16:42:16.577
Jul 29 16:42:16.603: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set 07/29/23 16:42:16.603
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Jul 29 16:42:17.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6986" for this suite. 07/29/23 16:42:17.63
{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","completed":282,"skipped":5150,"failed":0}
------------------------------
â€¢ [3.178 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:42:14.472
    Jul 29 16:42:14.472: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename replication-controller 07/29/23 16:42:14.474
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:42:14.505
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:42:14.51
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should surface a failure condition on a common issue like exceeded quota [Conformance]
      test/e2e/apps/rc.go:82
    Jul 29 16:42:14.514: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
    STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 07/29/23 16:42:15.547
    STEP: Checking rc "condition-test" has the desired failure condition set 07/29/23 16:42:15.558
    STEP: Scaling down rc "condition-test" to satisfy pod quota 07/29/23 16:42:16.577
    Jul 29 16:42:16.603: INFO: Updating replication controller "condition-test"
    STEP: Checking rc "condition-test" has no failure condition set 07/29/23 16:42:16.603
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Jul 29 16:42:17.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-6986" for this suite. 07/29/23 16:42:17.63
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:42:17.66
Jul 29 16:42:17.661: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename daemonsets 07/29/23 16:42:17.664
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:42:17.701
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:42:17.71
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822
STEP: Creating simple DaemonSet "daemon-set" 07/29/23 16:42:17.77
STEP: Check that daemon pods launch on every node of the cluster. 07/29/23 16:42:17.783
Jul 29 16:42:17.802: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jul 29 16:42:17.803: INFO: Node wa4quivohpee-1 is running 0 daemon pod, expected 1
Jul 29 16:42:18.819: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jul 29 16:42:18.819: INFO: Node wa4quivohpee-1 is running 0 daemon pod, expected 1
Jul 29 16:42:19.823: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jul 29 16:42:19.823: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: listing all DeamonSets 07/29/23 16:42:19.831
STEP: DeleteCollection of the DaemonSets 07/29/23 16:42:19.845
STEP: Verify that ReplicaSets have been deleted 07/29/23 16:42:19.871
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
Jul 29 16:42:19.917: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"29702"},"items":null}

Jul 29 16:42:19.925: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"29703"},"items":[{"metadata":{"name":"daemon-set-4sr86","generateName":"daemon-set-","namespace":"daemonsets-5058","uid":"2ffdbb28-f057-4957-bb09-583bdbf6ffa0","resourceVersion":"29700","creationTimestamp":"2023-07-29T16:42:17Z","deletionTimestamp":"2023-07-29T16:42:49Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"1cf86bd7-68a1-4c9c-9f67-15664c1ce13b","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-07-29T16:42:17Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1cf86bd7-68a1-4c9c-9f67-15664c1ce13b\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-07-29T16:42:19Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.64.225\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-s6lhw","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-s6lhw","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"wa4quivohpee-1","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["wa4quivohpee-1"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-29T16:42:17Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-29T16:42:19Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-29T16:42:19Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-29T16:42:17Z"}],"hostIP":"192.168.121.28","podIP":"10.233.64.225","podIPs":[{"ip":"10.233.64.225"}],"startTime":"2023-07-29T16:42:17Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-07-29T16:42:19Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"cri-o://9068bb87f537ffafd4f4c5c997c2ae1890c3c8284afed144f4b6222c0bc84cc8","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-9ztb7","generateName":"daemon-set-","namespace":"daemonsets-5058","uid":"192f8872-22bd-4eb5-9be2-e7e384676f7e","resourceVersion":"29702","creationTimestamp":"2023-07-29T16:42:17Z","deletionTimestamp":"2023-07-29T16:42:49Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"1cf86bd7-68a1-4c9c-9f67-15664c1ce13b","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-07-29T16:42:17Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1cf86bd7-68a1-4c9c-9f67-15664c1ce13b\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-07-29T16:42:19Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.65.39\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-4mvnw","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-4mvnw","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"wa4quivohpee-3","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["wa4quivohpee-3"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-29T16:42:17Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-29T16:42:19Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-29T16:42:19Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-29T16:42:17Z"}],"hostIP":"192.168.121.234","podIP":"10.233.65.39","podIPs":[{"ip":"10.233.65.39"}],"startTime":"2023-07-29T16:42:17Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-07-29T16:42:18Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"cri-o://3165c8e2b69e566579e5c91fe5494cc4e585be3e4a77266fde4b2f399888a1cb","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-x8d47","generateName":"daemon-set-","namespace":"daemonsets-5058","uid":"ca7ef9ab-3d0b-4999-bcac-43af5595f420","resourceVersion":"29699","creationTimestamp":"2023-07-29T16:42:17Z","deletionTimestamp":"2023-07-29T16:42:49Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"1cf86bd7-68a1-4c9c-9f67-15664c1ce13b","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-07-29T16:42:17Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1cf86bd7-68a1-4c9c-9f67-15664c1ce13b\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-07-29T16:42:19Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.28\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-m6xkg","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-m6xkg","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"wa4quivohpee-2","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["wa4quivohpee-2"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-29T16:42:17Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-29T16:42:19Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-29T16:42:19Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-29T16:42:17Z"}],"hostIP":"192.168.121.206","podIP":"10.233.66.28","podIPs":[{"ip":"10.233.66.28"}],"startTime":"2023-07-29T16:42:17Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-07-29T16:42:18Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"cri-o://97488eb41b1577d06fc4ad1b31fd9cc4933e1d6ccf04f585209ab1d5cff28fe9","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Jul 29 16:42:19.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5058" for this suite. 07/29/23 16:42:19.973
{"msg":"PASSED [sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]","completed":283,"skipped":5159,"failed":0}
------------------------------
â€¢ [2.330 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:42:17.66
    Jul 29 16:42:17.661: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename daemonsets 07/29/23 16:42:17.664
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:42:17.701
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:42:17.71
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should list and delete a collection of DaemonSets [Conformance]
      test/e2e/apps/daemon_set.go:822
    STEP: Creating simple DaemonSet "daemon-set" 07/29/23 16:42:17.77
    STEP: Check that daemon pods launch on every node of the cluster. 07/29/23 16:42:17.783
    Jul 29 16:42:17.802: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jul 29 16:42:17.803: INFO: Node wa4quivohpee-1 is running 0 daemon pod, expected 1
    Jul 29 16:42:18.819: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jul 29 16:42:18.819: INFO: Node wa4quivohpee-1 is running 0 daemon pod, expected 1
    Jul 29 16:42:19.823: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jul 29 16:42:19.823: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: listing all DeamonSets 07/29/23 16:42:19.831
    STEP: DeleteCollection of the DaemonSets 07/29/23 16:42:19.845
    STEP: Verify that ReplicaSets have been deleted 07/29/23 16:42:19.871
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    Jul 29 16:42:19.917: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"29702"},"items":null}

    Jul 29 16:42:19.925: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"29703"},"items":[{"metadata":{"name":"daemon-set-4sr86","generateName":"daemon-set-","namespace":"daemonsets-5058","uid":"2ffdbb28-f057-4957-bb09-583bdbf6ffa0","resourceVersion":"29700","creationTimestamp":"2023-07-29T16:42:17Z","deletionTimestamp":"2023-07-29T16:42:49Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"1cf86bd7-68a1-4c9c-9f67-15664c1ce13b","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-07-29T16:42:17Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1cf86bd7-68a1-4c9c-9f67-15664c1ce13b\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-07-29T16:42:19Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.64.225\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-s6lhw","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-s6lhw","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"wa4quivohpee-1","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["wa4quivohpee-1"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-29T16:42:17Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-29T16:42:19Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-29T16:42:19Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-29T16:42:17Z"}],"hostIP":"192.168.121.28","podIP":"10.233.64.225","podIPs":[{"ip":"10.233.64.225"}],"startTime":"2023-07-29T16:42:17Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-07-29T16:42:19Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"cri-o://9068bb87f537ffafd4f4c5c997c2ae1890c3c8284afed144f4b6222c0bc84cc8","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-9ztb7","generateName":"daemon-set-","namespace":"daemonsets-5058","uid":"192f8872-22bd-4eb5-9be2-e7e384676f7e","resourceVersion":"29702","creationTimestamp":"2023-07-29T16:42:17Z","deletionTimestamp":"2023-07-29T16:42:49Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"1cf86bd7-68a1-4c9c-9f67-15664c1ce13b","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-07-29T16:42:17Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1cf86bd7-68a1-4c9c-9f67-15664c1ce13b\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-07-29T16:42:19Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.65.39\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-4mvnw","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-4mvnw","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"wa4quivohpee-3","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["wa4quivohpee-3"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-29T16:42:17Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-29T16:42:19Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-29T16:42:19Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-29T16:42:17Z"}],"hostIP":"192.168.121.234","podIP":"10.233.65.39","podIPs":[{"ip":"10.233.65.39"}],"startTime":"2023-07-29T16:42:17Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-07-29T16:42:18Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"cri-o://3165c8e2b69e566579e5c91fe5494cc4e585be3e4a77266fde4b2f399888a1cb","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-x8d47","generateName":"daemon-set-","namespace":"daemonsets-5058","uid":"ca7ef9ab-3d0b-4999-bcac-43af5595f420","resourceVersion":"29699","creationTimestamp":"2023-07-29T16:42:17Z","deletionTimestamp":"2023-07-29T16:42:49Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"1cf86bd7-68a1-4c9c-9f67-15664c1ce13b","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-07-29T16:42:17Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1cf86bd7-68a1-4c9c-9f67-15664c1ce13b\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-07-29T16:42:19Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.28\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-m6xkg","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-m6xkg","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"wa4quivohpee-2","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["wa4quivohpee-2"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-29T16:42:17Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-29T16:42:19Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-29T16:42:19Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-29T16:42:17Z"}],"hostIP":"192.168.121.206","podIP":"10.233.66.28","podIPs":[{"ip":"10.233.66.28"}],"startTime":"2023-07-29T16:42:17Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-07-29T16:42:18Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"cri-o://97488eb41b1577d06fc4ad1b31fd9cc4933e1d6ccf04f585209ab1d5cff28fe9","started":true}],"qosClass":"BestEffort"}}]}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Jul 29 16:42:19.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-5058" for this suite. 07/29/23 16:42:19.973
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:42:19.996
Jul 29 16:42:19.996: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename dns 07/29/23 16:42:19.998
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:42:20.035
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:42:20.046
[It] should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
STEP: Creating a test externalName service 07/29/23 16:42:20.057
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1952.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-1952.svc.cluster.local; sleep 1; done
 07/29/23 16:42:20.073
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1952.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-1952.svc.cluster.local; sleep 1; done
 07/29/23 16:42:20.073
STEP: creating a pod to probe DNS 07/29/23 16:42:20.074
STEP: submitting the pod to kubernetes 07/29/23 16:42:20.074
Jul 29 16:42:20.095: INFO: Waiting up to 15m0s for pod "dns-test-da48cd65-0ccd-4ed5-ae50-a108aaa6e2cd" in namespace "dns-1952" to be "running"
Jul 29 16:42:20.103: INFO: Pod "dns-test-da48cd65-0ccd-4ed5-ae50-a108aaa6e2cd": Phase="Pending", Reason="", readiness=false. Elapsed: 7.801465ms
Jul 29 16:42:22.112: INFO: Pod "dns-test-da48cd65-0ccd-4ed5-ae50-a108aaa6e2cd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016225034s
Jul 29 16:42:24.119: INFO: Pod "dns-test-da48cd65-0ccd-4ed5-ae50-a108aaa6e2cd": Phase="Running", Reason="", readiness=true. Elapsed: 4.023923651s
Jul 29 16:42:24.120: INFO: Pod "dns-test-da48cd65-0ccd-4ed5-ae50-a108aaa6e2cd" satisfied condition "running"
STEP: retrieving the pod 07/29/23 16:42:24.12
STEP: looking for the results for each expected name from probers 07/29/23 16:42:24.126
Jul 29 16:42:24.145: INFO: DNS probes using dns-test-da48cd65-0ccd-4ed5-ae50-a108aaa6e2cd succeeded

STEP: deleting the pod 07/29/23 16:42:24.145
STEP: changing the externalName to bar.example.com 07/29/23 16:42:24.183
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1952.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-1952.svc.cluster.local; sleep 1; done
 07/29/23 16:42:24.206
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1952.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-1952.svc.cluster.local; sleep 1; done
 07/29/23 16:42:24.206
STEP: creating a second pod to probe DNS 07/29/23 16:42:24.206
STEP: submitting the pod to kubernetes 07/29/23 16:42:24.206
Jul 29 16:42:24.216: INFO: Waiting up to 15m0s for pod "dns-test-d9f0e34a-b795-457a-97d1-71948ee73d4c" in namespace "dns-1952" to be "running"
Jul 29 16:42:24.226: INFO: Pod "dns-test-d9f0e34a-b795-457a-97d1-71948ee73d4c": Phase="Pending", Reason="", readiness=false. Elapsed: 9.914884ms
Jul 29 16:42:26.237: INFO: Pod "dns-test-d9f0e34a-b795-457a-97d1-71948ee73d4c": Phase="Running", Reason="", readiness=true. Elapsed: 2.020772123s
Jul 29 16:42:26.237: INFO: Pod "dns-test-d9f0e34a-b795-457a-97d1-71948ee73d4c" satisfied condition "running"
STEP: retrieving the pod 07/29/23 16:42:26.237
STEP: looking for the results for each expected name from probers 07/29/23 16:42:26.242
Jul 29 16:42:26.256: INFO: File wheezy_udp@dns-test-service-3.dns-1952.svc.cluster.local from pod  dns-1952/dns-test-d9f0e34a-b795-457a-97d1-71948ee73d4c contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul 29 16:42:26.263: INFO: File jessie_udp@dns-test-service-3.dns-1952.svc.cluster.local from pod  dns-1952/dns-test-d9f0e34a-b795-457a-97d1-71948ee73d4c contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul 29 16:42:26.264: INFO: Lookups using dns-1952/dns-test-d9f0e34a-b795-457a-97d1-71948ee73d4c failed for: [wheezy_udp@dns-test-service-3.dns-1952.svc.cluster.local jessie_udp@dns-test-service-3.dns-1952.svc.cluster.local]

Jul 29 16:42:31.273: INFO: File wheezy_udp@dns-test-service-3.dns-1952.svc.cluster.local from pod  dns-1952/dns-test-d9f0e34a-b795-457a-97d1-71948ee73d4c contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul 29 16:42:31.282: INFO: File jessie_udp@dns-test-service-3.dns-1952.svc.cluster.local from pod  dns-1952/dns-test-d9f0e34a-b795-457a-97d1-71948ee73d4c contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul 29 16:42:31.282: INFO: Lookups using dns-1952/dns-test-d9f0e34a-b795-457a-97d1-71948ee73d4c failed for: [wheezy_udp@dns-test-service-3.dns-1952.svc.cluster.local jessie_udp@dns-test-service-3.dns-1952.svc.cluster.local]

Jul 29 16:42:36.273: INFO: File wheezy_udp@dns-test-service-3.dns-1952.svc.cluster.local from pod  dns-1952/dns-test-d9f0e34a-b795-457a-97d1-71948ee73d4c contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul 29 16:42:36.282: INFO: File jessie_udp@dns-test-service-3.dns-1952.svc.cluster.local from pod  dns-1952/dns-test-d9f0e34a-b795-457a-97d1-71948ee73d4c contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul 29 16:42:36.282: INFO: Lookups using dns-1952/dns-test-d9f0e34a-b795-457a-97d1-71948ee73d4c failed for: [wheezy_udp@dns-test-service-3.dns-1952.svc.cluster.local jessie_udp@dns-test-service-3.dns-1952.svc.cluster.local]

Jul 29 16:42:41.273: INFO: File wheezy_udp@dns-test-service-3.dns-1952.svc.cluster.local from pod  dns-1952/dns-test-d9f0e34a-b795-457a-97d1-71948ee73d4c contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul 29 16:42:41.284: INFO: File jessie_udp@dns-test-service-3.dns-1952.svc.cluster.local from pod  dns-1952/dns-test-d9f0e34a-b795-457a-97d1-71948ee73d4c contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul 29 16:42:41.284: INFO: Lookups using dns-1952/dns-test-d9f0e34a-b795-457a-97d1-71948ee73d4c failed for: [wheezy_udp@dns-test-service-3.dns-1952.svc.cluster.local jessie_udp@dns-test-service-3.dns-1952.svc.cluster.local]

Jul 29 16:42:46.273: INFO: File wheezy_udp@dns-test-service-3.dns-1952.svc.cluster.local from pod  dns-1952/dns-test-d9f0e34a-b795-457a-97d1-71948ee73d4c contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul 29 16:42:46.281: INFO: File jessie_udp@dns-test-service-3.dns-1952.svc.cluster.local from pod  dns-1952/dns-test-d9f0e34a-b795-457a-97d1-71948ee73d4c contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul 29 16:42:46.281: INFO: Lookups using dns-1952/dns-test-d9f0e34a-b795-457a-97d1-71948ee73d4c failed for: [wheezy_udp@dns-test-service-3.dns-1952.svc.cluster.local jessie_udp@dns-test-service-3.dns-1952.svc.cluster.local]

Jul 29 16:42:51.275: INFO: File wheezy_udp@dns-test-service-3.dns-1952.svc.cluster.local from pod  dns-1952/dns-test-d9f0e34a-b795-457a-97d1-71948ee73d4c contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul 29 16:42:51.283: INFO: File jessie_udp@dns-test-service-3.dns-1952.svc.cluster.local from pod  dns-1952/dns-test-d9f0e34a-b795-457a-97d1-71948ee73d4c contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul 29 16:42:51.283: INFO: Lookups using dns-1952/dns-test-d9f0e34a-b795-457a-97d1-71948ee73d4c failed for: [wheezy_udp@dns-test-service-3.dns-1952.svc.cluster.local jessie_udp@dns-test-service-3.dns-1952.svc.cluster.local]

Jul 29 16:42:56.287: INFO: DNS probes using dns-test-d9f0e34a-b795-457a-97d1-71948ee73d4c succeeded

STEP: deleting the pod 07/29/23 16:42:56.288
STEP: changing the service to type=ClusterIP 07/29/23 16:42:56.318
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1952.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-1952.svc.cluster.local; sleep 1; done
 07/29/23 16:42:56.371
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1952.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-1952.svc.cluster.local; sleep 1; done
 07/29/23 16:42:56.372
STEP: creating a third pod to probe DNS 07/29/23 16:42:56.372
STEP: submitting the pod to kubernetes 07/29/23 16:42:56.379
Jul 29 16:42:56.409: INFO: Waiting up to 15m0s for pod "dns-test-90f63609-9ddf-4c67-9ff9-0e8786d0b3b3" in namespace "dns-1952" to be "running"
Jul 29 16:42:56.416: INFO: Pod "dns-test-90f63609-9ddf-4c67-9ff9-0e8786d0b3b3": Phase="Pending", Reason="", readiness=false. Elapsed: 7.56774ms
Jul 29 16:42:58.424: INFO: Pod "dns-test-90f63609-9ddf-4c67-9ff9-0e8786d0b3b3": Phase="Running", Reason="", readiness=true. Elapsed: 2.015454881s
Jul 29 16:42:58.424: INFO: Pod "dns-test-90f63609-9ddf-4c67-9ff9-0e8786d0b3b3" satisfied condition "running"
STEP: retrieving the pod 07/29/23 16:42:58.424
STEP: looking for the results for each expected name from probers 07/29/23 16:42:58.431
Jul 29 16:42:58.455: INFO: DNS probes using dns-test-90f63609-9ddf-4c67-9ff9-0e8786d0b3b3 succeeded

STEP: deleting the pod 07/29/23 16:42:58.455
STEP: deleting the test externalName service 07/29/23 16:42:58.475
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jul 29 16:42:58.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1952" for this suite. 07/29/23 16:42:58.513
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","completed":284,"skipped":5191,"failed":0}
------------------------------
â€¢ [SLOW TEST] [38.536 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:42:19.996
    Jul 29 16:42:19.996: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename dns 07/29/23 16:42:19.998
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:42:20.035
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:42:20.046
    [It] should provide DNS for ExternalName services [Conformance]
      test/e2e/network/dns.go:333
    STEP: Creating a test externalName service 07/29/23 16:42:20.057
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1952.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-1952.svc.cluster.local; sleep 1; done
     07/29/23 16:42:20.073
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1952.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-1952.svc.cluster.local; sleep 1; done
     07/29/23 16:42:20.073
    STEP: creating a pod to probe DNS 07/29/23 16:42:20.074
    STEP: submitting the pod to kubernetes 07/29/23 16:42:20.074
    Jul 29 16:42:20.095: INFO: Waiting up to 15m0s for pod "dns-test-da48cd65-0ccd-4ed5-ae50-a108aaa6e2cd" in namespace "dns-1952" to be "running"
    Jul 29 16:42:20.103: INFO: Pod "dns-test-da48cd65-0ccd-4ed5-ae50-a108aaa6e2cd": Phase="Pending", Reason="", readiness=false. Elapsed: 7.801465ms
    Jul 29 16:42:22.112: INFO: Pod "dns-test-da48cd65-0ccd-4ed5-ae50-a108aaa6e2cd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016225034s
    Jul 29 16:42:24.119: INFO: Pod "dns-test-da48cd65-0ccd-4ed5-ae50-a108aaa6e2cd": Phase="Running", Reason="", readiness=true. Elapsed: 4.023923651s
    Jul 29 16:42:24.120: INFO: Pod "dns-test-da48cd65-0ccd-4ed5-ae50-a108aaa6e2cd" satisfied condition "running"
    STEP: retrieving the pod 07/29/23 16:42:24.12
    STEP: looking for the results for each expected name from probers 07/29/23 16:42:24.126
    Jul 29 16:42:24.145: INFO: DNS probes using dns-test-da48cd65-0ccd-4ed5-ae50-a108aaa6e2cd succeeded

    STEP: deleting the pod 07/29/23 16:42:24.145
    STEP: changing the externalName to bar.example.com 07/29/23 16:42:24.183
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1952.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-1952.svc.cluster.local; sleep 1; done
     07/29/23 16:42:24.206
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1952.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-1952.svc.cluster.local; sleep 1; done
     07/29/23 16:42:24.206
    STEP: creating a second pod to probe DNS 07/29/23 16:42:24.206
    STEP: submitting the pod to kubernetes 07/29/23 16:42:24.206
    Jul 29 16:42:24.216: INFO: Waiting up to 15m0s for pod "dns-test-d9f0e34a-b795-457a-97d1-71948ee73d4c" in namespace "dns-1952" to be "running"
    Jul 29 16:42:24.226: INFO: Pod "dns-test-d9f0e34a-b795-457a-97d1-71948ee73d4c": Phase="Pending", Reason="", readiness=false. Elapsed: 9.914884ms
    Jul 29 16:42:26.237: INFO: Pod "dns-test-d9f0e34a-b795-457a-97d1-71948ee73d4c": Phase="Running", Reason="", readiness=true. Elapsed: 2.020772123s
    Jul 29 16:42:26.237: INFO: Pod "dns-test-d9f0e34a-b795-457a-97d1-71948ee73d4c" satisfied condition "running"
    STEP: retrieving the pod 07/29/23 16:42:26.237
    STEP: looking for the results for each expected name from probers 07/29/23 16:42:26.242
    Jul 29 16:42:26.256: INFO: File wheezy_udp@dns-test-service-3.dns-1952.svc.cluster.local from pod  dns-1952/dns-test-d9f0e34a-b795-457a-97d1-71948ee73d4c contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jul 29 16:42:26.263: INFO: File jessie_udp@dns-test-service-3.dns-1952.svc.cluster.local from pod  dns-1952/dns-test-d9f0e34a-b795-457a-97d1-71948ee73d4c contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jul 29 16:42:26.264: INFO: Lookups using dns-1952/dns-test-d9f0e34a-b795-457a-97d1-71948ee73d4c failed for: [wheezy_udp@dns-test-service-3.dns-1952.svc.cluster.local jessie_udp@dns-test-service-3.dns-1952.svc.cluster.local]

    Jul 29 16:42:31.273: INFO: File wheezy_udp@dns-test-service-3.dns-1952.svc.cluster.local from pod  dns-1952/dns-test-d9f0e34a-b795-457a-97d1-71948ee73d4c contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jul 29 16:42:31.282: INFO: File jessie_udp@dns-test-service-3.dns-1952.svc.cluster.local from pod  dns-1952/dns-test-d9f0e34a-b795-457a-97d1-71948ee73d4c contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jul 29 16:42:31.282: INFO: Lookups using dns-1952/dns-test-d9f0e34a-b795-457a-97d1-71948ee73d4c failed for: [wheezy_udp@dns-test-service-3.dns-1952.svc.cluster.local jessie_udp@dns-test-service-3.dns-1952.svc.cluster.local]

    Jul 29 16:42:36.273: INFO: File wheezy_udp@dns-test-service-3.dns-1952.svc.cluster.local from pod  dns-1952/dns-test-d9f0e34a-b795-457a-97d1-71948ee73d4c contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jul 29 16:42:36.282: INFO: File jessie_udp@dns-test-service-3.dns-1952.svc.cluster.local from pod  dns-1952/dns-test-d9f0e34a-b795-457a-97d1-71948ee73d4c contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jul 29 16:42:36.282: INFO: Lookups using dns-1952/dns-test-d9f0e34a-b795-457a-97d1-71948ee73d4c failed for: [wheezy_udp@dns-test-service-3.dns-1952.svc.cluster.local jessie_udp@dns-test-service-3.dns-1952.svc.cluster.local]

    Jul 29 16:42:41.273: INFO: File wheezy_udp@dns-test-service-3.dns-1952.svc.cluster.local from pod  dns-1952/dns-test-d9f0e34a-b795-457a-97d1-71948ee73d4c contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jul 29 16:42:41.284: INFO: File jessie_udp@dns-test-service-3.dns-1952.svc.cluster.local from pod  dns-1952/dns-test-d9f0e34a-b795-457a-97d1-71948ee73d4c contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jul 29 16:42:41.284: INFO: Lookups using dns-1952/dns-test-d9f0e34a-b795-457a-97d1-71948ee73d4c failed for: [wheezy_udp@dns-test-service-3.dns-1952.svc.cluster.local jessie_udp@dns-test-service-3.dns-1952.svc.cluster.local]

    Jul 29 16:42:46.273: INFO: File wheezy_udp@dns-test-service-3.dns-1952.svc.cluster.local from pod  dns-1952/dns-test-d9f0e34a-b795-457a-97d1-71948ee73d4c contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jul 29 16:42:46.281: INFO: File jessie_udp@dns-test-service-3.dns-1952.svc.cluster.local from pod  dns-1952/dns-test-d9f0e34a-b795-457a-97d1-71948ee73d4c contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jul 29 16:42:46.281: INFO: Lookups using dns-1952/dns-test-d9f0e34a-b795-457a-97d1-71948ee73d4c failed for: [wheezy_udp@dns-test-service-3.dns-1952.svc.cluster.local jessie_udp@dns-test-service-3.dns-1952.svc.cluster.local]

    Jul 29 16:42:51.275: INFO: File wheezy_udp@dns-test-service-3.dns-1952.svc.cluster.local from pod  dns-1952/dns-test-d9f0e34a-b795-457a-97d1-71948ee73d4c contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jul 29 16:42:51.283: INFO: File jessie_udp@dns-test-service-3.dns-1952.svc.cluster.local from pod  dns-1952/dns-test-d9f0e34a-b795-457a-97d1-71948ee73d4c contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jul 29 16:42:51.283: INFO: Lookups using dns-1952/dns-test-d9f0e34a-b795-457a-97d1-71948ee73d4c failed for: [wheezy_udp@dns-test-service-3.dns-1952.svc.cluster.local jessie_udp@dns-test-service-3.dns-1952.svc.cluster.local]

    Jul 29 16:42:56.287: INFO: DNS probes using dns-test-d9f0e34a-b795-457a-97d1-71948ee73d4c succeeded

    STEP: deleting the pod 07/29/23 16:42:56.288
    STEP: changing the service to type=ClusterIP 07/29/23 16:42:56.318
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1952.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-1952.svc.cluster.local; sleep 1; done
     07/29/23 16:42:56.371
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1952.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-1952.svc.cluster.local; sleep 1; done
     07/29/23 16:42:56.372
    STEP: creating a third pod to probe DNS 07/29/23 16:42:56.372
    STEP: submitting the pod to kubernetes 07/29/23 16:42:56.379
    Jul 29 16:42:56.409: INFO: Waiting up to 15m0s for pod "dns-test-90f63609-9ddf-4c67-9ff9-0e8786d0b3b3" in namespace "dns-1952" to be "running"
    Jul 29 16:42:56.416: INFO: Pod "dns-test-90f63609-9ddf-4c67-9ff9-0e8786d0b3b3": Phase="Pending", Reason="", readiness=false. Elapsed: 7.56774ms
    Jul 29 16:42:58.424: INFO: Pod "dns-test-90f63609-9ddf-4c67-9ff9-0e8786d0b3b3": Phase="Running", Reason="", readiness=true. Elapsed: 2.015454881s
    Jul 29 16:42:58.424: INFO: Pod "dns-test-90f63609-9ddf-4c67-9ff9-0e8786d0b3b3" satisfied condition "running"
    STEP: retrieving the pod 07/29/23 16:42:58.424
    STEP: looking for the results for each expected name from probers 07/29/23 16:42:58.431
    Jul 29 16:42:58.455: INFO: DNS probes using dns-test-90f63609-9ddf-4c67-9ff9-0e8786d0b3b3 succeeded

    STEP: deleting the pod 07/29/23 16:42:58.455
    STEP: deleting the test externalName service 07/29/23 16:42:58.475
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jul 29 16:42:58.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-1952" for this suite. 07/29/23 16:42:58.513
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:42:58.548
Jul 29 16:42:58.548: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename svcaccounts 07/29/23 16:42:58.551
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:42:58.584
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:42:58.589
[It] should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75
Jul 29 16:42:58.630: INFO: Waiting up to 5m0s for pod "pod-service-account-669b1d90-a24e-4abb-9f3f-d883d5273ed2" in namespace "svcaccounts-596" to be "running"
Jul 29 16:42:58.666: INFO: Pod "pod-service-account-669b1d90-a24e-4abb-9f3f-d883d5273ed2": Phase="Pending", Reason="", readiness=false. Elapsed: 36.382393ms
Jul 29 16:43:00.674: INFO: Pod "pod-service-account-669b1d90-a24e-4abb-9f3f-d883d5273ed2": Phase="Running", Reason="", readiness=true. Elapsed: 2.043633158s
Jul 29 16:43:00.674: INFO: Pod "pod-service-account-669b1d90-a24e-4abb-9f3f-d883d5273ed2" satisfied condition "running"
STEP: reading a file in the container 07/29/23 16:43:00.674
Jul 29 16:43:00.675: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-596 pod-service-account-669b1d90-a24e-4abb-9f3f-d883d5273ed2 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container 07/29/23 16:43:00.951
Jul 29 16:43:00.955: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-596 pod-service-account-669b1d90-a24e-4abb-9f3f-d883d5273ed2 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container 07/29/23 16:43:01.286
Jul 29 16:43:01.288: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-596 pod-service-account-669b1d90-a24e-4abb-9f3f-d883d5273ed2 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
Jul 29 16:43:01.577: INFO: Got root ca configmap in namespace "svcaccounts-596"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Jul 29 16:43:01.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-596" for this suite. 07/29/23 16:43:01.594
{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","completed":285,"skipped":5201,"failed":0}
------------------------------
â€¢ [3.062 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:42:58.548
    Jul 29 16:42:58.548: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename svcaccounts 07/29/23 16:42:58.551
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:42:58.584
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:42:58.589
    [It] should mount an API token into pods  [Conformance]
      test/e2e/auth/service_accounts.go:75
    Jul 29 16:42:58.630: INFO: Waiting up to 5m0s for pod "pod-service-account-669b1d90-a24e-4abb-9f3f-d883d5273ed2" in namespace "svcaccounts-596" to be "running"
    Jul 29 16:42:58.666: INFO: Pod "pod-service-account-669b1d90-a24e-4abb-9f3f-d883d5273ed2": Phase="Pending", Reason="", readiness=false. Elapsed: 36.382393ms
    Jul 29 16:43:00.674: INFO: Pod "pod-service-account-669b1d90-a24e-4abb-9f3f-d883d5273ed2": Phase="Running", Reason="", readiness=true. Elapsed: 2.043633158s
    Jul 29 16:43:00.674: INFO: Pod "pod-service-account-669b1d90-a24e-4abb-9f3f-d883d5273ed2" satisfied condition "running"
    STEP: reading a file in the container 07/29/23 16:43:00.674
    Jul 29 16:43:00.675: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-596 pod-service-account-669b1d90-a24e-4abb-9f3f-d883d5273ed2 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
    STEP: reading a file in the container 07/29/23 16:43:00.951
    Jul 29 16:43:00.955: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-596 pod-service-account-669b1d90-a24e-4abb-9f3f-d883d5273ed2 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
    STEP: reading a file in the container 07/29/23 16:43:01.286
    Jul 29 16:43:01.288: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-596 pod-service-account-669b1d90-a24e-4abb-9f3f-d883d5273ed2 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
    Jul 29 16:43:01.577: INFO: Got root ca configmap in namespace "svcaccounts-596"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Jul 29 16:43:01.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-596" for this suite. 07/29/23 16:43:01.594
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1248
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:43:01.611
Jul 29 16:43:01.611: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename kubectl 07/29/23 16:43:01.619
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:43:01.654
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:43:01.66
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1248
STEP: validating cluster-info 07/29/23 16:43:01.664
Jul 29 16:43:01.666: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-340 cluster-info'
Jul 29 16:43:01.808: INFO: stderr: ""
Jul 29 16:43:01.808: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.233.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jul 29 16:43:01.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-340" for this suite. 07/29/23 16:43:01.819
{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","completed":286,"skipped":5212,"failed":0}
------------------------------
â€¢ [0.221 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  test/e2e/kubectl/kubectl.go:1242
    should check if Kubernetes control plane services is included in cluster-info  [Conformance]
    test/e2e/kubectl/kubectl.go:1248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:43:01.611
    Jul 29 16:43:01.611: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename kubectl 07/29/23 16:43:01.619
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:43:01.654
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:43:01.66
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
      test/e2e/kubectl/kubectl.go:1248
    STEP: validating cluster-info 07/29/23 16:43:01.664
    Jul 29 16:43:01.666: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-340 cluster-info'
    Jul 29 16:43:01.808: INFO: stderr: ""
    Jul 29 16:43:01.808: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.233.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jul 29 16:43:01.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-340" for this suite. 07/29/23 16:43:01.819
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:43:01.834
Jul 29 16:43:01.834: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename emptydir 07/29/23 16:43:01.836
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:43:01.864
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:43:01.869
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126
STEP: Creating a pod to test emptydir 0644 on tmpfs 07/29/23 16:43:01.874
Jul 29 16:43:01.899: INFO: Waiting up to 5m0s for pod "pod-163e9f52-e68f-4ac5-9570-e8880d8463ff" in namespace "emptydir-9258" to be "Succeeded or Failed"
Jul 29 16:43:01.924: INFO: Pod "pod-163e9f52-e68f-4ac5-9570-e8880d8463ff": Phase="Pending", Reason="", readiness=false. Elapsed: 24.990436ms
Jul 29 16:43:03.931: INFO: Pod "pod-163e9f52-e68f-4ac5-9570-e8880d8463ff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031706991s
Jul 29 16:43:05.932: INFO: Pod "pod-163e9f52-e68f-4ac5-9570-e8880d8463ff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033041186s
STEP: Saw pod success 07/29/23 16:43:05.932
Jul 29 16:43:05.933: INFO: Pod "pod-163e9f52-e68f-4ac5-9570-e8880d8463ff" satisfied condition "Succeeded or Failed"
Jul 29 16:43:05.942: INFO: Trying to get logs from node wa4quivohpee-3 pod pod-163e9f52-e68f-4ac5-9570-e8880d8463ff container test-container: <nil>
STEP: delete the pod 07/29/23 16:43:05.956
Jul 29 16:43:05.978: INFO: Waiting for pod pod-163e9f52-e68f-4ac5-9570-e8880d8463ff to disappear
Jul 29 16:43:05.984: INFO: Pod pod-163e9f52-e68f-4ac5-9570-e8880d8463ff no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jul 29 16:43:05.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9258" for this suite. 07/29/23 16:43:05.995
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":287,"skipped":5231,"failed":0}
------------------------------
â€¢ [4.176 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:43:01.834
    Jul 29 16:43:01.834: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename emptydir 07/29/23 16:43:01.836
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:43:01.864
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:43:01.869
    [It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:126
    STEP: Creating a pod to test emptydir 0644 on tmpfs 07/29/23 16:43:01.874
    Jul 29 16:43:01.899: INFO: Waiting up to 5m0s for pod "pod-163e9f52-e68f-4ac5-9570-e8880d8463ff" in namespace "emptydir-9258" to be "Succeeded or Failed"
    Jul 29 16:43:01.924: INFO: Pod "pod-163e9f52-e68f-4ac5-9570-e8880d8463ff": Phase="Pending", Reason="", readiness=false. Elapsed: 24.990436ms
    Jul 29 16:43:03.931: INFO: Pod "pod-163e9f52-e68f-4ac5-9570-e8880d8463ff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031706991s
    Jul 29 16:43:05.932: INFO: Pod "pod-163e9f52-e68f-4ac5-9570-e8880d8463ff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033041186s
    STEP: Saw pod success 07/29/23 16:43:05.932
    Jul 29 16:43:05.933: INFO: Pod "pod-163e9f52-e68f-4ac5-9570-e8880d8463ff" satisfied condition "Succeeded or Failed"
    Jul 29 16:43:05.942: INFO: Trying to get logs from node wa4quivohpee-3 pod pod-163e9f52-e68f-4ac5-9570-e8880d8463ff container test-container: <nil>
    STEP: delete the pod 07/29/23 16:43:05.956
    Jul 29 16:43:05.978: INFO: Waiting for pod pod-163e9f52-e68f-4ac5-9570-e8880d8463ff to disappear
    Jul 29 16:43:05.984: INFO: Pod pod-163e9f52-e68f-4ac5-9570-e8880d8463ff no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jul 29 16:43:05.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-9258" for this suite. 07/29/23 16:43:05.995
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:43:06.013
Jul 29 16:43:06.013: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename emptydir-wrapper 07/29/23 16:43:06.018
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:43:06.055
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:43:06.059
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
STEP: Creating 50 configmaps 07/29/23 16:43:06.067
STEP: Creating RC which spawns configmap-volume pods 07/29/23 16:43:06.502
Jul 29 16:43:06.535: INFO: Pod name wrapped-volume-race-469eb3f5-73b6-4e73-9e47-24a923f88c77: Found 0 pods out of 5
Jul 29 16:43:11.557: INFO: Pod name wrapped-volume-race-469eb3f5-73b6-4e73-9e47-24a923f88c77: Found 5 pods out of 5
STEP: Ensuring each pod is running 07/29/23 16:43:11.557
Jul 29 16:43:11.557: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-469eb3f5-73b6-4e73-9e47-24a923f88c77-85427" in namespace "emptydir-wrapper-8504" to be "running"
Jul 29 16:43:11.565: INFO: Pod "wrapped-volume-race-469eb3f5-73b6-4e73-9e47-24a923f88c77-85427": Phase="Pending", Reason="", readiness=false. Elapsed: 7.458546ms
Jul 29 16:43:13.583: INFO: Pod "wrapped-volume-race-469eb3f5-73b6-4e73-9e47-24a923f88c77-85427": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025569932s
Jul 29 16:43:15.580: INFO: Pod "wrapped-volume-race-469eb3f5-73b6-4e73-9e47-24a923f88c77-85427": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023102769s
Jul 29 16:43:17.574: INFO: Pod "wrapped-volume-race-469eb3f5-73b6-4e73-9e47-24a923f88c77-85427": Phase="Pending", Reason="", readiness=false. Elapsed: 6.017029591s
Jul 29 16:43:19.579: INFO: Pod "wrapped-volume-race-469eb3f5-73b6-4e73-9e47-24a923f88c77-85427": Phase="Pending", Reason="", readiness=false. Elapsed: 8.021496285s
Jul 29 16:43:21.575: INFO: Pod "wrapped-volume-race-469eb3f5-73b6-4e73-9e47-24a923f88c77-85427": Phase="Pending", Reason="", readiness=false. Elapsed: 10.018168144s
Jul 29 16:43:23.574: INFO: Pod "wrapped-volume-race-469eb3f5-73b6-4e73-9e47-24a923f88c77-85427": Phase="Running", Reason="", readiness=true. Elapsed: 12.016419406s
Jul 29 16:43:23.574: INFO: Pod "wrapped-volume-race-469eb3f5-73b6-4e73-9e47-24a923f88c77-85427" satisfied condition "running"
Jul 29 16:43:23.574: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-469eb3f5-73b6-4e73-9e47-24a923f88c77-c5jpd" in namespace "emptydir-wrapper-8504" to be "running"
Jul 29 16:43:23.583: INFO: Pod "wrapped-volume-race-469eb3f5-73b6-4e73-9e47-24a923f88c77-c5jpd": Phase="Running", Reason="", readiness=true. Elapsed: 8.800558ms
Jul 29 16:43:23.583: INFO: Pod "wrapped-volume-race-469eb3f5-73b6-4e73-9e47-24a923f88c77-c5jpd" satisfied condition "running"
Jul 29 16:43:23.583: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-469eb3f5-73b6-4e73-9e47-24a923f88c77-hk29b" in namespace "emptydir-wrapper-8504" to be "running"
Jul 29 16:43:23.594: INFO: Pod "wrapped-volume-race-469eb3f5-73b6-4e73-9e47-24a923f88c77-hk29b": Phase="Running", Reason="", readiness=true. Elapsed: 10.489397ms
Jul 29 16:43:23.594: INFO: Pod "wrapped-volume-race-469eb3f5-73b6-4e73-9e47-24a923f88c77-hk29b" satisfied condition "running"
Jul 29 16:43:23.594: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-469eb3f5-73b6-4e73-9e47-24a923f88c77-qc28k" in namespace "emptydir-wrapper-8504" to be "running"
Jul 29 16:43:23.608: INFO: Pod "wrapped-volume-race-469eb3f5-73b6-4e73-9e47-24a923f88c77-qc28k": Phase="Running", Reason="", readiness=true. Elapsed: 14.25432ms
Jul 29 16:43:23.608: INFO: Pod "wrapped-volume-race-469eb3f5-73b6-4e73-9e47-24a923f88c77-qc28k" satisfied condition "running"
Jul 29 16:43:23.608: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-469eb3f5-73b6-4e73-9e47-24a923f88c77-xj7ks" in namespace "emptydir-wrapper-8504" to be "running"
Jul 29 16:43:23.629: INFO: Pod "wrapped-volume-race-469eb3f5-73b6-4e73-9e47-24a923f88c77-xj7ks": Phase="Running", Reason="", readiness=true. Elapsed: 20.606461ms
Jul 29 16:43:23.629: INFO: Pod "wrapped-volume-race-469eb3f5-73b6-4e73-9e47-24a923f88c77-xj7ks" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-469eb3f5-73b6-4e73-9e47-24a923f88c77 in namespace emptydir-wrapper-8504, will wait for the garbage collector to delete the pods 07/29/23 16:43:23.629
Jul 29 16:43:23.706: INFO: Deleting ReplicationController wrapped-volume-race-469eb3f5-73b6-4e73-9e47-24a923f88c77 took: 14.798358ms
Jul 29 16:43:23.807: INFO: Terminating ReplicationController wrapped-volume-race-469eb3f5-73b6-4e73-9e47-24a923f88c77 pods took: 100.975113ms
STEP: Creating RC which spawns configmap-volume pods 07/29/23 16:43:25.919
Jul 29 16:43:25.953: INFO: Pod name wrapped-volume-race-6fe40fcc-1ded-4693-90a3-b7ceb4f4775d: Found 0 pods out of 5
Jul 29 16:43:30.971: INFO: Pod name wrapped-volume-race-6fe40fcc-1ded-4693-90a3-b7ceb4f4775d: Found 5 pods out of 5
STEP: Ensuring each pod is running 07/29/23 16:43:30.971
Jul 29 16:43:30.972: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-6fe40fcc-1ded-4693-90a3-b7ceb4f4775d-d8tmm" in namespace "emptydir-wrapper-8504" to be "running"
Jul 29 16:43:30.982: INFO: Pod "wrapped-volume-race-6fe40fcc-1ded-4693-90a3-b7ceb4f4775d-d8tmm": Phase="Pending", Reason="", readiness=false. Elapsed: 10.447201ms
Jul 29 16:43:32.996: INFO: Pod "wrapped-volume-race-6fe40fcc-1ded-4693-90a3-b7ceb4f4775d-d8tmm": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024735773s
Jul 29 16:43:34.994: INFO: Pod "wrapped-volume-race-6fe40fcc-1ded-4693-90a3-b7ceb4f4775d-d8tmm": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022709981s
Jul 29 16:43:36.993: INFO: Pod "wrapped-volume-race-6fe40fcc-1ded-4693-90a3-b7ceb4f4775d-d8tmm": Phase="Pending", Reason="", readiness=false. Elapsed: 6.021720339s
Jul 29 16:43:39.007: INFO: Pod "wrapped-volume-race-6fe40fcc-1ded-4693-90a3-b7ceb4f4775d-d8tmm": Phase="Pending", Reason="", readiness=false. Elapsed: 8.035752226s
Jul 29 16:43:41.000: INFO: Pod "wrapped-volume-race-6fe40fcc-1ded-4693-90a3-b7ceb4f4775d-d8tmm": Phase="Pending", Reason="", readiness=false. Elapsed: 10.028494152s
Jul 29 16:43:42.993: INFO: Pod "wrapped-volume-race-6fe40fcc-1ded-4693-90a3-b7ceb4f4775d-d8tmm": Phase="Running", Reason="", readiness=true. Elapsed: 12.021440371s
Jul 29 16:43:42.993: INFO: Pod "wrapped-volume-race-6fe40fcc-1ded-4693-90a3-b7ceb4f4775d-d8tmm" satisfied condition "running"
Jul 29 16:43:42.993: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-6fe40fcc-1ded-4693-90a3-b7ceb4f4775d-k8skn" in namespace "emptydir-wrapper-8504" to be "running"
Jul 29 16:43:43.003: INFO: Pod "wrapped-volume-race-6fe40fcc-1ded-4693-90a3-b7ceb4f4775d-k8skn": Phase="Running", Reason="", readiness=true. Elapsed: 9.298584ms
Jul 29 16:43:43.003: INFO: Pod "wrapped-volume-race-6fe40fcc-1ded-4693-90a3-b7ceb4f4775d-k8skn" satisfied condition "running"
Jul 29 16:43:43.003: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-6fe40fcc-1ded-4693-90a3-b7ceb4f4775d-ng5z9" in namespace "emptydir-wrapper-8504" to be "running"
Jul 29 16:43:43.014: INFO: Pod "wrapped-volume-race-6fe40fcc-1ded-4693-90a3-b7ceb4f4775d-ng5z9": Phase="Running", Reason="", readiness=true. Elapsed: 11.452047ms
Jul 29 16:43:43.014: INFO: Pod "wrapped-volume-race-6fe40fcc-1ded-4693-90a3-b7ceb4f4775d-ng5z9" satisfied condition "running"
Jul 29 16:43:43.014: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-6fe40fcc-1ded-4693-90a3-b7ceb4f4775d-rptt8" in namespace "emptydir-wrapper-8504" to be "running"
Jul 29 16:43:43.026: INFO: Pod "wrapped-volume-race-6fe40fcc-1ded-4693-90a3-b7ceb4f4775d-rptt8": Phase="Running", Reason="", readiness=true. Elapsed: 11.682518ms
Jul 29 16:43:43.026: INFO: Pod "wrapped-volume-race-6fe40fcc-1ded-4693-90a3-b7ceb4f4775d-rptt8" satisfied condition "running"
Jul 29 16:43:43.026: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-6fe40fcc-1ded-4693-90a3-b7ceb4f4775d-tvqjx" in namespace "emptydir-wrapper-8504" to be "running"
Jul 29 16:43:43.034: INFO: Pod "wrapped-volume-race-6fe40fcc-1ded-4693-90a3-b7ceb4f4775d-tvqjx": Phase="Running", Reason="", readiness=true. Elapsed: 8.12608ms
Jul 29 16:43:43.035: INFO: Pod "wrapped-volume-race-6fe40fcc-1ded-4693-90a3-b7ceb4f4775d-tvqjx" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-6fe40fcc-1ded-4693-90a3-b7ceb4f4775d in namespace emptydir-wrapper-8504, will wait for the garbage collector to delete the pods 07/29/23 16:43:43.035
Jul 29 16:43:43.110: INFO: Deleting ReplicationController wrapped-volume-race-6fe40fcc-1ded-4693-90a3-b7ceb4f4775d took: 16.611505ms
Jul 29 16:43:43.310: INFO: Terminating ReplicationController wrapped-volume-race-6fe40fcc-1ded-4693-90a3-b7ceb4f4775d pods took: 200.273365ms
STEP: Creating RC which spawns configmap-volume pods 07/29/23 16:43:46.52
Jul 29 16:43:46.554: INFO: Pod name wrapped-volume-race-158d57a7-b5a6-4384-a048-cab44e1961f4: Found 0 pods out of 5
Jul 29 16:43:51.571: INFO: Pod name wrapped-volume-race-158d57a7-b5a6-4384-a048-cab44e1961f4: Found 5 pods out of 5
STEP: Ensuring each pod is running 07/29/23 16:43:51.571
Jul 29 16:43:51.572: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-158d57a7-b5a6-4384-a048-cab44e1961f4-45tfb" in namespace "emptydir-wrapper-8504" to be "running"
Jul 29 16:43:51.580: INFO: Pod "wrapped-volume-race-158d57a7-b5a6-4384-a048-cab44e1961f4-45tfb": Phase="Pending", Reason="", readiness=false. Elapsed: 7.882008ms
Jul 29 16:43:53.590: INFO: Pod "wrapped-volume-race-158d57a7-b5a6-4384-a048-cab44e1961f4-45tfb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018318006s
Jul 29 16:43:55.588: INFO: Pod "wrapped-volume-race-158d57a7-b5a6-4384-a048-cab44e1961f4-45tfb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016590081s
Jul 29 16:43:57.593: INFO: Pod "wrapped-volume-race-158d57a7-b5a6-4384-a048-cab44e1961f4-45tfb": Phase="Pending", Reason="", readiness=false. Elapsed: 6.021495861s
Jul 29 16:43:59.589: INFO: Pod "wrapped-volume-race-158d57a7-b5a6-4384-a048-cab44e1961f4-45tfb": Phase="Pending", Reason="", readiness=false. Elapsed: 8.016984945s
Jul 29 16:44:01.592: INFO: Pod "wrapped-volume-race-158d57a7-b5a6-4384-a048-cab44e1961f4-45tfb": Phase="Pending", Reason="", readiness=false. Elapsed: 10.020192865s
Jul 29 16:44:03.589: INFO: Pod "wrapped-volume-race-158d57a7-b5a6-4384-a048-cab44e1961f4-45tfb": Phase="Running", Reason="", readiness=true. Elapsed: 12.016923113s
Jul 29 16:44:03.589: INFO: Pod "wrapped-volume-race-158d57a7-b5a6-4384-a048-cab44e1961f4-45tfb" satisfied condition "running"
Jul 29 16:44:03.589: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-158d57a7-b5a6-4384-a048-cab44e1961f4-55hdt" in namespace "emptydir-wrapper-8504" to be "running"
Jul 29 16:44:03.596: INFO: Pod "wrapped-volume-race-158d57a7-b5a6-4384-a048-cab44e1961f4-55hdt": Phase="Running", Reason="", readiness=true. Elapsed: 7.127234ms
Jul 29 16:44:03.596: INFO: Pod "wrapped-volume-race-158d57a7-b5a6-4384-a048-cab44e1961f4-55hdt" satisfied condition "running"
Jul 29 16:44:03.596: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-158d57a7-b5a6-4384-a048-cab44e1961f4-64z7k" in namespace "emptydir-wrapper-8504" to be "running"
Jul 29 16:44:03.604: INFO: Pod "wrapped-volume-race-158d57a7-b5a6-4384-a048-cab44e1961f4-64z7k": Phase="Running", Reason="", readiness=true. Elapsed: 8.2062ms
Jul 29 16:44:03.604: INFO: Pod "wrapped-volume-race-158d57a7-b5a6-4384-a048-cab44e1961f4-64z7k" satisfied condition "running"
Jul 29 16:44:03.604: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-158d57a7-b5a6-4384-a048-cab44e1961f4-6lv2t" in namespace "emptydir-wrapper-8504" to be "running"
Jul 29 16:44:03.615: INFO: Pod "wrapped-volume-race-158d57a7-b5a6-4384-a048-cab44e1961f4-6lv2t": Phase="Running", Reason="", readiness=true. Elapsed: 10.860978ms
Jul 29 16:44:03.616: INFO: Pod "wrapped-volume-race-158d57a7-b5a6-4384-a048-cab44e1961f4-6lv2t" satisfied condition "running"
Jul 29 16:44:03.616: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-158d57a7-b5a6-4384-a048-cab44e1961f4-s65br" in namespace "emptydir-wrapper-8504" to be "running"
Jul 29 16:44:03.624: INFO: Pod "wrapped-volume-race-158d57a7-b5a6-4384-a048-cab44e1961f4-s65br": Phase="Running", Reason="", readiness=true. Elapsed: 8.129987ms
Jul 29 16:44:03.624: INFO: Pod "wrapped-volume-race-158d57a7-b5a6-4384-a048-cab44e1961f4-s65br" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-158d57a7-b5a6-4384-a048-cab44e1961f4 in namespace emptydir-wrapper-8504, will wait for the garbage collector to delete the pods 07/29/23 16:44:03.624
Jul 29 16:44:03.699: INFO: Deleting ReplicationController wrapped-volume-race-158d57a7-b5a6-4384-a048-cab44e1961f4 took: 14.141956ms
Jul 29 16:44:03.900: INFO: Terminating ReplicationController wrapped-volume-race-158d57a7-b5a6-4384-a048-cab44e1961f4 pods took: 201.306222ms
STEP: Cleaning up the configMaps 07/29/23 16:44:06.8
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:187
Jul 29 16:44:07.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-8504" for this suite. 07/29/23 16:44:07.382
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","completed":288,"skipped":5233,"failed":0}
------------------------------
â€¢ [SLOW TEST] [61.397 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:43:06.013
    Jul 29 16:43:06.013: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename emptydir-wrapper 07/29/23 16:43:06.018
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:43:06.055
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:43:06.059
    [It] should not cause race condition when used for configmaps [Serial] [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:189
    STEP: Creating 50 configmaps 07/29/23 16:43:06.067
    STEP: Creating RC which spawns configmap-volume pods 07/29/23 16:43:06.502
    Jul 29 16:43:06.535: INFO: Pod name wrapped-volume-race-469eb3f5-73b6-4e73-9e47-24a923f88c77: Found 0 pods out of 5
    Jul 29 16:43:11.557: INFO: Pod name wrapped-volume-race-469eb3f5-73b6-4e73-9e47-24a923f88c77: Found 5 pods out of 5
    STEP: Ensuring each pod is running 07/29/23 16:43:11.557
    Jul 29 16:43:11.557: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-469eb3f5-73b6-4e73-9e47-24a923f88c77-85427" in namespace "emptydir-wrapper-8504" to be "running"
    Jul 29 16:43:11.565: INFO: Pod "wrapped-volume-race-469eb3f5-73b6-4e73-9e47-24a923f88c77-85427": Phase="Pending", Reason="", readiness=false. Elapsed: 7.458546ms
    Jul 29 16:43:13.583: INFO: Pod "wrapped-volume-race-469eb3f5-73b6-4e73-9e47-24a923f88c77-85427": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025569932s
    Jul 29 16:43:15.580: INFO: Pod "wrapped-volume-race-469eb3f5-73b6-4e73-9e47-24a923f88c77-85427": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023102769s
    Jul 29 16:43:17.574: INFO: Pod "wrapped-volume-race-469eb3f5-73b6-4e73-9e47-24a923f88c77-85427": Phase="Pending", Reason="", readiness=false. Elapsed: 6.017029591s
    Jul 29 16:43:19.579: INFO: Pod "wrapped-volume-race-469eb3f5-73b6-4e73-9e47-24a923f88c77-85427": Phase="Pending", Reason="", readiness=false. Elapsed: 8.021496285s
    Jul 29 16:43:21.575: INFO: Pod "wrapped-volume-race-469eb3f5-73b6-4e73-9e47-24a923f88c77-85427": Phase="Pending", Reason="", readiness=false. Elapsed: 10.018168144s
    Jul 29 16:43:23.574: INFO: Pod "wrapped-volume-race-469eb3f5-73b6-4e73-9e47-24a923f88c77-85427": Phase="Running", Reason="", readiness=true. Elapsed: 12.016419406s
    Jul 29 16:43:23.574: INFO: Pod "wrapped-volume-race-469eb3f5-73b6-4e73-9e47-24a923f88c77-85427" satisfied condition "running"
    Jul 29 16:43:23.574: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-469eb3f5-73b6-4e73-9e47-24a923f88c77-c5jpd" in namespace "emptydir-wrapper-8504" to be "running"
    Jul 29 16:43:23.583: INFO: Pod "wrapped-volume-race-469eb3f5-73b6-4e73-9e47-24a923f88c77-c5jpd": Phase="Running", Reason="", readiness=true. Elapsed: 8.800558ms
    Jul 29 16:43:23.583: INFO: Pod "wrapped-volume-race-469eb3f5-73b6-4e73-9e47-24a923f88c77-c5jpd" satisfied condition "running"
    Jul 29 16:43:23.583: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-469eb3f5-73b6-4e73-9e47-24a923f88c77-hk29b" in namespace "emptydir-wrapper-8504" to be "running"
    Jul 29 16:43:23.594: INFO: Pod "wrapped-volume-race-469eb3f5-73b6-4e73-9e47-24a923f88c77-hk29b": Phase="Running", Reason="", readiness=true. Elapsed: 10.489397ms
    Jul 29 16:43:23.594: INFO: Pod "wrapped-volume-race-469eb3f5-73b6-4e73-9e47-24a923f88c77-hk29b" satisfied condition "running"
    Jul 29 16:43:23.594: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-469eb3f5-73b6-4e73-9e47-24a923f88c77-qc28k" in namespace "emptydir-wrapper-8504" to be "running"
    Jul 29 16:43:23.608: INFO: Pod "wrapped-volume-race-469eb3f5-73b6-4e73-9e47-24a923f88c77-qc28k": Phase="Running", Reason="", readiness=true. Elapsed: 14.25432ms
    Jul 29 16:43:23.608: INFO: Pod "wrapped-volume-race-469eb3f5-73b6-4e73-9e47-24a923f88c77-qc28k" satisfied condition "running"
    Jul 29 16:43:23.608: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-469eb3f5-73b6-4e73-9e47-24a923f88c77-xj7ks" in namespace "emptydir-wrapper-8504" to be "running"
    Jul 29 16:43:23.629: INFO: Pod "wrapped-volume-race-469eb3f5-73b6-4e73-9e47-24a923f88c77-xj7ks": Phase="Running", Reason="", readiness=true. Elapsed: 20.606461ms
    Jul 29 16:43:23.629: INFO: Pod "wrapped-volume-race-469eb3f5-73b6-4e73-9e47-24a923f88c77-xj7ks" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-469eb3f5-73b6-4e73-9e47-24a923f88c77 in namespace emptydir-wrapper-8504, will wait for the garbage collector to delete the pods 07/29/23 16:43:23.629
    Jul 29 16:43:23.706: INFO: Deleting ReplicationController wrapped-volume-race-469eb3f5-73b6-4e73-9e47-24a923f88c77 took: 14.798358ms
    Jul 29 16:43:23.807: INFO: Terminating ReplicationController wrapped-volume-race-469eb3f5-73b6-4e73-9e47-24a923f88c77 pods took: 100.975113ms
    STEP: Creating RC which spawns configmap-volume pods 07/29/23 16:43:25.919
    Jul 29 16:43:25.953: INFO: Pod name wrapped-volume-race-6fe40fcc-1ded-4693-90a3-b7ceb4f4775d: Found 0 pods out of 5
    Jul 29 16:43:30.971: INFO: Pod name wrapped-volume-race-6fe40fcc-1ded-4693-90a3-b7ceb4f4775d: Found 5 pods out of 5
    STEP: Ensuring each pod is running 07/29/23 16:43:30.971
    Jul 29 16:43:30.972: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-6fe40fcc-1ded-4693-90a3-b7ceb4f4775d-d8tmm" in namespace "emptydir-wrapper-8504" to be "running"
    Jul 29 16:43:30.982: INFO: Pod "wrapped-volume-race-6fe40fcc-1ded-4693-90a3-b7ceb4f4775d-d8tmm": Phase="Pending", Reason="", readiness=false. Elapsed: 10.447201ms
    Jul 29 16:43:32.996: INFO: Pod "wrapped-volume-race-6fe40fcc-1ded-4693-90a3-b7ceb4f4775d-d8tmm": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024735773s
    Jul 29 16:43:34.994: INFO: Pod "wrapped-volume-race-6fe40fcc-1ded-4693-90a3-b7ceb4f4775d-d8tmm": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022709981s
    Jul 29 16:43:36.993: INFO: Pod "wrapped-volume-race-6fe40fcc-1ded-4693-90a3-b7ceb4f4775d-d8tmm": Phase="Pending", Reason="", readiness=false. Elapsed: 6.021720339s
    Jul 29 16:43:39.007: INFO: Pod "wrapped-volume-race-6fe40fcc-1ded-4693-90a3-b7ceb4f4775d-d8tmm": Phase="Pending", Reason="", readiness=false. Elapsed: 8.035752226s
    Jul 29 16:43:41.000: INFO: Pod "wrapped-volume-race-6fe40fcc-1ded-4693-90a3-b7ceb4f4775d-d8tmm": Phase="Pending", Reason="", readiness=false. Elapsed: 10.028494152s
    Jul 29 16:43:42.993: INFO: Pod "wrapped-volume-race-6fe40fcc-1ded-4693-90a3-b7ceb4f4775d-d8tmm": Phase="Running", Reason="", readiness=true. Elapsed: 12.021440371s
    Jul 29 16:43:42.993: INFO: Pod "wrapped-volume-race-6fe40fcc-1ded-4693-90a3-b7ceb4f4775d-d8tmm" satisfied condition "running"
    Jul 29 16:43:42.993: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-6fe40fcc-1ded-4693-90a3-b7ceb4f4775d-k8skn" in namespace "emptydir-wrapper-8504" to be "running"
    Jul 29 16:43:43.003: INFO: Pod "wrapped-volume-race-6fe40fcc-1ded-4693-90a3-b7ceb4f4775d-k8skn": Phase="Running", Reason="", readiness=true. Elapsed: 9.298584ms
    Jul 29 16:43:43.003: INFO: Pod "wrapped-volume-race-6fe40fcc-1ded-4693-90a3-b7ceb4f4775d-k8skn" satisfied condition "running"
    Jul 29 16:43:43.003: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-6fe40fcc-1ded-4693-90a3-b7ceb4f4775d-ng5z9" in namespace "emptydir-wrapper-8504" to be "running"
    Jul 29 16:43:43.014: INFO: Pod "wrapped-volume-race-6fe40fcc-1ded-4693-90a3-b7ceb4f4775d-ng5z9": Phase="Running", Reason="", readiness=true. Elapsed: 11.452047ms
    Jul 29 16:43:43.014: INFO: Pod "wrapped-volume-race-6fe40fcc-1ded-4693-90a3-b7ceb4f4775d-ng5z9" satisfied condition "running"
    Jul 29 16:43:43.014: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-6fe40fcc-1ded-4693-90a3-b7ceb4f4775d-rptt8" in namespace "emptydir-wrapper-8504" to be "running"
    Jul 29 16:43:43.026: INFO: Pod "wrapped-volume-race-6fe40fcc-1ded-4693-90a3-b7ceb4f4775d-rptt8": Phase="Running", Reason="", readiness=true. Elapsed: 11.682518ms
    Jul 29 16:43:43.026: INFO: Pod "wrapped-volume-race-6fe40fcc-1ded-4693-90a3-b7ceb4f4775d-rptt8" satisfied condition "running"
    Jul 29 16:43:43.026: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-6fe40fcc-1ded-4693-90a3-b7ceb4f4775d-tvqjx" in namespace "emptydir-wrapper-8504" to be "running"
    Jul 29 16:43:43.034: INFO: Pod "wrapped-volume-race-6fe40fcc-1ded-4693-90a3-b7ceb4f4775d-tvqjx": Phase="Running", Reason="", readiness=true. Elapsed: 8.12608ms
    Jul 29 16:43:43.035: INFO: Pod "wrapped-volume-race-6fe40fcc-1ded-4693-90a3-b7ceb4f4775d-tvqjx" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-6fe40fcc-1ded-4693-90a3-b7ceb4f4775d in namespace emptydir-wrapper-8504, will wait for the garbage collector to delete the pods 07/29/23 16:43:43.035
    Jul 29 16:43:43.110: INFO: Deleting ReplicationController wrapped-volume-race-6fe40fcc-1ded-4693-90a3-b7ceb4f4775d took: 16.611505ms
    Jul 29 16:43:43.310: INFO: Terminating ReplicationController wrapped-volume-race-6fe40fcc-1ded-4693-90a3-b7ceb4f4775d pods took: 200.273365ms
    STEP: Creating RC which spawns configmap-volume pods 07/29/23 16:43:46.52
    Jul 29 16:43:46.554: INFO: Pod name wrapped-volume-race-158d57a7-b5a6-4384-a048-cab44e1961f4: Found 0 pods out of 5
    Jul 29 16:43:51.571: INFO: Pod name wrapped-volume-race-158d57a7-b5a6-4384-a048-cab44e1961f4: Found 5 pods out of 5
    STEP: Ensuring each pod is running 07/29/23 16:43:51.571
    Jul 29 16:43:51.572: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-158d57a7-b5a6-4384-a048-cab44e1961f4-45tfb" in namespace "emptydir-wrapper-8504" to be "running"
    Jul 29 16:43:51.580: INFO: Pod "wrapped-volume-race-158d57a7-b5a6-4384-a048-cab44e1961f4-45tfb": Phase="Pending", Reason="", readiness=false. Elapsed: 7.882008ms
    Jul 29 16:43:53.590: INFO: Pod "wrapped-volume-race-158d57a7-b5a6-4384-a048-cab44e1961f4-45tfb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018318006s
    Jul 29 16:43:55.588: INFO: Pod "wrapped-volume-race-158d57a7-b5a6-4384-a048-cab44e1961f4-45tfb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016590081s
    Jul 29 16:43:57.593: INFO: Pod "wrapped-volume-race-158d57a7-b5a6-4384-a048-cab44e1961f4-45tfb": Phase="Pending", Reason="", readiness=false. Elapsed: 6.021495861s
    Jul 29 16:43:59.589: INFO: Pod "wrapped-volume-race-158d57a7-b5a6-4384-a048-cab44e1961f4-45tfb": Phase="Pending", Reason="", readiness=false. Elapsed: 8.016984945s
    Jul 29 16:44:01.592: INFO: Pod "wrapped-volume-race-158d57a7-b5a6-4384-a048-cab44e1961f4-45tfb": Phase="Pending", Reason="", readiness=false. Elapsed: 10.020192865s
    Jul 29 16:44:03.589: INFO: Pod "wrapped-volume-race-158d57a7-b5a6-4384-a048-cab44e1961f4-45tfb": Phase="Running", Reason="", readiness=true. Elapsed: 12.016923113s
    Jul 29 16:44:03.589: INFO: Pod "wrapped-volume-race-158d57a7-b5a6-4384-a048-cab44e1961f4-45tfb" satisfied condition "running"
    Jul 29 16:44:03.589: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-158d57a7-b5a6-4384-a048-cab44e1961f4-55hdt" in namespace "emptydir-wrapper-8504" to be "running"
    Jul 29 16:44:03.596: INFO: Pod "wrapped-volume-race-158d57a7-b5a6-4384-a048-cab44e1961f4-55hdt": Phase="Running", Reason="", readiness=true. Elapsed: 7.127234ms
    Jul 29 16:44:03.596: INFO: Pod "wrapped-volume-race-158d57a7-b5a6-4384-a048-cab44e1961f4-55hdt" satisfied condition "running"
    Jul 29 16:44:03.596: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-158d57a7-b5a6-4384-a048-cab44e1961f4-64z7k" in namespace "emptydir-wrapper-8504" to be "running"
    Jul 29 16:44:03.604: INFO: Pod "wrapped-volume-race-158d57a7-b5a6-4384-a048-cab44e1961f4-64z7k": Phase="Running", Reason="", readiness=true. Elapsed: 8.2062ms
    Jul 29 16:44:03.604: INFO: Pod "wrapped-volume-race-158d57a7-b5a6-4384-a048-cab44e1961f4-64z7k" satisfied condition "running"
    Jul 29 16:44:03.604: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-158d57a7-b5a6-4384-a048-cab44e1961f4-6lv2t" in namespace "emptydir-wrapper-8504" to be "running"
    Jul 29 16:44:03.615: INFO: Pod "wrapped-volume-race-158d57a7-b5a6-4384-a048-cab44e1961f4-6lv2t": Phase="Running", Reason="", readiness=true. Elapsed: 10.860978ms
    Jul 29 16:44:03.616: INFO: Pod "wrapped-volume-race-158d57a7-b5a6-4384-a048-cab44e1961f4-6lv2t" satisfied condition "running"
    Jul 29 16:44:03.616: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-158d57a7-b5a6-4384-a048-cab44e1961f4-s65br" in namespace "emptydir-wrapper-8504" to be "running"
    Jul 29 16:44:03.624: INFO: Pod "wrapped-volume-race-158d57a7-b5a6-4384-a048-cab44e1961f4-s65br": Phase="Running", Reason="", readiness=true. Elapsed: 8.129987ms
    Jul 29 16:44:03.624: INFO: Pod "wrapped-volume-race-158d57a7-b5a6-4384-a048-cab44e1961f4-s65br" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-158d57a7-b5a6-4384-a048-cab44e1961f4 in namespace emptydir-wrapper-8504, will wait for the garbage collector to delete the pods 07/29/23 16:44:03.624
    Jul 29 16:44:03.699: INFO: Deleting ReplicationController wrapped-volume-race-158d57a7-b5a6-4384-a048-cab44e1961f4 took: 14.141956ms
    Jul 29 16:44:03.900: INFO: Terminating ReplicationController wrapped-volume-race-158d57a7-b5a6-4384-a048-cab44e1961f4 pods took: 201.306222ms
    STEP: Cleaning up the configMaps 07/29/23 16:44:06.8
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:187
    Jul 29 16:44:07.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-wrapper-8504" for this suite. 07/29/23 16:44:07.382
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:44:07.416
Jul 29 16:44:07.416: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename projected 07/29/23 16:44:07.418
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:44:07.452
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:44:07.458
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234
STEP: Creating a pod to test downward API volume plugin 07/29/23 16:44:07.464
Jul 29 16:44:07.480: INFO: Waiting up to 5m0s for pod "downwardapi-volume-75d4d7b7-ddea-4987-a599-009cb988f5f0" in namespace "projected-3050" to be "Succeeded or Failed"
Jul 29 16:44:07.484: INFO: Pod "downwardapi-volume-75d4d7b7-ddea-4987-a599-009cb988f5f0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.595623ms
Jul 29 16:44:09.493: INFO: Pod "downwardapi-volume-75d4d7b7-ddea-4987-a599-009cb988f5f0": Phase="Running", Reason="", readiness=true. Elapsed: 2.01353339s
Jul 29 16:44:11.493: INFO: Pod "downwardapi-volume-75d4d7b7-ddea-4987-a599-009cb988f5f0": Phase="Running", Reason="", readiness=false. Elapsed: 4.01350054s
Jul 29 16:44:13.493: INFO: Pod "downwardapi-volume-75d4d7b7-ddea-4987-a599-009cb988f5f0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013729801s
STEP: Saw pod success 07/29/23 16:44:13.494
Jul 29 16:44:13.495: INFO: Pod "downwardapi-volume-75d4d7b7-ddea-4987-a599-009cb988f5f0" satisfied condition "Succeeded or Failed"
Jul 29 16:44:13.504: INFO: Trying to get logs from node wa4quivohpee-3 pod downwardapi-volume-75d4d7b7-ddea-4987-a599-009cb988f5f0 container client-container: <nil>
STEP: delete the pod 07/29/23 16:44:13.523
Jul 29 16:44:13.551: INFO: Waiting for pod downwardapi-volume-75d4d7b7-ddea-4987-a599-009cb988f5f0 to disappear
Jul 29 16:44:13.557: INFO: Pod downwardapi-volume-75d4d7b7-ddea-4987-a599-009cb988f5f0 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jul 29 16:44:13.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3050" for this suite. 07/29/23 16:44:13.565
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","completed":289,"skipped":5252,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.166 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:44:07.416
    Jul 29 16:44:07.416: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename projected 07/29/23 16:44:07.418
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:44:07.452
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:44:07.458
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:234
    STEP: Creating a pod to test downward API volume plugin 07/29/23 16:44:07.464
    Jul 29 16:44:07.480: INFO: Waiting up to 5m0s for pod "downwardapi-volume-75d4d7b7-ddea-4987-a599-009cb988f5f0" in namespace "projected-3050" to be "Succeeded or Failed"
    Jul 29 16:44:07.484: INFO: Pod "downwardapi-volume-75d4d7b7-ddea-4987-a599-009cb988f5f0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.595623ms
    Jul 29 16:44:09.493: INFO: Pod "downwardapi-volume-75d4d7b7-ddea-4987-a599-009cb988f5f0": Phase="Running", Reason="", readiness=true. Elapsed: 2.01353339s
    Jul 29 16:44:11.493: INFO: Pod "downwardapi-volume-75d4d7b7-ddea-4987-a599-009cb988f5f0": Phase="Running", Reason="", readiness=false. Elapsed: 4.01350054s
    Jul 29 16:44:13.493: INFO: Pod "downwardapi-volume-75d4d7b7-ddea-4987-a599-009cb988f5f0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013729801s
    STEP: Saw pod success 07/29/23 16:44:13.494
    Jul 29 16:44:13.495: INFO: Pod "downwardapi-volume-75d4d7b7-ddea-4987-a599-009cb988f5f0" satisfied condition "Succeeded or Failed"
    Jul 29 16:44:13.504: INFO: Trying to get logs from node wa4quivohpee-3 pod downwardapi-volume-75d4d7b7-ddea-4987-a599-009cb988f5f0 container client-container: <nil>
    STEP: delete the pod 07/29/23 16:44:13.523
    Jul 29 16:44:13.551: INFO: Waiting for pod downwardapi-volume-75d4d7b7-ddea-4987-a599-009cb988f5f0 to disappear
    Jul 29 16:44:13.557: INFO: Pod downwardapi-volume-75d4d7b7-ddea-4987-a599-009cb988f5f0 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jul 29 16:44:13.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3050" for this suite. 07/29/23 16:44:13.565
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:44:13.586
Jul 29 16:44:13.588: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename sched-pred 07/29/23 16:44:13.59
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:44:13.632
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:44:13.637
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Jul 29 16:44:13.643: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jul 29 16:44:13.660: INFO: Waiting for terminating namespaces to be deleted...
Jul 29 16:44:13.669: INFO: 
Logging pods the apiserver thinks is on node wa4quivohpee-1 before test
Jul 29 16:44:13.690: INFO: cilium-gfkpl from kube-system started at 2023-07-29 15:23:18 +0000 UTC (1 container statuses recorded)
Jul 29 16:44:13.690: INFO: 	Container cilium-agent ready: true, restart count 0
Jul 29 16:44:13.690: INFO: cilium-node-init-n8876 from kube-system started at 2023-07-29 15:23:18 +0000 UTC (1 container statuses recorded)
Jul 29 16:44:13.690: INFO: 	Container node-init ready: true, restart count 0
Jul 29 16:44:13.691: INFO: coredns-565d847f94-xm9ff from kube-system started at 2023-07-29 15:34:18 +0000 UTC (1 container statuses recorded)
Jul 29 16:44:13.691: INFO: 	Container coredns ready: true, restart count 0
Jul 29 16:44:13.691: INFO: kube-addon-manager-wa4quivohpee-1 from kube-system started at 2023-07-29 15:22:59 +0000 UTC (1 container statuses recorded)
Jul 29 16:44:13.691: INFO: 	Container kube-addon-manager ready: true, restart count 0
Jul 29 16:44:13.691: INFO: kube-apiserver-wa4quivohpee-1 from kube-system started at 2023-07-29 15:14:24 +0000 UTC (1 container statuses recorded)
Jul 29 16:44:13.691: INFO: 	Container kube-apiserver ready: true, restart count 0
Jul 29 16:44:13.691: INFO: kube-controller-manager-wa4quivohpee-1 from kube-system started at 2023-07-29 15:14:24 +0000 UTC (1 container statuses recorded)
Jul 29 16:44:13.691: INFO: 	Container kube-controller-manager ready: true, restart count 0
Jul 29 16:44:13.691: INFO: kube-proxy-gdffl from kube-system started at 2023-07-29 15:14:37 +0000 UTC (1 container statuses recorded)
Jul 29 16:44:13.691: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 29 16:44:13.692: INFO: kube-scheduler-wa4quivohpee-1 from kube-system started at 2023-07-29 15:14:23 +0000 UTC (1 container statuses recorded)
Jul 29 16:44:13.692: INFO: 	Container kube-scheduler ready: true, restart count 0
Jul 29 16:44:13.692: INFO: sonobuoy-systemd-logs-daemon-set-b6f0dae2c3174900-mtck5 from sonobuoy started at 2023-07-29 15:24:46 +0000 UTC (2 container statuses recorded)
Jul 29 16:44:13.692: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 29 16:44:13.692: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 29 16:44:13.692: INFO: 
Logging pods the apiserver thinks is on node wa4quivohpee-2 before test
Jul 29 16:44:13.713: INFO: cilium-node-init-946z9 from kube-system started at 2023-07-29 15:23:18 +0000 UTC (1 container statuses recorded)
Jul 29 16:44:13.714: INFO: 	Container node-init ready: true, restart count 0
Jul 29 16:44:13.714: INFO: cilium-rmjkx from kube-system started at 2023-07-29 15:23:18 +0000 UTC (1 container statuses recorded)
Jul 29 16:44:13.714: INFO: 	Container cilium-agent ready: true, restart count 0
Jul 29 16:44:13.714: INFO: coredns-565d847f94-wmg75 from kube-system started at 2023-07-29 15:24:27 +0000 UTC (1 container statuses recorded)
Jul 29 16:44:13.714: INFO: 	Container coredns ready: true, restart count 0
Jul 29 16:44:13.714: INFO: kube-addon-manager-wa4quivohpee-2 from kube-system started at 2023-07-29 15:22:59 +0000 UTC (1 container statuses recorded)
Jul 29 16:44:13.715: INFO: 	Container kube-addon-manager ready: true, restart count 0
Jul 29 16:44:13.715: INFO: kube-apiserver-wa4quivohpee-2 from kube-system started at 2023-07-29 15:15:22 +0000 UTC (1 container statuses recorded)
Jul 29 16:44:13.715: INFO: 	Container kube-apiserver ready: true, restart count 0
Jul 29 16:44:13.715: INFO: kube-controller-manager-wa4quivohpee-2 from kube-system started at 2023-07-29 15:15:22 +0000 UTC (1 container statuses recorded)
Jul 29 16:44:13.715: INFO: 	Container kube-controller-manager ready: true, restart count 0
Jul 29 16:44:13.715: INFO: kube-proxy-74hn2 from kube-system started at 2023-07-29 15:15:19 +0000 UTC (1 container statuses recorded)
Jul 29 16:44:13.715: INFO: 	Container kube-proxy ready: true, restart count 1
Jul 29 16:44:13.716: INFO: kube-scheduler-wa4quivohpee-2 from kube-system started at 2023-07-29 15:15:22 +0000 UTC (1 container statuses recorded)
Jul 29 16:44:13.716: INFO: 	Container kube-scheduler ready: true, restart count 0
Jul 29 16:44:13.716: INFO: sonobuoy-systemd-logs-daemon-set-b6f0dae2c3174900-cnfp2 from sonobuoy started at 2023-07-29 15:24:46 +0000 UTC (2 container statuses recorded)
Jul 29 16:44:13.716: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 29 16:44:13.716: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 29 16:44:13.716: INFO: 
Logging pods the apiserver thinks is on node wa4quivohpee-3 before test
Jul 29 16:44:13.740: INFO: cilium-node-init-j4pkd from kube-system started at 2023-07-29 15:23:18 +0000 UTC (1 container statuses recorded)
Jul 29 16:44:13.740: INFO: 	Container node-init ready: true, restart count 0
Jul 29 16:44:13.740: INFO: cilium-operator-69cffcb958-48gkl from kube-system started at 2023-07-29 15:23:19 +0000 UTC (1 container statuses recorded)
Jul 29 16:44:13.740: INFO: 	Container cilium-operator ready: true, restart count 0
Jul 29 16:44:13.740: INFO: cilium-zl28x from kube-system started at 2023-07-29 15:23:18 +0000 UTC (1 container statuses recorded)
Jul 29 16:44:13.741: INFO: 	Container cilium-agent ready: true, restart count 0
Jul 29 16:44:13.741: INFO: kube-proxy-ts2pg from kube-system started at 2023-07-29 15:16:17 +0000 UTC (1 container statuses recorded)
Jul 29 16:44:13.741: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 29 16:44:13.741: INFO: sonobuoy from sonobuoy started at 2023-07-29 15:24:31 +0000 UTC (1 container statuses recorded)
Jul 29 16:44:13.741: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jul 29 16:44:13.741: INFO: sonobuoy-e2e-job-5c9a7da3e6b74e15 from sonobuoy started at 2023-07-29 15:24:45 +0000 UTC (2 container statuses recorded)
Jul 29 16:44:13.741: INFO: 	Container e2e ready: true, restart count 0
Jul 29 16:44:13.741: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 29 16:44:13.741: INFO: sonobuoy-systemd-logs-daemon-set-b6f0dae2c3174900-cqnfx from sonobuoy started at 2023-07-29 15:24:46 +0000 UTC (2 container statuses recorded)
Jul 29 16:44:13.741: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 29 16:44:13.742: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461
STEP: Trying to launch a pod without a label to get a node which can launch it. 07/29/23 16:44:13.742
Jul 29 16:44:13.763: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-2827" to be "running"
Jul 29 16:44:13.782: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 19.164373ms
Jul 29 16:44:15.790: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.026954177s
Jul 29 16:44:15.790: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 07/29/23 16:44:15.796
STEP: Trying to apply a random label on the found node. 07/29/23 16:44:15.827
STEP: verifying the node has the label kubernetes.io/e2e-d0981fc6-0641-46c3-8ade-fc1b92da3f29 42 07/29/23 16:44:15.854
STEP: Trying to relaunch the pod, now with labels. 07/29/23 16:44:15.865
Jul 29 16:44:15.879: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-2827" to be "not pending"
Jul 29 16:44:15.886: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 6.603306ms
Jul 29 16:44:17.897: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.018242s
Jul 29 16:44:17.898: INFO: Pod "with-labels" satisfied condition "not pending"
STEP: removing the label kubernetes.io/e2e-d0981fc6-0641-46c3-8ade-fc1b92da3f29 off the node wa4quivohpee-3 07/29/23 16:44:17.905
STEP: verifying the node doesn't have the label kubernetes.io/e2e-d0981fc6-0641-46c3-8ade-fc1b92da3f29 07/29/23 16:44:17.929
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Jul 29 16:44:17.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2827" for this suite. 07/29/23 16:44:17.954
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","completed":290,"skipped":5275,"failed":0}
------------------------------
â€¢ [4.382 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:44:13.586
    Jul 29 16:44:13.588: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename sched-pred 07/29/23 16:44:13.59
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:44:13.632
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:44:13.637
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Jul 29 16:44:13.643: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jul 29 16:44:13.660: INFO: Waiting for terminating namespaces to be deleted...
    Jul 29 16:44:13.669: INFO: 
    Logging pods the apiserver thinks is on node wa4quivohpee-1 before test
    Jul 29 16:44:13.690: INFO: cilium-gfkpl from kube-system started at 2023-07-29 15:23:18 +0000 UTC (1 container statuses recorded)
    Jul 29 16:44:13.690: INFO: 	Container cilium-agent ready: true, restart count 0
    Jul 29 16:44:13.690: INFO: cilium-node-init-n8876 from kube-system started at 2023-07-29 15:23:18 +0000 UTC (1 container statuses recorded)
    Jul 29 16:44:13.690: INFO: 	Container node-init ready: true, restart count 0
    Jul 29 16:44:13.691: INFO: coredns-565d847f94-xm9ff from kube-system started at 2023-07-29 15:34:18 +0000 UTC (1 container statuses recorded)
    Jul 29 16:44:13.691: INFO: 	Container coredns ready: true, restart count 0
    Jul 29 16:44:13.691: INFO: kube-addon-manager-wa4quivohpee-1 from kube-system started at 2023-07-29 15:22:59 +0000 UTC (1 container statuses recorded)
    Jul 29 16:44:13.691: INFO: 	Container kube-addon-manager ready: true, restart count 0
    Jul 29 16:44:13.691: INFO: kube-apiserver-wa4quivohpee-1 from kube-system started at 2023-07-29 15:14:24 +0000 UTC (1 container statuses recorded)
    Jul 29 16:44:13.691: INFO: 	Container kube-apiserver ready: true, restart count 0
    Jul 29 16:44:13.691: INFO: kube-controller-manager-wa4quivohpee-1 from kube-system started at 2023-07-29 15:14:24 +0000 UTC (1 container statuses recorded)
    Jul 29 16:44:13.691: INFO: 	Container kube-controller-manager ready: true, restart count 0
    Jul 29 16:44:13.691: INFO: kube-proxy-gdffl from kube-system started at 2023-07-29 15:14:37 +0000 UTC (1 container statuses recorded)
    Jul 29 16:44:13.691: INFO: 	Container kube-proxy ready: true, restart count 0
    Jul 29 16:44:13.692: INFO: kube-scheduler-wa4quivohpee-1 from kube-system started at 2023-07-29 15:14:23 +0000 UTC (1 container statuses recorded)
    Jul 29 16:44:13.692: INFO: 	Container kube-scheduler ready: true, restart count 0
    Jul 29 16:44:13.692: INFO: sonobuoy-systemd-logs-daemon-set-b6f0dae2c3174900-mtck5 from sonobuoy started at 2023-07-29 15:24:46 +0000 UTC (2 container statuses recorded)
    Jul 29 16:44:13.692: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jul 29 16:44:13.692: INFO: 	Container systemd-logs ready: true, restart count 0
    Jul 29 16:44:13.692: INFO: 
    Logging pods the apiserver thinks is on node wa4quivohpee-2 before test
    Jul 29 16:44:13.713: INFO: cilium-node-init-946z9 from kube-system started at 2023-07-29 15:23:18 +0000 UTC (1 container statuses recorded)
    Jul 29 16:44:13.714: INFO: 	Container node-init ready: true, restart count 0
    Jul 29 16:44:13.714: INFO: cilium-rmjkx from kube-system started at 2023-07-29 15:23:18 +0000 UTC (1 container statuses recorded)
    Jul 29 16:44:13.714: INFO: 	Container cilium-agent ready: true, restart count 0
    Jul 29 16:44:13.714: INFO: coredns-565d847f94-wmg75 from kube-system started at 2023-07-29 15:24:27 +0000 UTC (1 container statuses recorded)
    Jul 29 16:44:13.714: INFO: 	Container coredns ready: true, restart count 0
    Jul 29 16:44:13.714: INFO: kube-addon-manager-wa4quivohpee-2 from kube-system started at 2023-07-29 15:22:59 +0000 UTC (1 container statuses recorded)
    Jul 29 16:44:13.715: INFO: 	Container kube-addon-manager ready: true, restart count 0
    Jul 29 16:44:13.715: INFO: kube-apiserver-wa4quivohpee-2 from kube-system started at 2023-07-29 15:15:22 +0000 UTC (1 container statuses recorded)
    Jul 29 16:44:13.715: INFO: 	Container kube-apiserver ready: true, restart count 0
    Jul 29 16:44:13.715: INFO: kube-controller-manager-wa4quivohpee-2 from kube-system started at 2023-07-29 15:15:22 +0000 UTC (1 container statuses recorded)
    Jul 29 16:44:13.715: INFO: 	Container kube-controller-manager ready: true, restart count 0
    Jul 29 16:44:13.715: INFO: kube-proxy-74hn2 from kube-system started at 2023-07-29 15:15:19 +0000 UTC (1 container statuses recorded)
    Jul 29 16:44:13.715: INFO: 	Container kube-proxy ready: true, restart count 1
    Jul 29 16:44:13.716: INFO: kube-scheduler-wa4quivohpee-2 from kube-system started at 2023-07-29 15:15:22 +0000 UTC (1 container statuses recorded)
    Jul 29 16:44:13.716: INFO: 	Container kube-scheduler ready: true, restart count 0
    Jul 29 16:44:13.716: INFO: sonobuoy-systemd-logs-daemon-set-b6f0dae2c3174900-cnfp2 from sonobuoy started at 2023-07-29 15:24:46 +0000 UTC (2 container statuses recorded)
    Jul 29 16:44:13.716: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jul 29 16:44:13.716: INFO: 	Container systemd-logs ready: true, restart count 0
    Jul 29 16:44:13.716: INFO: 
    Logging pods the apiserver thinks is on node wa4quivohpee-3 before test
    Jul 29 16:44:13.740: INFO: cilium-node-init-j4pkd from kube-system started at 2023-07-29 15:23:18 +0000 UTC (1 container statuses recorded)
    Jul 29 16:44:13.740: INFO: 	Container node-init ready: true, restart count 0
    Jul 29 16:44:13.740: INFO: cilium-operator-69cffcb958-48gkl from kube-system started at 2023-07-29 15:23:19 +0000 UTC (1 container statuses recorded)
    Jul 29 16:44:13.740: INFO: 	Container cilium-operator ready: true, restart count 0
    Jul 29 16:44:13.740: INFO: cilium-zl28x from kube-system started at 2023-07-29 15:23:18 +0000 UTC (1 container statuses recorded)
    Jul 29 16:44:13.741: INFO: 	Container cilium-agent ready: true, restart count 0
    Jul 29 16:44:13.741: INFO: kube-proxy-ts2pg from kube-system started at 2023-07-29 15:16:17 +0000 UTC (1 container statuses recorded)
    Jul 29 16:44:13.741: INFO: 	Container kube-proxy ready: true, restart count 0
    Jul 29 16:44:13.741: INFO: sonobuoy from sonobuoy started at 2023-07-29 15:24:31 +0000 UTC (1 container statuses recorded)
    Jul 29 16:44:13.741: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jul 29 16:44:13.741: INFO: sonobuoy-e2e-job-5c9a7da3e6b74e15 from sonobuoy started at 2023-07-29 15:24:45 +0000 UTC (2 container statuses recorded)
    Jul 29 16:44:13.741: INFO: 	Container e2e ready: true, restart count 0
    Jul 29 16:44:13.741: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jul 29 16:44:13.741: INFO: sonobuoy-systemd-logs-daemon-set-b6f0dae2c3174900-cqnfx from sonobuoy started at 2023-07-29 15:24:46 +0000 UTC (2 container statuses recorded)
    Jul 29 16:44:13.741: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jul 29 16:44:13.742: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if matching  [Conformance]
      test/e2e/scheduling/predicates.go:461
    STEP: Trying to launch a pod without a label to get a node which can launch it. 07/29/23 16:44:13.742
    Jul 29 16:44:13.763: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-2827" to be "running"
    Jul 29 16:44:13.782: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 19.164373ms
    Jul 29 16:44:15.790: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.026954177s
    Jul 29 16:44:15.790: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 07/29/23 16:44:15.796
    STEP: Trying to apply a random label on the found node. 07/29/23 16:44:15.827
    STEP: verifying the node has the label kubernetes.io/e2e-d0981fc6-0641-46c3-8ade-fc1b92da3f29 42 07/29/23 16:44:15.854
    STEP: Trying to relaunch the pod, now with labels. 07/29/23 16:44:15.865
    Jul 29 16:44:15.879: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-2827" to be "not pending"
    Jul 29 16:44:15.886: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 6.603306ms
    Jul 29 16:44:17.897: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.018242s
    Jul 29 16:44:17.898: INFO: Pod "with-labels" satisfied condition "not pending"
    STEP: removing the label kubernetes.io/e2e-d0981fc6-0641-46c3-8ade-fc1b92da3f29 off the node wa4quivohpee-3 07/29/23 16:44:17.905
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-d0981fc6-0641-46c3-8ade-fc1b92da3f29 07/29/23 16:44:17.929
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Jul 29 16:44:17.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-2827" for this suite. 07/29/23 16:44:17.954
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] Services
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3620
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:44:17.976
Jul 29 16:44:17.976: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename services 07/29/23 16:44:17.979
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:44:18.01
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:44:18.017
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should delete a collection of services [Conformance]
  test/e2e/network/service.go:3620
STEP: creating a collection of services 07/29/23 16:44:18.027
Jul 29 16:44:18.027: INFO: Creating e2e-svc-a-pwv58
Jul 29 16:44:18.047: INFO: Creating e2e-svc-b-9dfdw
Jul 29 16:44:18.076: INFO: Creating e2e-svc-c-jbh96
STEP: deleting service collection 07/29/23 16:44:18.103
Jul 29 16:44:18.159: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jul 29 16:44:18.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8414" for this suite. 07/29/23 16:44:18.166
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should delete a collection of services [Conformance]","completed":291,"skipped":5280,"failed":0}
------------------------------
â€¢ [0.199 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3620

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:44:17.976
    Jul 29 16:44:17.976: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename services 07/29/23 16:44:17.979
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:44:18.01
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:44:18.017
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should delete a collection of services [Conformance]
      test/e2e/network/service.go:3620
    STEP: creating a collection of services 07/29/23 16:44:18.027
    Jul 29 16:44:18.027: INFO: Creating e2e-svc-a-pwv58
    Jul 29 16:44:18.047: INFO: Creating e2e-svc-b-9dfdw
    Jul 29 16:44:18.076: INFO: Creating e2e-svc-c-jbh96
    STEP: deleting service collection 07/29/23 16:44:18.103
    Jul 29 16:44:18.159: INFO: Collection of services has been deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jul 29 16:44:18.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-8414" for this suite. 07/29/23 16:44:18.166
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:44:18.178
Jul 29 16:44:18.178: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename dns 07/29/23 16:44:18.182
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:44:18.211
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:44:18.218
[It] should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-5351.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-5351.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
 07/29/23 16:44:18.224
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-5351.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-5351.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
 07/29/23 16:44:18.225
STEP: creating a pod to probe /etc/hosts 07/29/23 16:44:18.225
STEP: submitting the pod to kubernetes 07/29/23 16:44:18.225
Jul 29 16:44:18.248: INFO: Waiting up to 15m0s for pod "dns-test-07cb8a37-22fe-4479-9c64-73a35c07f8dc" in namespace "dns-5351" to be "running"
Jul 29 16:44:18.259: INFO: Pod "dns-test-07cb8a37-22fe-4479-9c64-73a35c07f8dc": Phase="Pending", Reason="", readiness=false. Elapsed: 11.016659ms
Jul 29 16:44:20.268: INFO: Pod "dns-test-07cb8a37-22fe-4479-9c64-73a35c07f8dc": Phase="Running", Reason="", readiness=true. Elapsed: 2.019553636s
Jul 29 16:44:20.268: INFO: Pod "dns-test-07cb8a37-22fe-4479-9c64-73a35c07f8dc" satisfied condition "running"
STEP: retrieving the pod 07/29/23 16:44:20.268
STEP: looking for the results for each expected name from probers 07/29/23 16:44:20.275
Jul 29 16:44:20.301: INFO: DNS probes using dns-5351/dns-test-07cb8a37-22fe-4479-9c64-73a35c07f8dc succeeded

STEP: deleting the pod 07/29/23 16:44:20.301
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jul 29 16:44:20.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5351" for this suite. 07/29/23 16:44:20.345
{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [Conformance]","completed":292,"skipped":5297,"failed":0}
------------------------------
â€¢ [2.187 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:44:18.178
    Jul 29 16:44:18.178: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename dns 07/29/23 16:44:18.182
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:44:18.211
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:44:18.218
    [It] should provide /etc/hosts entries for the cluster [Conformance]
      test/e2e/network/dns.go:117
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-5351.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-5351.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
     07/29/23 16:44:18.224
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-5351.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-5351.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
     07/29/23 16:44:18.225
    STEP: creating a pod to probe /etc/hosts 07/29/23 16:44:18.225
    STEP: submitting the pod to kubernetes 07/29/23 16:44:18.225
    Jul 29 16:44:18.248: INFO: Waiting up to 15m0s for pod "dns-test-07cb8a37-22fe-4479-9c64-73a35c07f8dc" in namespace "dns-5351" to be "running"
    Jul 29 16:44:18.259: INFO: Pod "dns-test-07cb8a37-22fe-4479-9c64-73a35c07f8dc": Phase="Pending", Reason="", readiness=false. Elapsed: 11.016659ms
    Jul 29 16:44:20.268: INFO: Pod "dns-test-07cb8a37-22fe-4479-9c64-73a35c07f8dc": Phase="Running", Reason="", readiness=true. Elapsed: 2.019553636s
    Jul 29 16:44:20.268: INFO: Pod "dns-test-07cb8a37-22fe-4479-9c64-73a35c07f8dc" satisfied condition "running"
    STEP: retrieving the pod 07/29/23 16:44:20.268
    STEP: looking for the results for each expected name from probers 07/29/23 16:44:20.275
    Jul 29 16:44:20.301: INFO: DNS probes using dns-5351/dns-test-07cb8a37-22fe-4479-9c64-73a35c07f8dc succeeded

    STEP: deleting the pod 07/29/23 16:44:20.301
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jul 29 16:44:20.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-5351" for this suite. 07/29/23 16:44:20.345
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:44:20.38
Jul 29 16:44:20.380: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename job 07/29/23 16:44:20.382
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:44:20.412
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:44:20.417
[It] should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464
STEP: Creating a job 07/29/23 16:44:20.422
STEP: Ensure pods equal to paralellism count is attached to the job 07/29/23 16:44:20.432
STEP: patching /status 07/29/23 16:44:24.449
STEP: updating /status 07/29/23 16:44:24.462
STEP: get /status 07/29/23 16:44:24.519
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Jul 29 16:44:24.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-6731" for this suite. 07/29/23 16:44:24.538
{"msg":"PASSED [sig-apps] Job should apply changes to a job status [Conformance]","completed":293,"skipped":5320,"failed":0}
------------------------------
â€¢ [4.170 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:44:20.38
    Jul 29 16:44:20.380: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename job 07/29/23 16:44:20.382
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:44:20.412
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:44:20.417
    [It] should apply changes to a job status [Conformance]
      test/e2e/apps/job.go:464
    STEP: Creating a job 07/29/23 16:44:20.422
    STEP: Ensure pods equal to paralellism count is attached to the job 07/29/23 16:44:20.432
    STEP: patching /status 07/29/23 16:44:24.449
    STEP: updating /status 07/29/23 16:44:24.462
    STEP: get /status 07/29/23 16:44:24.519
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Jul 29 16:44:24.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-6731" for this suite. 07/29/23 16:44:24.538
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:44:24.553
Jul 29 16:44:24.553: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename pods 07/29/23 16:44:24.555
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:44:24.588
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:44:24.593
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617
Jul 29 16:44:24.598: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: creating the pod 07/29/23 16:44:24.6
STEP: submitting the pod to kubernetes 07/29/23 16:44:24.6
Jul 29 16:44:24.621: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-da7a6082-7615-456b-872e-b01ec7f68d62" in namespace "pods-5600" to be "running and ready"
Jul 29 16:44:24.631: INFO: Pod "pod-logs-websocket-da7a6082-7615-456b-872e-b01ec7f68d62": Phase="Pending", Reason="", readiness=false. Elapsed: 9.670638ms
Jul 29 16:44:24.631: INFO: The phase of Pod pod-logs-websocket-da7a6082-7615-456b-872e-b01ec7f68d62 is Pending, waiting for it to be Running (with Ready = true)
Jul 29 16:44:26.638: INFO: Pod "pod-logs-websocket-da7a6082-7615-456b-872e-b01ec7f68d62": Phase="Running", Reason="", readiness=true. Elapsed: 2.01662742s
Jul 29 16:44:26.638: INFO: The phase of Pod pod-logs-websocket-da7a6082-7615-456b-872e-b01ec7f68d62 is Running (Ready = true)
Jul 29 16:44:26.638: INFO: Pod "pod-logs-websocket-da7a6082-7615-456b-872e-b01ec7f68d62" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jul 29 16:44:26.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5600" for this suite. 07/29/23 16:44:26.707
{"msg":"PASSED [sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","completed":294,"skipped":5343,"failed":0}
------------------------------
â€¢ [2.176 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:44:24.553
    Jul 29 16:44:24.553: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename pods 07/29/23 16:44:24.555
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:44:24.588
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:44:24.593
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:617
    Jul 29 16:44:24.598: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: creating the pod 07/29/23 16:44:24.6
    STEP: submitting the pod to kubernetes 07/29/23 16:44:24.6
    Jul 29 16:44:24.621: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-da7a6082-7615-456b-872e-b01ec7f68d62" in namespace "pods-5600" to be "running and ready"
    Jul 29 16:44:24.631: INFO: Pod "pod-logs-websocket-da7a6082-7615-456b-872e-b01ec7f68d62": Phase="Pending", Reason="", readiness=false. Elapsed: 9.670638ms
    Jul 29 16:44:24.631: INFO: The phase of Pod pod-logs-websocket-da7a6082-7615-456b-872e-b01ec7f68d62 is Pending, waiting for it to be Running (with Ready = true)
    Jul 29 16:44:26.638: INFO: Pod "pod-logs-websocket-da7a6082-7615-456b-872e-b01ec7f68d62": Phase="Running", Reason="", readiness=true. Elapsed: 2.01662742s
    Jul 29 16:44:26.638: INFO: The phase of Pod pod-logs-websocket-da7a6082-7615-456b-872e-b01ec7f68d62 is Running (Ready = true)
    Jul 29 16:44:26.638: INFO: Pod "pod-logs-websocket-da7a6082-7615-456b-872e-b01ec7f68d62" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jul 29 16:44:26.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-5600" for this suite. 07/29/23 16:44:26.707
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:130
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:44:26.743
Jul 29 16:44:26.743: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename container-lifecycle-hook 07/29/23 16:44:26.747
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:44:26.786
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:44:26.792
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 07/29/23 16:44:26.803
Jul 29 16:44:26.823: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-7036" to be "running and ready"
Jul 29 16:44:26.828: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 4.860976ms
Jul 29 16:44:26.829: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jul 29 16:44:28.839: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.01515413s
Jul 29 16:44:28.839: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jul 29 16:44:28.839: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:130
STEP: create the pod with lifecycle hook 07/29/23 16:44:28.844
Jul 29 16:44:28.858: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-7036" to be "running and ready"
Jul 29 16:44:28.866: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 8.241251ms
Jul 29 16:44:28.867: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jul 29 16:44:30.883: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.024589663s
Jul 29 16:44:30.883: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
Jul 29 16:44:30.883: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
STEP: check poststart hook 07/29/23 16:44:30.889
STEP: delete the pod with lifecycle hook 07/29/23 16:44:30.915
Jul 29 16:44:30.930: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul 29 16:44:30.936: INFO: Pod pod-with-poststart-http-hook still exists
Jul 29 16:44:32.936: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul 29 16:44:32.947: INFO: Pod pod-with-poststart-http-hook still exists
Jul 29 16:44:34.937: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul 29 16:44:34.946: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Jul 29 16:44:34.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7036" for this suite. 07/29/23 16:44:34.956
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","completed":295,"skipped":5394,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.227 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:44:26.743
    Jul 29 16:44:26.743: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename container-lifecycle-hook 07/29/23 16:44:26.747
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:44:26.786
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:44:26.792
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 07/29/23 16:44:26.803
    Jul 29 16:44:26.823: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-7036" to be "running and ready"
    Jul 29 16:44:26.828: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 4.860976ms
    Jul 29 16:44:26.829: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jul 29 16:44:28.839: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.01515413s
    Jul 29 16:44:28.839: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jul 29 16:44:28.839: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:130
    STEP: create the pod with lifecycle hook 07/29/23 16:44:28.844
    Jul 29 16:44:28.858: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-7036" to be "running and ready"
    Jul 29 16:44:28.866: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 8.241251ms
    Jul 29 16:44:28.867: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Jul 29 16:44:30.883: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.024589663s
    Jul 29 16:44:30.883: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
    Jul 29 16:44:30.883: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
    STEP: check poststart hook 07/29/23 16:44:30.889
    STEP: delete the pod with lifecycle hook 07/29/23 16:44:30.915
    Jul 29 16:44:30.930: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Jul 29 16:44:30.936: INFO: Pod pod-with-poststart-http-hook still exists
    Jul 29 16:44:32.936: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Jul 29 16:44:32.947: INFO: Pod pod-with-poststart-http-hook still exists
    Jul 29 16:44:34.937: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Jul 29 16:44:34.946: INFO: Pod pod-with-poststart-http-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Jul 29 16:44:34.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-7036" for this suite. 07/29/23 16:44:34.956
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Ingress API
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
[BeforeEach] [sig-network] Ingress API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:44:34.971
Jul 29 16:44:34.972: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename ingress 07/29/23 16:44:34.974
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:44:35.016
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:44:35.022
[It] should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
STEP: getting /apis 07/29/23 16:44:35.027
STEP: getting /apis/networking.k8s.io 07/29/23 16:44:35.031
STEP: getting /apis/networking.k8s.iov1 07/29/23 16:44:35.033
STEP: creating 07/29/23 16:44:35.035
STEP: getting 07/29/23 16:44:35.061
STEP: listing 07/29/23 16:44:35.065
STEP: watching 07/29/23 16:44:35.071
Jul 29 16:44:35.071: INFO: starting watch
STEP: cluster-wide listing 07/29/23 16:44:35.073
STEP: cluster-wide watching 07/29/23 16:44:35.078
Jul 29 16:44:35.079: INFO: starting watch
STEP: patching 07/29/23 16:44:35.081
STEP: updating 07/29/23 16:44:35.09
Jul 29 16:44:35.105: INFO: waiting for watch events with expected annotations
Jul 29 16:44:35.105: INFO: saw patched and updated annotations
STEP: patching /status 07/29/23 16:44:35.105
STEP: updating /status 07/29/23 16:44:35.117
STEP: get /status 07/29/23 16:44:35.134
STEP: deleting 07/29/23 16:44:35.141
STEP: deleting a collection 07/29/23 16:44:35.158
[AfterEach] [sig-network] Ingress API
  test/e2e/framework/framework.go:187
Jul 29 16:44:35.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-7835" for this suite. 07/29/23 16:44:35.19
{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","completed":296,"skipped":5394,"failed":0}
------------------------------
â€¢ [0.229 seconds]
[sig-network] Ingress API
test/e2e/network/common/framework.go:23
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Ingress API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:44:34.971
    Jul 29 16:44:34.972: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename ingress 07/29/23 16:44:34.974
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:44:35.016
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:44:35.022
    [It] should support creating Ingress API operations [Conformance]
      test/e2e/network/ingress.go:552
    STEP: getting /apis 07/29/23 16:44:35.027
    STEP: getting /apis/networking.k8s.io 07/29/23 16:44:35.031
    STEP: getting /apis/networking.k8s.iov1 07/29/23 16:44:35.033
    STEP: creating 07/29/23 16:44:35.035
    STEP: getting 07/29/23 16:44:35.061
    STEP: listing 07/29/23 16:44:35.065
    STEP: watching 07/29/23 16:44:35.071
    Jul 29 16:44:35.071: INFO: starting watch
    STEP: cluster-wide listing 07/29/23 16:44:35.073
    STEP: cluster-wide watching 07/29/23 16:44:35.078
    Jul 29 16:44:35.079: INFO: starting watch
    STEP: patching 07/29/23 16:44:35.081
    STEP: updating 07/29/23 16:44:35.09
    Jul 29 16:44:35.105: INFO: waiting for watch events with expected annotations
    Jul 29 16:44:35.105: INFO: saw patched and updated annotations
    STEP: patching /status 07/29/23 16:44:35.105
    STEP: updating /status 07/29/23 16:44:35.117
    STEP: get /status 07/29/23 16:44:35.134
    STEP: deleting 07/29/23 16:44:35.141
    STEP: deleting a collection 07/29/23 16:44:35.158
    [AfterEach] [sig-network] Ingress API
      test/e2e/framework/framework.go:187
    Jul 29 16:44:35.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ingress-7835" for this suite. 07/29/23 16:44:35.19
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:44:35.202
Jul 29 16:44:35.202: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename crd-publish-openapi 07/29/23 16:44:35.203
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:44:35.281
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:44:35.287
[It] works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68
Jul 29 16:44:35.292: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 07/29/23 16:44:41.452
Jul 29 16:44:41.454: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=crd-publish-openapi-5394 --namespace=crd-publish-openapi-5394 create -f -'
Jul 29 16:44:43.014: INFO: stderr: ""
Jul 29 16:44:43.014: INFO: stdout: "e2e-test-crd-publish-openapi-8313-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jul 29 16:44:43.014: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=crd-publish-openapi-5394 --namespace=crd-publish-openapi-5394 delete e2e-test-crd-publish-openapi-8313-crds test-foo'
Jul 29 16:44:43.165: INFO: stderr: ""
Jul 29 16:44:43.165: INFO: stdout: "e2e-test-crd-publish-openapi-8313-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Jul 29 16:44:43.165: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=crd-publish-openapi-5394 --namespace=crd-publish-openapi-5394 apply -f -'
Jul 29 16:44:44.526: INFO: stderr: ""
Jul 29 16:44:44.526: INFO: stdout: "e2e-test-crd-publish-openapi-8313-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jul 29 16:44:44.526: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=crd-publish-openapi-5394 --namespace=crd-publish-openapi-5394 delete e2e-test-crd-publish-openapi-8313-crds test-foo'
Jul 29 16:44:44.700: INFO: stderr: ""
Jul 29 16:44:44.700: INFO: stdout: "e2e-test-crd-publish-openapi-8313-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 07/29/23 16:44:44.701
Jul 29 16:44:44.701: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=crd-publish-openapi-5394 --namespace=crd-publish-openapi-5394 create -f -'
Jul 29 16:44:45.109: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 07/29/23 16:44:45.11
Jul 29 16:44:45.110: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=crd-publish-openapi-5394 --namespace=crd-publish-openapi-5394 create -f -'
Jul 29 16:44:45.517: INFO: rc: 1
Jul 29 16:44:45.517: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=crd-publish-openapi-5394 --namespace=crd-publish-openapi-5394 apply -f -'
Jul 29 16:44:45.933: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request without required properties 07/29/23 16:44:45.933
Jul 29 16:44:45.933: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=crd-publish-openapi-5394 --namespace=crd-publish-openapi-5394 create -f -'
Jul 29 16:44:46.313: INFO: rc: 1
Jul 29 16:44:46.313: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=crd-publish-openapi-5394 --namespace=crd-publish-openapi-5394 apply -f -'
Jul 29 16:44:46.757: INFO: rc: 1
STEP: kubectl explain works to explain CR properties 07/29/23 16:44:46.757
Jul 29 16:44:46.758: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=crd-publish-openapi-5394 explain e2e-test-crd-publish-openapi-8313-crds'
Jul 29 16:44:47.204: INFO: stderr: ""
Jul 29 16:44:47.204: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8313-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively 07/29/23 16:44:47.205
Jul 29 16:44:47.205: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=crd-publish-openapi-5394 explain e2e-test-crd-publish-openapi-8313-crds.metadata'
Jul 29 16:44:47.636: INFO: stderr: ""
Jul 29 16:44:47.636: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8313-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Jul 29 16:44:47.637: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=crd-publish-openapi-5394 explain e2e-test-crd-publish-openapi-8313-crds.spec'
Jul 29 16:44:48.083: INFO: stderr: ""
Jul 29 16:44:48.083: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8313-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Jul 29 16:44:48.083: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=crd-publish-openapi-5394 explain e2e-test-crd-publish-openapi-8313-crds.spec.bars'
Jul 29 16:44:48.512: INFO: stderr: ""
Jul 29 16:44:48.512: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8313-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist 07/29/23 16:44:48.512
Jul 29 16:44:48.513: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=crd-publish-openapi-5394 explain e2e-test-crd-publish-openapi-8313-crds.spec.bars2'
Jul 29 16:44:48.911: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jul 29 16:44:54.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5394" for this suite. 07/29/23 16:44:54.873
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","completed":297,"skipped":5400,"failed":0}
------------------------------
â€¢ [SLOW TEST] [19.683 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:44:35.202
    Jul 29 16:44:35.202: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename crd-publish-openapi 07/29/23 16:44:35.203
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:44:35.281
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:44:35.287
    [It] works for CRD with validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:68
    Jul 29 16:44:35.292: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 07/29/23 16:44:41.452
    Jul 29 16:44:41.454: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=crd-publish-openapi-5394 --namespace=crd-publish-openapi-5394 create -f -'
    Jul 29 16:44:43.014: INFO: stderr: ""
    Jul 29 16:44:43.014: INFO: stdout: "e2e-test-crd-publish-openapi-8313-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Jul 29 16:44:43.014: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=crd-publish-openapi-5394 --namespace=crd-publish-openapi-5394 delete e2e-test-crd-publish-openapi-8313-crds test-foo'
    Jul 29 16:44:43.165: INFO: stderr: ""
    Jul 29 16:44:43.165: INFO: stdout: "e2e-test-crd-publish-openapi-8313-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    Jul 29 16:44:43.165: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=crd-publish-openapi-5394 --namespace=crd-publish-openapi-5394 apply -f -'
    Jul 29 16:44:44.526: INFO: stderr: ""
    Jul 29 16:44:44.526: INFO: stdout: "e2e-test-crd-publish-openapi-8313-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Jul 29 16:44:44.526: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=crd-publish-openapi-5394 --namespace=crd-publish-openapi-5394 delete e2e-test-crd-publish-openapi-8313-crds test-foo'
    Jul 29 16:44:44.700: INFO: stderr: ""
    Jul 29 16:44:44.700: INFO: stdout: "e2e-test-crd-publish-openapi-8313-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 07/29/23 16:44:44.701
    Jul 29 16:44:44.701: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=crd-publish-openapi-5394 --namespace=crd-publish-openapi-5394 create -f -'
    Jul 29 16:44:45.109: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 07/29/23 16:44:45.11
    Jul 29 16:44:45.110: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=crd-publish-openapi-5394 --namespace=crd-publish-openapi-5394 create -f -'
    Jul 29 16:44:45.517: INFO: rc: 1
    Jul 29 16:44:45.517: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=crd-publish-openapi-5394 --namespace=crd-publish-openapi-5394 apply -f -'
    Jul 29 16:44:45.933: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request without required properties 07/29/23 16:44:45.933
    Jul 29 16:44:45.933: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=crd-publish-openapi-5394 --namespace=crd-publish-openapi-5394 create -f -'
    Jul 29 16:44:46.313: INFO: rc: 1
    Jul 29 16:44:46.313: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=crd-publish-openapi-5394 --namespace=crd-publish-openapi-5394 apply -f -'
    Jul 29 16:44:46.757: INFO: rc: 1
    STEP: kubectl explain works to explain CR properties 07/29/23 16:44:46.757
    Jul 29 16:44:46.758: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=crd-publish-openapi-5394 explain e2e-test-crd-publish-openapi-8313-crds'
    Jul 29 16:44:47.204: INFO: stderr: ""
    Jul 29 16:44:47.204: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8313-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
    STEP: kubectl explain works to explain CR properties recursively 07/29/23 16:44:47.205
    Jul 29 16:44:47.205: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=crd-publish-openapi-5394 explain e2e-test-crd-publish-openapi-8313-crds.metadata'
    Jul 29 16:44:47.636: INFO: stderr: ""
    Jul 29 16:44:47.636: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8313-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
    Jul 29 16:44:47.637: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=crd-publish-openapi-5394 explain e2e-test-crd-publish-openapi-8313-crds.spec'
    Jul 29 16:44:48.083: INFO: stderr: ""
    Jul 29 16:44:48.083: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8313-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
    Jul 29 16:44:48.083: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=crd-publish-openapi-5394 explain e2e-test-crd-publish-openapi-8313-crds.spec.bars'
    Jul 29 16:44:48.512: INFO: stderr: ""
    Jul 29 16:44:48.512: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8313-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
    STEP: kubectl explain works to return error when explain is called on property that doesn't exist 07/29/23 16:44:48.512
    Jul 29 16:44:48.513: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=crd-publish-openapi-5394 explain e2e-test-crd-publish-openapi-8313-crds.spec.bars2'
    Jul 29 16:44:48.911: INFO: rc: 1
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jul 29 16:44:54.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-5394" for this suite. 07/29/23 16:44:54.873
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:44:54.888
Jul 29 16:44:54.888: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename webhook 07/29/23 16:44:54.891
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:44:54.918
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:44:54.923
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 07/29/23 16:44:54.953
STEP: Create role binding to let webhook read extension-apiserver-authentication 07/29/23 16:44:56.014
STEP: Deploying the webhook pod 07/29/23 16:44:56.027
STEP: Wait for the deployment to be ready 07/29/23 16:44:56.051
Jul 29 16:44:56.070: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 07/29/23 16:44:58.098
STEP: Verifying the service has paired with the endpoint 07/29/23 16:44:58.127
Jul 29 16:44:59.128: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196
STEP: Registering the webhook via the AdmissionRegistration API 07/29/23 16:44:59.137
STEP: create a pod that should be denied by the webhook 07/29/23 16:44:59.166
STEP: create a pod that causes the webhook to hang 07/29/23 16:44:59.196
STEP: create a configmap that should be denied by the webhook 07/29/23 16:45:09.213
STEP: create a configmap that should be admitted by the webhook 07/29/23 16:45:09.235
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 07/29/23 16:45:09.256
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 07/29/23 16:45:09.275
STEP: create a namespace that bypass the webhook 07/29/23 16:45:09.288
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 07/29/23 16:45:09.305
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jul 29 16:45:09.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2316" for this suite. 07/29/23 16:45:09.363
STEP: Destroying namespace "webhook-2316-markers" for this suite. 07/29/23 16:45:09.39
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","completed":298,"skipped":5400,"failed":0}
------------------------------
â€¢ [SLOW TEST] [14.601 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:44:54.888
    Jul 29 16:44:54.888: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename webhook 07/29/23 16:44:54.891
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:44:54.918
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:44:54.923
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 07/29/23 16:44:54.953
    STEP: Create role binding to let webhook read extension-apiserver-authentication 07/29/23 16:44:56.014
    STEP: Deploying the webhook pod 07/29/23 16:44:56.027
    STEP: Wait for the deployment to be ready 07/29/23 16:44:56.051
    Jul 29 16:44:56.070: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 07/29/23 16:44:58.098
    STEP: Verifying the service has paired with the endpoint 07/29/23 16:44:58.127
    Jul 29 16:44:59.128: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny pod and configmap creation [Conformance]
      test/e2e/apimachinery/webhook.go:196
    STEP: Registering the webhook via the AdmissionRegistration API 07/29/23 16:44:59.137
    STEP: create a pod that should be denied by the webhook 07/29/23 16:44:59.166
    STEP: create a pod that causes the webhook to hang 07/29/23 16:44:59.196
    STEP: create a configmap that should be denied by the webhook 07/29/23 16:45:09.213
    STEP: create a configmap that should be admitted by the webhook 07/29/23 16:45:09.235
    STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 07/29/23 16:45:09.256
    STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 07/29/23 16:45:09.275
    STEP: create a namespace that bypass the webhook 07/29/23 16:45:09.288
    STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 07/29/23 16:45:09.305
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jul 29 16:45:09.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-2316" for this suite. 07/29/23 16:45:09.363
    STEP: Destroying namespace "webhook-2316-markers" for this suite. 07/29/23 16:45:09.39
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:45:09.492
Jul 29 16:45:09.492: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename dns 07/29/23 16:45:09.497
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:45:09.548
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:45:09.552
[It] should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
STEP: Creating a test headless service 07/29/23 16:45:09.558
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4835.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-4835.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4835.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4835.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-4835.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-4835.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-4835.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-4835.svc.cluster.local;sleep 1; done
 07/29/23 16:45:09.579
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4835.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-4835.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4835.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-4835.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-4835.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-4835.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-4835.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-4835.svc.cluster.local;sleep 1; done
 07/29/23 16:45:09.58
STEP: creating a pod to probe DNS 07/29/23 16:45:09.58
STEP: submitting the pod to kubernetes 07/29/23 16:45:09.581
Jul 29 16:45:09.618: INFO: Waiting up to 15m0s for pod "dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc" in namespace "dns-4835" to be "running"
Jul 29 16:45:09.636: INFO: Pod "dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc": Phase="Pending", Reason="", readiness=false. Elapsed: 17.788489ms
Jul 29 16:45:11.643: INFO: Pod "dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc": Phase="Running", Reason="", readiness=true. Elapsed: 2.024687582s
Jul 29 16:45:11.643: INFO: Pod "dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc" satisfied condition "running"
STEP: retrieving the pod 07/29/23 16:45:11.643
STEP: looking for the results for each expected name from probers 07/29/23 16:45:11.649
Jul 29 16:45:11.658: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4835.svc.cluster.local from pod dns-4835/dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc: the server could not find the requested resource (get pods dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc)
Jul 29 16:45:11.663: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4835.svc.cluster.local from pod dns-4835/dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc: the server could not find the requested resource (get pods dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc)
Jul 29 16:45:11.670: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4835.svc.cluster.local from pod dns-4835/dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc: the server could not find the requested resource (get pods dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc)
Jul 29 16:45:11.674: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4835.svc.cluster.local from pod dns-4835/dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc: the server could not find the requested resource (get pods dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc)
Jul 29 16:45:11.679: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4835.svc.cluster.local from pod dns-4835/dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc: the server could not find the requested resource (get pods dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc)
Jul 29 16:45:11.685: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4835.svc.cluster.local from pod dns-4835/dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc: the server could not find the requested resource (get pods dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc)
Jul 29 16:45:11.693: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4835.svc.cluster.local from pod dns-4835/dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc: the server could not find the requested resource (get pods dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc)
Jul 29 16:45:11.698: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4835.svc.cluster.local from pod dns-4835/dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc: the server could not find the requested resource (get pods dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc)
Jul 29 16:45:11.698: INFO: Lookups using dns-4835/dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4835.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4835.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4835.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4835.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4835.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4835.svc.cluster.local jessie_udp@dns-test-service-2.dns-4835.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4835.svc.cluster.local]

Jul 29 16:45:16.707: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4835.svc.cluster.local from pod dns-4835/dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc: the server could not find the requested resource (get pods dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc)
Jul 29 16:45:16.715: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4835.svc.cluster.local from pod dns-4835/dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc: the server could not find the requested resource (get pods dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc)
Jul 29 16:45:16.722: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4835.svc.cluster.local from pod dns-4835/dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc: the server could not find the requested resource (get pods dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc)
Jul 29 16:45:16.731: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4835.svc.cluster.local from pod dns-4835/dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc: the server could not find the requested resource (get pods dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc)
Jul 29 16:45:16.738: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4835.svc.cluster.local from pod dns-4835/dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc: the server could not find the requested resource (get pods dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc)
Jul 29 16:45:16.749: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4835.svc.cluster.local from pod dns-4835/dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc: the server could not find the requested resource (get pods dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc)
Jul 29 16:45:16.757: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4835.svc.cluster.local from pod dns-4835/dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc: the server could not find the requested resource (get pods dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc)
Jul 29 16:45:16.761: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4835.svc.cluster.local from pod dns-4835/dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc: the server could not find the requested resource (get pods dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc)
Jul 29 16:45:16.761: INFO: Lookups using dns-4835/dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4835.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4835.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4835.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4835.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4835.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4835.svc.cluster.local jessie_udp@dns-test-service-2.dns-4835.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4835.svc.cluster.local]

Jul 29 16:45:21.712: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4835.svc.cluster.local from pod dns-4835/dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc: the server could not find the requested resource (get pods dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc)
Jul 29 16:45:21.719: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4835.svc.cluster.local from pod dns-4835/dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc: the server could not find the requested resource (get pods dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc)
Jul 29 16:45:21.729: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4835.svc.cluster.local from pod dns-4835/dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc: the server could not find the requested resource (get pods dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc)
Jul 29 16:45:21.736: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4835.svc.cluster.local from pod dns-4835/dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc: the server could not find the requested resource (get pods dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc)
Jul 29 16:45:21.749: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4835.svc.cluster.local from pod dns-4835/dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc: the server could not find the requested resource (get pods dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc)
Jul 29 16:45:21.760: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4835.svc.cluster.local from pod dns-4835/dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc: the server could not find the requested resource (get pods dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc)
Jul 29 16:45:21.770: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4835.svc.cluster.local from pod dns-4835/dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc: the server could not find the requested resource (get pods dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc)
Jul 29 16:45:21.775: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4835.svc.cluster.local from pod dns-4835/dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc: the server could not find the requested resource (get pods dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc)
Jul 29 16:45:21.776: INFO: Lookups using dns-4835/dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4835.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4835.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4835.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4835.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4835.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4835.svc.cluster.local jessie_udp@dns-test-service-2.dns-4835.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4835.svc.cluster.local]

Jul 29 16:45:26.707: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4835.svc.cluster.local from pod dns-4835/dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc: the server could not find the requested resource (get pods dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc)
Jul 29 16:45:26.713: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4835.svc.cluster.local from pod dns-4835/dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc: the server could not find the requested resource (get pods dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc)
Jul 29 16:45:26.719: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4835.svc.cluster.local from pod dns-4835/dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc: the server could not find the requested resource (get pods dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc)
Jul 29 16:45:26.725: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4835.svc.cluster.local from pod dns-4835/dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc: the server could not find the requested resource (get pods dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc)
Jul 29 16:45:26.731: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4835.svc.cluster.local from pod dns-4835/dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc: the server could not find the requested resource (get pods dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc)
Jul 29 16:45:26.737: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4835.svc.cluster.local from pod dns-4835/dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc: the server could not find the requested resource (get pods dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc)
Jul 29 16:45:26.743: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4835.svc.cluster.local from pod dns-4835/dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc: the server could not find the requested resource (get pods dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc)
Jul 29 16:45:26.748: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4835.svc.cluster.local from pod dns-4835/dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc: the server could not find the requested resource (get pods dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc)
Jul 29 16:45:26.748: INFO: Lookups using dns-4835/dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4835.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4835.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4835.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4835.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4835.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4835.svc.cluster.local jessie_udp@dns-test-service-2.dns-4835.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4835.svc.cluster.local]

Jul 29 16:45:31.708: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4835.svc.cluster.local from pod dns-4835/dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc: the server could not find the requested resource (get pods dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc)
Jul 29 16:45:31.717: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4835.svc.cluster.local from pod dns-4835/dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc: the server could not find the requested resource (get pods dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc)
Jul 29 16:45:31.724: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4835.svc.cluster.local from pod dns-4835/dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc: the server could not find the requested resource (get pods dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc)
Jul 29 16:45:31.731: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4835.svc.cluster.local from pod dns-4835/dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc: the server could not find the requested resource (get pods dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc)
Jul 29 16:45:31.736: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4835.svc.cluster.local from pod dns-4835/dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc: the server could not find the requested resource (get pods dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc)
Jul 29 16:45:31.743: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4835.svc.cluster.local from pod dns-4835/dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc: the server could not find the requested resource (get pods dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc)
Jul 29 16:45:31.749: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4835.svc.cluster.local from pod dns-4835/dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc: the server could not find the requested resource (get pods dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc)
Jul 29 16:45:31.755: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4835.svc.cluster.local from pod dns-4835/dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc: the server could not find the requested resource (get pods dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc)
Jul 29 16:45:31.755: INFO: Lookups using dns-4835/dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4835.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4835.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4835.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4835.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4835.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4835.svc.cluster.local jessie_udp@dns-test-service-2.dns-4835.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4835.svc.cluster.local]

Jul 29 16:45:36.711: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4835.svc.cluster.local from pod dns-4835/dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc: the server could not find the requested resource (get pods dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc)
Jul 29 16:45:36.725: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4835.svc.cluster.local from pod dns-4835/dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc: the server could not find the requested resource (get pods dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc)
Jul 29 16:45:36.734: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4835.svc.cluster.local from pod dns-4835/dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc: the server could not find the requested resource (get pods dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc)
Jul 29 16:45:36.741: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4835.svc.cluster.local from pod dns-4835/dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc: the server could not find the requested resource (get pods dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc)
Jul 29 16:45:36.747: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4835.svc.cluster.local from pod dns-4835/dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc: the server could not find the requested resource (get pods dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc)
Jul 29 16:45:36.753: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4835.svc.cluster.local from pod dns-4835/dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc: the server could not find the requested resource (get pods dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc)
Jul 29 16:45:36.759: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4835.svc.cluster.local from pod dns-4835/dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc: the server could not find the requested resource (get pods dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc)
Jul 29 16:45:36.764: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4835.svc.cluster.local from pod dns-4835/dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc: the server could not find the requested resource (get pods dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc)
Jul 29 16:45:36.764: INFO: Lookups using dns-4835/dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4835.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4835.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4835.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4835.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4835.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4835.svc.cluster.local jessie_udp@dns-test-service-2.dns-4835.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4835.svc.cluster.local]

Jul 29 16:45:41.782: INFO: DNS probes using dns-4835/dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc succeeded

STEP: deleting the pod 07/29/23 16:45:41.782
STEP: deleting the test headless service 07/29/23 16:45:41.826
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jul 29 16:45:41.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4835" for this suite. 07/29/23 16:45:41.865
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","completed":299,"skipped":5407,"failed":0}
------------------------------
â€¢ [SLOW TEST] [32.392 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:45:09.492
    Jul 29 16:45:09.492: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename dns 07/29/23 16:45:09.497
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:45:09.548
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:45:09.552
    [It] should provide DNS for pods for Subdomain [Conformance]
      test/e2e/network/dns.go:290
    STEP: Creating a test headless service 07/29/23 16:45:09.558
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4835.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-4835.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4835.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4835.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-4835.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-4835.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-4835.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-4835.svc.cluster.local;sleep 1; done
     07/29/23 16:45:09.579
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4835.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-4835.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4835.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-4835.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-4835.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-4835.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-4835.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-4835.svc.cluster.local;sleep 1; done
     07/29/23 16:45:09.58
    STEP: creating a pod to probe DNS 07/29/23 16:45:09.58
    STEP: submitting the pod to kubernetes 07/29/23 16:45:09.581
    Jul 29 16:45:09.618: INFO: Waiting up to 15m0s for pod "dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc" in namespace "dns-4835" to be "running"
    Jul 29 16:45:09.636: INFO: Pod "dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc": Phase="Pending", Reason="", readiness=false. Elapsed: 17.788489ms
    Jul 29 16:45:11.643: INFO: Pod "dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc": Phase="Running", Reason="", readiness=true. Elapsed: 2.024687582s
    Jul 29 16:45:11.643: INFO: Pod "dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc" satisfied condition "running"
    STEP: retrieving the pod 07/29/23 16:45:11.643
    STEP: looking for the results for each expected name from probers 07/29/23 16:45:11.649
    Jul 29 16:45:11.658: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4835.svc.cluster.local from pod dns-4835/dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc: the server could not find the requested resource (get pods dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc)
    Jul 29 16:45:11.663: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4835.svc.cluster.local from pod dns-4835/dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc: the server could not find the requested resource (get pods dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc)
    Jul 29 16:45:11.670: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4835.svc.cluster.local from pod dns-4835/dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc: the server could not find the requested resource (get pods dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc)
    Jul 29 16:45:11.674: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4835.svc.cluster.local from pod dns-4835/dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc: the server could not find the requested resource (get pods dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc)
    Jul 29 16:45:11.679: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4835.svc.cluster.local from pod dns-4835/dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc: the server could not find the requested resource (get pods dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc)
    Jul 29 16:45:11.685: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4835.svc.cluster.local from pod dns-4835/dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc: the server could not find the requested resource (get pods dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc)
    Jul 29 16:45:11.693: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4835.svc.cluster.local from pod dns-4835/dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc: the server could not find the requested resource (get pods dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc)
    Jul 29 16:45:11.698: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4835.svc.cluster.local from pod dns-4835/dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc: the server could not find the requested resource (get pods dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc)
    Jul 29 16:45:11.698: INFO: Lookups using dns-4835/dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4835.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4835.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4835.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4835.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4835.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4835.svc.cluster.local jessie_udp@dns-test-service-2.dns-4835.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4835.svc.cluster.local]

    Jul 29 16:45:16.707: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4835.svc.cluster.local from pod dns-4835/dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc: the server could not find the requested resource (get pods dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc)
    Jul 29 16:45:16.715: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4835.svc.cluster.local from pod dns-4835/dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc: the server could not find the requested resource (get pods dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc)
    Jul 29 16:45:16.722: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4835.svc.cluster.local from pod dns-4835/dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc: the server could not find the requested resource (get pods dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc)
    Jul 29 16:45:16.731: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4835.svc.cluster.local from pod dns-4835/dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc: the server could not find the requested resource (get pods dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc)
    Jul 29 16:45:16.738: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4835.svc.cluster.local from pod dns-4835/dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc: the server could not find the requested resource (get pods dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc)
    Jul 29 16:45:16.749: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4835.svc.cluster.local from pod dns-4835/dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc: the server could not find the requested resource (get pods dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc)
    Jul 29 16:45:16.757: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4835.svc.cluster.local from pod dns-4835/dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc: the server could not find the requested resource (get pods dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc)
    Jul 29 16:45:16.761: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4835.svc.cluster.local from pod dns-4835/dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc: the server could not find the requested resource (get pods dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc)
    Jul 29 16:45:16.761: INFO: Lookups using dns-4835/dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4835.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4835.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4835.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4835.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4835.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4835.svc.cluster.local jessie_udp@dns-test-service-2.dns-4835.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4835.svc.cluster.local]

    Jul 29 16:45:21.712: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4835.svc.cluster.local from pod dns-4835/dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc: the server could not find the requested resource (get pods dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc)
    Jul 29 16:45:21.719: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4835.svc.cluster.local from pod dns-4835/dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc: the server could not find the requested resource (get pods dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc)
    Jul 29 16:45:21.729: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4835.svc.cluster.local from pod dns-4835/dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc: the server could not find the requested resource (get pods dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc)
    Jul 29 16:45:21.736: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4835.svc.cluster.local from pod dns-4835/dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc: the server could not find the requested resource (get pods dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc)
    Jul 29 16:45:21.749: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4835.svc.cluster.local from pod dns-4835/dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc: the server could not find the requested resource (get pods dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc)
    Jul 29 16:45:21.760: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4835.svc.cluster.local from pod dns-4835/dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc: the server could not find the requested resource (get pods dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc)
    Jul 29 16:45:21.770: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4835.svc.cluster.local from pod dns-4835/dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc: the server could not find the requested resource (get pods dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc)
    Jul 29 16:45:21.775: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4835.svc.cluster.local from pod dns-4835/dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc: the server could not find the requested resource (get pods dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc)
    Jul 29 16:45:21.776: INFO: Lookups using dns-4835/dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4835.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4835.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4835.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4835.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4835.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4835.svc.cluster.local jessie_udp@dns-test-service-2.dns-4835.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4835.svc.cluster.local]

    Jul 29 16:45:26.707: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4835.svc.cluster.local from pod dns-4835/dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc: the server could not find the requested resource (get pods dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc)
    Jul 29 16:45:26.713: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4835.svc.cluster.local from pod dns-4835/dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc: the server could not find the requested resource (get pods dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc)
    Jul 29 16:45:26.719: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4835.svc.cluster.local from pod dns-4835/dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc: the server could not find the requested resource (get pods dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc)
    Jul 29 16:45:26.725: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4835.svc.cluster.local from pod dns-4835/dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc: the server could not find the requested resource (get pods dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc)
    Jul 29 16:45:26.731: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4835.svc.cluster.local from pod dns-4835/dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc: the server could not find the requested resource (get pods dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc)
    Jul 29 16:45:26.737: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4835.svc.cluster.local from pod dns-4835/dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc: the server could not find the requested resource (get pods dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc)
    Jul 29 16:45:26.743: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4835.svc.cluster.local from pod dns-4835/dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc: the server could not find the requested resource (get pods dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc)
    Jul 29 16:45:26.748: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4835.svc.cluster.local from pod dns-4835/dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc: the server could not find the requested resource (get pods dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc)
    Jul 29 16:45:26.748: INFO: Lookups using dns-4835/dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4835.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4835.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4835.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4835.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4835.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4835.svc.cluster.local jessie_udp@dns-test-service-2.dns-4835.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4835.svc.cluster.local]

    Jul 29 16:45:31.708: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4835.svc.cluster.local from pod dns-4835/dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc: the server could not find the requested resource (get pods dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc)
    Jul 29 16:45:31.717: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4835.svc.cluster.local from pod dns-4835/dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc: the server could not find the requested resource (get pods dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc)
    Jul 29 16:45:31.724: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4835.svc.cluster.local from pod dns-4835/dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc: the server could not find the requested resource (get pods dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc)
    Jul 29 16:45:31.731: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4835.svc.cluster.local from pod dns-4835/dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc: the server could not find the requested resource (get pods dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc)
    Jul 29 16:45:31.736: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4835.svc.cluster.local from pod dns-4835/dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc: the server could not find the requested resource (get pods dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc)
    Jul 29 16:45:31.743: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4835.svc.cluster.local from pod dns-4835/dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc: the server could not find the requested resource (get pods dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc)
    Jul 29 16:45:31.749: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4835.svc.cluster.local from pod dns-4835/dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc: the server could not find the requested resource (get pods dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc)
    Jul 29 16:45:31.755: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4835.svc.cluster.local from pod dns-4835/dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc: the server could not find the requested resource (get pods dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc)
    Jul 29 16:45:31.755: INFO: Lookups using dns-4835/dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4835.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4835.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4835.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4835.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4835.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4835.svc.cluster.local jessie_udp@dns-test-service-2.dns-4835.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4835.svc.cluster.local]

    Jul 29 16:45:36.711: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4835.svc.cluster.local from pod dns-4835/dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc: the server could not find the requested resource (get pods dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc)
    Jul 29 16:45:36.725: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4835.svc.cluster.local from pod dns-4835/dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc: the server could not find the requested resource (get pods dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc)
    Jul 29 16:45:36.734: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4835.svc.cluster.local from pod dns-4835/dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc: the server could not find the requested resource (get pods dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc)
    Jul 29 16:45:36.741: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4835.svc.cluster.local from pod dns-4835/dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc: the server could not find the requested resource (get pods dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc)
    Jul 29 16:45:36.747: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4835.svc.cluster.local from pod dns-4835/dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc: the server could not find the requested resource (get pods dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc)
    Jul 29 16:45:36.753: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4835.svc.cluster.local from pod dns-4835/dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc: the server could not find the requested resource (get pods dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc)
    Jul 29 16:45:36.759: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4835.svc.cluster.local from pod dns-4835/dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc: the server could not find the requested resource (get pods dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc)
    Jul 29 16:45:36.764: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4835.svc.cluster.local from pod dns-4835/dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc: the server could not find the requested resource (get pods dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc)
    Jul 29 16:45:36.764: INFO: Lookups using dns-4835/dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4835.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4835.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4835.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4835.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4835.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4835.svc.cluster.local jessie_udp@dns-test-service-2.dns-4835.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4835.svc.cluster.local]

    Jul 29 16:45:41.782: INFO: DNS probes using dns-4835/dns-test-14b8883f-3fc2-434f-833d-682e0310f5bc succeeded

    STEP: deleting the pod 07/29/23 16:45:41.782
    STEP: deleting the test headless service 07/29/23 16:45:41.826
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jul 29 16:45:41.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-4835" for this suite. 07/29/23 16:45:41.865
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:45:41.887
Jul 29 16:45:41.887: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename taint-multiple-pods 07/29/23 16:45:41.891
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:45:41.936
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:45:41.945
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/node/taints.go:348
Jul 29 16:45:41.951: INFO: Waiting up to 1m0s for all nodes to be ready
Jul 29 16:46:42.002: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420
Jul 29 16:46:42.008: INFO: Starting informer...
STEP: Starting pods... 07/29/23 16:46:42.008
Jul 29 16:46:42.245: INFO: Pod1 is running on wa4quivohpee-3. Tainting Node
Jul 29 16:46:42.462: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-5302" to be "running"
Jul 29 16:46:42.471: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 9.685077ms
Jul 29 16:46:44.482: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.019983641s
Jul 29 16:46:44.482: INFO: Pod "taint-eviction-b1" satisfied condition "running"
Jul 29 16:46:44.482: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-5302" to be "running"
Jul 29 16:46:44.488: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 6.206074ms
Jul 29 16:46:44.488: INFO: Pod "taint-eviction-b2" satisfied condition "running"
Jul 29 16:46:44.488: INFO: Pod2 is running on wa4quivohpee-3. Tainting Node
STEP: Trying to apply a taint on the Node 07/29/23 16:46:44.489
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 07/29/23 16:46:44.519
STEP: Waiting for Pod1 and Pod2 to be deleted 07/29/23 16:46:44.525
Jul 29 16:46:50.315: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Jul 29 16:47:10.389: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 07/29/23 16:47:10.414
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/framework.go:187
Jul 29 16:47:10.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-5302" for this suite. 07/29/23 16:47:10.432
{"msg":"PASSED [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","completed":300,"skipped":5415,"failed":0}
------------------------------
â€¢ [SLOW TEST] [88.557 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:45:41.887
    Jul 29 16:45:41.887: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename taint-multiple-pods 07/29/23 16:45:41.891
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:45:41.936
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:45:41.945
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/node/taints.go:348
    Jul 29 16:45:41.951: INFO: Waiting up to 1m0s for all nodes to be ready
    Jul 29 16:46:42.002: INFO: Waiting for terminating namespaces to be deleted...
    [It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
      test/e2e/node/taints.go:420
    Jul 29 16:46:42.008: INFO: Starting informer...
    STEP: Starting pods... 07/29/23 16:46:42.008
    Jul 29 16:46:42.245: INFO: Pod1 is running on wa4quivohpee-3. Tainting Node
    Jul 29 16:46:42.462: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-5302" to be "running"
    Jul 29 16:46:42.471: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 9.685077ms
    Jul 29 16:46:44.482: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.019983641s
    Jul 29 16:46:44.482: INFO: Pod "taint-eviction-b1" satisfied condition "running"
    Jul 29 16:46:44.482: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-5302" to be "running"
    Jul 29 16:46:44.488: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 6.206074ms
    Jul 29 16:46:44.488: INFO: Pod "taint-eviction-b2" satisfied condition "running"
    Jul 29 16:46:44.488: INFO: Pod2 is running on wa4quivohpee-3. Tainting Node
    STEP: Trying to apply a taint on the Node 07/29/23 16:46:44.489
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 07/29/23 16:46:44.519
    STEP: Waiting for Pod1 and Pod2 to be deleted 07/29/23 16:46:44.525
    Jul 29 16:46:50.315: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
    Jul 29 16:47:10.389: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 07/29/23 16:47:10.414
    [AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/framework.go:187
    Jul 29 16:47:10.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "taint-multiple-pods-5302" for this suite. 07/29/23 16:47:10.432
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:47:10.45
Jul 29 16:47:10.450: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename configmap 07/29/23 16:47:10.453
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:47:10.49
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:47:10.498
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123
STEP: Creating configMap with name configmap-test-upd-eb5a8721-e7ce-44f9-a455-8fc56215ec4a 07/29/23 16:47:10.512
STEP: Creating the pod 07/29/23 16:47:10.519
Jul 29 16:47:10.535: INFO: Waiting up to 5m0s for pod "pod-configmaps-7523d1dd-9ba3-467d-ad3d-1979ef4ba4c2" in namespace "configmap-447" to be "running and ready"
Jul 29 16:47:10.542: INFO: Pod "pod-configmaps-7523d1dd-9ba3-467d-ad3d-1979ef4ba4c2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.018792ms
Jul 29 16:47:10.542: INFO: The phase of Pod pod-configmaps-7523d1dd-9ba3-467d-ad3d-1979ef4ba4c2 is Pending, waiting for it to be Running (with Ready = true)
Jul 29 16:47:12.551: INFO: Pod "pod-configmaps-7523d1dd-9ba3-467d-ad3d-1979ef4ba4c2": Phase="Running", Reason="", readiness=true. Elapsed: 2.016005505s
Jul 29 16:47:12.551: INFO: The phase of Pod pod-configmaps-7523d1dd-9ba3-467d-ad3d-1979ef4ba4c2 is Running (Ready = true)
Jul 29 16:47:12.551: INFO: Pod "pod-configmaps-7523d1dd-9ba3-467d-ad3d-1979ef4ba4c2" satisfied condition "running and ready"
STEP: Updating configmap configmap-test-upd-eb5a8721-e7ce-44f9-a455-8fc56215ec4a 07/29/23 16:47:12.587
STEP: waiting to observe update in volume 07/29/23 16:47:12.6
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jul 29 16:47:14.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-447" for this suite. 07/29/23 16:47:14.636
{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","completed":301,"skipped":5473,"failed":0}
------------------------------
â€¢ [4.235 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:47:10.45
    Jul 29 16:47:10.450: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename configmap 07/29/23 16:47:10.453
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:47:10.49
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:47:10.498
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:123
    STEP: Creating configMap with name configmap-test-upd-eb5a8721-e7ce-44f9-a455-8fc56215ec4a 07/29/23 16:47:10.512
    STEP: Creating the pod 07/29/23 16:47:10.519
    Jul 29 16:47:10.535: INFO: Waiting up to 5m0s for pod "pod-configmaps-7523d1dd-9ba3-467d-ad3d-1979ef4ba4c2" in namespace "configmap-447" to be "running and ready"
    Jul 29 16:47:10.542: INFO: Pod "pod-configmaps-7523d1dd-9ba3-467d-ad3d-1979ef4ba4c2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.018792ms
    Jul 29 16:47:10.542: INFO: The phase of Pod pod-configmaps-7523d1dd-9ba3-467d-ad3d-1979ef4ba4c2 is Pending, waiting for it to be Running (with Ready = true)
    Jul 29 16:47:12.551: INFO: Pod "pod-configmaps-7523d1dd-9ba3-467d-ad3d-1979ef4ba4c2": Phase="Running", Reason="", readiness=true. Elapsed: 2.016005505s
    Jul 29 16:47:12.551: INFO: The phase of Pod pod-configmaps-7523d1dd-9ba3-467d-ad3d-1979ef4ba4c2 is Running (Ready = true)
    Jul 29 16:47:12.551: INFO: Pod "pod-configmaps-7523d1dd-9ba3-467d-ad3d-1979ef4ba4c2" satisfied condition "running and ready"
    STEP: Updating configmap configmap-test-upd-eb5a8721-e7ce-44f9-a455-8fc56215ec4a 07/29/23 16:47:12.587
    STEP: waiting to observe update in volume 07/29/23 16:47:12.6
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jul 29 16:47:14.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-447" for this suite. 07/29/23 16:47:14.636
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3185
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:47:14.689
Jul 29 16:47:14.690: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename services 07/29/23 16:47:14.692
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:47:14.732
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:47:14.737
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3185
STEP: fetching services 07/29/23 16:47:14.74
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jul 29 16:47:14.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2135" for this suite. 07/29/23 16:47:14.754
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","completed":302,"skipped":5492,"failed":0}
------------------------------
â€¢ [0.074 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:47:14.689
    Jul 29 16:47:14.690: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename services 07/29/23 16:47:14.692
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:47:14.732
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:47:14.737
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should find a service from listing all namespaces [Conformance]
      test/e2e/network/service.go:3185
    STEP: fetching services 07/29/23 16:47:14.74
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jul 29 16:47:14.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-2135" for this suite. 07/29/23 16:47:14.754
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:47:14.764
Jul 29 16:47:14.764: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename sched-pred 07/29/23 16:47:14.767
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:47:14.8
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:47:14.804
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Jul 29 16:47:14.808: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jul 29 16:47:14.823: INFO: Waiting for terminating namespaces to be deleted...
Jul 29 16:47:14.828: INFO: 
Logging pods the apiserver thinks is on node wa4quivohpee-1 before test
Jul 29 16:47:14.845: INFO: cilium-gfkpl from kube-system started at 2023-07-29 15:23:18 +0000 UTC (1 container statuses recorded)
Jul 29 16:47:14.847: INFO: 	Container cilium-agent ready: true, restart count 0
Jul 29 16:47:14.848: INFO: cilium-node-init-n8876 from kube-system started at 2023-07-29 15:23:18 +0000 UTC (1 container statuses recorded)
Jul 29 16:47:14.848: INFO: 	Container node-init ready: true, restart count 0
Jul 29 16:47:14.848: INFO: coredns-565d847f94-xm9ff from kube-system started at 2023-07-29 15:34:18 +0000 UTC (1 container statuses recorded)
Jul 29 16:47:14.848: INFO: 	Container coredns ready: true, restart count 0
Jul 29 16:47:14.848: INFO: kube-addon-manager-wa4quivohpee-1 from kube-system started at 2023-07-29 15:22:59 +0000 UTC (1 container statuses recorded)
Jul 29 16:47:14.848: INFO: 	Container kube-addon-manager ready: true, restart count 0
Jul 29 16:47:14.848: INFO: kube-apiserver-wa4quivohpee-1 from kube-system started at 2023-07-29 15:14:24 +0000 UTC (1 container statuses recorded)
Jul 29 16:47:14.848: INFO: 	Container kube-apiserver ready: true, restart count 0
Jul 29 16:47:14.849: INFO: kube-controller-manager-wa4quivohpee-1 from kube-system started at 2023-07-29 15:14:24 +0000 UTC (1 container statuses recorded)
Jul 29 16:47:14.849: INFO: 	Container kube-controller-manager ready: true, restart count 0
Jul 29 16:47:14.849: INFO: kube-proxy-gdffl from kube-system started at 2023-07-29 15:14:37 +0000 UTC (1 container statuses recorded)
Jul 29 16:47:14.849: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 29 16:47:14.849: INFO: kube-scheduler-wa4quivohpee-1 from kube-system started at 2023-07-29 15:14:23 +0000 UTC (1 container statuses recorded)
Jul 29 16:47:14.849: INFO: 	Container kube-scheduler ready: true, restart count 0
Jul 29 16:47:14.849: INFO: sonobuoy-systemd-logs-daemon-set-b6f0dae2c3174900-mtck5 from sonobuoy started at 2023-07-29 15:24:46 +0000 UTC (2 container statuses recorded)
Jul 29 16:47:14.849: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 29 16:47:14.849: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 29 16:47:14.849: INFO: 
Logging pods the apiserver thinks is on node wa4quivohpee-2 before test
Jul 29 16:47:14.866: INFO: cilium-node-init-946z9 from kube-system started at 2023-07-29 15:23:18 +0000 UTC (1 container statuses recorded)
Jul 29 16:47:14.867: INFO: 	Container node-init ready: true, restart count 0
Jul 29 16:47:14.867: INFO: cilium-rmjkx from kube-system started at 2023-07-29 15:23:18 +0000 UTC (1 container statuses recorded)
Jul 29 16:47:14.867: INFO: 	Container cilium-agent ready: true, restart count 0
Jul 29 16:47:14.867: INFO: coredns-565d847f94-wmg75 from kube-system started at 2023-07-29 15:24:27 +0000 UTC (1 container statuses recorded)
Jul 29 16:47:14.867: INFO: 	Container coredns ready: true, restart count 0
Jul 29 16:47:14.867: INFO: kube-addon-manager-wa4quivohpee-2 from kube-system started at 2023-07-29 15:22:59 +0000 UTC (1 container statuses recorded)
Jul 29 16:47:14.867: INFO: 	Container kube-addon-manager ready: true, restart count 0
Jul 29 16:47:14.867: INFO: kube-apiserver-wa4quivohpee-2 from kube-system started at 2023-07-29 15:15:22 +0000 UTC (1 container statuses recorded)
Jul 29 16:47:14.867: INFO: 	Container kube-apiserver ready: true, restart count 0
Jul 29 16:47:14.867: INFO: kube-controller-manager-wa4quivohpee-2 from kube-system started at 2023-07-29 15:15:22 +0000 UTC (1 container statuses recorded)
Jul 29 16:47:14.868: INFO: 	Container kube-controller-manager ready: true, restart count 0
Jul 29 16:47:14.868: INFO: kube-proxy-74hn2 from kube-system started at 2023-07-29 15:15:19 +0000 UTC (1 container statuses recorded)
Jul 29 16:47:14.868: INFO: 	Container kube-proxy ready: true, restart count 1
Jul 29 16:47:14.868: INFO: kube-scheduler-wa4quivohpee-2 from kube-system started at 2023-07-29 15:15:22 +0000 UTC (1 container statuses recorded)
Jul 29 16:47:14.868: INFO: 	Container kube-scheduler ready: true, restart count 0
Jul 29 16:47:14.868: INFO: sonobuoy-systemd-logs-daemon-set-b6f0dae2c3174900-cnfp2 from sonobuoy started at 2023-07-29 15:24:46 +0000 UTC (2 container statuses recorded)
Jul 29 16:47:14.868: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 29 16:47:14.868: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 29 16:47:14.868: INFO: 
Logging pods the apiserver thinks is on node wa4quivohpee-3 before test
Jul 29 16:47:14.889: INFO: pod-configmaps-7523d1dd-9ba3-467d-ad3d-1979ef4ba4c2 from configmap-447 started at 2023-07-29 16:47:10 +0000 UTC (1 container statuses recorded)
Jul 29 16:47:14.890: INFO: 	Container agnhost-container ready: true, restart count 0
Jul 29 16:47:14.890: INFO: cilium-node-init-j4pkd from kube-system started at 2023-07-29 15:23:18 +0000 UTC (1 container statuses recorded)
Jul 29 16:47:14.890: INFO: 	Container node-init ready: true, restart count 0
Jul 29 16:47:14.890: INFO: cilium-operator-69cffcb958-48gkl from kube-system started at 2023-07-29 15:23:19 +0000 UTC (1 container statuses recorded)
Jul 29 16:47:14.890: INFO: 	Container cilium-operator ready: true, restart count 0
Jul 29 16:47:14.890: INFO: cilium-zl28x from kube-system started at 2023-07-29 15:23:18 +0000 UTC (1 container statuses recorded)
Jul 29 16:47:14.890: INFO: 	Container cilium-agent ready: true, restart count 0
Jul 29 16:47:14.890: INFO: kube-proxy-ts2pg from kube-system started at 2023-07-29 15:16:17 +0000 UTC (1 container statuses recorded)
Jul 29 16:47:14.890: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 29 16:47:14.890: INFO: sonobuoy from sonobuoy started at 2023-07-29 15:24:31 +0000 UTC (1 container statuses recorded)
Jul 29 16:47:14.890: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jul 29 16:47:14.890: INFO: sonobuoy-e2e-job-5c9a7da3e6b74e15 from sonobuoy started at 2023-07-29 15:24:45 +0000 UTC (2 container statuses recorded)
Jul 29 16:47:14.890: INFO: 	Container e2e ready: true, restart count 0
Jul 29 16:47:14.890: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 29 16:47:14.890: INFO: sonobuoy-systemd-logs-daemon-set-b6f0dae2c3174900-cqnfx from sonobuoy started at 2023-07-29 15:24:46 +0000 UTC (2 container statuses recorded)
Jul 29 16:47:14.890: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 29 16:47:14.890: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438
STEP: Trying to schedule Pod with nonempty NodeSelector. 07/29/23 16:47:14.89
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.177664858ff10ab5], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match Pod's node affinity/selector. preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling.] 07/29/23 16:47:14.946
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Jul 29 16:47:15.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-4465" for this suite. 07/29/23 16:47:15.955
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","completed":303,"skipped":5493,"failed":0}
------------------------------
â€¢ [1.203 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:47:14.764
    Jul 29 16:47:14.764: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename sched-pred 07/29/23 16:47:14.767
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:47:14.8
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:47:14.804
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Jul 29 16:47:14.808: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jul 29 16:47:14.823: INFO: Waiting for terminating namespaces to be deleted...
    Jul 29 16:47:14.828: INFO: 
    Logging pods the apiserver thinks is on node wa4quivohpee-1 before test
    Jul 29 16:47:14.845: INFO: cilium-gfkpl from kube-system started at 2023-07-29 15:23:18 +0000 UTC (1 container statuses recorded)
    Jul 29 16:47:14.847: INFO: 	Container cilium-agent ready: true, restart count 0
    Jul 29 16:47:14.848: INFO: cilium-node-init-n8876 from kube-system started at 2023-07-29 15:23:18 +0000 UTC (1 container statuses recorded)
    Jul 29 16:47:14.848: INFO: 	Container node-init ready: true, restart count 0
    Jul 29 16:47:14.848: INFO: coredns-565d847f94-xm9ff from kube-system started at 2023-07-29 15:34:18 +0000 UTC (1 container statuses recorded)
    Jul 29 16:47:14.848: INFO: 	Container coredns ready: true, restart count 0
    Jul 29 16:47:14.848: INFO: kube-addon-manager-wa4quivohpee-1 from kube-system started at 2023-07-29 15:22:59 +0000 UTC (1 container statuses recorded)
    Jul 29 16:47:14.848: INFO: 	Container kube-addon-manager ready: true, restart count 0
    Jul 29 16:47:14.848: INFO: kube-apiserver-wa4quivohpee-1 from kube-system started at 2023-07-29 15:14:24 +0000 UTC (1 container statuses recorded)
    Jul 29 16:47:14.848: INFO: 	Container kube-apiserver ready: true, restart count 0
    Jul 29 16:47:14.849: INFO: kube-controller-manager-wa4quivohpee-1 from kube-system started at 2023-07-29 15:14:24 +0000 UTC (1 container statuses recorded)
    Jul 29 16:47:14.849: INFO: 	Container kube-controller-manager ready: true, restart count 0
    Jul 29 16:47:14.849: INFO: kube-proxy-gdffl from kube-system started at 2023-07-29 15:14:37 +0000 UTC (1 container statuses recorded)
    Jul 29 16:47:14.849: INFO: 	Container kube-proxy ready: true, restart count 0
    Jul 29 16:47:14.849: INFO: kube-scheduler-wa4quivohpee-1 from kube-system started at 2023-07-29 15:14:23 +0000 UTC (1 container statuses recorded)
    Jul 29 16:47:14.849: INFO: 	Container kube-scheduler ready: true, restart count 0
    Jul 29 16:47:14.849: INFO: sonobuoy-systemd-logs-daemon-set-b6f0dae2c3174900-mtck5 from sonobuoy started at 2023-07-29 15:24:46 +0000 UTC (2 container statuses recorded)
    Jul 29 16:47:14.849: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jul 29 16:47:14.849: INFO: 	Container systemd-logs ready: true, restart count 0
    Jul 29 16:47:14.849: INFO: 
    Logging pods the apiserver thinks is on node wa4quivohpee-2 before test
    Jul 29 16:47:14.866: INFO: cilium-node-init-946z9 from kube-system started at 2023-07-29 15:23:18 +0000 UTC (1 container statuses recorded)
    Jul 29 16:47:14.867: INFO: 	Container node-init ready: true, restart count 0
    Jul 29 16:47:14.867: INFO: cilium-rmjkx from kube-system started at 2023-07-29 15:23:18 +0000 UTC (1 container statuses recorded)
    Jul 29 16:47:14.867: INFO: 	Container cilium-agent ready: true, restart count 0
    Jul 29 16:47:14.867: INFO: coredns-565d847f94-wmg75 from kube-system started at 2023-07-29 15:24:27 +0000 UTC (1 container statuses recorded)
    Jul 29 16:47:14.867: INFO: 	Container coredns ready: true, restart count 0
    Jul 29 16:47:14.867: INFO: kube-addon-manager-wa4quivohpee-2 from kube-system started at 2023-07-29 15:22:59 +0000 UTC (1 container statuses recorded)
    Jul 29 16:47:14.867: INFO: 	Container kube-addon-manager ready: true, restart count 0
    Jul 29 16:47:14.867: INFO: kube-apiserver-wa4quivohpee-2 from kube-system started at 2023-07-29 15:15:22 +0000 UTC (1 container statuses recorded)
    Jul 29 16:47:14.867: INFO: 	Container kube-apiserver ready: true, restart count 0
    Jul 29 16:47:14.867: INFO: kube-controller-manager-wa4quivohpee-2 from kube-system started at 2023-07-29 15:15:22 +0000 UTC (1 container statuses recorded)
    Jul 29 16:47:14.868: INFO: 	Container kube-controller-manager ready: true, restart count 0
    Jul 29 16:47:14.868: INFO: kube-proxy-74hn2 from kube-system started at 2023-07-29 15:15:19 +0000 UTC (1 container statuses recorded)
    Jul 29 16:47:14.868: INFO: 	Container kube-proxy ready: true, restart count 1
    Jul 29 16:47:14.868: INFO: kube-scheduler-wa4quivohpee-2 from kube-system started at 2023-07-29 15:15:22 +0000 UTC (1 container statuses recorded)
    Jul 29 16:47:14.868: INFO: 	Container kube-scheduler ready: true, restart count 0
    Jul 29 16:47:14.868: INFO: sonobuoy-systemd-logs-daemon-set-b6f0dae2c3174900-cnfp2 from sonobuoy started at 2023-07-29 15:24:46 +0000 UTC (2 container statuses recorded)
    Jul 29 16:47:14.868: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jul 29 16:47:14.868: INFO: 	Container systemd-logs ready: true, restart count 0
    Jul 29 16:47:14.868: INFO: 
    Logging pods the apiserver thinks is on node wa4quivohpee-3 before test
    Jul 29 16:47:14.889: INFO: pod-configmaps-7523d1dd-9ba3-467d-ad3d-1979ef4ba4c2 from configmap-447 started at 2023-07-29 16:47:10 +0000 UTC (1 container statuses recorded)
    Jul 29 16:47:14.890: INFO: 	Container agnhost-container ready: true, restart count 0
    Jul 29 16:47:14.890: INFO: cilium-node-init-j4pkd from kube-system started at 2023-07-29 15:23:18 +0000 UTC (1 container statuses recorded)
    Jul 29 16:47:14.890: INFO: 	Container node-init ready: true, restart count 0
    Jul 29 16:47:14.890: INFO: cilium-operator-69cffcb958-48gkl from kube-system started at 2023-07-29 15:23:19 +0000 UTC (1 container statuses recorded)
    Jul 29 16:47:14.890: INFO: 	Container cilium-operator ready: true, restart count 0
    Jul 29 16:47:14.890: INFO: cilium-zl28x from kube-system started at 2023-07-29 15:23:18 +0000 UTC (1 container statuses recorded)
    Jul 29 16:47:14.890: INFO: 	Container cilium-agent ready: true, restart count 0
    Jul 29 16:47:14.890: INFO: kube-proxy-ts2pg from kube-system started at 2023-07-29 15:16:17 +0000 UTC (1 container statuses recorded)
    Jul 29 16:47:14.890: INFO: 	Container kube-proxy ready: true, restart count 0
    Jul 29 16:47:14.890: INFO: sonobuoy from sonobuoy started at 2023-07-29 15:24:31 +0000 UTC (1 container statuses recorded)
    Jul 29 16:47:14.890: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jul 29 16:47:14.890: INFO: sonobuoy-e2e-job-5c9a7da3e6b74e15 from sonobuoy started at 2023-07-29 15:24:45 +0000 UTC (2 container statuses recorded)
    Jul 29 16:47:14.890: INFO: 	Container e2e ready: true, restart count 0
    Jul 29 16:47:14.890: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jul 29 16:47:14.890: INFO: sonobuoy-systemd-logs-daemon-set-b6f0dae2c3174900-cqnfx from sonobuoy started at 2023-07-29 15:24:46 +0000 UTC (2 container statuses recorded)
    Jul 29 16:47:14.890: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jul 29 16:47:14.890: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if not matching  [Conformance]
      test/e2e/scheduling/predicates.go:438
    STEP: Trying to schedule Pod with nonempty NodeSelector. 07/29/23 16:47:14.89
    STEP: Considering event: 
    Type = [Warning], Name = [restricted-pod.177664858ff10ab5], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match Pod's node affinity/selector. preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling.] 07/29/23 16:47:14.946
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Jul 29 16:47:15.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-4465" for this suite. 07/29/23 16:47:15.955
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:47:15.975
Jul 29 16:47:15.975: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename gc 07/29/23 16:47:15.979
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:47:16.004
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:47:16.009
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
STEP: create the rc1 07/29/23 16:47:16.023
STEP: create the rc2 07/29/23 16:47:16.034
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 07/29/23 16:47:21.125
STEP: delete the rc simpletest-rc-to-be-deleted 07/29/23 16:47:28.792
STEP: wait for the rc to be deleted 07/29/23 16:47:28.967
Jul 29 16:47:34.270: INFO: 97 pods remaining
Jul 29 16:47:34.271: INFO: 70 pods has nil DeletionTimestamp
Jul 29 16:47:34.271: INFO: 
Jul 29 16:47:39.002: INFO: 84 pods remaining
Jul 29 16:47:39.002: INFO: 50 pods has nil DeletionTimestamp
Jul 29 16:47:39.002: INFO: 
STEP: Gathering metrics 07/29/23 16:47:44.008
Jul 29 16:47:44.321: INFO: Waiting up to 5m0s for pod "kube-controller-manager-wa4quivohpee-2" in namespace "kube-system" to be "running and ready"
Jul 29 16:47:44.329: INFO: Pod "kube-controller-manager-wa4quivohpee-2": Phase="Running", Reason="", readiness=true. Elapsed: 8.562645ms
Jul 29 16:47:44.329: INFO: The phase of Pod kube-controller-manager-wa4quivohpee-2 is Running (Ready = true)
Jul 29 16:47:44.329: INFO: Pod "kube-controller-manager-wa4quivohpee-2" satisfied condition "running and ready"
Jul 29 16:47:44.602: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Jul 29 16:47:44.602: INFO: Deleting pod "simpletest-rc-to-be-deleted-26pjm" in namespace "gc-4655"
Jul 29 16:47:44.677: INFO: Deleting pod "simpletest-rc-to-be-deleted-27rj8" in namespace "gc-4655"
Jul 29 16:47:44.773: INFO: Deleting pod "simpletest-rc-to-be-deleted-2cldr" in namespace "gc-4655"
Jul 29 16:47:44.868: INFO: Deleting pod "simpletest-rc-to-be-deleted-2lfrt" in namespace "gc-4655"
Jul 29 16:47:44.913: INFO: Deleting pod "simpletest-rc-to-be-deleted-4spqh" in namespace "gc-4655"
Jul 29 16:47:44.946: INFO: Deleting pod "simpletest-rc-to-be-deleted-584hl" in namespace "gc-4655"
Jul 29 16:47:45.002: INFO: Deleting pod "simpletest-rc-to-be-deleted-5k56g" in namespace "gc-4655"
Jul 29 16:47:45.045: INFO: Deleting pod "simpletest-rc-to-be-deleted-5l58w" in namespace "gc-4655"
Jul 29 16:47:45.109: INFO: Deleting pod "simpletest-rc-to-be-deleted-5mz7t" in namespace "gc-4655"
Jul 29 16:47:45.157: INFO: Deleting pod "simpletest-rc-to-be-deleted-5npds" in namespace "gc-4655"
Jul 29 16:47:45.296: INFO: Deleting pod "simpletest-rc-to-be-deleted-5vj9v" in namespace "gc-4655"
Jul 29 16:47:45.421: INFO: Deleting pod "simpletest-rc-to-be-deleted-5wfsf" in namespace "gc-4655"
Jul 29 16:47:45.532: INFO: Deleting pod "simpletest-rc-to-be-deleted-68lgl" in namespace "gc-4655"
Jul 29 16:47:45.603: INFO: Deleting pod "simpletest-rc-to-be-deleted-68sz5" in namespace "gc-4655"
Jul 29 16:47:45.686: INFO: Deleting pod "simpletest-rc-to-be-deleted-68x49" in namespace "gc-4655"
Jul 29 16:47:45.778: INFO: Deleting pod "simpletest-rc-to-be-deleted-6r7h5" in namespace "gc-4655"
Jul 29 16:47:45.844: INFO: Deleting pod "simpletest-rc-to-be-deleted-86vsx" in namespace "gc-4655"
Jul 29 16:47:45.945: INFO: Deleting pod "simpletest-rc-to-be-deleted-8fpf5" in namespace "gc-4655"
Jul 29 16:47:46.047: INFO: Deleting pod "simpletest-rc-to-be-deleted-8tjqk" in namespace "gc-4655"
Jul 29 16:47:46.112: INFO: Deleting pod "simpletest-rc-to-be-deleted-9c4sp" in namespace "gc-4655"
Jul 29 16:47:46.234: INFO: Deleting pod "simpletest-rc-to-be-deleted-9dqvx" in namespace "gc-4655"
Jul 29 16:47:46.291: INFO: Deleting pod "simpletest-rc-to-be-deleted-9f9z7" in namespace "gc-4655"
Jul 29 16:47:46.355: INFO: Deleting pod "simpletest-rc-to-be-deleted-bdlwj" in namespace "gc-4655"
Jul 29 16:47:46.453: INFO: Deleting pod "simpletest-rc-to-be-deleted-bwwm8" in namespace "gc-4655"
Jul 29 16:47:46.548: INFO: Deleting pod "simpletest-rc-to-be-deleted-bz5lv" in namespace "gc-4655"
Jul 29 16:47:46.792: INFO: Deleting pod "simpletest-rc-to-be-deleted-cdr5s" in namespace "gc-4655"
Jul 29 16:47:46.879: INFO: Deleting pod "simpletest-rc-to-be-deleted-cgt22" in namespace "gc-4655"
Jul 29 16:47:46.991: INFO: Deleting pod "simpletest-rc-to-be-deleted-ck9cs" in namespace "gc-4655"
Jul 29 16:47:47.075: INFO: Deleting pod "simpletest-rc-to-be-deleted-cx4q4" in namespace "gc-4655"
Jul 29 16:47:47.121: INFO: Deleting pod "simpletest-rc-to-be-deleted-dmpvk" in namespace "gc-4655"
Jul 29 16:47:47.157: INFO: Deleting pod "simpletest-rc-to-be-deleted-drhrq" in namespace "gc-4655"
Jul 29 16:47:47.207: INFO: Deleting pod "simpletest-rc-to-be-deleted-f2plm" in namespace "gc-4655"
Jul 29 16:47:47.255: INFO: Deleting pod "simpletest-rc-to-be-deleted-fjmjk" in namespace "gc-4655"
Jul 29 16:47:47.315: INFO: Deleting pod "simpletest-rc-to-be-deleted-fjnmt" in namespace "gc-4655"
Jul 29 16:47:47.377: INFO: Deleting pod "simpletest-rc-to-be-deleted-fn5w9" in namespace "gc-4655"
Jul 29 16:47:47.484: INFO: Deleting pod "simpletest-rc-to-be-deleted-fp8tf" in namespace "gc-4655"
Jul 29 16:47:47.611: INFO: Deleting pod "simpletest-rc-to-be-deleted-fw7qz" in namespace "gc-4655"
Jul 29 16:47:47.643: INFO: Deleting pod "simpletest-rc-to-be-deleted-g42ng" in namespace "gc-4655"
Jul 29 16:47:47.884: INFO: Deleting pod "simpletest-rc-to-be-deleted-gdscg" in namespace "gc-4655"
Jul 29 16:47:47.929: INFO: Deleting pod "simpletest-rc-to-be-deleted-gvqp4" in namespace "gc-4655"
Jul 29 16:47:48.007: INFO: Deleting pod "simpletest-rc-to-be-deleted-hmm89" in namespace "gc-4655"
Jul 29 16:47:48.110: INFO: Deleting pod "simpletest-rc-to-be-deleted-j776v" in namespace "gc-4655"
Jul 29 16:47:48.180: INFO: Deleting pod "simpletest-rc-to-be-deleted-jkvt5" in namespace "gc-4655"
Jul 29 16:47:48.227: INFO: Deleting pod "simpletest-rc-to-be-deleted-jldpm" in namespace "gc-4655"
Jul 29 16:47:48.283: INFO: Deleting pod "simpletest-rc-to-be-deleted-jts7r" in namespace "gc-4655"
Jul 29 16:47:48.336: INFO: Deleting pod "simpletest-rc-to-be-deleted-k4rwj" in namespace "gc-4655"
Jul 29 16:47:48.422: INFO: Deleting pod "simpletest-rc-to-be-deleted-kkp8c" in namespace "gc-4655"
Jul 29 16:47:48.576: INFO: Deleting pod "simpletest-rc-to-be-deleted-klw82" in namespace "gc-4655"
Jul 29 16:47:48.608: INFO: Deleting pod "simpletest-rc-to-be-deleted-kmrt8" in namespace "gc-4655"
Jul 29 16:47:48.725: INFO: Deleting pod "simpletest-rc-to-be-deleted-knd64" in namespace "gc-4655"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Jul 29 16:47:48.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4655" for this suite. 07/29/23 16:47:48.82
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","completed":304,"skipped":5515,"failed":0}
------------------------------
â€¢ [SLOW TEST] [32.874 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:47:15.975
    Jul 29 16:47:15.975: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename gc 07/29/23 16:47:15.979
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:47:16.004
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:47:16.009
    [It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
      test/e2e/apimachinery/garbage_collector.go:735
    STEP: create the rc1 07/29/23 16:47:16.023
    STEP: create the rc2 07/29/23 16:47:16.034
    STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 07/29/23 16:47:21.125
    STEP: delete the rc simpletest-rc-to-be-deleted 07/29/23 16:47:28.792
    STEP: wait for the rc to be deleted 07/29/23 16:47:28.967
    Jul 29 16:47:34.270: INFO: 97 pods remaining
    Jul 29 16:47:34.271: INFO: 70 pods has nil DeletionTimestamp
    Jul 29 16:47:34.271: INFO: 
    Jul 29 16:47:39.002: INFO: 84 pods remaining
    Jul 29 16:47:39.002: INFO: 50 pods has nil DeletionTimestamp
    Jul 29 16:47:39.002: INFO: 
    STEP: Gathering metrics 07/29/23 16:47:44.008
    Jul 29 16:47:44.321: INFO: Waiting up to 5m0s for pod "kube-controller-manager-wa4quivohpee-2" in namespace "kube-system" to be "running and ready"
    Jul 29 16:47:44.329: INFO: Pod "kube-controller-manager-wa4quivohpee-2": Phase="Running", Reason="", readiness=true. Elapsed: 8.562645ms
    Jul 29 16:47:44.329: INFO: The phase of Pod kube-controller-manager-wa4quivohpee-2 is Running (Ready = true)
    Jul 29 16:47:44.329: INFO: Pod "kube-controller-manager-wa4quivohpee-2" satisfied condition "running and ready"
    Jul 29 16:47:44.602: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Jul 29 16:47:44.602: INFO: Deleting pod "simpletest-rc-to-be-deleted-26pjm" in namespace "gc-4655"
    Jul 29 16:47:44.677: INFO: Deleting pod "simpletest-rc-to-be-deleted-27rj8" in namespace "gc-4655"
    Jul 29 16:47:44.773: INFO: Deleting pod "simpletest-rc-to-be-deleted-2cldr" in namespace "gc-4655"
    Jul 29 16:47:44.868: INFO: Deleting pod "simpletest-rc-to-be-deleted-2lfrt" in namespace "gc-4655"
    Jul 29 16:47:44.913: INFO: Deleting pod "simpletest-rc-to-be-deleted-4spqh" in namespace "gc-4655"
    Jul 29 16:47:44.946: INFO: Deleting pod "simpletest-rc-to-be-deleted-584hl" in namespace "gc-4655"
    Jul 29 16:47:45.002: INFO: Deleting pod "simpletest-rc-to-be-deleted-5k56g" in namespace "gc-4655"
    Jul 29 16:47:45.045: INFO: Deleting pod "simpletest-rc-to-be-deleted-5l58w" in namespace "gc-4655"
    Jul 29 16:47:45.109: INFO: Deleting pod "simpletest-rc-to-be-deleted-5mz7t" in namespace "gc-4655"
    Jul 29 16:47:45.157: INFO: Deleting pod "simpletest-rc-to-be-deleted-5npds" in namespace "gc-4655"
    Jul 29 16:47:45.296: INFO: Deleting pod "simpletest-rc-to-be-deleted-5vj9v" in namespace "gc-4655"
    Jul 29 16:47:45.421: INFO: Deleting pod "simpletest-rc-to-be-deleted-5wfsf" in namespace "gc-4655"
    Jul 29 16:47:45.532: INFO: Deleting pod "simpletest-rc-to-be-deleted-68lgl" in namespace "gc-4655"
    Jul 29 16:47:45.603: INFO: Deleting pod "simpletest-rc-to-be-deleted-68sz5" in namespace "gc-4655"
    Jul 29 16:47:45.686: INFO: Deleting pod "simpletest-rc-to-be-deleted-68x49" in namespace "gc-4655"
    Jul 29 16:47:45.778: INFO: Deleting pod "simpletest-rc-to-be-deleted-6r7h5" in namespace "gc-4655"
    Jul 29 16:47:45.844: INFO: Deleting pod "simpletest-rc-to-be-deleted-86vsx" in namespace "gc-4655"
    Jul 29 16:47:45.945: INFO: Deleting pod "simpletest-rc-to-be-deleted-8fpf5" in namespace "gc-4655"
    Jul 29 16:47:46.047: INFO: Deleting pod "simpletest-rc-to-be-deleted-8tjqk" in namespace "gc-4655"
    Jul 29 16:47:46.112: INFO: Deleting pod "simpletest-rc-to-be-deleted-9c4sp" in namespace "gc-4655"
    Jul 29 16:47:46.234: INFO: Deleting pod "simpletest-rc-to-be-deleted-9dqvx" in namespace "gc-4655"
    Jul 29 16:47:46.291: INFO: Deleting pod "simpletest-rc-to-be-deleted-9f9z7" in namespace "gc-4655"
    Jul 29 16:47:46.355: INFO: Deleting pod "simpletest-rc-to-be-deleted-bdlwj" in namespace "gc-4655"
    Jul 29 16:47:46.453: INFO: Deleting pod "simpletest-rc-to-be-deleted-bwwm8" in namespace "gc-4655"
    Jul 29 16:47:46.548: INFO: Deleting pod "simpletest-rc-to-be-deleted-bz5lv" in namespace "gc-4655"
    Jul 29 16:47:46.792: INFO: Deleting pod "simpletest-rc-to-be-deleted-cdr5s" in namespace "gc-4655"
    Jul 29 16:47:46.879: INFO: Deleting pod "simpletest-rc-to-be-deleted-cgt22" in namespace "gc-4655"
    Jul 29 16:47:46.991: INFO: Deleting pod "simpletest-rc-to-be-deleted-ck9cs" in namespace "gc-4655"
    Jul 29 16:47:47.075: INFO: Deleting pod "simpletest-rc-to-be-deleted-cx4q4" in namespace "gc-4655"
    Jul 29 16:47:47.121: INFO: Deleting pod "simpletest-rc-to-be-deleted-dmpvk" in namespace "gc-4655"
    Jul 29 16:47:47.157: INFO: Deleting pod "simpletest-rc-to-be-deleted-drhrq" in namespace "gc-4655"
    Jul 29 16:47:47.207: INFO: Deleting pod "simpletest-rc-to-be-deleted-f2plm" in namespace "gc-4655"
    Jul 29 16:47:47.255: INFO: Deleting pod "simpletest-rc-to-be-deleted-fjmjk" in namespace "gc-4655"
    Jul 29 16:47:47.315: INFO: Deleting pod "simpletest-rc-to-be-deleted-fjnmt" in namespace "gc-4655"
    Jul 29 16:47:47.377: INFO: Deleting pod "simpletest-rc-to-be-deleted-fn5w9" in namespace "gc-4655"
    Jul 29 16:47:47.484: INFO: Deleting pod "simpletest-rc-to-be-deleted-fp8tf" in namespace "gc-4655"
    Jul 29 16:47:47.611: INFO: Deleting pod "simpletest-rc-to-be-deleted-fw7qz" in namespace "gc-4655"
    Jul 29 16:47:47.643: INFO: Deleting pod "simpletest-rc-to-be-deleted-g42ng" in namespace "gc-4655"
    Jul 29 16:47:47.884: INFO: Deleting pod "simpletest-rc-to-be-deleted-gdscg" in namespace "gc-4655"
    Jul 29 16:47:47.929: INFO: Deleting pod "simpletest-rc-to-be-deleted-gvqp4" in namespace "gc-4655"
    Jul 29 16:47:48.007: INFO: Deleting pod "simpletest-rc-to-be-deleted-hmm89" in namespace "gc-4655"
    Jul 29 16:47:48.110: INFO: Deleting pod "simpletest-rc-to-be-deleted-j776v" in namespace "gc-4655"
    Jul 29 16:47:48.180: INFO: Deleting pod "simpletest-rc-to-be-deleted-jkvt5" in namespace "gc-4655"
    Jul 29 16:47:48.227: INFO: Deleting pod "simpletest-rc-to-be-deleted-jldpm" in namespace "gc-4655"
    Jul 29 16:47:48.283: INFO: Deleting pod "simpletest-rc-to-be-deleted-jts7r" in namespace "gc-4655"
    Jul 29 16:47:48.336: INFO: Deleting pod "simpletest-rc-to-be-deleted-k4rwj" in namespace "gc-4655"
    Jul 29 16:47:48.422: INFO: Deleting pod "simpletest-rc-to-be-deleted-kkp8c" in namespace "gc-4655"
    Jul 29 16:47:48.576: INFO: Deleting pod "simpletest-rc-to-be-deleted-klw82" in namespace "gc-4655"
    Jul 29 16:47:48.608: INFO: Deleting pod "simpletest-rc-to-be-deleted-kmrt8" in namespace "gc-4655"
    Jul 29 16:47:48.725: INFO: Deleting pod "simpletest-rc-to-be-deleted-knd64" in namespace "gc-4655"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Jul 29 16:47:48.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-4655" for this suite. 07/29/23 16:47:48.82
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] DNS
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:47:48.85
Jul 29 16:47:48.850: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename dns 07/29/23 16:47:48.857
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:47:48.907
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:47:48.93
[It] should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 07/29/23 16:47:48.936
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 07/29/23 16:47:48.936
STEP: creating a pod to probe DNS 07/29/23 16:47:48.936
STEP: submitting the pod to kubernetes 07/29/23 16:47:48.937
Jul 29 16:47:48.976: INFO: Waiting up to 15m0s for pod "dns-test-eb0d017d-06b8-4691-9eaa-1d7b154dd35c" in namespace "dns-1033" to be "running"
Jul 29 16:47:49.022: INFO: Pod "dns-test-eb0d017d-06b8-4691-9eaa-1d7b154dd35c": Phase="Pending", Reason="", readiness=false. Elapsed: 46.710203ms
Jul 29 16:47:51.030: INFO: Pod "dns-test-eb0d017d-06b8-4691-9eaa-1d7b154dd35c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.054089218s
Jul 29 16:47:53.037: INFO: Pod "dns-test-eb0d017d-06b8-4691-9eaa-1d7b154dd35c": Phase="Running", Reason="", readiness=true. Elapsed: 4.061793081s
Jul 29 16:47:53.038: INFO: Pod "dns-test-eb0d017d-06b8-4691-9eaa-1d7b154dd35c" satisfied condition "running"
STEP: retrieving the pod 07/29/23 16:47:53.038
STEP: looking for the results for each expected name from probers 07/29/23 16:47:53.05
Jul 29 16:47:53.131: INFO: DNS probes using dns-1033/dns-test-eb0d017d-06b8-4691-9eaa-1d7b154dd35c succeeded

STEP: deleting the pod 07/29/23 16:47:53.131
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jul 29 16:47:53.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1033" for this suite. 07/29/23 16:47:53.392
{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","completed":305,"skipped":5518,"failed":0}
------------------------------
â€¢ [4.584 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:47:48.85
    Jul 29 16:47:48.850: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename dns 07/29/23 16:47:48.857
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:47:48.907
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:47:48.93
    [It] should provide DNS for the cluster  [Conformance]
      test/e2e/network/dns.go:50
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     07/29/23 16:47:48.936
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     07/29/23 16:47:48.936
    STEP: creating a pod to probe DNS 07/29/23 16:47:48.936
    STEP: submitting the pod to kubernetes 07/29/23 16:47:48.937
    Jul 29 16:47:48.976: INFO: Waiting up to 15m0s for pod "dns-test-eb0d017d-06b8-4691-9eaa-1d7b154dd35c" in namespace "dns-1033" to be "running"
    Jul 29 16:47:49.022: INFO: Pod "dns-test-eb0d017d-06b8-4691-9eaa-1d7b154dd35c": Phase="Pending", Reason="", readiness=false. Elapsed: 46.710203ms
    Jul 29 16:47:51.030: INFO: Pod "dns-test-eb0d017d-06b8-4691-9eaa-1d7b154dd35c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.054089218s
    Jul 29 16:47:53.037: INFO: Pod "dns-test-eb0d017d-06b8-4691-9eaa-1d7b154dd35c": Phase="Running", Reason="", readiness=true. Elapsed: 4.061793081s
    Jul 29 16:47:53.038: INFO: Pod "dns-test-eb0d017d-06b8-4691-9eaa-1d7b154dd35c" satisfied condition "running"
    STEP: retrieving the pod 07/29/23 16:47:53.038
    STEP: looking for the results for each expected name from probers 07/29/23 16:47:53.05
    Jul 29 16:47:53.131: INFO: DNS probes using dns-1033/dns-test-eb0d017d-06b8-4691-9eaa-1d7b154dd35c succeeded

    STEP: deleting the pod 07/29/23 16:47:53.131
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jul 29 16:47:53.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-1033" for this suite. 07/29/23 16:47:53.392
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:47:53.457
Jul 29 16:47:53.457: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename crd-publish-openapi 07/29/23 16:47:53.46
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:47:53.524
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:47:53.528
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193
Jul 29 16:47:53.534: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 07/29/23 16:48:00.247
Jul 29 16:48:00.248: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=crd-publish-openapi-508 --namespace=crd-publish-openapi-508 create -f -'
Jul 29 16:48:01.857: INFO: stderr: ""
Jul 29 16:48:01.857: INFO: stdout: "e2e-test-crd-publish-openapi-2658-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jul 29 16:48:01.858: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=crd-publish-openapi-508 --namespace=crd-publish-openapi-508 delete e2e-test-crd-publish-openapi-2658-crds test-cr'
Jul 29 16:48:02.006: INFO: stderr: ""
Jul 29 16:48:02.006: INFO: stdout: "e2e-test-crd-publish-openapi-2658-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Jul 29 16:48:02.007: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=crd-publish-openapi-508 --namespace=crd-publish-openapi-508 apply -f -'
Jul 29 16:48:02.403: INFO: stderr: ""
Jul 29 16:48:02.403: INFO: stdout: "e2e-test-crd-publish-openapi-2658-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jul 29 16:48:02.403: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=crd-publish-openapi-508 --namespace=crd-publish-openapi-508 delete e2e-test-crd-publish-openapi-2658-crds test-cr'
Jul 29 16:48:02.696: INFO: stderr: ""
Jul 29 16:48:02.696: INFO: stdout: "e2e-test-crd-publish-openapi-2658-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 07/29/23 16:48:02.696
Jul 29 16:48:02.697: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=crd-publish-openapi-508 explain e2e-test-crd-publish-openapi-2658-crds'
Jul 29 16:48:03.845: INFO: stderr: ""
Jul 29 16:48:03.845: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2658-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jul 29 16:48:09.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-508" for this suite. 07/29/23 16:48:09.673
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","completed":306,"skipped":5585,"failed":0}
------------------------------
â€¢ [SLOW TEST] [16.230 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:47:53.457
    Jul 29 16:47:53.457: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename crd-publish-openapi 07/29/23 16:47:53.46
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:47:53.524
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:47:53.528
    [It] works for CRD preserving unknown fields at the schema root [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:193
    Jul 29 16:47:53.534: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 07/29/23 16:48:00.247
    Jul 29 16:48:00.248: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=crd-publish-openapi-508 --namespace=crd-publish-openapi-508 create -f -'
    Jul 29 16:48:01.857: INFO: stderr: ""
    Jul 29 16:48:01.857: INFO: stdout: "e2e-test-crd-publish-openapi-2658-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Jul 29 16:48:01.858: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=crd-publish-openapi-508 --namespace=crd-publish-openapi-508 delete e2e-test-crd-publish-openapi-2658-crds test-cr'
    Jul 29 16:48:02.006: INFO: stderr: ""
    Jul 29 16:48:02.006: INFO: stdout: "e2e-test-crd-publish-openapi-2658-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    Jul 29 16:48:02.007: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=crd-publish-openapi-508 --namespace=crd-publish-openapi-508 apply -f -'
    Jul 29 16:48:02.403: INFO: stderr: ""
    Jul 29 16:48:02.403: INFO: stdout: "e2e-test-crd-publish-openapi-2658-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Jul 29 16:48:02.403: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=crd-publish-openapi-508 --namespace=crd-publish-openapi-508 delete e2e-test-crd-publish-openapi-2658-crds test-cr'
    Jul 29 16:48:02.696: INFO: stderr: ""
    Jul 29 16:48:02.696: INFO: stdout: "e2e-test-crd-publish-openapi-2658-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 07/29/23 16:48:02.696
    Jul 29 16:48:02.697: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=crd-publish-openapi-508 explain e2e-test-crd-publish-openapi-2658-crds'
    Jul 29 16:48:03.845: INFO: stderr: ""
    Jul 29 16:48:03.845: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2658-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jul 29 16:48:09.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-508" for this suite. 07/29/23 16:48:09.673
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:48:09.69
Jul 29 16:48:09.690: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename webhook 07/29/23 16:48:09.692
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:48:09.723
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:48:09.727
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 07/29/23 16:48:09.747
STEP: Create role binding to let webhook read extension-apiserver-authentication 07/29/23 16:48:10.383
STEP: Deploying the webhook pod 07/29/23 16:48:10.398
STEP: Wait for the deployment to be ready 07/29/23 16:48:10.419
Jul 29 16:48:10.438: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 07/29/23 16:48:12.458
STEP: Verifying the service has paired with the endpoint 07/29/23 16:48:12.492
Jul 29 16:48:13.494: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220
Jul 29 16:48:13.500: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Registering the custom resource webhook via the AdmissionRegistration API 07/29/23 16:48:14.021
STEP: Creating a custom resource that should be denied by the webhook 07/29/23 16:48:14.05
STEP: Creating a custom resource whose deletion would be denied by the webhook 07/29/23 16:48:16.208
STEP: Updating the custom resource with disallowed data should be denied 07/29/23 16:48:16.223
STEP: Deleting the custom resource should be denied 07/29/23 16:48:16.244
STEP: Remove the offending key and value from the custom resource data 07/29/23 16:48:16.257
STEP: Deleting the updated custom resource should be successful 07/29/23 16:48:16.275
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jul 29 16:48:16.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6381" for this suite. 07/29/23 16:48:16.832
STEP: Destroying namespace "webhook-6381-markers" for this suite. 07/29/23 16:48:16.843
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","completed":307,"skipped":5595,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.276 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:48:09.69
    Jul 29 16:48:09.690: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename webhook 07/29/23 16:48:09.692
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:48:09.723
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:48:09.727
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 07/29/23 16:48:09.747
    STEP: Create role binding to let webhook read extension-apiserver-authentication 07/29/23 16:48:10.383
    STEP: Deploying the webhook pod 07/29/23 16:48:10.398
    STEP: Wait for the deployment to be ready 07/29/23 16:48:10.419
    Jul 29 16:48:10.438: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 07/29/23 16:48:12.458
    STEP: Verifying the service has paired with the endpoint 07/29/23 16:48:12.492
    Jul 29 16:48:13.494: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny custom resource creation, update and deletion [Conformance]
      test/e2e/apimachinery/webhook.go:220
    Jul 29 16:48:13.500: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Registering the custom resource webhook via the AdmissionRegistration API 07/29/23 16:48:14.021
    STEP: Creating a custom resource that should be denied by the webhook 07/29/23 16:48:14.05
    STEP: Creating a custom resource whose deletion would be denied by the webhook 07/29/23 16:48:16.208
    STEP: Updating the custom resource with disallowed data should be denied 07/29/23 16:48:16.223
    STEP: Deleting the custom resource should be denied 07/29/23 16:48:16.244
    STEP: Remove the offending key and value from the custom resource data 07/29/23 16:48:16.257
    STEP: Deleting the updated custom resource should be successful 07/29/23 16:48:16.275
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jul 29 16:48:16.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-6381" for this suite. 07/29/23 16:48:16.832
    STEP: Destroying namespace "webhook-6381-markers" for this suite. 07/29/23 16:48:16.843
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:48:16.974
Jul 29 16:48:16.974: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename downward-api 07/29/23 16:48:16.981
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:48:17.046
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:48:17.055
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192
STEP: Creating a pod to test downward API volume plugin 07/29/23 16:48:17.115
Jul 29 16:48:17.171: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ac7ab5c9-8a1e-49e6-94ac-651c236e418b" in namespace "downward-api-2366" to be "Succeeded or Failed"
Jul 29 16:48:17.297: INFO: Pod "downwardapi-volume-ac7ab5c9-8a1e-49e6-94ac-651c236e418b": Phase="Pending", Reason="", readiness=false. Elapsed: 125.979988ms
Jul 29 16:48:19.311: INFO: Pod "downwardapi-volume-ac7ab5c9-8a1e-49e6-94ac-651c236e418b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.140597452s
Jul 29 16:48:21.306: INFO: Pod "downwardapi-volume-ac7ab5c9-8a1e-49e6-94ac-651c236e418b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.134988996s
STEP: Saw pod success 07/29/23 16:48:21.306
Jul 29 16:48:21.306: INFO: Pod "downwardapi-volume-ac7ab5c9-8a1e-49e6-94ac-651c236e418b" satisfied condition "Succeeded or Failed"
Jul 29 16:48:21.316: INFO: Trying to get logs from node wa4quivohpee-3 pod downwardapi-volume-ac7ab5c9-8a1e-49e6-94ac-651c236e418b container client-container: <nil>
STEP: delete the pod 07/29/23 16:48:21.348
Jul 29 16:48:21.379: INFO: Waiting for pod downwardapi-volume-ac7ab5c9-8a1e-49e6-94ac-651c236e418b to disappear
Jul 29 16:48:21.384: INFO: Pod downwardapi-volume-ac7ab5c9-8a1e-49e6-94ac-651c236e418b no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jul 29 16:48:21.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2366" for this suite. 07/29/23 16:48:21.395
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","completed":308,"skipped":5625,"failed":0}
------------------------------
â€¢ [4.431 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:48:16.974
    Jul 29 16:48:16.974: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename downward-api 07/29/23 16:48:16.981
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:48:17.046
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:48:17.055
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:192
    STEP: Creating a pod to test downward API volume plugin 07/29/23 16:48:17.115
    Jul 29 16:48:17.171: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ac7ab5c9-8a1e-49e6-94ac-651c236e418b" in namespace "downward-api-2366" to be "Succeeded or Failed"
    Jul 29 16:48:17.297: INFO: Pod "downwardapi-volume-ac7ab5c9-8a1e-49e6-94ac-651c236e418b": Phase="Pending", Reason="", readiness=false. Elapsed: 125.979988ms
    Jul 29 16:48:19.311: INFO: Pod "downwardapi-volume-ac7ab5c9-8a1e-49e6-94ac-651c236e418b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.140597452s
    Jul 29 16:48:21.306: INFO: Pod "downwardapi-volume-ac7ab5c9-8a1e-49e6-94ac-651c236e418b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.134988996s
    STEP: Saw pod success 07/29/23 16:48:21.306
    Jul 29 16:48:21.306: INFO: Pod "downwardapi-volume-ac7ab5c9-8a1e-49e6-94ac-651c236e418b" satisfied condition "Succeeded or Failed"
    Jul 29 16:48:21.316: INFO: Trying to get logs from node wa4quivohpee-3 pod downwardapi-volume-ac7ab5c9-8a1e-49e6-94ac-651c236e418b container client-container: <nil>
    STEP: delete the pod 07/29/23 16:48:21.348
    Jul 29 16:48:21.379: INFO: Waiting for pod downwardapi-volume-ac7ab5c9-8a1e-49e6-94ac-651c236e418b to disappear
    Jul 29 16:48:21.384: INFO: Pod downwardapi-volume-ac7ab5c9-8a1e-49e6-94ac-651c236e418b no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jul 29 16:48:21.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-2366" for this suite. 07/29/23 16:48:21.395
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:48:21.417
Jul 29 16:48:21.417: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename crd-publish-openapi 07/29/23 16:48:21.419
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:48:21.449
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:48:21.454
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441
STEP: set up a multi version CRD 07/29/23 16:48:21.459
Jul 29 16:48:21.460: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: mark a version not serverd 07/29/23 16:48:34.413
STEP: check the unserved version gets removed 07/29/23 16:48:34.458
STEP: check the other version is not changed 07/29/23 16:48:39.821
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jul 29 16:48:50.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3289" for this suite. 07/29/23 16:48:50.412
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","completed":309,"skipped":5678,"failed":0}
------------------------------
â€¢ [SLOW TEST] [29.009 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:48:21.417
    Jul 29 16:48:21.417: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename crd-publish-openapi 07/29/23 16:48:21.419
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:48:21.449
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:48:21.454
    [It] removes definition from spec when one version gets changed to not be served [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:441
    STEP: set up a multi version CRD 07/29/23 16:48:21.459
    Jul 29 16:48:21.460: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: mark a version not serverd 07/29/23 16:48:34.413
    STEP: check the unserved version gets removed 07/29/23 16:48:34.458
    STEP: check the other version is not changed 07/29/23 16:48:39.821
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jul 29 16:48:50.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-3289" for this suite. 07/29/23 16:48:50.412
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:48:50.434
Jul 29 16:48:50.435: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename projected 07/29/23 16:48:50.437
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:48:50.466
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:48:50.469
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56
STEP: Creating configMap with name projected-configmap-test-volume-25ce3ae3-fc29-4d10-94c2-007fd4e27f0b 07/29/23 16:48:50.474
STEP: Creating a pod to test consume configMaps 07/29/23 16:48:50.483
Jul 29 16:48:50.503: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5dee252d-ece7-4b22-8731-9a751007224d" in namespace "projected-624" to be "Succeeded or Failed"
Jul 29 16:48:50.511: INFO: Pod "pod-projected-configmaps-5dee252d-ece7-4b22-8731-9a751007224d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.758604ms
Jul 29 16:48:52.540: INFO: Pod "pod-projected-configmaps-5dee252d-ece7-4b22-8731-9a751007224d": Phase="Running", Reason="", readiness=false. Elapsed: 2.03685717s
Jul 29 16:48:54.520: INFO: Pod "pod-projected-configmaps-5dee252d-ece7-4b22-8731-9a751007224d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016759902s
STEP: Saw pod success 07/29/23 16:48:54.52
Jul 29 16:48:54.520: INFO: Pod "pod-projected-configmaps-5dee252d-ece7-4b22-8731-9a751007224d" satisfied condition "Succeeded or Failed"
Jul 29 16:48:54.527: INFO: Trying to get logs from node wa4quivohpee-3 pod pod-projected-configmaps-5dee252d-ece7-4b22-8731-9a751007224d container agnhost-container: <nil>
STEP: delete the pod 07/29/23 16:48:54.542
Jul 29 16:48:54.563: INFO: Waiting for pod pod-projected-configmaps-5dee252d-ece7-4b22-8731-9a751007224d to disappear
Jul 29 16:48:54.570: INFO: Pod pod-projected-configmaps-5dee252d-ece7-4b22-8731-9a751007224d no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jul 29 16:48:54.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-624" for this suite. 07/29/23 16:48:54.58
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":310,"skipped":5704,"failed":0}
------------------------------
â€¢ [4.158 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:48:50.434
    Jul 29 16:48:50.435: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename projected 07/29/23 16:48:50.437
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:48:50.466
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:48:50.469
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:56
    STEP: Creating configMap with name projected-configmap-test-volume-25ce3ae3-fc29-4d10-94c2-007fd4e27f0b 07/29/23 16:48:50.474
    STEP: Creating a pod to test consume configMaps 07/29/23 16:48:50.483
    Jul 29 16:48:50.503: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5dee252d-ece7-4b22-8731-9a751007224d" in namespace "projected-624" to be "Succeeded or Failed"
    Jul 29 16:48:50.511: INFO: Pod "pod-projected-configmaps-5dee252d-ece7-4b22-8731-9a751007224d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.758604ms
    Jul 29 16:48:52.540: INFO: Pod "pod-projected-configmaps-5dee252d-ece7-4b22-8731-9a751007224d": Phase="Running", Reason="", readiness=false. Elapsed: 2.03685717s
    Jul 29 16:48:54.520: INFO: Pod "pod-projected-configmaps-5dee252d-ece7-4b22-8731-9a751007224d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016759902s
    STEP: Saw pod success 07/29/23 16:48:54.52
    Jul 29 16:48:54.520: INFO: Pod "pod-projected-configmaps-5dee252d-ece7-4b22-8731-9a751007224d" satisfied condition "Succeeded or Failed"
    Jul 29 16:48:54.527: INFO: Trying to get logs from node wa4quivohpee-3 pod pod-projected-configmaps-5dee252d-ece7-4b22-8731-9a751007224d container agnhost-container: <nil>
    STEP: delete the pod 07/29/23 16:48:54.542
    Jul 29 16:48:54.563: INFO: Waiting for pod pod-projected-configmaps-5dee252d-ece7-4b22-8731-9a751007224d to disappear
    Jul 29 16:48:54.570: INFO: Pod pod-projected-configmaps-5dee252d-ece7-4b22-8731-9a751007224d no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jul 29 16:48:54.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-624" for this suite. 07/29/23 16:48:54.58
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:97
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:48:54.601
Jul 29 16:48:54.602: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename container-lifecycle-hook 07/29/23 16:48:54.605
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:48:54.641
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:48:54.645
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 07/29/23 16:48:54.658
Jul 29 16:48:54.700: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-6580" to be "running and ready"
Jul 29 16:48:54.706: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 5.893878ms
Jul 29 16:48:54.706: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jul 29 16:48:56.714: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.013799409s
Jul 29 16:48:56.715: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jul 29 16:48:56.715: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:97
STEP: create the pod with lifecycle hook 07/29/23 16:48:56.727
Jul 29 16:48:56.742: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-6580" to be "running and ready"
Jul 29 16:48:56.747: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 5.343606ms
Jul 29 16:48:56.748: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jul 29 16:48:58.756: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.014138427s
Jul 29 16:48:58.756: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
Jul 29 16:48:58.757: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
STEP: check poststart hook 07/29/23 16:48:58.763
STEP: delete the pod with lifecycle hook 07/29/23 16:48:58.777
Jul 29 16:48:58.790: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 29 16:48:58.799: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 29 16:49:00.799: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 29 16:49:00.807: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 29 16:49:02.799: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 29 16:49:02.812: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Jul 29 16:49:02.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6580" for this suite. 07/29/23 16:49:02.822
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","completed":311,"skipped":5730,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.235 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:48:54.601
    Jul 29 16:48:54.602: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename container-lifecycle-hook 07/29/23 16:48:54.605
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:48:54.641
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:48:54.645
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 07/29/23 16:48:54.658
    Jul 29 16:48:54.700: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-6580" to be "running and ready"
    Jul 29 16:48:54.706: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 5.893878ms
    Jul 29 16:48:54.706: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jul 29 16:48:56.714: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.013799409s
    Jul 29 16:48:56.715: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jul 29 16:48:56.715: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:97
    STEP: create the pod with lifecycle hook 07/29/23 16:48:56.727
    Jul 29 16:48:56.742: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-6580" to be "running and ready"
    Jul 29 16:48:56.747: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 5.343606ms
    Jul 29 16:48:56.748: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Jul 29 16:48:58.756: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.014138427s
    Jul 29 16:48:58.756: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
    Jul 29 16:48:58.757: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
    STEP: check poststart hook 07/29/23 16:48:58.763
    STEP: delete the pod with lifecycle hook 07/29/23 16:48:58.777
    Jul 29 16:48:58.790: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Jul 29 16:48:58.799: INFO: Pod pod-with-poststart-exec-hook still exists
    Jul 29 16:49:00.799: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Jul 29 16:49:00.807: INFO: Pod pod-with-poststart-exec-hook still exists
    Jul 29 16:49:02.799: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Jul 29 16:49:02.812: INFO: Pod pod-with-poststart-exec-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Jul 29 16:49:02.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-6580" for this suite. 07/29/23 16:49:02.822
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] ConfigMap
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:49:02.84
Jul 29 16:49:02.840: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename configmap 07/29/23 16:49:02.843
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:49:02.876
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:49:02.881
[It] should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168
STEP: creating a ConfigMap 07/29/23 16:49:02.889
STEP: fetching the ConfigMap 07/29/23 16:49:02.899
STEP: patching the ConfigMap 07/29/23 16:49:02.906
STEP: listing all ConfigMaps in all namespaces with a label selector 07/29/23 16:49:02.918
STEP: deleting the ConfigMap by collection with a label selector 07/29/23 16:49:02.926
STEP: listing all ConfigMaps in test namespace 07/29/23 16:49:02.943
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Jul 29 16:49:02.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-793" for this suite. 07/29/23 16:49:02.966
{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","completed":312,"skipped":5732,"failed":0}
------------------------------
â€¢ [0.147 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:49:02.84
    Jul 29 16:49:02.840: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename configmap 07/29/23 16:49:02.843
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:49:02.876
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:49:02.881
    [It] should run through a ConfigMap lifecycle [Conformance]
      test/e2e/common/node/configmap.go:168
    STEP: creating a ConfigMap 07/29/23 16:49:02.889
    STEP: fetching the ConfigMap 07/29/23 16:49:02.899
    STEP: patching the ConfigMap 07/29/23 16:49:02.906
    STEP: listing all ConfigMaps in all namespaces with a label selector 07/29/23 16:49:02.918
    STEP: deleting the ConfigMap by collection with a label selector 07/29/23 16:49:02.926
    STEP: listing all ConfigMaps in test namespace 07/29/23 16:49:02.943
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Jul 29 16:49:02.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-793" for this suite. 07/29/23 16:49:02.966
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:49:02.996
Jul 29 16:49:02.996: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename projected 07/29/23 16:49:03.008
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:49:03.066
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:49:03.071
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260
STEP: Creating a pod to test downward API volume plugin 07/29/23 16:49:03.078
Jul 29 16:49:03.096: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d9347500-6e48-4912-b39f-902868672db0" in namespace "projected-5601" to be "Succeeded or Failed"
Jul 29 16:49:03.102: INFO: Pod "downwardapi-volume-d9347500-6e48-4912-b39f-902868672db0": Phase="Pending", Reason="", readiness=false. Elapsed: 5.851466ms
Jul 29 16:49:05.109: INFO: Pod "downwardapi-volume-d9347500-6e48-4912-b39f-902868672db0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012395776s
Jul 29 16:49:07.112: INFO: Pod "downwardapi-volume-d9347500-6e48-4912-b39f-902868672db0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015753034s
STEP: Saw pod success 07/29/23 16:49:07.112
Jul 29 16:49:07.112: INFO: Pod "downwardapi-volume-d9347500-6e48-4912-b39f-902868672db0" satisfied condition "Succeeded or Failed"
Jul 29 16:49:07.118: INFO: Trying to get logs from node wa4quivohpee-3 pod downwardapi-volume-d9347500-6e48-4912-b39f-902868672db0 container client-container: <nil>
STEP: delete the pod 07/29/23 16:49:07.127
Jul 29 16:49:07.148: INFO: Waiting for pod downwardapi-volume-d9347500-6e48-4912-b39f-902868672db0 to disappear
Jul 29 16:49:07.154: INFO: Pod downwardapi-volume-d9347500-6e48-4912-b39f-902868672db0 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jul 29 16:49:07.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5601" for this suite. 07/29/23 16:49:07.161
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","completed":313,"skipped":5748,"failed":0}
------------------------------
â€¢ [4.177 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:49:02.996
    Jul 29 16:49:02.996: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename projected 07/29/23 16:49:03.008
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:49:03.066
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:49:03.071
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:260
    STEP: Creating a pod to test downward API volume plugin 07/29/23 16:49:03.078
    Jul 29 16:49:03.096: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d9347500-6e48-4912-b39f-902868672db0" in namespace "projected-5601" to be "Succeeded or Failed"
    Jul 29 16:49:03.102: INFO: Pod "downwardapi-volume-d9347500-6e48-4912-b39f-902868672db0": Phase="Pending", Reason="", readiness=false. Elapsed: 5.851466ms
    Jul 29 16:49:05.109: INFO: Pod "downwardapi-volume-d9347500-6e48-4912-b39f-902868672db0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012395776s
    Jul 29 16:49:07.112: INFO: Pod "downwardapi-volume-d9347500-6e48-4912-b39f-902868672db0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015753034s
    STEP: Saw pod success 07/29/23 16:49:07.112
    Jul 29 16:49:07.112: INFO: Pod "downwardapi-volume-d9347500-6e48-4912-b39f-902868672db0" satisfied condition "Succeeded or Failed"
    Jul 29 16:49:07.118: INFO: Trying to get logs from node wa4quivohpee-3 pod downwardapi-volume-d9347500-6e48-4912-b39f-902868672db0 container client-container: <nil>
    STEP: delete the pod 07/29/23 16:49:07.127
    Jul 29 16:49:07.148: INFO: Waiting for pod downwardapi-volume-d9347500-6e48-4912-b39f-902868672db0 to disappear
    Jul 29 16:49:07.154: INFO: Pod downwardapi-volume-d9347500-6e48-4912-b39f-902868672db0 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jul 29 16:49:07.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5601" for this suite. 07/29/23 16:49:07.161
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:49:07.175
Jul 29 16:49:07.175: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename webhook 07/29/23 16:49:07.178
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:49:07.201
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:49:07.211
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 07/29/23 16:49:07.255
STEP: Create role binding to let webhook read extension-apiserver-authentication 07/29/23 16:49:07.78
STEP: Deploying the webhook pod 07/29/23 16:49:07.797
STEP: Wait for the deployment to be ready 07/29/23 16:49:07.812
Jul 29 16:49:07.826: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 07/29/23 16:49:09.847
STEP: Verifying the service has paired with the endpoint 07/29/23 16:49:09.861
Jul 29 16:49:10.861: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581
STEP: Listing all of the created validation webhooks 07/29/23 16:49:10.962
STEP: Creating a configMap that does not comply to the validation webhook rules 07/29/23 16:49:11.03
STEP: Deleting the collection of validation webhooks 07/29/23 16:49:11.107
STEP: Creating a configMap that does not comply to the validation webhook rules 07/29/23 16:49:11.198
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jul 29 16:49:11.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7242" for this suite. 07/29/23 16:49:11.222
STEP: Destroying namespace "webhook-7242-markers" for this suite. 07/29/23 16:49:11.231
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","completed":314,"skipped":5753,"failed":0}
------------------------------
â€¢ [4.166 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:49:07.175
    Jul 29 16:49:07.175: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename webhook 07/29/23 16:49:07.178
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:49:07.201
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:49:07.211
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 07/29/23 16:49:07.255
    STEP: Create role binding to let webhook read extension-apiserver-authentication 07/29/23 16:49:07.78
    STEP: Deploying the webhook pod 07/29/23 16:49:07.797
    STEP: Wait for the deployment to be ready 07/29/23 16:49:07.812
    Jul 29 16:49:07.826: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 07/29/23 16:49:09.847
    STEP: Verifying the service has paired with the endpoint 07/29/23 16:49:09.861
    Jul 29 16:49:10.861: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing validating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:581
    STEP: Listing all of the created validation webhooks 07/29/23 16:49:10.962
    STEP: Creating a configMap that does not comply to the validation webhook rules 07/29/23 16:49:11.03
    STEP: Deleting the collection of validation webhooks 07/29/23 16:49:11.107
    STEP: Creating a configMap that does not comply to the validation webhook rules 07/29/23 16:49:11.198
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jul 29 16:49:11.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-7242" for this suite. 07/29/23 16:49:11.222
    STEP: Destroying namespace "webhook-7242-markers" for this suite. 07/29/23 16:49:11.231
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:49:11.352
Jul 29 16:49:11.354: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename crd-publish-openapi 07/29/23 16:49:11.358
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:49:11.399
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:49:11.405
[It] works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 07/29/23 16:49:11.418
Jul 29 16:49:11.419: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 07/29/23 16:49:31.181
Jul 29 16:49:31.184: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
Jul 29 16:49:36.793: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jul 29 16:49:56.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6952" for this suite. 07/29/23 16:49:56.636
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","completed":315,"skipped":5779,"failed":0}
------------------------------
â€¢ [SLOW TEST] [45.293 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:49:11.352
    Jul 29 16:49:11.354: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename crd-publish-openapi 07/29/23 16:49:11.358
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:49:11.399
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:49:11.405
    [It] works for multiple CRDs of same group but different versions [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:308
    STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 07/29/23 16:49:11.418
    Jul 29 16:49:11.419: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 07/29/23 16:49:31.181
    Jul 29 16:49:31.184: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    Jul 29 16:49:36.793: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jul 29 16:49:56.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-6952" for this suite. 07/29/23 16:49:56.636
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:350
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:49:56.652
Jul 29 16:49:56.652: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename kubectl 07/29/23 16:49:56.654
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:49:56.685
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:49:56.691
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:324
[It] should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:350
STEP: creating a replication controller 07/29/23 16:49:56.696
Jul 29 16:49:56.697: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-3862 create -f -'
Jul 29 16:49:57.823: INFO: stderr: ""
Jul 29 16:49:57.823: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 07/29/23 16:49:57.823
Jul 29 16:49:57.824: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-3862 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jul 29 16:49:58.003: INFO: stderr: ""
Jul 29 16:49:58.003: INFO: stdout: "update-demo-nautilus-jk7hd update-demo-nautilus-pz2kc "
Jul 29 16:49:58.004: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-3862 get pods update-demo-nautilus-jk7hd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jul 29 16:49:58.166: INFO: stderr: ""
Jul 29 16:49:58.166: INFO: stdout: ""
Jul 29 16:49:58.166: INFO: update-demo-nautilus-jk7hd is created but not running
Jul 29 16:50:03.166: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-3862 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jul 29 16:50:03.349: INFO: stderr: ""
Jul 29 16:50:03.349: INFO: stdout: "update-demo-nautilus-jk7hd update-demo-nautilus-pz2kc "
Jul 29 16:50:03.349: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-3862 get pods update-demo-nautilus-jk7hd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jul 29 16:50:03.494: INFO: stderr: ""
Jul 29 16:50:03.494: INFO: stdout: "true"
Jul 29 16:50:03.495: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-3862 get pods update-demo-nautilus-jk7hd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jul 29 16:50:03.614: INFO: stderr: ""
Jul 29 16:50:03.614: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jul 29 16:50:03.614: INFO: validating pod update-demo-nautilus-jk7hd
Jul 29 16:50:03.631: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 29 16:50:03.632: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 29 16:50:03.632: INFO: update-demo-nautilus-jk7hd is verified up and running
Jul 29 16:50:03.632: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-3862 get pods update-demo-nautilus-pz2kc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jul 29 16:50:03.791: INFO: stderr: ""
Jul 29 16:50:03.791: INFO: stdout: "true"
Jul 29 16:50:03.792: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-3862 get pods update-demo-nautilus-pz2kc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jul 29 16:50:03.910: INFO: stderr: ""
Jul 29 16:50:03.910: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jul 29 16:50:03.911: INFO: validating pod update-demo-nautilus-pz2kc
Jul 29 16:50:03.927: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 29 16:50:03.927: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 29 16:50:03.927: INFO: update-demo-nautilus-pz2kc is verified up and running
STEP: scaling down the replication controller 07/29/23 16:50:03.927
Jul 29 16:50:03.942: INFO: scanned /root for discovery docs: <nil>
Jul 29 16:50:03.942: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-3862 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Jul 29 16:50:05.168: INFO: stderr: ""
Jul 29 16:50:05.168: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 07/29/23 16:50:05.168
Jul 29 16:50:05.169: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-3862 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jul 29 16:50:05.293: INFO: stderr: ""
Jul 29 16:50:05.293: INFO: stdout: "update-demo-nautilus-pz2kc "
Jul 29 16:50:05.293: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-3862 get pods update-demo-nautilus-pz2kc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jul 29 16:50:05.429: INFO: stderr: ""
Jul 29 16:50:05.430: INFO: stdout: "true"
Jul 29 16:50:05.430: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-3862 get pods update-demo-nautilus-pz2kc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jul 29 16:50:05.559: INFO: stderr: ""
Jul 29 16:50:05.559: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jul 29 16:50:05.559: INFO: validating pod update-demo-nautilus-pz2kc
Jul 29 16:50:05.566: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 29 16:50:05.567: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 29 16:50:05.567: INFO: update-demo-nautilus-pz2kc is verified up and running
STEP: scaling up the replication controller 07/29/23 16:50:05.567
Jul 29 16:50:05.577: INFO: scanned /root for discovery docs: <nil>
Jul 29 16:50:05.577: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-3862 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Jul 29 16:50:06.741: INFO: stderr: ""
Jul 29 16:50:06.741: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 07/29/23 16:50:06.741
Jul 29 16:50:06.742: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-3862 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jul 29 16:50:06.893: INFO: stderr: ""
Jul 29 16:50:06.893: INFO: stdout: "update-demo-nautilus-26728 update-demo-nautilus-pz2kc "
Jul 29 16:50:06.893: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-3862 get pods update-demo-nautilus-26728 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jul 29 16:50:07.032: INFO: stderr: ""
Jul 29 16:50:07.032: INFO: stdout: "true"
Jul 29 16:50:07.033: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-3862 get pods update-demo-nautilus-26728 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jul 29 16:50:07.169: INFO: stderr: ""
Jul 29 16:50:07.169: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jul 29 16:50:07.169: INFO: validating pod update-demo-nautilus-26728
Jul 29 16:50:07.194: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 29 16:50:07.194: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 29 16:50:07.194: INFO: update-demo-nautilus-26728 is verified up and running
Jul 29 16:50:07.195: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-3862 get pods update-demo-nautilus-pz2kc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jul 29 16:50:07.368: INFO: stderr: ""
Jul 29 16:50:07.368: INFO: stdout: "true"
Jul 29 16:50:07.369: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-3862 get pods update-demo-nautilus-pz2kc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jul 29 16:50:07.514: INFO: stderr: ""
Jul 29 16:50:07.514: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jul 29 16:50:07.514: INFO: validating pod update-demo-nautilus-pz2kc
Jul 29 16:50:07.522: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 29 16:50:07.522: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 29 16:50:07.522: INFO: update-demo-nautilus-pz2kc is verified up and running
STEP: using delete to clean up resources 07/29/23 16:50:07.522
Jul 29 16:50:07.523: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-3862 delete --grace-period=0 --force -f -'
Jul 29 16:50:07.656: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 29 16:50:07.656: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jul 29 16:50:07.656: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-3862 get rc,svc -l name=update-demo --no-headers'
Jul 29 16:50:07.861: INFO: stderr: "No resources found in kubectl-3862 namespace.\n"
Jul 29 16:50:07.861: INFO: stdout: ""
Jul 29 16:50:07.862: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-3862 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul 29 16:50:08.025: INFO: stderr: ""
Jul 29 16:50:08.025: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jul 29 16:50:08.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3862" for this suite. 07/29/23 16:50:08.034
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","completed":316,"skipped":5793,"failed":0}
------------------------------
â€¢ [SLOW TEST] [11.407 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:322
    should scale a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:350

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:49:56.652
    Jul 29 16:49:56.652: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename kubectl 07/29/23 16:49:56.654
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:49:56.685
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:49:56.691
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:324
    [It] should scale a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:350
    STEP: creating a replication controller 07/29/23 16:49:56.696
    Jul 29 16:49:56.697: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-3862 create -f -'
    Jul 29 16:49:57.823: INFO: stderr: ""
    Jul 29 16:49:57.823: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 07/29/23 16:49:57.823
    Jul 29 16:49:57.824: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-3862 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jul 29 16:49:58.003: INFO: stderr: ""
    Jul 29 16:49:58.003: INFO: stdout: "update-demo-nautilus-jk7hd update-demo-nautilus-pz2kc "
    Jul 29 16:49:58.004: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-3862 get pods update-demo-nautilus-jk7hd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jul 29 16:49:58.166: INFO: stderr: ""
    Jul 29 16:49:58.166: INFO: stdout: ""
    Jul 29 16:49:58.166: INFO: update-demo-nautilus-jk7hd is created but not running
    Jul 29 16:50:03.166: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-3862 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jul 29 16:50:03.349: INFO: stderr: ""
    Jul 29 16:50:03.349: INFO: stdout: "update-demo-nautilus-jk7hd update-demo-nautilus-pz2kc "
    Jul 29 16:50:03.349: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-3862 get pods update-demo-nautilus-jk7hd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jul 29 16:50:03.494: INFO: stderr: ""
    Jul 29 16:50:03.494: INFO: stdout: "true"
    Jul 29 16:50:03.495: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-3862 get pods update-demo-nautilus-jk7hd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jul 29 16:50:03.614: INFO: stderr: ""
    Jul 29 16:50:03.614: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jul 29 16:50:03.614: INFO: validating pod update-demo-nautilus-jk7hd
    Jul 29 16:50:03.631: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jul 29 16:50:03.632: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jul 29 16:50:03.632: INFO: update-demo-nautilus-jk7hd is verified up and running
    Jul 29 16:50:03.632: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-3862 get pods update-demo-nautilus-pz2kc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jul 29 16:50:03.791: INFO: stderr: ""
    Jul 29 16:50:03.791: INFO: stdout: "true"
    Jul 29 16:50:03.792: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-3862 get pods update-demo-nautilus-pz2kc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jul 29 16:50:03.910: INFO: stderr: ""
    Jul 29 16:50:03.910: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jul 29 16:50:03.911: INFO: validating pod update-demo-nautilus-pz2kc
    Jul 29 16:50:03.927: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jul 29 16:50:03.927: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jul 29 16:50:03.927: INFO: update-demo-nautilus-pz2kc is verified up and running
    STEP: scaling down the replication controller 07/29/23 16:50:03.927
    Jul 29 16:50:03.942: INFO: scanned /root for discovery docs: <nil>
    Jul 29 16:50:03.942: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-3862 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
    Jul 29 16:50:05.168: INFO: stderr: ""
    Jul 29 16:50:05.168: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 07/29/23 16:50:05.168
    Jul 29 16:50:05.169: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-3862 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jul 29 16:50:05.293: INFO: stderr: ""
    Jul 29 16:50:05.293: INFO: stdout: "update-demo-nautilus-pz2kc "
    Jul 29 16:50:05.293: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-3862 get pods update-demo-nautilus-pz2kc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jul 29 16:50:05.429: INFO: stderr: ""
    Jul 29 16:50:05.430: INFO: stdout: "true"
    Jul 29 16:50:05.430: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-3862 get pods update-demo-nautilus-pz2kc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jul 29 16:50:05.559: INFO: stderr: ""
    Jul 29 16:50:05.559: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jul 29 16:50:05.559: INFO: validating pod update-demo-nautilus-pz2kc
    Jul 29 16:50:05.566: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jul 29 16:50:05.567: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jul 29 16:50:05.567: INFO: update-demo-nautilus-pz2kc is verified up and running
    STEP: scaling up the replication controller 07/29/23 16:50:05.567
    Jul 29 16:50:05.577: INFO: scanned /root for discovery docs: <nil>
    Jul 29 16:50:05.577: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-3862 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
    Jul 29 16:50:06.741: INFO: stderr: ""
    Jul 29 16:50:06.741: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 07/29/23 16:50:06.741
    Jul 29 16:50:06.742: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-3862 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jul 29 16:50:06.893: INFO: stderr: ""
    Jul 29 16:50:06.893: INFO: stdout: "update-demo-nautilus-26728 update-demo-nautilus-pz2kc "
    Jul 29 16:50:06.893: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-3862 get pods update-demo-nautilus-26728 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jul 29 16:50:07.032: INFO: stderr: ""
    Jul 29 16:50:07.032: INFO: stdout: "true"
    Jul 29 16:50:07.033: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-3862 get pods update-demo-nautilus-26728 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jul 29 16:50:07.169: INFO: stderr: ""
    Jul 29 16:50:07.169: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jul 29 16:50:07.169: INFO: validating pod update-demo-nautilus-26728
    Jul 29 16:50:07.194: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jul 29 16:50:07.194: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jul 29 16:50:07.194: INFO: update-demo-nautilus-26728 is verified up and running
    Jul 29 16:50:07.195: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-3862 get pods update-demo-nautilus-pz2kc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jul 29 16:50:07.368: INFO: stderr: ""
    Jul 29 16:50:07.368: INFO: stdout: "true"
    Jul 29 16:50:07.369: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-3862 get pods update-demo-nautilus-pz2kc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jul 29 16:50:07.514: INFO: stderr: ""
    Jul 29 16:50:07.514: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jul 29 16:50:07.514: INFO: validating pod update-demo-nautilus-pz2kc
    Jul 29 16:50:07.522: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jul 29 16:50:07.522: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jul 29 16:50:07.522: INFO: update-demo-nautilus-pz2kc is verified up and running
    STEP: using delete to clean up resources 07/29/23 16:50:07.522
    Jul 29 16:50:07.523: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-3862 delete --grace-period=0 --force -f -'
    Jul 29 16:50:07.656: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jul 29 16:50:07.656: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Jul 29 16:50:07.656: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-3862 get rc,svc -l name=update-demo --no-headers'
    Jul 29 16:50:07.861: INFO: stderr: "No resources found in kubectl-3862 namespace.\n"
    Jul 29 16:50:07.861: INFO: stdout: ""
    Jul 29 16:50:07.862: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-3862 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Jul 29 16:50:08.025: INFO: stderr: ""
    Jul 29 16:50:08.025: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jul 29 16:50:08.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-3862" for this suite. 07/29/23 16:50:08.034
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:733
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:50:08.077
Jul 29 16:50:08.077: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename sched-preemption 07/29/23 16:50:08.079
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:50:08.122
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:50:08.128
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Jul 29 16:50:08.161: INFO: Waiting up to 1m0s for all nodes to be ready
Jul 29 16:51:08.220: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:51:08.227
Jul 29 16:51:08.227: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename sched-preemption-path 07/29/23 16:51:08.23
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:51:08.294
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:51:08.3
[BeforeEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:690
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:733
Jul 29 16:51:08.337: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
Jul 29 16:51:08.346: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  test/e2e/framework/framework.go:187
Jul 29 16:51:08.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-9615" for this suite. 07/29/23 16:51:08.39
[AfterEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:706
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Jul 29 16:51:08.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-6481" for this suite. 07/29/23 16:51:08.429
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","completed":317,"skipped":5861,"failed":0}
------------------------------
â€¢ [SLOW TEST] [60.491 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  test/e2e/scheduling/preemption.go:683
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    test/e2e/scheduling/preemption.go:733

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:50:08.077
    Jul 29 16:50:08.077: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename sched-preemption 07/29/23 16:50:08.079
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:50:08.122
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:50:08.128
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Jul 29 16:50:08.161: INFO: Waiting up to 1m0s for all nodes to be ready
    Jul 29 16:51:08.220: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PriorityClass endpoints
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:51:08.227
    Jul 29 16:51:08.227: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename sched-preemption-path 07/29/23 16:51:08.23
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:51:08.294
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:51:08.3
    [BeforeEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:690
    [It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
      test/e2e/scheduling/preemption.go:733
    Jul 29 16:51:08.337: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
    Jul 29 16:51:08.346: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
    [AfterEach] PriorityClass endpoints
      test/e2e/framework/framework.go:187
    Jul 29 16:51:08.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-path-9615" for this suite. 07/29/23 16:51:08.39
    [AfterEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:706
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Jul 29 16:51:08.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-6481" for this suite. 07/29/23 16:51:08.429
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:51:08.57
Jul 29 16:51:08.570: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename container-probe 07/29/23 16:51:08.572
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:51:08.608
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:51:08.612
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68
Jul 29 16:51:08.635: INFO: Waiting up to 5m0s for pod "test-webserver-98f1fdca-3c6b-4ebb-b283-9ebcdfeb2bb2" in namespace "container-probe-5346" to be "running and ready"
Jul 29 16:51:08.644: INFO: Pod "test-webserver-98f1fdca-3c6b-4ebb-b283-9ebcdfeb2bb2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.364196ms
Jul 29 16:51:08.644: INFO: The phase of Pod test-webserver-98f1fdca-3c6b-4ebb-b283-9ebcdfeb2bb2 is Pending, waiting for it to be Running (with Ready = true)
Jul 29 16:51:10.656: INFO: Pod "test-webserver-98f1fdca-3c6b-4ebb-b283-9ebcdfeb2bb2": Phase="Running", Reason="", readiness=false. Elapsed: 2.020737365s
Jul 29 16:51:10.656: INFO: The phase of Pod test-webserver-98f1fdca-3c6b-4ebb-b283-9ebcdfeb2bb2 is Running (Ready = false)
Jul 29 16:51:12.653: INFO: Pod "test-webserver-98f1fdca-3c6b-4ebb-b283-9ebcdfeb2bb2": Phase="Running", Reason="", readiness=false. Elapsed: 4.017796821s
Jul 29 16:51:12.654: INFO: The phase of Pod test-webserver-98f1fdca-3c6b-4ebb-b283-9ebcdfeb2bb2 is Running (Ready = false)
Jul 29 16:51:14.677: INFO: Pod "test-webserver-98f1fdca-3c6b-4ebb-b283-9ebcdfeb2bb2": Phase="Running", Reason="", readiness=false. Elapsed: 6.041691628s
Jul 29 16:51:14.678: INFO: The phase of Pod test-webserver-98f1fdca-3c6b-4ebb-b283-9ebcdfeb2bb2 is Running (Ready = false)
Jul 29 16:51:16.655: INFO: Pod "test-webserver-98f1fdca-3c6b-4ebb-b283-9ebcdfeb2bb2": Phase="Running", Reason="", readiness=false. Elapsed: 8.01897792s
Jul 29 16:51:16.655: INFO: The phase of Pod test-webserver-98f1fdca-3c6b-4ebb-b283-9ebcdfeb2bb2 is Running (Ready = false)
Jul 29 16:51:18.687: INFO: Pod "test-webserver-98f1fdca-3c6b-4ebb-b283-9ebcdfeb2bb2": Phase="Running", Reason="", readiness=false. Elapsed: 10.05132373s
Jul 29 16:51:18.687: INFO: The phase of Pod test-webserver-98f1fdca-3c6b-4ebb-b283-9ebcdfeb2bb2 is Running (Ready = false)
Jul 29 16:51:20.653: INFO: Pod "test-webserver-98f1fdca-3c6b-4ebb-b283-9ebcdfeb2bb2": Phase="Running", Reason="", readiness=false. Elapsed: 12.016923267s
Jul 29 16:51:20.653: INFO: The phase of Pod test-webserver-98f1fdca-3c6b-4ebb-b283-9ebcdfeb2bb2 is Running (Ready = false)
Jul 29 16:51:22.697: INFO: Pod "test-webserver-98f1fdca-3c6b-4ebb-b283-9ebcdfeb2bb2": Phase="Running", Reason="", readiness=false. Elapsed: 14.060967146s
Jul 29 16:51:22.697: INFO: The phase of Pod test-webserver-98f1fdca-3c6b-4ebb-b283-9ebcdfeb2bb2 is Running (Ready = false)
Jul 29 16:51:24.654: INFO: Pod "test-webserver-98f1fdca-3c6b-4ebb-b283-9ebcdfeb2bb2": Phase="Running", Reason="", readiness=false. Elapsed: 16.018381686s
Jul 29 16:51:24.654: INFO: The phase of Pod test-webserver-98f1fdca-3c6b-4ebb-b283-9ebcdfeb2bb2 is Running (Ready = false)
Jul 29 16:51:26.653: INFO: Pod "test-webserver-98f1fdca-3c6b-4ebb-b283-9ebcdfeb2bb2": Phase="Running", Reason="", readiness=false. Elapsed: 18.017389162s
Jul 29 16:51:26.653: INFO: The phase of Pod test-webserver-98f1fdca-3c6b-4ebb-b283-9ebcdfeb2bb2 is Running (Ready = false)
Jul 29 16:51:28.652: INFO: Pod "test-webserver-98f1fdca-3c6b-4ebb-b283-9ebcdfeb2bb2": Phase="Running", Reason="", readiness=false. Elapsed: 20.016685205s
Jul 29 16:51:28.652: INFO: The phase of Pod test-webserver-98f1fdca-3c6b-4ebb-b283-9ebcdfeb2bb2 is Running (Ready = false)
Jul 29 16:51:30.677: INFO: Pod "test-webserver-98f1fdca-3c6b-4ebb-b283-9ebcdfeb2bb2": Phase="Running", Reason="", readiness=true. Elapsed: 22.041147537s
Jul 29 16:51:30.677: INFO: The phase of Pod test-webserver-98f1fdca-3c6b-4ebb-b283-9ebcdfeb2bb2 is Running (Ready = true)
Jul 29 16:51:30.677: INFO: Pod "test-webserver-98f1fdca-3c6b-4ebb-b283-9ebcdfeb2bb2" satisfied condition "running and ready"
Jul 29 16:51:30.685: INFO: Container started at 2023-07-29 16:51:09 +0000 UTC, pod became ready at 2023-07-29 16:51:29 +0000 UTC
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jul 29 16:51:30.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5346" for this suite. 07/29/23 16:51:30.706
{"msg":"PASSED [sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","completed":318,"skipped":5870,"failed":0}
------------------------------
â€¢ [SLOW TEST] [22.154 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:51:08.57
    Jul 29 16:51:08.570: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename container-probe 07/29/23 16:51:08.572
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:51:08.608
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:51:08.612
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:68
    Jul 29 16:51:08.635: INFO: Waiting up to 5m0s for pod "test-webserver-98f1fdca-3c6b-4ebb-b283-9ebcdfeb2bb2" in namespace "container-probe-5346" to be "running and ready"
    Jul 29 16:51:08.644: INFO: Pod "test-webserver-98f1fdca-3c6b-4ebb-b283-9ebcdfeb2bb2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.364196ms
    Jul 29 16:51:08.644: INFO: The phase of Pod test-webserver-98f1fdca-3c6b-4ebb-b283-9ebcdfeb2bb2 is Pending, waiting for it to be Running (with Ready = true)
    Jul 29 16:51:10.656: INFO: Pod "test-webserver-98f1fdca-3c6b-4ebb-b283-9ebcdfeb2bb2": Phase="Running", Reason="", readiness=false. Elapsed: 2.020737365s
    Jul 29 16:51:10.656: INFO: The phase of Pod test-webserver-98f1fdca-3c6b-4ebb-b283-9ebcdfeb2bb2 is Running (Ready = false)
    Jul 29 16:51:12.653: INFO: Pod "test-webserver-98f1fdca-3c6b-4ebb-b283-9ebcdfeb2bb2": Phase="Running", Reason="", readiness=false. Elapsed: 4.017796821s
    Jul 29 16:51:12.654: INFO: The phase of Pod test-webserver-98f1fdca-3c6b-4ebb-b283-9ebcdfeb2bb2 is Running (Ready = false)
    Jul 29 16:51:14.677: INFO: Pod "test-webserver-98f1fdca-3c6b-4ebb-b283-9ebcdfeb2bb2": Phase="Running", Reason="", readiness=false. Elapsed: 6.041691628s
    Jul 29 16:51:14.678: INFO: The phase of Pod test-webserver-98f1fdca-3c6b-4ebb-b283-9ebcdfeb2bb2 is Running (Ready = false)
    Jul 29 16:51:16.655: INFO: Pod "test-webserver-98f1fdca-3c6b-4ebb-b283-9ebcdfeb2bb2": Phase="Running", Reason="", readiness=false. Elapsed: 8.01897792s
    Jul 29 16:51:16.655: INFO: The phase of Pod test-webserver-98f1fdca-3c6b-4ebb-b283-9ebcdfeb2bb2 is Running (Ready = false)
    Jul 29 16:51:18.687: INFO: Pod "test-webserver-98f1fdca-3c6b-4ebb-b283-9ebcdfeb2bb2": Phase="Running", Reason="", readiness=false. Elapsed: 10.05132373s
    Jul 29 16:51:18.687: INFO: The phase of Pod test-webserver-98f1fdca-3c6b-4ebb-b283-9ebcdfeb2bb2 is Running (Ready = false)
    Jul 29 16:51:20.653: INFO: Pod "test-webserver-98f1fdca-3c6b-4ebb-b283-9ebcdfeb2bb2": Phase="Running", Reason="", readiness=false. Elapsed: 12.016923267s
    Jul 29 16:51:20.653: INFO: The phase of Pod test-webserver-98f1fdca-3c6b-4ebb-b283-9ebcdfeb2bb2 is Running (Ready = false)
    Jul 29 16:51:22.697: INFO: Pod "test-webserver-98f1fdca-3c6b-4ebb-b283-9ebcdfeb2bb2": Phase="Running", Reason="", readiness=false. Elapsed: 14.060967146s
    Jul 29 16:51:22.697: INFO: The phase of Pod test-webserver-98f1fdca-3c6b-4ebb-b283-9ebcdfeb2bb2 is Running (Ready = false)
    Jul 29 16:51:24.654: INFO: Pod "test-webserver-98f1fdca-3c6b-4ebb-b283-9ebcdfeb2bb2": Phase="Running", Reason="", readiness=false. Elapsed: 16.018381686s
    Jul 29 16:51:24.654: INFO: The phase of Pod test-webserver-98f1fdca-3c6b-4ebb-b283-9ebcdfeb2bb2 is Running (Ready = false)
    Jul 29 16:51:26.653: INFO: Pod "test-webserver-98f1fdca-3c6b-4ebb-b283-9ebcdfeb2bb2": Phase="Running", Reason="", readiness=false. Elapsed: 18.017389162s
    Jul 29 16:51:26.653: INFO: The phase of Pod test-webserver-98f1fdca-3c6b-4ebb-b283-9ebcdfeb2bb2 is Running (Ready = false)
    Jul 29 16:51:28.652: INFO: Pod "test-webserver-98f1fdca-3c6b-4ebb-b283-9ebcdfeb2bb2": Phase="Running", Reason="", readiness=false. Elapsed: 20.016685205s
    Jul 29 16:51:28.652: INFO: The phase of Pod test-webserver-98f1fdca-3c6b-4ebb-b283-9ebcdfeb2bb2 is Running (Ready = false)
    Jul 29 16:51:30.677: INFO: Pod "test-webserver-98f1fdca-3c6b-4ebb-b283-9ebcdfeb2bb2": Phase="Running", Reason="", readiness=true. Elapsed: 22.041147537s
    Jul 29 16:51:30.677: INFO: The phase of Pod test-webserver-98f1fdca-3c6b-4ebb-b283-9ebcdfeb2bb2 is Running (Ready = true)
    Jul 29 16:51:30.677: INFO: Pod "test-webserver-98f1fdca-3c6b-4ebb-b283-9ebcdfeb2bb2" satisfied condition "running and ready"
    Jul 29 16:51:30.685: INFO: Container started at 2023-07-29 16:51:09 +0000 UTC, pod became ready at 2023-07-29 16:51:29 +0000 UTC
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jul 29 16:51:30.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-5346" for this suite. 07/29/23 16:51:30.706
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch
  should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1650
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:51:30.728
Jul 29 16:51:30.728: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename kubectl 07/29/23 16:51:30.734
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:51:30.773
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:51:30.778
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1650
STEP: creating Agnhost RC 07/29/23 16:51:30.784
Jul 29 16:51:30.784: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-3356 create -f -'
Jul 29 16:51:31.243: INFO: stderr: ""
Jul 29 16:51:31.243: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 07/29/23 16:51:31.243
Jul 29 16:51:32.258: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 29 16:51:32.258: INFO: Found 0 / 1
Jul 29 16:51:33.254: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 29 16:51:33.255: INFO: Found 1 / 1
Jul 29 16:51:33.255: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods 07/29/23 16:51:33.255
Jul 29 16:51:33.262: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 29 16:51:33.262: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jul 29 16:51:33.262: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-3356 patch pod agnhost-primary-kr2kn -p {"metadata":{"annotations":{"x":"y"}}}'
Jul 29 16:51:33.423: INFO: stderr: ""
Jul 29 16:51:33.423: INFO: stdout: "pod/agnhost-primary-kr2kn patched\n"
STEP: checking annotations 07/29/23 16:51:33.423
Jul 29 16:51:33.430: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 29 16:51:33.430: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jul 29 16:51:33.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3356" for this suite. 07/29/23 16:51:33.439
{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","completed":319,"skipped":5876,"failed":0}
------------------------------
â€¢ [2.724 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl patch
  test/e2e/kubectl/kubectl.go:1644
    should add annotations for pods in rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:51:30.728
    Jul 29 16:51:30.728: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename kubectl 07/29/23 16:51:30.734
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:51:30.773
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:51:30.778
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should add annotations for pods in rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1650
    STEP: creating Agnhost RC 07/29/23 16:51:30.784
    Jul 29 16:51:30.784: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-3356 create -f -'
    Jul 29 16:51:31.243: INFO: stderr: ""
    Jul 29 16:51:31.243: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 07/29/23 16:51:31.243
    Jul 29 16:51:32.258: INFO: Selector matched 1 pods for map[app:agnhost]
    Jul 29 16:51:32.258: INFO: Found 0 / 1
    Jul 29 16:51:33.254: INFO: Selector matched 1 pods for map[app:agnhost]
    Jul 29 16:51:33.255: INFO: Found 1 / 1
    Jul 29 16:51:33.255: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    STEP: patching all pods 07/29/23 16:51:33.255
    Jul 29 16:51:33.262: INFO: Selector matched 1 pods for map[app:agnhost]
    Jul 29 16:51:33.262: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Jul 29 16:51:33.262: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-3356 patch pod agnhost-primary-kr2kn -p {"metadata":{"annotations":{"x":"y"}}}'
    Jul 29 16:51:33.423: INFO: stderr: ""
    Jul 29 16:51:33.423: INFO: stdout: "pod/agnhost-primary-kr2kn patched\n"
    STEP: checking annotations 07/29/23 16:51:33.423
    Jul 29 16:51:33.430: INFO: Selector matched 1 pods for map[app:agnhost]
    Jul 29 16:51:33.430: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jul 29 16:51:33.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-3356" for this suite. 07/29/23 16:51:33.439
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:51:33.454
Jul 29 16:51:33.454: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename daemonsets 07/29/23 16:51:33.457
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:51:33.484
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:51:33.49
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431
Jul 29 16:51:33.551: INFO: Create a RollingUpdate DaemonSet
Jul 29 16:51:33.566: INFO: Check that daemon pods launch on every node of the cluster
Jul 29 16:51:33.582: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jul 29 16:51:33.582: INFO: Node wa4quivohpee-1 is running 0 daemon pod, expected 1
Jul 29 16:51:34.609: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jul 29 16:51:34.609: INFO: Node wa4quivohpee-1 is running 0 daemon pod, expected 1
Jul 29 16:51:35.608: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jul 29 16:51:35.608: INFO: Node wa4quivohpee-1 is running 0 daemon pod, expected 1
Jul 29 16:51:36.599: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jul 29 16:51:36.599: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
Jul 29 16:51:36.599: INFO: Update the DaemonSet to trigger a rollout
Jul 29 16:51:36.625: INFO: Updating DaemonSet daemon-set
Jul 29 16:51:38.661: INFO: Roll back the DaemonSet before rollout is complete
Jul 29 16:51:38.761: INFO: Updating DaemonSet daemon-set
Jul 29 16:51:38.762: INFO: Make sure DaemonSet rollback is complete
Jul 29 16:51:38.768: INFO: Wrong image for pod: daemon-set-hwgxn. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-2, got: foo:non-existent.
Jul 29 16:51:38.768: INFO: Pod daemon-set-hwgxn is not available
Jul 29 16:51:44.819: INFO: Pod daemon-set-865f2 is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 07/29/23 16:51:44.844
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3357, will wait for the garbage collector to delete the pods 07/29/23 16:51:44.844
Jul 29 16:51:44.912: INFO: Deleting DaemonSet.extensions daemon-set took: 10.898626ms
Jul 29 16:51:45.013: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.089165ms
Jul 29 16:52:16.423: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jul 29 16:52:16.423: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jul 29 16:52:16.429: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"35123"},"items":null}

Jul 29 16:52:16.433: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"35123"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Jul 29 16:52:16.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3357" for this suite. 07/29/23 16:52:16.494
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","completed":320,"skipped":5882,"failed":0}
------------------------------
â€¢ [SLOW TEST] [43.072 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:51:33.454
    Jul 29 16:51:33.454: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename daemonsets 07/29/23 16:51:33.457
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:51:33.484
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:51:33.49
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should rollback without unnecessary restarts [Conformance]
      test/e2e/apps/daemon_set.go:431
    Jul 29 16:51:33.551: INFO: Create a RollingUpdate DaemonSet
    Jul 29 16:51:33.566: INFO: Check that daemon pods launch on every node of the cluster
    Jul 29 16:51:33.582: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jul 29 16:51:33.582: INFO: Node wa4quivohpee-1 is running 0 daemon pod, expected 1
    Jul 29 16:51:34.609: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jul 29 16:51:34.609: INFO: Node wa4quivohpee-1 is running 0 daemon pod, expected 1
    Jul 29 16:51:35.608: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jul 29 16:51:35.608: INFO: Node wa4quivohpee-1 is running 0 daemon pod, expected 1
    Jul 29 16:51:36.599: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jul 29 16:51:36.599: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    Jul 29 16:51:36.599: INFO: Update the DaemonSet to trigger a rollout
    Jul 29 16:51:36.625: INFO: Updating DaemonSet daemon-set
    Jul 29 16:51:38.661: INFO: Roll back the DaemonSet before rollout is complete
    Jul 29 16:51:38.761: INFO: Updating DaemonSet daemon-set
    Jul 29 16:51:38.762: INFO: Make sure DaemonSet rollback is complete
    Jul 29 16:51:38.768: INFO: Wrong image for pod: daemon-set-hwgxn. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-2, got: foo:non-existent.
    Jul 29 16:51:38.768: INFO: Pod daemon-set-hwgxn is not available
    Jul 29 16:51:44.819: INFO: Pod daemon-set-865f2 is not available
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 07/29/23 16:51:44.844
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3357, will wait for the garbage collector to delete the pods 07/29/23 16:51:44.844
    Jul 29 16:51:44.912: INFO: Deleting DaemonSet.extensions daemon-set took: 10.898626ms
    Jul 29 16:51:45.013: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.089165ms
    Jul 29 16:52:16.423: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jul 29 16:52:16.423: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jul 29 16:52:16.429: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"35123"},"items":null}

    Jul 29 16:52:16.433: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"35123"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Jul 29 16:52:16.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-3357" for this suite. 07/29/23 16:52:16.494
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:52:16.529
Jul 29 16:52:16.529: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename deployment 07/29/23 16:52:16.533
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:52:16.585
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:52:16.59
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
Jul 29 16:52:16.614: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Jul 29 16:52:21.625: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 07/29/23 16:52:21.625
Jul 29 16:52:21.626: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 07/29/23 16:52:21.645
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jul 29 16:52:21.664: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-7020  d4983bdd-1d09-4459-b04c-c7b1504e993e 35156 1 2023-07-29 16:52:21 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-07-29 16:52:21 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001ac8d88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Jul 29 16:52:21.670: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jul 29 16:52:21.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7020" for this suite. 07/29/23 16:52:21.686
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","completed":321,"skipped":5899,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.174 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:52:16.529
    Jul 29 16:52:16.529: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename deployment 07/29/23 16:52:16.533
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:52:16.585
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:52:16.59
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should delete old replica sets [Conformance]
      test/e2e/apps/deployment.go:122
    Jul 29 16:52:16.614: INFO: Pod name cleanup-pod: Found 0 pods out of 1
    Jul 29 16:52:21.625: INFO: Pod name cleanup-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 07/29/23 16:52:21.625
    Jul 29 16:52:21.626: INFO: Creating deployment test-cleanup-deployment
    STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 07/29/23 16:52:21.645
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jul 29 16:52:21.664: INFO: Deployment "test-cleanup-deployment":
    &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-7020  d4983bdd-1d09-4459-b04c-c7b1504e993e 35156 1 2023-07-29 16:52:21 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-07-29 16:52:21 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001ac8d88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

    Jul 29 16:52:21.670: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jul 29 16:52:21.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-7020" for this suite. 07/29/23 16:52:21.686
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:52:21.714
Jul 29 16:52:21.714: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename secrets 07/29/23 16:52:21.716
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:52:21.756
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:52:21.762
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88
STEP: Creating secret with name secret-test-map-405bc69e-02e1-4d09-a9a9-91ccfa64516a 07/29/23 16:52:21.768
STEP: Creating a pod to test consume secrets 07/29/23 16:52:21.777
Jul 29 16:52:21.795: INFO: Waiting up to 5m0s for pod "pod-secrets-4f15e5e9-b1a2-49f9-869d-c7d01379c54a" in namespace "secrets-9472" to be "Succeeded or Failed"
Jul 29 16:52:21.804: INFO: Pod "pod-secrets-4f15e5e9-b1a2-49f9-869d-c7d01379c54a": Phase="Pending", Reason="", readiness=false. Elapsed: 9.464886ms
Jul 29 16:52:23.813: INFO: Pod "pod-secrets-4f15e5e9-b1a2-49f9-869d-c7d01379c54a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018038628s
Jul 29 16:52:25.812: INFO: Pod "pod-secrets-4f15e5e9-b1a2-49f9-869d-c7d01379c54a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017458502s
STEP: Saw pod success 07/29/23 16:52:25.813
Jul 29 16:52:25.813: INFO: Pod "pod-secrets-4f15e5e9-b1a2-49f9-869d-c7d01379c54a" satisfied condition "Succeeded or Failed"
Jul 29 16:52:25.824: INFO: Trying to get logs from node wa4quivohpee-3 pod pod-secrets-4f15e5e9-b1a2-49f9-869d-c7d01379c54a container secret-volume-test: <nil>
STEP: delete the pod 07/29/23 16:52:25.859
Jul 29 16:52:25.877: INFO: Waiting for pod pod-secrets-4f15e5e9-b1a2-49f9-869d-c7d01379c54a to disappear
Jul 29 16:52:25.882: INFO: Pod pod-secrets-4f15e5e9-b1a2-49f9-869d-c7d01379c54a no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jul 29 16:52:25.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9472" for this suite. 07/29/23 16:52:25.89
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":322,"skipped":5934,"failed":0}
------------------------------
â€¢ [4.188 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:52:21.714
    Jul 29 16:52:21.714: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename secrets 07/29/23 16:52:21.716
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:52:21.756
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:52:21.762
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:88
    STEP: Creating secret with name secret-test-map-405bc69e-02e1-4d09-a9a9-91ccfa64516a 07/29/23 16:52:21.768
    STEP: Creating a pod to test consume secrets 07/29/23 16:52:21.777
    Jul 29 16:52:21.795: INFO: Waiting up to 5m0s for pod "pod-secrets-4f15e5e9-b1a2-49f9-869d-c7d01379c54a" in namespace "secrets-9472" to be "Succeeded or Failed"
    Jul 29 16:52:21.804: INFO: Pod "pod-secrets-4f15e5e9-b1a2-49f9-869d-c7d01379c54a": Phase="Pending", Reason="", readiness=false. Elapsed: 9.464886ms
    Jul 29 16:52:23.813: INFO: Pod "pod-secrets-4f15e5e9-b1a2-49f9-869d-c7d01379c54a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018038628s
    Jul 29 16:52:25.812: INFO: Pod "pod-secrets-4f15e5e9-b1a2-49f9-869d-c7d01379c54a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017458502s
    STEP: Saw pod success 07/29/23 16:52:25.813
    Jul 29 16:52:25.813: INFO: Pod "pod-secrets-4f15e5e9-b1a2-49f9-869d-c7d01379c54a" satisfied condition "Succeeded or Failed"
    Jul 29 16:52:25.824: INFO: Trying to get logs from node wa4quivohpee-3 pod pod-secrets-4f15e5e9-b1a2-49f9-869d-c7d01379c54a container secret-volume-test: <nil>
    STEP: delete the pod 07/29/23 16:52:25.859
    Jul 29 16:52:25.877: INFO: Waiting for pod pod-secrets-4f15e5e9-b1a2-49f9-869d-c7d01379c54a to disappear
    Jul 29 16:52:25.882: INFO: Pod pod-secrets-4f15e5e9-b1a2-49f9-869d-c7d01379c54a no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jul 29 16:52:25.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-9472" for this suite. 07/29/23 16:52:25.89
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:52:25.915
Jul 29 16:52:25.915: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename projected 07/29/23 16:52:25.918
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:52:25.95
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:52:25.954
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161
STEP: Creating the pod 07/29/23 16:52:25.958
Jul 29 16:52:25.973: INFO: Waiting up to 5m0s for pod "annotationupdate2ff745e5-a488-43c8-8eb7-71559ef967e7" in namespace "projected-3753" to be "running and ready"
Jul 29 16:52:25.985: INFO: Pod "annotationupdate2ff745e5-a488-43c8-8eb7-71559ef967e7": Phase="Pending", Reason="", readiness=false. Elapsed: 11.495061ms
Jul 29 16:52:25.985: INFO: The phase of Pod annotationupdate2ff745e5-a488-43c8-8eb7-71559ef967e7 is Pending, waiting for it to be Running (with Ready = true)
Jul 29 16:52:27.993: INFO: Pod "annotationupdate2ff745e5-a488-43c8-8eb7-71559ef967e7": Phase="Running", Reason="", readiness=true. Elapsed: 2.019533798s
Jul 29 16:52:27.993: INFO: The phase of Pod annotationupdate2ff745e5-a488-43c8-8eb7-71559ef967e7 is Running (Ready = true)
Jul 29 16:52:27.993: INFO: Pod "annotationupdate2ff745e5-a488-43c8-8eb7-71559ef967e7" satisfied condition "running and ready"
Jul 29 16:52:28.531: INFO: Successfully updated pod "annotationupdate2ff745e5-a488-43c8-8eb7-71559ef967e7"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jul 29 16:52:32.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3753" for this suite. 07/29/23 16:52:32.583
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","completed":323,"skipped":5987,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.680 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:52:25.915
    Jul 29 16:52:25.915: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename projected 07/29/23 16:52:25.918
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:52:25.95
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:52:25.954
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:161
    STEP: Creating the pod 07/29/23 16:52:25.958
    Jul 29 16:52:25.973: INFO: Waiting up to 5m0s for pod "annotationupdate2ff745e5-a488-43c8-8eb7-71559ef967e7" in namespace "projected-3753" to be "running and ready"
    Jul 29 16:52:25.985: INFO: Pod "annotationupdate2ff745e5-a488-43c8-8eb7-71559ef967e7": Phase="Pending", Reason="", readiness=false. Elapsed: 11.495061ms
    Jul 29 16:52:25.985: INFO: The phase of Pod annotationupdate2ff745e5-a488-43c8-8eb7-71559ef967e7 is Pending, waiting for it to be Running (with Ready = true)
    Jul 29 16:52:27.993: INFO: Pod "annotationupdate2ff745e5-a488-43c8-8eb7-71559ef967e7": Phase="Running", Reason="", readiness=true. Elapsed: 2.019533798s
    Jul 29 16:52:27.993: INFO: The phase of Pod annotationupdate2ff745e5-a488-43c8-8eb7-71559ef967e7 is Running (Ready = true)
    Jul 29 16:52:27.993: INFO: Pod "annotationupdate2ff745e5-a488-43c8-8eb7-71559ef967e7" satisfied condition "running and ready"
    Jul 29 16:52:28.531: INFO: Successfully updated pod "annotationupdate2ff745e5-a488-43c8-8eb7-71559ef967e7"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jul 29 16:52:32.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3753" for this suite. 07/29/23 16:52:32.583
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin]
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:52:32.605
Jul 29 16:52:32.605: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename certificates 07/29/23 16:52:32.609
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:52:32.648
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:52:32.652
[It] should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
STEP: getting /apis 07/29/23 16:52:33.714
STEP: getting /apis/certificates.k8s.io 07/29/23 16:52:33.719
STEP: getting /apis/certificates.k8s.io/v1 07/29/23 16:52:33.72
STEP: creating 07/29/23 16:52:33.722
STEP: getting 07/29/23 16:52:33.751
STEP: listing 07/29/23 16:52:33.755
STEP: watching 07/29/23 16:52:33.762
Jul 29 16:52:33.762: INFO: starting watch
STEP: patching 07/29/23 16:52:33.764
STEP: updating 07/29/23 16:52:33.777
Jul 29 16:52:33.787: INFO: waiting for watch events with expected annotations
Jul 29 16:52:33.787: INFO: saw patched and updated annotations
STEP: getting /approval 07/29/23 16:52:33.787
STEP: patching /approval 07/29/23 16:52:33.792
STEP: updating /approval 07/29/23 16:52:33.802
STEP: getting /status 07/29/23 16:52:33.811
STEP: patching /status 07/29/23 16:52:33.818
STEP: updating /status 07/29/23 16:52:33.836
STEP: deleting 07/29/23 16:52:33.851
STEP: deleting a collection 07/29/23 16:52:33.88
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jul 29 16:52:33.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-2484" for this suite. 07/29/23 16:52:33.916
{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","completed":324,"skipped":6011,"failed":0}
------------------------------
â€¢ [1.322 seconds]
[sig-auth] Certificates API [Privileged:ClusterAdmin]
test/e2e/auth/framework.go:23
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:52:32.605
    Jul 29 16:52:32.605: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename certificates 07/29/23 16:52:32.609
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:52:32.648
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:52:32.652
    [It] should support CSR API operations [Conformance]
      test/e2e/auth/certificates.go:200
    STEP: getting /apis 07/29/23 16:52:33.714
    STEP: getting /apis/certificates.k8s.io 07/29/23 16:52:33.719
    STEP: getting /apis/certificates.k8s.io/v1 07/29/23 16:52:33.72
    STEP: creating 07/29/23 16:52:33.722
    STEP: getting 07/29/23 16:52:33.751
    STEP: listing 07/29/23 16:52:33.755
    STEP: watching 07/29/23 16:52:33.762
    Jul 29 16:52:33.762: INFO: starting watch
    STEP: patching 07/29/23 16:52:33.764
    STEP: updating 07/29/23 16:52:33.777
    Jul 29 16:52:33.787: INFO: waiting for watch events with expected annotations
    Jul 29 16:52:33.787: INFO: saw patched and updated annotations
    STEP: getting /approval 07/29/23 16:52:33.787
    STEP: patching /approval 07/29/23 16:52:33.792
    STEP: updating /approval 07/29/23 16:52:33.802
    STEP: getting /status 07/29/23 16:52:33.811
    STEP: patching /status 07/29/23 16:52:33.818
    STEP: updating /status 07/29/23 16:52:33.836
    STEP: deleting 07/29/23 16:52:33.851
    STEP: deleting a collection 07/29/23 16:52:33.88
    [AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jul 29 16:52:33.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "certificates-2484" for this suite. 07/29/23 16:52:33.916
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:737
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:52:33.944
Jul 29 16:52:33.944: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename statefulset 07/29/23 16:52:33.946
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:52:33.977
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:52:33.98
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-3494 07/29/23 16:52:33.984
[It] Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:737
STEP: Looking for a node to schedule stateful set and pod 07/29/23 16:52:33.995
STEP: Creating pod with conflicting port in namespace statefulset-3494 07/29/23 16:52:34.005
STEP: Waiting until pod test-pod will start running in namespace statefulset-3494 07/29/23 16:52:34.019
Jul 29 16:52:34.019: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-3494" to be "running"
Jul 29 16:52:34.026: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.026596ms
Jul 29 16:52:36.033: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.013931327s
Jul 29 16:52:36.033: INFO: Pod "test-pod" satisfied condition "running"
STEP: Creating statefulset with conflicting port in namespace statefulset-3494 07/29/23 16:52:36.033
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-3494 07/29/23 16:52:36.041
Jul 29 16:52:36.069: INFO: Observed stateful pod in namespace: statefulset-3494, name: ss-0, uid: c96db085-1b05-4d50-bfec-fa2053fd969e, status phase: Pending. Waiting for statefulset controller to delete.
Jul 29 16:52:36.091: INFO: Observed stateful pod in namespace: statefulset-3494, name: ss-0, uid: c96db085-1b05-4d50-bfec-fa2053fd969e, status phase: Failed. Waiting for statefulset controller to delete.
Jul 29 16:52:36.119: INFO: Observed stateful pod in namespace: statefulset-3494, name: ss-0, uid: c96db085-1b05-4d50-bfec-fa2053fd969e, status phase: Failed. Waiting for statefulset controller to delete.
Jul 29 16:52:36.124: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-3494
STEP: Removing pod with conflicting port in namespace statefulset-3494 07/29/23 16:52:36.124
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-3494 and will be in running state 07/29/23 16:52:36.139
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jul 29 16:52:54.242: INFO: Deleting all statefulset in ns statefulset-3494
Jul 29 16:52:54.249: INFO: Scaling statefulset ss to 0
Jul 29 16:53:04.287: INFO: Waiting for statefulset status.replicas updated to 0
Jul 29 16:53:04.296: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jul 29 16:53:04.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3494" for this suite. 07/29/23 16:53:04.325
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","completed":325,"skipped":6074,"failed":0}
------------------------------
â€¢ [SLOW TEST] [30.392 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Should recreate evicted statefulset [Conformance]
    test/e2e/apps/statefulset.go:737

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:52:33.944
    Jul 29 16:52:33.944: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename statefulset 07/29/23 16:52:33.946
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:52:33.977
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:52:33.98
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-3494 07/29/23 16:52:33.984
    [It] Should recreate evicted statefulset [Conformance]
      test/e2e/apps/statefulset.go:737
    STEP: Looking for a node to schedule stateful set and pod 07/29/23 16:52:33.995
    STEP: Creating pod with conflicting port in namespace statefulset-3494 07/29/23 16:52:34.005
    STEP: Waiting until pod test-pod will start running in namespace statefulset-3494 07/29/23 16:52:34.019
    Jul 29 16:52:34.019: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-3494" to be "running"
    Jul 29 16:52:34.026: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.026596ms
    Jul 29 16:52:36.033: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.013931327s
    Jul 29 16:52:36.033: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Creating statefulset with conflicting port in namespace statefulset-3494 07/29/23 16:52:36.033
    STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-3494 07/29/23 16:52:36.041
    Jul 29 16:52:36.069: INFO: Observed stateful pod in namespace: statefulset-3494, name: ss-0, uid: c96db085-1b05-4d50-bfec-fa2053fd969e, status phase: Pending. Waiting for statefulset controller to delete.
    Jul 29 16:52:36.091: INFO: Observed stateful pod in namespace: statefulset-3494, name: ss-0, uid: c96db085-1b05-4d50-bfec-fa2053fd969e, status phase: Failed. Waiting for statefulset controller to delete.
    Jul 29 16:52:36.119: INFO: Observed stateful pod in namespace: statefulset-3494, name: ss-0, uid: c96db085-1b05-4d50-bfec-fa2053fd969e, status phase: Failed. Waiting for statefulset controller to delete.
    Jul 29 16:52:36.124: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-3494
    STEP: Removing pod with conflicting port in namespace statefulset-3494 07/29/23 16:52:36.124
    STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-3494 and will be in running state 07/29/23 16:52:36.139
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jul 29 16:52:54.242: INFO: Deleting all statefulset in ns statefulset-3494
    Jul 29 16:52:54.249: INFO: Scaling statefulset ss to 0
    Jul 29 16:53:04.287: INFO: Waiting for statefulset status.replicas updated to 0
    Jul 29 16:53:04.296: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jul 29 16:53:04.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-3494" for this suite. 07/29/23 16:53:04.325
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:53:04.35
Jul 29 16:53:04.351: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename containers 07/29/23 16:53:04.354
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:53:04.386
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:53:04.392
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86
STEP: Creating a pod to test override all 07/29/23 16:53:04.397
Jul 29 16:53:04.410: INFO: Waiting up to 5m0s for pod "client-containers-e5058180-1e90-4d4e-9d91-884ed0100d51" in namespace "containers-5897" to be "Succeeded or Failed"
Jul 29 16:53:04.420: INFO: Pod "client-containers-e5058180-1e90-4d4e-9d91-884ed0100d51": Phase="Pending", Reason="", readiness=false. Elapsed: 8.967045ms
Jul 29 16:53:06.430: INFO: Pod "client-containers-e5058180-1e90-4d4e-9d91-884ed0100d51": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019314188s
Jul 29 16:53:08.428: INFO: Pod "client-containers-e5058180-1e90-4d4e-9d91-884ed0100d51": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017686194s
STEP: Saw pod success 07/29/23 16:53:08.428
Jul 29 16:53:08.429: INFO: Pod "client-containers-e5058180-1e90-4d4e-9d91-884ed0100d51" satisfied condition "Succeeded or Failed"
Jul 29 16:53:08.437: INFO: Trying to get logs from node wa4quivohpee-3 pod client-containers-e5058180-1e90-4d4e-9d91-884ed0100d51 container agnhost-container: <nil>
STEP: delete the pod 07/29/23 16:53:08.454
Jul 29 16:53:08.481: INFO: Waiting for pod client-containers-e5058180-1e90-4d4e-9d91-884ed0100d51 to disappear
Jul 29 16:53:08.487: INFO: Pod client-containers-e5058180-1e90-4d4e-9d91-884ed0100d51 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Jul 29 16:53:08.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5897" for this suite. 07/29/23 16:53:08.504
{"msg":"PASSED [sig-node] Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","completed":326,"skipped":6107,"failed":0}
------------------------------
â€¢ [4.165 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:53:04.35
    Jul 29 16:53:04.351: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename containers 07/29/23 16:53:04.354
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:53:04.386
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:53:04.392
    [It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:86
    STEP: Creating a pod to test override all 07/29/23 16:53:04.397
    Jul 29 16:53:04.410: INFO: Waiting up to 5m0s for pod "client-containers-e5058180-1e90-4d4e-9d91-884ed0100d51" in namespace "containers-5897" to be "Succeeded or Failed"
    Jul 29 16:53:04.420: INFO: Pod "client-containers-e5058180-1e90-4d4e-9d91-884ed0100d51": Phase="Pending", Reason="", readiness=false. Elapsed: 8.967045ms
    Jul 29 16:53:06.430: INFO: Pod "client-containers-e5058180-1e90-4d4e-9d91-884ed0100d51": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019314188s
    Jul 29 16:53:08.428: INFO: Pod "client-containers-e5058180-1e90-4d4e-9d91-884ed0100d51": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017686194s
    STEP: Saw pod success 07/29/23 16:53:08.428
    Jul 29 16:53:08.429: INFO: Pod "client-containers-e5058180-1e90-4d4e-9d91-884ed0100d51" satisfied condition "Succeeded or Failed"
    Jul 29 16:53:08.437: INFO: Trying to get logs from node wa4quivohpee-3 pod client-containers-e5058180-1e90-4d4e-9d91-884ed0100d51 container agnhost-container: <nil>
    STEP: delete the pod 07/29/23 16:53:08.454
    Jul 29 16:53:08.481: INFO: Waiting for pod client-containers-e5058180-1e90-4d4e-9d91-884ed0100d51 to disappear
    Jul 29 16:53:08.487: INFO: Pod client-containers-e5058180-1e90-4d4e-9d91-884ed0100d51 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Jul 29 16:53:08.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-5897" for this suite. 07/29/23 16:53:08.504
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:53:08.523
Jul 29 16:53:08.523: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename svcaccounts 07/29/23 16:53:08.527
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:53:08.558
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:53:08.562
[It] should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272
STEP: Creating a pod to test service account token:  07/29/23 16:53:08.566
Jul 29 16:53:08.581: INFO: Waiting up to 5m0s for pod "test-pod-595c07b1-8ecc-46c1-8769-e125ac5ab63b" in namespace "svcaccounts-4133" to be "Succeeded or Failed"
Jul 29 16:53:08.588: INFO: Pod "test-pod-595c07b1-8ecc-46c1-8769-e125ac5ab63b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.593167ms
Jul 29 16:53:10.614: INFO: Pod "test-pod-595c07b1-8ecc-46c1-8769-e125ac5ab63b": Phase="Running", Reason="", readiness=false. Elapsed: 2.033050271s
Jul 29 16:53:12.596: INFO: Pod "test-pod-595c07b1-8ecc-46c1-8769-e125ac5ab63b": Phase="Running", Reason="", readiness=false. Elapsed: 4.015126823s
Jul 29 16:53:14.599: INFO: Pod "test-pod-595c07b1-8ecc-46c1-8769-e125ac5ab63b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.017528782s
STEP: Saw pod success 07/29/23 16:53:14.599
Jul 29 16:53:14.599: INFO: Pod "test-pod-595c07b1-8ecc-46c1-8769-e125ac5ab63b" satisfied condition "Succeeded or Failed"
Jul 29 16:53:14.633: INFO: Trying to get logs from node wa4quivohpee-3 pod test-pod-595c07b1-8ecc-46c1-8769-e125ac5ab63b container agnhost-container: <nil>
STEP: delete the pod 07/29/23 16:53:14.651
Jul 29 16:53:14.699: INFO: Waiting for pod test-pod-595c07b1-8ecc-46c1-8769-e125ac5ab63b to disappear
Jul 29 16:53:14.707: INFO: Pod test-pod-595c07b1-8ecc-46c1-8769-e125ac5ab63b no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Jul 29 16:53:14.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-4133" for this suite. 07/29/23 16:53:14.723
{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","completed":327,"skipped":6129,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.213 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:53:08.523
    Jul 29 16:53:08.523: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename svcaccounts 07/29/23 16:53:08.527
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:53:08.558
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:53:08.562
    [It] should mount projected service account token [Conformance]
      test/e2e/auth/service_accounts.go:272
    STEP: Creating a pod to test service account token:  07/29/23 16:53:08.566
    Jul 29 16:53:08.581: INFO: Waiting up to 5m0s for pod "test-pod-595c07b1-8ecc-46c1-8769-e125ac5ab63b" in namespace "svcaccounts-4133" to be "Succeeded or Failed"
    Jul 29 16:53:08.588: INFO: Pod "test-pod-595c07b1-8ecc-46c1-8769-e125ac5ab63b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.593167ms
    Jul 29 16:53:10.614: INFO: Pod "test-pod-595c07b1-8ecc-46c1-8769-e125ac5ab63b": Phase="Running", Reason="", readiness=false. Elapsed: 2.033050271s
    Jul 29 16:53:12.596: INFO: Pod "test-pod-595c07b1-8ecc-46c1-8769-e125ac5ab63b": Phase="Running", Reason="", readiness=false. Elapsed: 4.015126823s
    Jul 29 16:53:14.599: INFO: Pod "test-pod-595c07b1-8ecc-46c1-8769-e125ac5ab63b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.017528782s
    STEP: Saw pod success 07/29/23 16:53:14.599
    Jul 29 16:53:14.599: INFO: Pod "test-pod-595c07b1-8ecc-46c1-8769-e125ac5ab63b" satisfied condition "Succeeded or Failed"
    Jul 29 16:53:14.633: INFO: Trying to get logs from node wa4quivohpee-3 pod test-pod-595c07b1-8ecc-46c1-8769-e125ac5ab63b container agnhost-container: <nil>
    STEP: delete the pod 07/29/23 16:53:14.651
    Jul 29 16:53:14.699: INFO: Waiting for pod test-pod-595c07b1-8ecc-46c1-8769-e125ac5ab63b to disappear
    Jul 29 16:53:14.707: INFO: Pod test-pod-595c07b1-8ecc-46c1-8769-e125ac5ab63b no longer exists
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Jul 29 16:53:14.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-4133" for this suite. 07/29/23 16:53:14.723
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:53:14.741
Jul 29 16:53:14.741: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename disruption 07/29/23 16:53:14.744
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:53:14.782
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:53:14.787
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107
STEP: creating the pdb 07/29/23 16:53:14.793
STEP: Waiting for the pdb to be processed 07/29/23 16:53:14.801
STEP: updating the pdb 07/29/23 16:53:16.816
STEP: Waiting for the pdb to be processed 07/29/23 16:53:16.834
STEP: patching the pdb 07/29/23 16:53:18.846
STEP: Waiting for the pdb to be processed 07/29/23 16:53:18.86
STEP: Waiting for the pdb to be deleted 07/29/23 16:53:20.886
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Jul 29 16:53:20.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-1454" for this suite. 07/29/23 16:53:20.902
{"msg":"PASSED [sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]","completed":328,"skipped":6148,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.172 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:53:14.741
    Jul 29 16:53:14.741: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename disruption 07/29/23 16:53:14.744
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:53:14.782
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:53:14.787
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should create a PodDisruptionBudget [Conformance]
      test/e2e/apps/disruption.go:107
    STEP: creating the pdb 07/29/23 16:53:14.793
    STEP: Waiting for the pdb to be processed 07/29/23 16:53:14.801
    STEP: updating the pdb 07/29/23 16:53:16.816
    STEP: Waiting for the pdb to be processed 07/29/23 16:53:16.834
    STEP: patching the pdb 07/29/23 16:53:18.846
    STEP: Waiting for the pdb to be processed 07/29/23 16:53:18.86
    STEP: Waiting for the pdb to be deleted 07/29/23 16:53:20.886
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Jul 29 16:53:20.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-1454" for this suite. 07/29/23 16:53:20.902
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch
  watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:53:20.918
Jul 29 16:53:20.918: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename crd-watch 07/29/23 16:53:20.92
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:53:20.95
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:53:20.955
[It] watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
Jul 29 16:53:20.959: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Creating first CR  07/29/23 16:53:23.658
Jul 29 16:53:23.666: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-07-29T16:53:23Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-07-29T16:53:23Z]] name:name1 resourceVersion:35574 uid:f47fe00a-4e23-4136-8327-25e5dc690c5a] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR 07/29/23 16:53:33.666
Jul 29 16:53:33.677: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-07-29T16:53:33Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-07-29T16:53:33Z]] name:name2 resourceVersion:35601 uid:9aac0c84-ef18-42eb-bc33-32b50b0eade1] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR 07/29/23 16:53:43.677
Jul 29 16:53:43.690: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-07-29T16:53:23Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-07-29T16:53:43Z]] name:name1 resourceVersion:35622 uid:f47fe00a-4e23-4136-8327-25e5dc690c5a] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR 07/29/23 16:53:53.691
Jul 29 16:53:53.707: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-07-29T16:53:33Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-07-29T16:53:53Z]] name:name2 resourceVersion:35643 uid:9aac0c84-ef18-42eb-bc33-32b50b0eade1] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR 07/29/23 16:54:03.707
Jul 29 16:54:03.729: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-07-29T16:53:23Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-07-29T16:53:43Z]] name:name1 resourceVersion:35664 uid:f47fe00a-4e23-4136-8327-25e5dc690c5a] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR 07/29/23 16:54:13.73
Jul 29 16:54:13.743: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-07-29T16:53:33Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-07-29T16:53:53Z]] name:name2 resourceVersion:35686 uid:9aac0c84-ef18-42eb-bc33-32b50b0eade1] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jul 29 16:54:24.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-9846" for this suite. 07/29/23 16:54:24.277
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","completed":329,"skipped":6148,"failed":0}
------------------------------
â€¢ [SLOW TEST] [63.373 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  test/e2e/apimachinery/crd_watch.go:44
    watch on custom resource definition objects [Conformance]
    test/e2e/apimachinery/crd_watch.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:53:20.918
    Jul 29 16:53:20.918: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename crd-watch 07/29/23 16:53:20.92
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:53:20.95
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:53:20.955
    [It] watch on custom resource definition objects [Conformance]
      test/e2e/apimachinery/crd_watch.go:51
    Jul 29 16:53:20.959: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Creating first CR  07/29/23 16:53:23.658
    Jul 29 16:53:23.666: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-07-29T16:53:23Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-07-29T16:53:23Z]] name:name1 resourceVersion:35574 uid:f47fe00a-4e23-4136-8327-25e5dc690c5a] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Creating second CR 07/29/23 16:53:33.666
    Jul 29 16:53:33.677: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-07-29T16:53:33Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-07-29T16:53:33Z]] name:name2 resourceVersion:35601 uid:9aac0c84-ef18-42eb-bc33-32b50b0eade1] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying first CR 07/29/23 16:53:43.677
    Jul 29 16:53:43.690: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-07-29T16:53:23Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-07-29T16:53:43Z]] name:name1 resourceVersion:35622 uid:f47fe00a-4e23-4136-8327-25e5dc690c5a] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying second CR 07/29/23 16:53:53.691
    Jul 29 16:53:53.707: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-07-29T16:53:33Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-07-29T16:53:53Z]] name:name2 resourceVersion:35643 uid:9aac0c84-ef18-42eb-bc33-32b50b0eade1] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting first CR 07/29/23 16:54:03.707
    Jul 29 16:54:03.729: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-07-29T16:53:23Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-07-29T16:53:43Z]] name:name1 resourceVersion:35664 uid:f47fe00a-4e23-4136-8327-25e5dc690c5a] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting second CR 07/29/23 16:54:13.73
    Jul 29 16:54:13.743: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-07-29T16:53:33Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-07-29T16:53:53Z]] name:name2 resourceVersion:35686 uid:9aac0c84-ef18-42eb-bc33-32b50b0eade1] num:map[num1:9223372036854775807 num2:1000000]]}
    [AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jul 29 16:54:24.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-watch-9846" for this suite. 07/29/23 16:54:24.277
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:54:24.294
Jul 29 16:54:24.294: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename sched-pred 07/29/23 16:54:24.296
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:54:24.332
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:54:24.336
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Jul 29 16:54:24.341: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jul 29 16:54:24.356: INFO: Waiting for terminating namespaces to be deleted...
Jul 29 16:54:24.360: INFO: 
Logging pods the apiserver thinks is on node wa4quivohpee-1 before test
Jul 29 16:54:24.374: INFO: cilium-gfkpl from kube-system started at 2023-07-29 15:23:18 +0000 UTC (1 container statuses recorded)
Jul 29 16:54:24.374: INFO: 	Container cilium-agent ready: true, restart count 0
Jul 29 16:54:24.374: INFO: cilium-node-init-n8876 from kube-system started at 2023-07-29 15:23:18 +0000 UTC (1 container statuses recorded)
Jul 29 16:54:24.374: INFO: 	Container node-init ready: true, restart count 0
Jul 29 16:54:24.374: INFO: coredns-565d847f94-xm9ff from kube-system started at 2023-07-29 15:34:18 +0000 UTC (1 container statuses recorded)
Jul 29 16:54:24.374: INFO: 	Container coredns ready: true, restart count 0
Jul 29 16:54:24.374: INFO: kube-addon-manager-wa4quivohpee-1 from kube-system started at 2023-07-29 15:22:59 +0000 UTC (1 container statuses recorded)
Jul 29 16:54:24.374: INFO: 	Container kube-addon-manager ready: true, restart count 0
Jul 29 16:54:24.374: INFO: kube-apiserver-wa4quivohpee-1 from kube-system started at 2023-07-29 15:14:24 +0000 UTC (1 container statuses recorded)
Jul 29 16:54:24.374: INFO: 	Container kube-apiserver ready: true, restart count 0
Jul 29 16:54:24.374: INFO: kube-controller-manager-wa4quivohpee-1 from kube-system started at 2023-07-29 15:14:24 +0000 UTC (1 container statuses recorded)
Jul 29 16:54:24.374: INFO: 	Container kube-controller-manager ready: true, restart count 0
Jul 29 16:54:24.374: INFO: kube-proxy-gdffl from kube-system started at 2023-07-29 15:14:37 +0000 UTC (1 container statuses recorded)
Jul 29 16:54:24.374: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 29 16:54:24.374: INFO: kube-scheduler-wa4quivohpee-1 from kube-system started at 2023-07-29 15:14:23 +0000 UTC (1 container statuses recorded)
Jul 29 16:54:24.374: INFO: 	Container kube-scheduler ready: true, restart count 0
Jul 29 16:54:24.374: INFO: sonobuoy-systemd-logs-daemon-set-b6f0dae2c3174900-mtck5 from sonobuoy started at 2023-07-29 15:24:46 +0000 UTC (2 container statuses recorded)
Jul 29 16:54:24.374: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 29 16:54:24.374: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 29 16:54:24.374: INFO: 
Logging pods the apiserver thinks is on node wa4quivohpee-2 before test
Jul 29 16:54:24.387: INFO: cilium-node-init-946z9 from kube-system started at 2023-07-29 15:23:18 +0000 UTC (1 container statuses recorded)
Jul 29 16:54:24.387: INFO: 	Container node-init ready: true, restart count 0
Jul 29 16:54:24.387: INFO: cilium-rmjkx from kube-system started at 2023-07-29 15:23:18 +0000 UTC (1 container statuses recorded)
Jul 29 16:54:24.387: INFO: 	Container cilium-agent ready: true, restart count 0
Jul 29 16:54:24.387: INFO: coredns-565d847f94-wmg75 from kube-system started at 2023-07-29 15:24:27 +0000 UTC (1 container statuses recorded)
Jul 29 16:54:24.387: INFO: 	Container coredns ready: true, restart count 0
Jul 29 16:54:24.387: INFO: kube-addon-manager-wa4quivohpee-2 from kube-system started at 2023-07-29 15:22:59 +0000 UTC (1 container statuses recorded)
Jul 29 16:54:24.387: INFO: 	Container kube-addon-manager ready: true, restart count 0
Jul 29 16:54:24.387: INFO: kube-apiserver-wa4quivohpee-2 from kube-system started at 2023-07-29 15:15:22 +0000 UTC (1 container statuses recorded)
Jul 29 16:54:24.387: INFO: 	Container kube-apiserver ready: true, restart count 0
Jul 29 16:54:24.387: INFO: kube-controller-manager-wa4quivohpee-2 from kube-system started at 2023-07-29 15:15:22 +0000 UTC (1 container statuses recorded)
Jul 29 16:54:24.387: INFO: 	Container kube-controller-manager ready: true, restart count 0
Jul 29 16:54:24.387: INFO: kube-proxy-74hn2 from kube-system started at 2023-07-29 15:15:19 +0000 UTC (1 container statuses recorded)
Jul 29 16:54:24.387: INFO: 	Container kube-proxy ready: true, restart count 1
Jul 29 16:54:24.387: INFO: kube-scheduler-wa4quivohpee-2 from kube-system started at 2023-07-29 15:15:22 +0000 UTC (1 container statuses recorded)
Jul 29 16:54:24.387: INFO: 	Container kube-scheduler ready: true, restart count 0
Jul 29 16:54:24.387: INFO: sonobuoy-systemd-logs-daemon-set-b6f0dae2c3174900-cnfp2 from sonobuoy started at 2023-07-29 15:24:46 +0000 UTC (2 container statuses recorded)
Jul 29 16:54:24.388: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 29 16:54:24.388: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 29 16:54:24.388: INFO: 
Logging pods the apiserver thinks is on node wa4quivohpee-3 before test
Jul 29 16:54:24.398: INFO: cilium-node-init-j4pkd from kube-system started at 2023-07-29 15:23:18 +0000 UTC (1 container statuses recorded)
Jul 29 16:54:24.398: INFO: 	Container node-init ready: true, restart count 0
Jul 29 16:54:24.398: INFO: cilium-operator-69cffcb958-48gkl from kube-system started at 2023-07-29 15:23:19 +0000 UTC (1 container statuses recorded)
Jul 29 16:54:24.398: INFO: 	Container cilium-operator ready: true, restart count 0
Jul 29 16:54:24.398: INFO: cilium-zl28x from kube-system started at 2023-07-29 15:23:18 +0000 UTC (1 container statuses recorded)
Jul 29 16:54:24.398: INFO: 	Container cilium-agent ready: true, restart count 0
Jul 29 16:54:24.398: INFO: kube-proxy-ts2pg from kube-system started at 2023-07-29 15:16:17 +0000 UTC (1 container statuses recorded)
Jul 29 16:54:24.398: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 29 16:54:24.399: INFO: sonobuoy from sonobuoy started at 2023-07-29 15:24:31 +0000 UTC (1 container statuses recorded)
Jul 29 16:54:24.399: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jul 29 16:54:24.399: INFO: sonobuoy-e2e-job-5c9a7da3e6b74e15 from sonobuoy started at 2023-07-29 15:24:45 +0000 UTC (2 container statuses recorded)
Jul 29 16:54:24.399: INFO: 	Container e2e ready: true, restart count 0
Jul 29 16:54:24.399: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 29 16:54:24.399: INFO: sonobuoy-systemd-logs-daemon-set-b6f0dae2c3174900-cqnfx from sonobuoy started at 2023-07-29 15:24:46 +0000 UTC (2 container statuses recorded)
Jul 29 16:54:24.399: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 29 16:54:24.399: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699
STEP: Trying to launch a pod without a label to get a node which can launch it. 07/29/23 16:54:24.399
Jul 29 16:54:24.414: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-7259" to be "running"
Jul 29 16:54:24.418: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 4.309384ms
Jul 29 16:54:26.424: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.010416664s
Jul 29 16:54:26.425: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 07/29/23 16:54:26.431
STEP: Trying to apply a random label on the found node. 07/29/23 16:54:26.455
STEP: verifying the node has the label kubernetes.io/e2e-ed30dc43-d4d9-478d-858f-87cd45fbc4b9 95 07/29/23 16:54:26.47
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 07/29/23 16:54:26.476
Jul 29 16:54:26.488: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-7259" to be "not pending"
Jul 29 16:54:26.494: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.741295ms
Jul 29 16:54:28.500: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.012064471s
Jul 29 16:54:28.500: INFO: Pod "pod4" satisfied condition "not pending"
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 192.168.121.234 on the node which pod4 resides and expect not scheduled 07/29/23 16:54:28.5
Jul 29 16:54:28.511: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-7259" to be "not pending"
Jul 29 16:54:28.524: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 13.011114ms
Jul 29 16:54:30.535: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024092304s
Jul 29 16:54:32.532: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020465941s
Jul 29 16:54:34.536: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.025026442s
Jul 29 16:54:36.532: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.020200502s
Jul 29 16:54:38.533: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.021175777s
Jul 29 16:54:40.533: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.021418701s
Jul 29 16:54:42.533: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.021766586s
Jul 29 16:54:44.532: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.020180624s
Jul 29 16:54:46.534: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.022397015s
Jul 29 16:54:48.538: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.026699292s
Jul 29 16:54:50.535: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.02334114s
Jul 29 16:54:52.544: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.032996202s
Jul 29 16:54:54.532: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.020177048s
Jul 29 16:54:56.533: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.021947239s
Jul 29 16:54:58.539: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.027695621s
Jul 29 16:55:00.533: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.021303129s
Jul 29 16:55:02.533: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.021895405s
Jul 29 16:55:04.534: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.02273736s
Jul 29 16:55:06.532: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.020451102s
Jul 29 16:55:08.537: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.025950384s
Jul 29 16:55:10.532: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.021007454s
Jul 29 16:55:12.531: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.019781679s
Jul 29 16:55:14.535: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.02360412s
Jul 29 16:55:16.554: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.042978639s
Jul 29 16:55:18.534: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.02307524s
Jul 29 16:55:20.531: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.019826155s
Jul 29 16:55:22.533: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.02200208s
Jul 29 16:55:24.532: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.020863821s
Jul 29 16:55:26.532: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.020187446s
Jul 29 16:55:28.534: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.023078245s
Jul 29 16:55:30.535: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.023500977s
Jul 29 16:55:32.531: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.019196727s
Jul 29 16:55:34.533: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.021878699s
Jul 29 16:55:36.534: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.022708453s
Jul 29 16:55:38.532: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.020570126s
Jul 29 16:55:40.532: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.020573695s
Jul 29 16:55:42.533: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.021336383s
Jul 29 16:55:44.533: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.021211705s
Jul 29 16:55:46.533: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.02161823s
Jul 29 16:55:48.535: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.023886124s
Jul 29 16:55:50.532: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.020352646s
Jul 29 16:55:52.540: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.028690446s
Jul 29 16:55:54.531: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.019660206s
Jul 29 16:55:56.531: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.019712018s
Jul 29 16:55:58.532: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.021069247s
Jul 29 16:56:00.533: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.021423034s
Jul 29 16:56:02.533: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.021911781s
Jul 29 16:56:04.536: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.024624779s
Jul 29 16:56:06.534: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.022990665s
Jul 29 16:56:08.532: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.020185598s
Jul 29 16:56:10.531: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.019935105s
Jul 29 16:56:12.532: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.020285294s
Jul 29 16:56:14.536: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.025122186s
Jul 29 16:56:16.537: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.025560767s
Jul 29 16:56:18.531: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.019223975s
Jul 29 16:56:20.535: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.023282282s
Jul 29 16:56:22.531: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.019843661s
Jul 29 16:56:24.533: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.022003775s
Jul 29 16:56:26.538: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.026229517s
Jul 29 16:56:28.533: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.021558883s
Jul 29 16:56:30.532: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.02114354s
Jul 29 16:56:32.532: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.020542274s
Jul 29 16:56:34.534: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.023159859s
Jul 29 16:56:36.535: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.023593742s
Jul 29 16:56:38.533: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.022031712s
Jul 29 16:56:40.535: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.024096982s
Jul 29 16:56:42.532: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.020911821s
Jul 29 16:56:44.533: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.021348106s
Jul 29 16:56:46.533: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.021302929s
Jul 29 16:56:48.531: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.019637064s
Jul 29 16:56:50.531: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.019461126s
Jul 29 16:56:52.532: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.02045311s
Jul 29 16:56:54.533: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.021759818s
Jul 29 16:56:56.532: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.020637369s
Jul 29 16:56:58.533: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.021433961s
Jul 29 16:57:00.535: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.023635582s
Jul 29 16:57:02.532: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.020640168s
Jul 29 16:57:04.534: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.022523655s
Jul 29 16:57:06.531: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.019972459s
Jul 29 16:57:08.531: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.019368189s
Jul 29 16:57:10.531: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.020029556s
Jul 29 16:57:12.533: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.021778967s
Jul 29 16:57:14.533: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.021831039s
Jul 29 16:57:16.535: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.02358684s
Jul 29 16:57:18.531: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.019488845s
Jul 29 16:57:20.531: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.019878724s
Jul 29 16:57:22.531: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.019997921s
Jul 29 16:57:24.536: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.024198631s
Jul 29 16:57:26.532: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.020889606s
Jul 29 16:57:28.532: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.020953286s
Jul 29 16:57:30.531: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.019225649s
Jul 29 16:57:32.532: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.020178163s
Jul 29 16:57:34.534: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.022783214s
Jul 29 16:57:36.531: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.019969132s
Jul 29 16:57:38.536: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.025137835s
Jul 29 16:57:40.530: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.018823021s
Jul 29 16:57:42.536: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.024552191s
Jul 29 16:57:44.531: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.020125651s
Jul 29 16:57:46.533: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.022116976s
Jul 29 16:57:48.533: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.02178175s
Jul 29 16:57:50.533: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.02152278s
Jul 29 16:57:52.533: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.021418709s
Jul 29 16:57:54.531: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.019636039s
Jul 29 16:57:56.532: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.020354896s
Jul 29 16:57:58.541: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.029773774s
Jul 29 16:58:00.535: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.023751525s
Jul 29 16:58:02.533: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.021711643s
Jul 29 16:58:04.538: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.0270083s
Jul 29 16:58:06.533: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.021952521s
Jul 29 16:58:08.534: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.022633636s
Jul 29 16:58:10.533: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.021889745s
Jul 29 16:58:12.531: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.019547067s
Jul 29 16:58:14.534: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.022791025s
Jul 29 16:58:16.530: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.019014323s
Jul 29 16:58:18.533: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.021746382s
Jul 29 16:58:20.534: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.021628248s
Jul 29 16:58:22.534: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.022942736s
Jul 29 16:58:24.531: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.019875611s
Jul 29 16:58:26.533: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.022010865s
Jul 29 16:58:28.535: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.023805579s
Jul 29 16:58:30.531: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.019952745s
Jul 29 16:58:32.531: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.019379946s
Jul 29 16:58:34.534: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.023084603s
Jul 29 16:58:36.531: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.020161189s
Jul 29 16:58:38.533: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.021826389s
Jul 29 16:58:40.531: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.019426295s
Jul 29 16:58:42.533: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.021352348s
Jul 29 16:58:44.534: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.022308011s
Jul 29 16:58:46.535: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.023543379s
Jul 29 16:58:48.537: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.025318706s
Jul 29 16:58:50.531: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.019871933s
Jul 29 16:58:52.534: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.02230865s
Jul 29 16:58:54.535: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.02345712s
Jul 29 16:58:56.533: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.021560448s
Jul 29 16:58:58.531: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.019750072s
Jul 29 16:59:00.532: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.020265852s
Jul 29 16:59:02.532: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.021139395s
Jul 29 16:59:04.534: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.023056283s
Jul 29 16:59:06.533: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.021536606s
Jul 29 16:59:08.531: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.019470836s
Jul 29 16:59:10.534: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.02295768s
Jul 29 16:59:12.533: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.022014131s
Jul 29 16:59:14.535: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.023487275s
Jul 29 16:59:16.531: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.0199325s
Jul 29 16:59:18.531: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.020000061s
Jul 29 16:59:20.533: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.021708183s
Jul 29 16:59:22.533: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.021797624s
Jul 29 16:59:24.535: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.023654348s
Jul 29 16:59:26.532: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.020956004s
Jul 29 16:59:28.531: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.020133235s
Jul 29 16:59:28.537: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.025716896s
STEP: removing the label kubernetes.io/e2e-ed30dc43-d4d9-478d-858f-87cd45fbc4b9 off the node wa4quivohpee-3 07/29/23 16:59:28.537
STEP: verifying the node doesn't have the label kubernetes.io/e2e-ed30dc43-d4d9-478d-858f-87cd45fbc4b9 07/29/23 16:59:28.575
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Jul 29 16:59:28.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7259" for this suite. 07/29/23 16:59:28.604
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","completed":330,"skipped":6164,"failed":0}
------------------------------
â€¢ [SLOW TEST] [304.324 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:54:24.294
    Jul 29 16:54:24.294: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename sched-pred 07/29/23 16:54:24.296
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:54:24.332
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:54:24.336
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Jul 29 16:54:24.341: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jul 29 16:54:24.356: INFO: Waiting for terminating namespaces to be deleted...
    Jul 29 16:54:24.360: INFO: 
    Logging pods the apiserver thinks is on node wa4quivohpee-1 before test
    Jul 29 16:54:24.374: INFO: cilium-gfkpl from kube-system started at 2023-07-29 15:23:18 +0000 UTC (1 container statuses recorded)
    Jul 29 16:54:24.374: INFO: 	Container cilium-agent ready: true, restart count 0
    Jul 29 16:54:24.374: INFO: cilium-node-init-n8876 from kube-system started at 2023-07-29 15:23:18 +0000 UTC (1 container statuses recorded)
    Jul 29 16:54:24.374: INFO: 	Container node-init ready: true, restart count 0
    Jul 29 16:54:24.374: INFO: coredns-565d847f94-xm9ff from kube-system started at 2023-07-29 15:34:18 +0000 UTC (1 container statuses recorded)
    Jul 29 16:54:24.374: INFO: 	Container coredns ready: true, restart count 0
    Jul 29 16:54:24.374: INFO: kube-addon-manager-wa4quivohpee-1 from kube-system started at 2023-07-29 15:22:59 +0000 UTC (1 container statuses recorded)
    Jul 29 16:54:24.374: INFO: 	Container kube-addon-manager ready: true, restart count 0
    Jul 29 16:54:24.374: INFO: kube-apiserver-wa4quivohpee-1 from kube-system started at 2023-07-29 15:14:24 +0000 UTC (1 container statuses recorded)
    Jul 29 16:54:24.374: INFO: 	Container kube-apiserver ready: true, restart count 0
    Jul 29 16:54:24.374: INFO: kube-controller-manager-wa4quivohpee-1 from kube-system started at 2023-07-29 15:14:24 +0000 UTC (1 container statuses recorded)
    Jul 29 16:54:24.374: INFO: 	Container kube-controller-manager ready: true, restart count 0
    Jul 29 16:54:24.374: INFO: kube-proxy-gdffl from kube-system started at 2023-07-29 15:14:37 +0000 UTC (1 container statuses recorded)
    Jul 29 16:54:24.374: INFO: 	Container kube-proxy ready: true, restart count 0
    Jul 29 16:54:24.374: INFO: kube-scheduler-wa4quivohpee-1 from kube-system started at 2023-07-29 15:14:23 +0000 UTC (1 container statuses recorded)
    Jul 29 16:54:24.374: INFO: 	Container kube-scheduler ready: true, restart count 0
    Jul 29 16:54:24.374: INFO: sonobuoy-systemd-logs-daemon-set-b6f0dae2c3174900-mtck5 from sonobuoy started at 2023-07-29 15:24:46 +0000 UTC (2 container statuses recorded)
    Jul 29 16:54:24.374: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jul 29 16:54:24.374: INFO: 	Container systemd-logs ready: true, restart count 0
    Jul 29 16:54:24.374: INFO: 
    Logging pods the apiserver thinks is on node wa4quivohpee-2 before test
    Jul 29 16:54:24.387: INFO: cilium-node-init-946z9 from kube-system started at 2023-07-29 15:23:18 +0000 UTC (1 container statuses recorded)
    Jul 29 16:54:24.387: INFO: 	Container node-init ready: true, restart count 0
    Jul 29 16:54:24.387: INFO: cilium-rmjkx from kube-system started at 2023-07-29 15:23:18 +0000 UTC (1 container statuses recorded)
    Jul 29 16:54:24.387: INFO: 	Container cilium-agent ready: true, restart count 0
    Jul 29 16:54:24.387: INFO: coredns-565d847f94-wmg75 from kube-system started at 2023-07-29 15:24:27 +0000 UTC (1 container statuses recorded)
    Jul 29 16:54:24.387: INFO: 	Container coredns ready: true, restart count 0
    Jul 29 16:54:24.387: INFO: kube-addon-manager-wa4quivohpee-2 from kube-system started at 2023-07-29 15:22:59 +0000 UTC (1 container statuses recorded)
    Jul 29 16:54:24.387: INFO: 	Container kube-addon-manager ready: true, restart count 0
    Jul 29 16:54:24.387: INFO: kube-apiserver-wa4quivohpee-2 from kube-system started at 2023-07-29 15:15:22 +0000 UTC (1 container statuses recorded)
    Jul 29 16:54:24.387: INFO: 	Container kube-apiserver ready: true, restart count 0
    Jul 29 16:54:24.387: INFO: kube-controller-manager-wa4quivohpee-2 from kube-system started at 2023-07-29 15:15:22 +0000 UTC (1 container statuses recorded)
    Jul 29 16:54:24.387: INFO: 	Container kube-controller-manager ready: true, restart count 0
    Jul 29 16:54:24.387: INFO: kube-proxy-74hn2 from kube-system started at 2023-07-29 15:15:19 +0000 UTC (1 container statuses recorded)
    Jul 29 16:54:24.387: INFO: 	Container kube-proxy ready: true, restart count 1
    Jul 29 16:54:24.387: INFO: kube-scheduler-wa4quivohpee-2 from kube-system started at 2023-07-29 15:15:22 +0000 UTC (1 container statuses recorded)
    Jul 29 16:54:24.387: INFO: 	Container kube-scheduler ready: true, restart count 0
    Jul 29 16:54:24.387: INFO: sonobuoy-systemd-logs-daemon-set-b6f0dae2c3174900-cnfp2 from sonobuoy started at 2023-07-29 15:24:46 +0000 UTC (2 container statuses recorded)
    Jul 29 16:54:24.388: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jul 29 16:54:24.388: INFO: 	Container systemd-logs ready: true, restart count 0
    Jul 29 16:54:24.388: INFO: 
    Logging pods the apiserver thinks is on node wa4quivohpee-3 before test
    Jul 29 16:54:24.398: INFO: cilium-node-init-j4pkd from kube-system started at 2023-07-29 15:23:18 +0000 UTC (1 container statuses recorded)
    Jul 29 16:54:24.398: INFO: 	Container node-init ready: true, restart count 0
    Jul 29 16:54:24.398: INFO: cilium-operator-69cffcb958-48gkl from kube-system started at 2023-07-29 15:23:19 +0000 UTC (1 container statuses recorded)
    Jul 29 16:54:24.398: INFO: 	Container cilium-operator ready: true, restart count 0
    Jul 29 16:54:24.398: INFO: cilium-zl28x from kube-system started at 2023-07-29 15:23:18 +0000 UTC (1 container statuses recorded)
    Jul 29 16:54:24.398: INFO: 	Container cilium-agent ready: true, restart count 0
    Jul 29 16:54:24.398: INFO: kube-proxy-ts2pg from kube-system started at 2023-07-29 15:16:17 +0000 UTC (1 container statuses recorded)
    Jul 29 16:54:24.398: INFO: 	Container kube-proxy ready: true, restart count 0
    Jul 29 16:54:24.399: INFO: sonobuoy from sonobuoy started at 2023-07-29 15:24:31 +0000 UTC (1 container statuses recorded)
    Jul 29 16:54:24.399: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jul 29 16:54:24.399: INFO: sonobuoy-e2e-job-5c9a7da3e6b74e15 from sonobuoy started at 2023-07-29 15:24:45 +0000 UTC (2 container statuses recorded)
    Jul 29 16:54:24.399: INFO: 	Container e2e ready: true, restart count 0
    Jul 29 16:54:24.399: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jul 29 16:54:24.399: INFO: sonobuoy-systemd-logs-daemon-set-b6f0dae2c3174900-cqnfx from sonobuoy started at 2023-07-29 15:24:46 +0000 UTC (2 container statuses recorded)
    Jul 29 16:54:24.399: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jul 29 16:54:24.399: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
      test/e2e/scheduling/predicates.go:699
    STEP: Trying to launch a pod without a label to get a node which can launch it. 07/29/23 16:54:24.399
    Jul 29 16:54:24.414: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-7259" to be "running"
    Jul 29 16:54:24.418: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 4.309384ms
    Jul 29 16:54:26.424: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.010416664s
    Jul 29 16:54:26.425: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 07/29/23 16:54:26.431
    STEP: Trying to apply a random label on the found node. 07/29/23 16:54:26.455
    STEP: verifying the node has the label kubernetes.io/e2e-ed30dc43-d4d9-478d-858f-87cd45fbc4b9 95 07/29/23 16:54:26.47
    STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 07/29/23 16:54:26.476
    Jul 29 16:54:26.488: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-7259" to be "not pending"
    Jul 29 16:54:26.494: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.741295ms
    Jul 29 16:54:28.500: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.012064471s
    Jul 29 16:54:28.500: INFO: Pod "pod4" satisfied condition "not pending"
    STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 192.168.121.234 on the node which pod4 resides and expect not scheduled 07/29/23 16:54:28.5
    Jul 29 16:54:28.511: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-7259" to be "not pending"
    Jul 29 16:54:28.524: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 13.011114ms
    Jul 29 16:54:30.535: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024092304s
    Jul 29 16:54:32.532: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020465941s
    Jul 29 16:54:34.536: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.025026442s
    Jul 29 16:54:36.532: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.020200502s
    Jul 29 16:54:38.533: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.021175777s
    Jul 29 16:54:40.533: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.021418701s
    Jul 29 16:54:42.533: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.021766586s
    Jul 29 16:54:44.532: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.020180624s
    Jul 29 16:54:46.534: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.022397015s
    Jul 29 16:54:48.538: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.026699292s
    Jul 29 16:54:50.535: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.02334114s
    Jul 29 16:54:52.544: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.032996202s
    Jul 29 16:54:54.532: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.020177048s
    Jul 29 16:54:56.533: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.021947239s
    Jul 29 16:54:58.539: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.027695621s
    Jul 29 16:55:00.533: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.021303129s
    Jul 29 16:55:02.533: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.021895405s
    Jul 29 16:55:04.534: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.02273736s
    Jul 29 16:55:06.532: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.020451102s
    Jul 29 16:55:08.537: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.025950384s
    Jul 29 16:55:10.532: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.021007454s
    Jul 29 16:55:12.531: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.019781679s
    Jul 29 16:55:14.535: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.02360412s
    Jul 29 16:55:16.554: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.042978639s
    Jul 29 16:55:18.534: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.02307524s
    Jul 29 16:55:20.531: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.019826155s
    Jul 29 16:55:22.533: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.02200208s
    Jul 29 16:55:24.532: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.020863821s
    Jul 29 16:55:26.532: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.020187446s
    Jul 29 16:55:28.534: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.023078245s
    Jul 29 16:55:30.535: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.023500977s
    Jul 29 16:55:32.531: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.019196727s
    Jul 29 16:55:34.533: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.021878699s
    Jul 29 16:55:36.534: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.022708453s
    Jul 29 16:55:38.532: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.020570126s
    Jul 29 16:55:40.532: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.020573695s
    Jul 29 16:55:42.533: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.021336383s
    Jul 29 16:55:44.533: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.021211705s
    Jul 29 16:55:46.533: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.02161823s
    Jul 29 16:55:48.535: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.023886124s
    Jul 29 16:55:50.532: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.020352646s
    Jul 29 16:55:52.540: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.028690446s
    Jul 29 16:55:54.531: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.019660206s
    Jul 29 16:55:56.531: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.019712018s
    Jul 29 16:55:58.532: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.021069247s
    Jul 29 16:56:00.533: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.021423034s
    Jul 29 16:56:02.533: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.021911781s
    Jul 29 16:56:04.536: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.024624779s
    Jul 29 16:56:06.534: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.022990665s
    Jul 29 16:56:08.532: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.020185598s
    Jul 29 16:56:10.531: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.019935105s
    Jul 29 16:56:12.532: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.020285294s
    Jul 29 16:56:14.536: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.025122186s
    Jul 29 16:56:16.537: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.025560767s
    Jul 29 16:56:18.531: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.019223975s
    Jul 29 16:56:20.535: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.023282282s
    Jul 29 16:56:22.531: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.019843661s
    Jul 29 16:56:24.533: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.022003775s
    Jul 29 16:56:26.538: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.026229517s
    Jul 29 16:56:28.533: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.021558883s
    Jul 29 16:56:30.532: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.02114354s
    Jul 29 16:56:32.532: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.020542274s
    Jul 29 16:56:34.534: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.023159859s
    Jul 29 16:56:36.535: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.023593742s
    Jul 29 16:56:38.533: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.022031712s
    Jul 29 16:56:40.535: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.024096982s
    Jul 29 16:56:42.532: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.020911821s
    Jul 29 16:56:44.533: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.021348106s
    Jul 29 16:56:46.533: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.021302929s
    Jul 29 16:56:48.531: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.019637064s
    Jul 29 16:56:50.531: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.019461126s
    Jul 29 16:56:52.532: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.02045311s
    Jul 29 16:56:54.533: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.021759818s
    Jul 29 16:56:56.532: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.020637369s
    Jul 29 16:56:58.533: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.021433961s
    Jul 29 16:57:00.535: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.023635582s
    Jul 29 16:57:02.532: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.020640168s
    Jul 29 16:57:04.534: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.022523655s
    Jul 29 16:57:06.531: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.019972459s
    Jul 29 16:57:08.531: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.019368189s
    Jul 29 16:57:10.531: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.020029556s
    Jul 29 16:57:12.533: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.021778967s
    Jul 29 16:57:14.533: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.021831039s
    Jul 29 16:57:16.535: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.02358684s
    Jul 29 16:57:18.531: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.019488845s
    Jul 29 16:57:20.531: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.019878724s
    Jul 29 16:57:22.531: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.019997921s
    Jul 29 16:57:24.536: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.024198631s
    Jul 29 16:57:26.532: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.020889606s
    Jul 29 16:57:28.532: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.020953286s
    Jul 29 16:57:30.531: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.019225649s
    Jul 29 16:57:32.532: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.020178163s
    Jul 29 16:57:34.534: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.022783214s
    Jul 29 16:57:36.531: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.019969132s
    Jul 29 16:57:38.536: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.025137835s
    Jul 29 16:57:40.530: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.018823021s
    Jul 29 16:57:42.536: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.024552191s
    Jul 29 16:57:44.531: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.020125651s
    Jul 29 16:57:46.533: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.022116976s
    Jul 29 16:57:48.533: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.02178175s
    Jul 29 16:57:50.533: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.02152278s
    Jul 29 16:57:52.533: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.021418709s
    Jul 29 16:57:54.531: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.019636039s
    Jul 29 16:57:56.532: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.020354896s
    Jul 29 16:57:58.541: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.029773774s
    Jul 29 16:58:00.535: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.023751525s
    Jul 29 16:58:02.533: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.021711643s
    Jul 29 16:58:04.538: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.0270083s
    Jul 29 16:58:06.533: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.021952521s
    Jul 29 16:58:08.534: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.022633636s
    Jul 29 16:58:10.533: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.021889745s
    Jul 29 16:58:12.531: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.019547067s
    Jul 29 16:58:14.534: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.022791025s
    Jul 29 16:58:16.530: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.019014323s
    Jul 29 16:58:18.533: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.021746382s
    Jul 29 16:58:20.534: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.021628248s
    Jul 29 16:58:22.534: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.022942736s
    Jul 29 16:58:24.531: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.019875611s
    Jul 29 16:58:26.533: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.022010865s
    Jul 29 16:58:28.535: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.023805579s
    Jul 29 16:58:30.531: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.019952745s
    Jul 29 16:58:32.531: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.019379946s
    Jul 29 16:58:34.534: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.023084603s
    Jul 29 16:58:36.531: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.020161189s
    Jul 29 16:58:38.533: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.021826389s
    Jul 29 16:58:40.531: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.019426295s
    Jul 29 16:58:42.533: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.021352348s
    Jul 29 16:58:44.534: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.022308011s
    Jul 29 16:58:46.535: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.023543379s
    Jul 29 16:58:48.537: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.025318706s
    Jul 29 16:58:50.531: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.019871933s
    Jul 29 16:58:52.534: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.02230865s
    Jul 29 16:58:54.535: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.02345712s
    Jul 29 16:58:56.533: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.021560448s
    Jul 29 16:58:58.531: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.019750072s
    Jul 29 16:59:00.532: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.020265852s
    Jul 29 16:59:02.532: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.021139395s
    Jul 29 16:59:04.534: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.023056283s
    Jul 29 16:59:06.533: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.021536606s
    Jul 29 16:59:08.531: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.019470836s
    Jul 29 16:59:10.534: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.02295768s
    Jul 29 16:59:12.533: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.022014131s
    Jul 29 16:59:14.535: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.023487275s
    Jul 29 16:59:16.531: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.0199325s
    Jul 29 16:59:18.531: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.020000061s
    Jul 29 16:59:20.533: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.021708183s
    Jul 29 16:59:22.533: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.021797624s
    Jul 29 16:59:24.535: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.023654348s
    Jul 29 16:59:26.532: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.020956004s
    Jul 29 16:59:28.531: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.020133235s
    Jul 29 16:59:28.537: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.025716896s
    STEP: removing the label kubernetes.io/e2e-ed30dc43-d4d9-478d-858f-87cd45fbc4b9 off the node wa4quivohpee-3 07/29/23 16:59:28.537
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-ed30dc43-d4d9-478d-858f-87cd45fbc4b9 07/29/23 16:59:28.575
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Jul 29 16:59:28.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-7259" for this suite. 07/29/23 16:59:28.604
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:59:28.627
Jul 29 16:59:28.628: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename crd-publish-openapi 07/29/23 16:59:28.631
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:59:28.669
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:59:28.675
[It] works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152
Jul 29 16:59:28.685: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 07/29/23 16:59:32.648
Jul 29 16:59:32.649: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=crd-publish-openapi-778 --namespace=crd-publish-openapi-778 create -f -'
Jul 29 16:59:34.423: INFO: stderr: ""
Jul 29 16:59:34.423: INFO: stdout: "e2e-test-crd-publish-openapi-796-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jul 29 16:59:34.428: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=crd-publish-openapi-778 --namespace=crd-publish-openapi-778 delete e2e-test-crd-publish-openapi-796-crds test-cr'
Jul 29 16:59:34.614: INFO: stderr: ""
Jul 29 16:59:34.614: INFO: stdout: "e2e-test-crd-publish-openapi-796-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Jul 29 16:59:34.616: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=crd-publish-openapi-778 --namespace=crd-publish-openapi-778 apply -f -'
Jul 29 16:59:35.174: INFO: stderr: ""
Jul 29 16:59:35.174: INFO: stdout: "e2e-test-crd-publish-openapi-796-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jul 29 16:59:35.175: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=crd-publish-openapi-778 --namespace=crd-publish-openapi-778 delete e2e-test-crd-publish-openapi-796-crds test-cr'
Jul 29 16:59:35.468: INFO: stderr: ""
Jul 29 16:59:35.468: INFO: stdout: "e2e-test-crd-publish-openapi-796-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema 07/29/23 16:59:35.468
Jul 29 16:59:35.468: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=crd-publish-openapi-778 explain e2e-test-crd-publish-openapi-796-crds'
Jul 29 16:59:36.522: INFO: stderr: ""
Jul 29 16:59:36.525: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-796-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jul 29 16:59:42.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-778" for this suite. 07/29/23 16:59:42.337
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","completed":331,"skipped":6167,"failed":0}
------------------------------
â€¢ [SLOW TEST] [13.723 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:59:28.627
    Jul 29 16:59:28.628: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename crd-publish-openapi 07/29/23 16:59:28.631
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:59:28.669
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:59:28.675
    [It] works for CRD without validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:152
    Jul 29 16:59:28.685: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 07/29/23 16:59:32.648
    Jul 29 16:59:32.649: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=crd-publish-openapi-778 --namespace=crd-publish-openapi-778 create -f -'
    Jul 29 16:59:34.423: INFO: stderr: ""
    Jul 29 16:59:34.423: INFO: stdout: "e2e-test-crd-publish-openapi-796-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Jul 29 16:59:34.428: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=crd-publish-openapi-778 --namespace=crd-publish-openapi-778 delete e2e-test-crd-publish-openapi-796-crds test-cr'
    Jul 29 16:59:34.614: INFO: stderr: ""
    Jul 29 16:59:34.614: INFO: stdout: "e2e-test-crd-publish-openapi-796-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    Jul 29 16:59:34.616: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=crd-publish-openapi-778 --namespace=crd-publish-openapi-778 apply -f -'
    Jul 29 16:59:35.174: INFO: stderr: ""
    Jul 29 16:59:35.174: INFO: stdout: "e2e-test-crd-publish-openapi-796-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Jul 29 16:59:35.175: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=crd-publish-openapi-778 --namespace=crd-publish-openapi-778 delete e2e-test-crd-publish-openapi-796-crds test-cr'
    Jul 29 16:59:35.468: INFO: stderr: ""
    Jul 29 16:59:35.468: INFO: stdout: "e2e-test-crd-publish-openapi-796-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR without validation schema 07/29/23 16:59:35.468
    Jul 29 16:59:35.468: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=crd-publish-openapi-778 explain e2e-test-crd-publish-openapi-796-crds'
    Jul 29 16:59:36.522: INFO: stderr: ""
    Jul 29 16:59:36.525: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-796-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jul 29 16:59:42.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-778" for this suite. 07/29/23 16:59:42.337
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:59:42.361
Jul 29 16:59:42.362: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename services 07/29/23 16:59:42.365
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:59:42.395
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:59:42.399
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791
STEP: creating service endpoint-test2 in namespace services-4064 07/29/23 16:59:42.403
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4064 to expose endpoints map[] 07/29/23 16:59:42.428
Jul 29 16:59:42.449: INFO: successfully validated that service endpoint-test2 in namespace services-4064 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-4064 07/29/23 16:59:42.449
Jul 29 16:59:42.470: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-4064" to be "running and ready"
Jul 29 16:59:42.476: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.848121ms
Jul 29 16:59:42.476: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jul 29 16:59:44.503: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.033422114s
Jul 29 16:59:44.503: INFO: The phase of Pod pod1 is Running (Ready = true)
Jul 29 16:59:44.504: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4064 to expose endpoints map[pod1:[80]] 07/29/23 16:59:44.508
Jul 29 16:59:44.529: INFO: successfully validated that service endpoint-test2 in namespace services-4064 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1 07/29/23 16:59:44.53
Jul 29 16:59:44.530: INFO: Creating new exec pod
Jul 29 16:59:44.538: INFO: Waiting up to 5m0s for pod "execpodhl7fh" in namespace "services-4064" to be "running"
Jul 29 16:59:44.544: INFO: Pod "execpodhl7fh": Phase="Pending", Reason="", readiness=false. Elapsed: 5.78485ms
Jul 29 16:59:46.552: INFO: Pod "execpodhl7fh": Phase="Running", Reason="", readiness=true. Elapsed: 2.013717689s
Jul 29 16:59:46.552: INFO: Pod "execpodhl7fh" satisfied condition "running"
Jul 29 16:59:47.555: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=services-4064 exec execpodhl7fh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Jul 29 16:59:47.857: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jul 29 16:59:47.857: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul 29 16:59:47.858: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=services-4064 exec execpodhl7fh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.55.0 80'
Jul 29 16:59:48.106: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.55.0 80\nConnection to 10.233.55.0 80 port [tcp/http] succeeded!\n"
Jul 29 16:59:48.106: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Creating pod pod2 in namespace services-4064 07/29/23 16:59:48.107
Jul 29 16:59:48.123: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-4064" to be "running and ready"
Jul 29 16:59:48.131: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.219219ms
Jul 29 16:59:48.131: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jul 29 16:59:50.141: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.018253157s
Jul 29 16:59:50.141: INFO: The phase of Pod pod2 is Running (Ready = true)
Jul 29 16:59:50.141: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4064 to expose endpoints map[pod1:[80] pod2:[80]] 07/29/23 16:59:50.148
Jul 29 16:59:50.171: INFO: successfully validated that service endpoint-test2 in namespace services-4064 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2 07/29/23 16:59:50.172
Jul 29 16:59:51.173: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=services-4064 exec execpodhl7fh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Jul 29 16:59:51.437: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jul 29 16:59:51.437: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul 29 16:59:51.438: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=services-4064 exec execpodhl7fh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.55.0 80'
Jul 29 16:59:51.682: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.55.0 80\nConnection to 10.233.55.0 80 port [tcp/http] succeeded!\n"
Jul 29 16:59:51.683: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-4064 07/29/23 16:59:51.683
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4064 to expose endpoints map[pod2:[80]] 07/29/23 16:59:51.712
Jul 29 16:59:51.739: INFO: successfully validated that service endpoint-test2 in namespace services-4064 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2 07/29/23 16:59:51.739
Jul 29 16:59:52.741: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=services-4064 exec execpodhl7fh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Jul 29 16:59:52.965: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jul 29 16:59:52.965: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul 29 16:59:52.966: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=services-4064 exec execpodhl7fh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.55.0 80'
Jul 29 16:59:53.240: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.55.0 80\nConnection to 10.233.55.0 80 port [tcp/http] succeeded!\n"
Jul 29 16:59:53.240: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod2 in namespace services-4064 07/29/23 16:59:53.24
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4064 to expose endpoints map[] 07/29/23 16:59:53.283
Jul 29 16:59:54.327: INFO: successfully validated that service endpoint-test2 in namespace services-4064 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jul 29 16:59:54.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4064" for this suite. 07/29/23 16:59:54.379
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","completed":332,"skipped":6190,"failed":0}
------------------------------
â€¢ [SLOW TEST] [12.036 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:59:42.361
    Jul 29 16:59:42.362: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename services 07/29/23 16:59:42.365
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:59:42.395
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:59:42.399
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should serve a basic endpoint from pods  [Conformance]
      test/e2e/network/service.go:791
    STEP: creating service endpoint-test2 in namespace services-4064 07/29/23 16:59:42.403
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4064 to expose endpoints map[] 07/29/23 16:59:42.428
    Jul 29 16:59:42.449: INFO: successfully validated that service endpoint-test2 in namespace services-4064 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-4064 07/29/23 16:59:42.449
    Jul 29 16:59:42.470: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-4064" to be "running and ready"
    Jul 29 16:59:42.476: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.848121ms
    Jul 29 16:59:42.476: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jul 29 16:59:44.503: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.033422114s
    Jul 29 16:59:44.503: INFO: The phase of Pod pod1 is Running (Ready = true)
    Jul 29 16:59:44.504: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4064 to expose endpoints map[pod1:[80]] 07/29/23 16:59:44.508
    Jul 29 16:59:44.529: INFO: successfully validated that service endpoint-test2 in namespace services-4064 exposes endpoints map[pod1:[80]]
    STEP: Checking if the Service forwards traffic to pod1 07/29/23 16:59:44.53
    Jul 29 16:59:44.530: INFO: Creating new exec pod
    Jul 29 16:59:44.538: INFO: Waiting up to 5m0s for pod "execpodhl7fh" in namespace "services-4064" to be "running"
    Jul 29 16:59:44.544: INFO: Pod "execpodhl7fh": Phase="Pending", Reason="", readiness=false. Elapsed: 5.78485ms
    Jul 29 16:59:46.552: INFO: Pod "execpodhl7fh": Phase="Running", Reason="", readiness=true. Elapsed: 2.013717689s
    Jul 29 16:59:46.552: INFO: Pod "execpodhl7fh" satisfied condition "running"
    Jul 29 16:59:47.555: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=services-4064 exec execpodhl7fh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    Jul 29 16:59:47.857: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Jul 29 16:59:47.857: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jul 29 16:59:47.858: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=services-4064 exec execpodhl7fh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.55.0 80'
    Jul 29 16:59:48.106: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.55.0 80\nConnection to 10.233.55.0 80 port [tcp/http] succeeded!\n"
    Jul 29 16:59:48.106: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Creating pod pod2 in namespace services-4064 07/29/23 16:59:48.107
    Jul 29 16:59:48.123: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-4064" to be "running and ready"
    Jul 29 16:59:48.131: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.219219ms
    Jul 29 16:59:48.131: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jul 29 16:59:50.141: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.018253157s
    Jul 29 16:59:50.141: INFO: The phase of Pod pod2 is Running (Ready = true)
    Jul 29 16:59:50.141: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4064 to expose endpoints map[pod1:[80] pod2:[80]] 07/29/23 16:59:50.148
    Jul 29 16:59:50.171: INFO: successfully validated that service endpoint-test2 in namespace services-4064 exposes endpoints map[pod1:[80] pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod1 and pod2 07/29/23 16:59:50.172
    Jul 29 16:59:51.173: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=services-4064 exec execpodhl7fh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    Jul 29 16:59:51.437: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Jul 29 16:59:51.437: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jul 29 16:59:51.438: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=services-4064 exec execpodhl7fh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.55.0 80'
    Jul 29 16:59:51.682: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.55.0 80\nConnection to 10.233.55.0 80 port [tcp/http] succeeded!\n"
    Jul 29 16:59:51.683: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod1 in namespace services-4064 07/29/23 16:59:51.683
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4064 to expose endpoints map[pod2:[80]] 07/29/23 16:59:51.712
    Jul 29 16:59:51.739: INFO: successfully validated that service endpoint-test2 in namespace services-4064 exposes endpoints map[pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod2 07/29/23 16:59:51.739
    Jul 29 16:59:52.741: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=services-4064 exec execpodhl7fh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    Jul 29 16:59:52.965: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Jul 29 16:59:52.965: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jul 29 16:59:52.966: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=services-4064 exec execpodhl7fh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.55.0 80'
    Jul 29 16:59:53.240: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.55.0 80\nConnection to 10.233.55.0 80 port [tcp/http] succeeded!\n"
    Jul 29 16:59:53.240: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod2 in namespace services-4064 07/29/23 16:59:53.24
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4064 to expose endpoints map[] 07/29/23 16:59:53.283
    Jul 29 16:59:54.327: INFO: successfully validated that service endpoint-test2 in namespace services-4064 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jul 29 16:59:54.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-4064" for this suite. 07/29/23 16:59:54.379
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:59:54.401
Jul 29 16:59:54.401: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename proxy 07/29/23 16:59:54.404
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:59:54.441
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:59:54.451
[It] A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
Jul 29 16:59:54.457: INFO: Creating pod...
Jul 29 16:59:54.474: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-6851" to be "running"
Jul 29 16:59:54.483: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 9.00507ms
Jul 29 16:59:56.492: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.01776927s
Jul 29 16:59:56.492: INFO: Pod "agnhost" satisfied condition "running"
Jul 29 16:59:56.492: INFO: Creating service...
Jul 29 16:59:56.524: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6851/pods/agnhost/proxy?method=DELETE
Jul 29 16:59:56.548: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jul 29 16:59:56.548: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6851/pods/agnhost/proxy?method=OPTIONS
Jul 29 16:59:56.560: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jul 29 16:59:56.560: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6851/pods/agnhost/proxy?method=PATCH
Jul 29 16:59:56.566: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jul 29 16:59:56.566: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6851/pods/agnhost/proxy?method=POST
Jul 29 16:59:56.573: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jul 29 16:59:56.574: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6851/pods/agnhost/proxy?method=PUT
Jul 29 16:59:56.582: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jul 29 16:59:56.582: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6851/services/e2e-proxy-test-service/proxy?method=DELETE
Jul 29 16:59:56.592: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jul 29 16:59:56.593: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6851/services/e2e-proxy-test-service/proxy?method=OPTIONS
Jul 29 16:59:56.605: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jul 29 16:59:56.605: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6851/services/e2e-proxy-test-service/proxy?method=PATCH
Jul 29 16:59:56.616: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jul 29 16:59:56.616: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6851/services/e2e-proxy-test-service/proxy?method=POST
Jul 29 16:59:56.626: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jul 29 16:59:56.626: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6851/services/e2e-proxy-test-service/proxy?method=PUT
Jul 29 16:59:56.634: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jul 29 16:59:56.634: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6851/pods/agnhost/proxy?method=GET
Jul 29 16:59:56.639: INFO: http.Client request:GET StatusCode:301
Jul 29 16:59:56.639: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6851/services/e2e-proxy-test-service/proxy?method=GET
Jul 29 16:59:56.645: INFO: http.Client request:GET StatusCode:301
Jul 29 16:59:56.645: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6851/pods/agnhost/proxy?method=HEAD
Jul 29 16:59:56.650: INFO: http.Client request:HEAD StatusCode:301
Jul 29 16:59:56.650: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6851/services/e2e-proxy-test-service/proxy?method=HEAD
Jul 29 16:59:56.658: INFO: http.Client request:HEAD StatusCode:301
[AfterEach] version v1
  test/e2e/framework/framework.go:187
Jul 29 16:59:56.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-6851" for this suite. 07/29/23 16:59:56.668
{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service Proxy [Conformance]","completed":333,"skipped":6193,"failed":0}
------------------------------
â€¢ [2.282 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service Proxy [Conformance]
    test/e2e/network/proxy.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:59:54.401
    Jul 29 16:59:54.401: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename proxy 07/29/23 16:59:54.404
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:59:54.441
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:59:54.451
    [It] A set of valid responses are returned for both pod and service Proxy [Conformance]
      test/e2e/network/proxy.go:380
    Jul 29 16:59:54.457: INFO: Creating pod...
    Jul 29 16:59:54.474: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-6851" to be "running"
    Jul 29 16:59:54.483: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 9.00507ms
    Jul 29 16:59:56.492: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.01776927s
    Jul 29 16:59:56.492: INFO: Pod "agnhost" satisfied condition "running"
    Jul 29 16:59:56.492: INFO: Creating service...
    Jul 29 16:59:56.524: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6851/pods/agnhost/proxy?method=DELETE
    Jul 29 16:59:56.548: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jul 29 16:59:56.548: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6851/pods/agnhost/proxy?method=OPTIONS
    Jul 29 16:59:56.560: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jul 29 16:59:56.560: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6851/pods/agnhost/proxy?method=PATCH
    Jul 29 16:59:56.566: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jul 29 16:59:56.566: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6851/pods/agnhost/proxy?method=POST
    Jul 29 16:59:56.573: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jul 29 16:59:56.574: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6851/pods/agnhost/proxy?method=PUT
    Jul 29 16:59:56.582: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Jul 29 16:59:56.582: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6851/services/e2e-proxy-test-service/proxy?method=DELETE
    Jul 29 16:59:56.592: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jul 29 16:59:56.593: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6851/services/e2e-proxy-test-service/proxy?method=OPTIONS
    Jul 29 16:59:56.605: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jul 29 16:59:56.605: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6851/services/e2e-proxy-test-service/proxy?method=PATCH
    Jul 29 16:59:56.616: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jul 29 16:59:56.616: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6851/services/e2e-proxy-test-service/proxy?method=POST
    Jul 29 16:59:56.626: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jul 29 16:59:56.626: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6851/services/e2e-proxy-test-service/proxy?method=PUT
    Jul 29 16:59:56.634: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Jul 29 16:59:56.634: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6851/pods/agnhost/proxy?method=GET
    Jul 29 16:59:56.639: INFO: http.Client request:GET StatusCode:301
    Jul 29 16:59:56.639: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6851/services/e2e-proxy-test-service/proxy?method=GET
    Jul 29 16:59:56.645: INFO: http.Client request:GET StatusCode:301
    Jul 29 16:59:56.645: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6851/pods/agnhost/proxy?method=HEAD
    Jul 29 16:59:56.650: INFO: http.Client request:HEAD StatusCode:301
    Jul 29 16:59:56.650: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6851/services/e2e-proxy-test-service/proxy?method=HEAD
    Jul 29 16:59:56.658: INFO: http.Client request:HEAD StatusCode:301
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    Jul 29 16:59:56.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-6851" for this suite. 07/29/23 16:59:56.668
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 16:59:56.686
Jul 29 16:59:56.687: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename job 07/29/23 16:59:56.689
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:59:56.72
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:59:56.724
[It] should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335
STEP: Creating a job 07/29/23 16:59:56.728
STEP: Ensuring active pods == parallelism 07/29/23 16:59:56.742
STEP: Orphaning one of the Job's Pods 07/29/23 17:00:00.752
Jul 29 17:00:01.283: INFO: Successfully updated pod "adopt-release-nckwf"
STEP: Checking that the Job readopts the Pod 07/29/23 17:00:01.283
Jul 29 17:00:01.283: INFO: Waiting up to 15m0s for pod "adopt-release-nckwf" in namespace "job-2209" to be "adopted"
Jul 29 17:00:01.293: INFO: Pod "adopt-release-nckwf": Phase="Running", Reason="", readiness=true. Elapsed: 9.140731ms
Jul 29 17:00:03.301: INFO: Pod "adopt-release-nckwf": Phase="Running", Reason="", readiness=true. Elapsed: 2.017413026s
Jul 29 17:00:03.301: INFO: Pod "adopt-release-nckwf" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod 07/29/23 17:00:03.301
Jul 29 17:00:03.823: INFO: Successfully updated pod "adopt-release-nckwf"
STEP: Checking that the Job releases the Pod 07/29/23 17:00:03.823
Jul 29 17:00:03.823: INFO: Waiting up to 15m0s for pod "adopt-release-nckwf" in namespace "job-2209" to be "released"
Jul 29 17:00:03.834: INFO: Pod "adopt-release-nckwf": Phase="Running", Reason="", readiness=true. Elapsed: 10.545664ms
Jul 29 17:00:05.843: INFO: Pod "adopt-release-nckwf": Phase="Running", Reason="", readiness=true. Elapsed: 2.019959644s
Jul 29 17:00:05.844: INFO: Pod "adopt-release-nckwf" satisfied condition "released"
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Jul 29 17:00:05.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-2209" for this suite. 07/29/23 17:00:05.858
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","completed":334,"skipped":6207,"failed":0}
------------------------------
â€¢ [SLOW TEST] [9.183 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 16:59:56.686
    Jul 29 16:59:56.687: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename job 07/29/23 16:59:56.689
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 16:59:56.72
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 16:59:56.724
    [It] should adopt matching orphans and release non-matching pods [Conformance]
      test/e2e/apps/job.go:335
    STEP: Creating a job 07/29/23 16:59:56.728
    STEP: Ensuring active pods == parallelism 07/29/23 16:59:56.742
    STEP: Orphaning one of the Job's Pods 07/29/23 17:00:00.752
    Jul 29 17:00:01.283: INFO: Successfully updated pod "adopt-release-nckwf"
    STEP: Checking that the Job readopts the Pod 07/29/23 17:00:01.283
    Jul 29 17:00:01.283: INFO: Waiting up to 15m0s for pod "adopt-release-nckwf" in namespace "job-2209" to be "adopted"
    Jul 29 17:00:01.293: INFO: Pod "adopt-release-nckwf": Phase="Running", Reason="", readiness=true. Elapsed: 9.140731ms
    Jul 29 17:00:03.301: INFO: Pod "adopt-release-nckwf": Phase="Running", Reason="", readiness=true. Elapsed: 2.017413026s
    Jul 29 17:00:03.301: INFO: Pod "adopt-release-nckwf" satisfied condition "adopted"
    STEP: Removing the labels from the Job's Pod 07/29/23 17:00:03.301
    Jul 29 17:00:03.823: INFO: Successfully updated pod "adopt-release-nckwf"
    STEP: Checking that the Job releases the Pod 07/29/23 17:00:03.823
    Jul 29 17:00:03.823: INFO: Waiting up to 15m0s for pod "adopt-release-nckwf" in namespace "job-2209" to be "released"
    Jul 29 17:00:03.834: INFO: Pod "adopt-release-nckwf": Phase="Running", Reason="", readiness=true. Elapsed: 10.545664ms
    Jul 29 17:00:05.843: INFO: Pod "adopt-release-nckwf": Phase="Running", Reason="", readiness=true. Elapsed: 2.019959644s
    Jul 29 17:00:05.844: INFO: Pod "adopt-release-nckwf" satisfied condition "released"
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Jul 29 17:00:05.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-2209" for this suite. 07/29/23 17:00:05.858
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 17:00:05.874
Jul 29 17:00:05.874: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename events 07/29/23 17:00:05.876
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 17:00:05.908
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 17:00:05.913
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
STEP: Create set of events 07/29/23 17:00:05.918
Jul 29 17:00:05.926: INFO: created test-event-1
Jul 29 17:00:05.933: INFO: created test-event-2
Jul 29 17:00:05.942: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace 07/29/23 17:00:05.942
STEP: delete collection of events 07/29/23 17:00:05.949
Jul 29 17:00:05.949: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 07/29/23 17:00:05.994
Jul 29 17:00:05.994: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:187
Jul 29 17:00:06.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-6972" for this suite. 07/29/23 17:00:06.008
{"msg":"PASSED [sig-instrumentation] Events should delete a collection of events [Conformance]","completed":335,"skipped":6230,"failed":0}
------------------------------
â€¢ [0.145 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 17:00:05.874
    Jul 29 17:00:05.874: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename events 07/29/23 17:00:05.876
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 17:00:05.908
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 17:00:05.913
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/core_events.go:175
    STEP: Create set of events 07/29/23 17:00:05.918
    Jul 29 17:00:05.926: INFO: created test-event-1
    Jul 29 17:00:05.933: INFO: created test-event-2
    Jul 29 17:00:05.942: INFO: created test-event-3
    STEP: get a list of Events with a label in the current namespace 07/29/23 17:00:05.942
    STEP: delete collection of events 07/29/23 17:00:05.949
    Jul 29 17:00:05.949: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 07/29/23 17:00:05.994
    Jul 29 17:00:05.994: INFO: requesting list of events to confirm quantity
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:187
    Jul 29 17:00:06.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-6972" for this suite. 07/29/23 17:00:06.008
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 17:00:06.031
Jul 29 17:00:06.031: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename emptydir 07/29/23 17:00:06.033
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 17:00:06.064
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 17:00:06.069
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136
STEP: Creating a pod to test emptydir 0666 on tmpfs 07/29/23 17:00:06.073
Jul 29 17:00:06.087: INFO: Waiting up to 5m0s for pod "pod-7a01fe03-4fde-4dc6-92d8-7af2956c7f18" in namespace "emptydir-4826" to be "Succeeded or Failed"
Jul 29 17:00:06.093: INFO: Pod "pod-7a01fe03-4fde-4dc6-92d8-7af2956c7f18": Phase="Pending", Reason="", readiness=false. Elapsed: 5.592788ms
Jul 29 17:00:08.102: INFO: Pod "pod-7a01fe03-4fde-4dc6-92d8-7af2956c7f18": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014546105s
Jul 29 17:00:10.104: INFO: Pod "pod-7a01fe03-4fde-4dc6-92d8-7af2956c7f18": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01674397s
STEP: Saw pod success 07/29/23 17:00:10.104
Jul 29 17:00:10.105: INFO: Pod "pod-7a01fe03-4fde-4dc6-92d8-7af2956c7f18" satisfied condition "Succeeded or Failed"
Jul 29 17:00:10.110: INFO: Trying to get logs from node wa4quivohpee-3 pod pod-7a01fe03-4fde-4dc6-92d8-7af2956c7f18 container test-container: <nil>
STEP: delete the pod 07/29/23 17:00:10.139
Jul 29 17:00:10.160: INFO: Waiting for pod pod-7a01fe03-4fde-4dc6-92d8-7af2956c7f18 to disappear
Jul 29 17:00:10.165: INFO: Pod pod-7a01fe03-4fde-4dc6-92d8-7af2956c7f18 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jul 29 17:00:10.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4826" for this suite. 07/29/23 17:00:10.176
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":336,"skipped":6256,"failed":0}
------------------------------
â€¢ [4.155 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 17:00:06.031
    Jul 29 17:00:06.031: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename emptydir 07/29/23 17:00:06.033
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 17:00:06.064
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 17:00:06.069
    [It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:136
    STEP: Creating a pod to test emptydir 0666 on tmpfs 07/29/23 17:00:06.073
    Jul 29 17:00:06.087: INFO: Waiting up to 5m0s for pod "pod-7a01fe03-4fde-4dc6-92d8-7af2956c7f18" in namespace "emptydir-4826" to be "Succeeded or Failed"
    Jul 29 17:00:06.093: INFO: Pod "pod-7a01fe03-4fde-4dc6-92d8-7af2956c7f18": Phase="Pending", Reason="", readiness=false. Elapsed: 5.592788ms
    Jul 29 17:00:08.102: INFO: Pod "pod-7a01fe03-4fde-4dc6-92d8-7af2956c7f18": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014546105s
    Jul 29 17:00:10.104: INFO: Pod "pod-7a01fe03-4fde-4dc6-92d8-7af2956c7f18": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01674397s
    STEP: Saw pod success 07/29/23 17:00:10.104
    Jul 29 17:00:10.105: INFO: Pod "pod-7a01fe03-4fde-4dc6-92d8-7af2956c7f18" satisfied condition "Succeeded or Failed"
    Jul 29 17:00:10.110: INFO: Trying to get logs from node wa4quivohpee-3 pod pod-7a01fe03-4fde-4dc6-92d8-7af2956c7f18 container test-container: <nil>
    STEP: delete the pod 07/29/23 17:00:10.139
    Jul 29 17:00:10.160: INFO: Waiting for pod pod-7a01fe03-4fde-4dc6-92d8-7af2956c7f18 to disappear
    Jul 29 17:00:10.165: INFO: Pod pod-7a01fe03-4fde-4dc6-92d8-7af2956c7f18 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jul 29 17:00:10.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-4826" for this suite. 07/29/23 17:00:10.176
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:304
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 17:00:10.189
Jul 29 17:00:10.189: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename statefulset 07/29/23 17:00:10.192
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 17:00:10.218
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 17:00:10.223
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-40 07/29/23 17:00:10.227
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:304
STEP: Creating a new StatefulSet 07/29/23 17:00:10.237
Jul 29 17:00:10.262: INFO: Found 0 stateful pods, waiting for 3
Jul 29 17:00:20.277: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 29 17:00:20.278: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 29 17:00:20.278: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Jul 29 17:00:20.307: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=statefulset-40 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 29 17:00:20.635: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 29 17:00:20.635: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 29 17:00:20.635: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 07/29/23 17:00:30.665
Jul 29 17:00:30.712: INFO: Updating stateful set ss2
STEP: Creating a new revision 07/29/23 17:00:30.712
STEP: Updating Pods in reverse ordinal order 07/29/23 17:00:40.751
Jul 29 17:00:40.760: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=statefulset-40 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 29 17:00:41.031: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jul 29 17:00:41.031: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul 29 17:00:41.031: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

STEP: Rolling back to a previous revision 07/29/23 17:00:51.073
Jul 29 17:00:51.074: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=statefulset-40 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 29 17:00:51.366: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 29 17:00:51.366: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 29 17:00:51.366: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul 29 17:01:01.446: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order 07/29/23 17:01:11.487
Jul 29 17:01:11.497: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=statefulset-40 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 29 17:01:11.740: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jul 29 17:01:11.740: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul 29 17:01:11.740: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jul 29 17:01:21.794: INFO: Deleting all statefulset in ns statefulset-40
Jul 29 17:01:21.800: INFO: Scaling statefulset ss2 to 0
Jul 29 17:01:31.838: INFO: Waiting for statefulset status.replicas updated to 0
Jul 29 17:01:31.847: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jul 29 17:01:31.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-40" for this suite. 07/29/23 17:01:31.881
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","completed":337,"skipped":6265,"failed":0}
------------------------------
â€¢ [SLOW TEST] [81.702 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should perform rolling updates and roll backs of template modifications [Conformance]
    test/e2e/apps/statefulset.go:304

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 17:00:10.189
    Jul 29 17:00:10.189: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename statefulset 07/29/23 17:00:10.192
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 17:00:10.218
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 17:00:10.223
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-40 07/29/23 17:00:10.227
    [It] should perform rolling updates and roll backs of template modifications [Conformance]
      test/e2e/apps/statefulset.go:304
    STEP: Creating a new StatefulSet 07/29/23 17:00:10.237
    Jul 29 17:00:10.262: INFO: Found 0 stateful pods, waiting for 3
    Jul 29 17:00:20.277: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Jul 29 17:00:20.278: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Jul 29 17:00:20.278: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    Jul 29 17:00:20.307: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=statefulset-40 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jul 29 17:00:20.635: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jul 29 17:00:20.635: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jul 29 17:00:20.635: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 07/29/23 17:00:30.665
    Jul 29 17:00:30.712: INFO: Updating stateful set ss2
    STEP: Creating a new revision 07/29/23 17:00:30.712
    STEP: Updating Pods in reverse ordinal order 07/29/23 17:00:40.751
    Jul 29 17:00:40.760: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=statefulset-40 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jul 29 17:00:41.031: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jul 29 17:00:41.031: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jul 29 17:00:41.031: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    STEP: Rolling back to a previous revision 07/29/23 17:00:51.073
    Jul 29 17:00:51.074: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=statefulset-40 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jul 29 17:00:51.366: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jul 29 17:00:51.366: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jul 29 17:00:51.366: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jul 29 17:01:01.446: INFO: Updating stateful set ss2
    STEP: Rolling back update in reverse ordinal order 07/29/23 17:01:11.487
    Jul 29 17:01:11.497: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=statefulset-40 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jul 29 17:01:11.740: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jul 29 17:01:11.740: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jul 29 17:01:11.740: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jul 29 17:01:21.794: INFO: Deleting all statefulset in ns statefulset-40
    Jul 29 17:01:21.800: INFO: Scaling statefulset ss2 to 0
    Jul 29 17:01:31.838: INFO: Waiting for statefulset status.replicas updated to 0
    Jul 29 17:01:31.847: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jul 29 17:01:31.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-40" for this suite. 07/29/23 17:01:31.881
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 17:01:31.896
Jul 29 17:01:31.896: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename kubelet-test 07/29/23 17:01:31.904
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 17:01:31.939
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 17:01:31.944
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
STEP: Waiting for pod completion 07/29/23 17:01:31.972
Jul 29 17:01:31.972: INFO: Waiting up to 3m0s for pod "agnhost-host-aliasesebe8fb55-1430-47ef-aa72-f74b5b2f2b9e" in namespace "kubelet-test-4966" to be "completed"
Jul 29 17:01:31.982: INFO: Pod "agnhost-host-aliasesebe8fb55-1430-47ef-aa72-f74b5b2f2b9e": Phase="Pending", Reason="", readiness=false. Elapsed: 9.522406ms
Jul 29 17:01:33.988: INFO: Pod "agnhost-host-aliasesebe8fb55-1430-47ef-aa72-f74b5b2f2b9e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015592496s
Jul 29 17:01:35.989: INFO: Pod "agnhost-host-aliasesebe8fb55-1430-47ef-aa72-f74b5b2f2b9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016801519s
Jul 29 17:01:35.989: INFO: Pod "agnhost-host-aliasesebe8fb55-1430-47ef-aa72-f74b5b2f2b9e" satisfied condition "completed"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Jul 29 17:01:36.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4966" for this suite. 07/29/23 17:01:36.012
{"msg":"PASSED [sig-node] Kubelet when scheduling an agnhost Pod with hostAliases should write entries to /etc/hosts [NodeConformance] [Conformance]","completed":338,"skipped":6278,"failed":0}
------------------------------
â€¢ [4.129 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling an agnhost Pod with hostAliases
  test/e2e/common/node/kubelet.go:140
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 17:01:31.896
    Jul 29 17:01:31.896: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename kubelet-test 07/29/23 17:01:31.904
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 17:01:31.939
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 17:01:31.944
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should write entries to /etc/hosts [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:148
    STEP: Waiting for pod completion 07/29/23 17:01:31.972
    Jul 29 17:01:31.972: INFO: Waiting up to 3m0s for pod "agnhost-host-aliasesebe8fb55-1430-47ef-aa72-f74b5b2f2b9e" in namespace "kubelet-test-4966" to be "completed"
    Jul 29 17:01:31.982: INFO: Pod "agnhost-host-aliasesebe8fb55-1430-47ef-aa72-f74b5b2f2b9e": Phase="Pending", Reason="", readiness=false. Elapsed: 9.522406ms
    Jul 29 17:01:33.988: INFO: Pod "agnhost-host-aliasesebe8fb55-1430-47ef-aa72-f74b5b2f2b9e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015592496s
    Jul 29 17:01:35.989: INFO: Pod "agnhost-host-aliasesebe8fb55-1430-47ef-aa72-f74b5b2f2b9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016801519s
    Jul 29 17:01:35.989: INFO: Pod "agnhost-host-aliasesebe8fb55-1430-47ef-aa72-f74b5b2f2b9e" satisfied condition "completed"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Jul 29 17:01:36.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-4966" for this suite. 07/29/23 17:01:36.012
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 17:01:36.038
Jul 29 17:01:36.038: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename projected 07/29/23 17:01:36.04
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 17:01:36.068
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 17:01:36.075
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173
STEP: Creating configMap with name cm-test-opt-del-a4298c42-9e63-4ca3-a756-97b756a4d44f 07/29/23 17:01:36.086
STEP: Creating configMap with name cm-test-opt-upd-d3c85a4a-be2b-4e0a-8beb-15ebdcf5b8d3 07/29/23 17:01:36.096
STEP: Creating the pod 07/29/23 17:01:36.105
Jul 29 17:01:36.123: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9e9c0959-68f7-4288-9852-00c4e78d7d8a" in namespace "projected-1952" to be "running and ready"
Jul 29 17:01:36.133: INFO: Pod "pod-projected-configmaps-9e9c0959-68f7-4288-9852-00c4e78d7d8a": Phase="Pending", Reason="", readiness=false. Elapsed: 10.289417ms
Jul 29 17:01:36.133: INFO: The phase of Pod pod-projected-configmaps-9e9c0959-68f7-4288-9852-00c4e78d7d8a is Pending, waiting for it to be Running (with Ready = true)
Jul 29 17:01:38.142: INFO: Pod "pod-projected-configmaps-9e9c0959-68f7-4288-9852-00c4e78d7d8a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018813012s
Jul 29 17:01:38.142: INFO: The phase of Pod pod-projected-configmaps-9e9c0959-68f7-4288-9852-00c4e78d7d8a is Pending, waiting for it to be Running (with Ready = true)
Jul 29 17:01:40.144: INFO: Pod "pod-projected-configmaps-9e9c0959-68f7-4288-9852-00c4e78d7d8a": Phase="Running", Reason="", readiness=true. Elapsed: 4.021588227s
Jul 29 17:01:40.145: INFO: The phase of Pod pod-projected-configmaps-9e9c0959-68f7-4288-9852-00c4e78d7d8a is Running (Ready = true)
Jul 29 17:01:40.145: INFO: Pod "pod-projected-configmaps-9e9c0959-68f7-4288-9852-00c4e78d7d8a" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-a4298c42-9e63-4ca3-a756-97b756a4d44f 07/29/23 17:01:40.186
STEP: Updating configmap cm-test-opt-upd-d3c85a4a-be2b-4e0a-8beb-15ebdcf5b8d3 07/29/23 17:01:40.208
STEP: Creating configMap with name cm-test-opt-create-1339715a-8295-46f8-9a51-640a273b4048 07/29/23 17:01:40.219
STEP: waiting to observe update in volume 07/29/23 17:01:40.227
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jul 29 17:03:07.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1952" for this suite. 07/29/23 17:03:07.12
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":339,"skipped":6295,"failed":0}
------------------------------
â€¢ [SLOW TEST] [91.097 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 17:01:36.038
    Jul 29 17:01:36.038: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename projected 07/29/23 17:01:36.04
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 17:01:36.068
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 17:01:36.075
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:173
    STEP: Creating configMap with name cm-test-opt-del-a4298c42-9e63-4ca3-a756-97b756a4d44f 07/29/23 17:01:36.086
    STEP: Creating configMap with name cm-test-opt-upd-d3c85a4a-be2b-4e0a-8beb-15ebdcf5b8d3 07/29/23 17:01:36.096
    STEP: Creating the pod 07/29/23 17:01:36.105
    Jul 29 17:01:36.123: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9e9c0959-68f7-4288-9852-00c4e78d7d8a" in namespace "projected-1952" to be "running and ready"
    Jul 29 17:01:36.133: INFO: Pod "pod-projected-configmaps-9e9c0959-68f7-4288-9852-00c4e78d7d8a": Phase="Pending", Reason="", readiness=false. Elapsed: 10.289417ms
    Jul 29 17:01:36.133: INFO: The phase of Pod pod-projected-configmaps-9e9c0959-68f7-4288-9852-00c4e78d7d8a is Pending, waiting for it to be Running (with Ready = true)
    Jul 29 17:01:38.142: INFO: Pod "pod-projected-configmaps-9e9c0959-68f7-4288-9852-00c4e78d7d8a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018813012s
    Jul 29 17:01:38.142: INFO: The phase of Pod pod-projected-configmaps-9e9c0959-68f7-4288-9852-00c4e78d7d8a is Pending, waiting for it to be Running (with Ready = true)
    Jul 29 17:01:40.144: INFO: Pod "pod-projected-configmaps-9e9c0959-68f7-4288-9852-00c4e78d7d8a": Phase="Running", Reason="", readiness=true. Elapsed: 4.021588227s
    Jul 29 17:01:40.145: INFO: The phase of Pod pod-projected-configmaps-9e9c0959-68f7-4288-9852-00c4e78d7d8a is Running (Ready = true)
    Jul 29 17:01:40.145: INFO: Pod "pod-projected-configmaps-9e9c0959-68f7-4288-9852-00c4e78d7d8a" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-a4298c42-9e63-4ca3-a756-97b756a4d44f 07/29/23 17:01:40.186
    STEP: Updating configmap cm-test-opt-upd-d3c85a4a-be2b-4e0a-8beb-15ebdcf5b8d3 07/29/23 17:01:40.208
    STEP: Creating configMap with name cm-test-opt-create-1339715a-8295-46f8-9a51-640a273b4048 07/29/23 17:01:40.219
    STEP: waiting to observe update in volume 07/29/23 17:01:40.227
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jul 29 17:03:07.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1952" for this suite. 07/29/23 17:03:07.12
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1810
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 17:03:07.139
Jul 29 17:03:07.140: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename kubectl 07/29/23 17:03:07.145
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 17:03:07.177
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 17:03:07.181
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1810
STEP: Starting the proxy 07/29/23 17:03:07.187
Jul 29 17:03:07.189: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-17 proxy --unix-socket=/tmp/kubectl-proxy-unix1727389540/test'
STEP: retrieving proxy /api/ output 07/29/23 17:03:07.299
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jul 29 17:03:07.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-17" for this suite. 07/29/23 17:03:07.312
{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","completed":340,"skipped":6302,"failed":0}
------------------------------
â€¢ [0.183 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1778
    should support --unix-socket=/path  [Conformance]
    test/e2e/kubectl/kubectl.go:1810

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 17:03:07.139
    Jul 29 17:03:07.140: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename kubectl 07/29/23 17:03:07.145
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 17:03:07.177
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 17:03:07.181
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should support --unix-socket=/path  [Conformance]
      test/e2e/kubectl/kubectl.go:1810
    STEP: Starting the proxy 07/29/23 17:03:07.187
    Jul 29 17:03:07.189: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-17 proxy --unix-socket=/tmp/kubectl-proxy-unix1727389540/test'
    STEP: retrieving proxy /api/ output 07/29/23 17:03:07.299
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jul 29 17:03:07.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-17" for this suite. 07/29/23 17:03:07.312
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 17:03:07.327
Jul 29 17:03:07.328: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename webhook 07/29/23 17:03:07.331
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 17:03:07.36
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 17:03:07.367
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 07/29/23 17:03:07.399
STEP: Create role binding to let webhook read extension-apiserver-authentication 07/29/23 17:03:08.004
STEP: Deploying the webhook pod 07/29/23 17:03:08.021
STEP: Wait for the deployment to be ready 07/29/23 17:03:08.041
Jul 29 17:03:08.053: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 07/29/23 17:03:10.075
STEP: Verifying the service has paired with the endpoint 07/29/23 17:03:10.093
Jul 29 17:03:11.094: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 07/29/23 17:03:11.101
STEP: create a configmap that should be updated by the webhook 07/29/23 17:03:11.138
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jul 29 17:03:11.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8777" for this suite. 07/29/23 17:03:11.184
STEP: Destroying namespace "webhook-8777-markers" for this suite. 07/29/23 17:03:11.197
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","completed":341,"skipped":6310,"failed":0}
------------------------------
â€¢ [3.969 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 17:03:07.327
    Jul 29 17:03:07.328: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename webhook 07/29/23 17:03:07.331
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 17:03:07.36
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 17:03:07.367
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 07/29/23 17:03:07.399
    STEP: Create role binding to let webhook read extension-apiserver-authentication 07/29/23 17:03:08.004
    STEP: Deploying the webhook pod 07/29/23 17:03:08.021
    STEP: Wait for the deployment to be ready 07/29/23 17:03:08.041
    Jul 29 17:03:08.053: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 07/29/23 17:03:10.075
    STEP: Verifying the service has paired with the endpoint 07/29/23 17:03:10.093
    Jul 29 17:03:11.094: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate configmap [Conformance]
      test/e2e/apimachinery/webhook.go:251
    STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 07/29/23 17:03:11.101
    STEP: create a configmap that should be updated by the webhook 07/29/23 17:03:11.138
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jul 29 17:03:11.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-8777" for this suite. 07/29/23 17:03:11.184
    STEP: Destroying namespace "webhook-8777-markers" for this suite. 07/29/23 17:03:11.197
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:194
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 17:03:11.296
Jul 29 17:03:11.296: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename container-runtime 07/29/23 17:03:11.3
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 17:03:11.352
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 17:03:11.363
[It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:194
STEP: create the container 07/29/23 17:03:11.375
STEP: wait for the container to reach Succeeded 07/29/23 17:03:11.407
STEP: get the container status 07/29/23 17:03:15.474
STEP: the container should be terminated 07/29/23 17:03:15.481
STEP: the termination message should be set 07/29/23 17:03:15.481
Jul 29 17:03:15.482: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 07/29/23 17:03:15.482
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Jul 29 17:03:15.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6884" for this suite. 07/29/23 17:03:15.512
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","completed":342,"skipped":6311,"failed":0}
------------------------------
â€¢ [4.225 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 17:03:11.296
    Jul 29 17:03:11.296: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename container-runtime 07/29/23 17:03:11.3
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 17:03:11.352
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 17:03:11.363
    [It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:194
    STEP: create the container 07/29/23 17:03:11.375
    STEP: wait for the container to reach Succeeded 07/29/23 17:03:11.407
    STEP: get the container status 07/29/23 17:03:15.474
    STEP: the container should be terminated 07/29/23 17:03:15.481
    STEP: the termination message should be set 07/29/23 17:03:15.481
    Jul 29 17:03:15.482: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 07/29/23 17:03:15.482
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Jul 29 17:03:15.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-6884" for this suite. 07/29/23 17:03:15.512
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 17:03:15.524
Jul 29 17:03:15.525: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename crd-webhook 07/29/23 17:03:15.527
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 17:03:15.561
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 17:03:15.566
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 07/29/23 17:03:15.571
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 07/29/23 17:03:15.903
STEP: Deploying the custom resource conversion webhook pod 07/29/23 17:03:15.913
STEP: Wait for the deployment to be ready 07/29/23 17:03:15.932
Jul 29 17:03:15.947: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 07/29/23 17:03:17.971
STEP: Verifying the service has paired with the endpoint 07/29/23 17:03:17.991
Jul 29 17:03:18.992: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
Jul 29 17:03:19.001: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Creating a v1 custom resource 07/29/23 17:03:21.882
STEP: Create a v2 custom resource 07/29/23 17:03:21.914
STEP: List CRs in v1 07/29/23 17:03:22.165
STEP: List CRs in v2 07/29/23 17:03:22.176
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jul 29 17:03:22.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-8774" for this suite. 07/29/23 17:03:22.729
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","completed":343,"skipped":6326,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.305 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 17:03:15.524
    Jul 29 17:03:15.525: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename crd-webhook 07/29/23 17:03:15.527
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 17:03:15.561
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 17:03:15.566
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 07/29/23 17:03:15.571
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 07/29/23 17:03:15.903
    STEP: Deploying the custom resource conversion webhook pod 07/29/23 17:03:15.913
    STEP: Wait for the deployment to be ready 07/29/23 17:03:15.932
    Jul 29 17:03:15.947: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 07/29/23 17:03:17.971
    STEP: Verifying the service has paired with the endpoint 07/29/23 17:03:17.991
    Jul 29 17:03:18.992: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert a non homogeneous list of CRs [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:184
    Jul 29 17:03:19.001: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Creating a v1 custom resource 07/29/23 17:03:21.882
    STEP: Create a v2 custom resource 07/29/23 17:03:21.914
    STEP: List CRs in v1 07/29/23 17:03:22.165
    STEP: List CRs in v2 07/29/23 17:03:22.176
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jul 29 17:03:22.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-webhook-8774" for this suite. 07/29/23 17:03:22.729
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] DisruptionController
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 17:03:22.836
Jul 29 17:03:22.837: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename disruption 07/29/23 17:03:22.842
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 17:03:22.91
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 17:03:22.923
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346
STEP: Creating a pdb that targets all three pods in a test replica set 07/29/23 17:03:22.933
STEP: Waiting for the pdb to be processed 07/29/23 17:03:22.944
STEP: First trying to evict a pod which shouldn't be evictable 07/29/23 17:03:24.972
STEP: Waiting for all pods to be running 07/29/23 17:03:24.972
Jul 29 17:03:24.978: INFO: pods: 0 < 3
STEP: locating a running pod 07/29/23 17:03:26.987
STEP: Updating the pdb to allow a pod to be evicted 07/29/23 17:03:27.009
STEP: Waiting for the pdb to be processed 07/29/23 17:03:27.032
STEP: Trying to evict the same pod we tried earlier which should now be evictable 07/29/23 17:03:29.047
STEP: Waiting for all pods to be running 07/29/23 17:03:29.047
STEP: Waiting for the pdb to observed all healthy pods 07/29/23 17:03:29.054
STEP: Patching the pdb to disallow a pod to be evicted 07/29/23 17:03:29.101
STEP: Waiting for the pdb to be processed 07/29/23 17:03:29.199
STEP: Waiting for all pods to be running 07/29/23 17:03:31.226
STEP: locating a running pod 07/29/23 17:03:31.235
STEP: Deleting the pdb to allow a pod to be evicted 07/29/23 17:03:31.253
STEP: Waiting for the pdb to be deleted 07/29/23 17:03:31.265
STEP: Trying to evict the same pod we tried earlier which should now be evictable 07/29/23 17:03:31.27
STEP: Waiting for all pods to be running 07/29/23 17:03:31.271
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Jul 29 17:03:31.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-6807" for this suite. 07/29/23 17:03:31.299
{"msg":"PASSED [sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]","completed":344,"skipped":6327,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.478 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 17:03:22.836
    Jul 29 17:03:22.837: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename disruption 07/29/23 17:03:22.842
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 17:03:22.91
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 17:03:22.923
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should block an eviction until the PDB is updated to allow it [Conformance]
      test/e2e/apps/disruption.go:346
    STEP: Creating a pdb that targets all three pods in a test replica set 07/29/23 17:03:22.933
    STEP: Waiting for the pdb to be processed 07/29/23 17:03:22.944
    STEP: First trying to evict a pod which shouldn't be evictable 07/29/23 17:03:24.972
    STEP: Waiting for all pods to be running 07/29/23 17:03:24.972
    Jul 29 17:03:24.978: INFO: pods: 0 < 3
    STEP: locating a running pod 07/29/23 17:03:26.987
    STEP: Updating the pdb to allow a pod to be evicted 07/29/23 17:03:27.009
    STEP: Waiting for the pdb to be processed 07/29/23 17:03:27.032
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 07/29/23 17:03:29.047
    STEP: Waiting for all pods to be running 07/29/23 17:03:29.047
    STEP: Waiting for the pdb to observed all healthy pods 07/29/23 17:03:29.054
    STEP: Patching the pdb to disallow a pod to be evicted 07/29/23 17:03:29.101
    STEP: Waiting for the pdb to be processed 07/29/23 17:03:29.199
    STEP: Waiting for all pods to be running 07/29/23 17:03:31.226
    STEP: locating a running pod 07/29/23 17:03:31.235
    STEP: Deleting the pdb to allow a pod to be evicted 07/29/23 17:03:31.253
    STEP: Waiting for the pdb to be deleted 07/29/23 17:03:31.265
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 07/29/23 17:03:31.27
    STEP: Waiting for all pods to be running 07/29/23 17:03:31.271
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Jul 29 17:03:31.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-6807" for this suite. 07/29/23 17:03:31.299
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Secrets
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 17:03:31.318
Jul 29 17:03:31.318: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename secrets 07/29/23 17:03:31.322
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 17:03:31.361
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 17:03:31.364
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jul 29 17:03:31.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7004" for this suite. 07/29/23 17:03:31.435
{"msg":"PASSED [sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]","completed":345,"skipped":6327,"failed":0}
------------------------------
â€¢ [0.128 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 17:03:31.318
    Jul 29 17:03:31.318: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename secrets 07/29/23 17:03:31.322
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 17:03:31.361
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 17:03:31.364
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/secrets_volume.go:385
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jul 29 17:03:31.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-7004" for this suite. 07/29/23 17:03:31.435
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 17:03:31.451
Jul 29 17:03:31.452: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename projected 07/29/23 17:03:31.453
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 17:03:31.485
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 17:03:31.489
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214
STEP: Creating secret with name s-test-opt-del-b32197fd-07e2-408d-8b8d-b3a417ef840d 07/29/23 17:03:31.502
STEP: Creating secret with name s-test-opt-upd-135886ad-42fe-4eb5-8ebf-c6c7976b51a9 07/29/23 17:03:31.509
STEP: Creating the pod 07/29/23 17:03:31.516
Jul 29 17:03:31.533: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6582ca03-8b8c-4eee-98f1-910042257f88" in namespace "projected-285" to be "running and ready"
Jul 29 17:03:31.538: INFO: Pod "pod-projected-secrets-6582ca03-8b8c-4eee-98f1-910042257f88": Phase="Pending", Reason="", readiness=false. Elapsed: 5.257612ms
Jul 29 17:03:31.539: INFO: The phase of Pod pod-projected-secrets-6582ca03-8b8c-4eee-98f1-910042257f88 is Pending, waiting for it to be Running (with Ready = true)
Jul 29 17:03:33.547: INFO: Pod "pod-projected-secrets-6582ca03-8b8c-4eee-98f1-910042257f88": Phase="Running", Reason="", readiness=true. Elapsed: 2.013764623s
Jul 29 17:03:33.547: INFO: The phase of Pod pod-projected-secrets-6582ca03-8b8c-4eee-98f1-910042257f88 is Running (Ready = true)
Jul 29 17:03:33.547: INFO: Pod "pod-projected-secrets-6582ca03-8b8c-4eee-98f1-910042257f88" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-b32197fd-07e2-408d-8b8d-b3a417ef840d 07/29/23 17:03:33.581
STEP: Updating secret s-test-opt-upd-135886ad-42fe-4eb5-8ebf-c6c7976b51a9 07/29/23 17:03:33.591
STEP: Creating secret with name s-test-opt-create-04a09a16-ce0d-444f-967d-cddc8c41a54b 07/29/23 17:03:33.6
STEP: waiting to observe update in volume 07/29/23 17:03:33.607
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Jul 29 17:03:35.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-285" for this suite. 07/29/23 17:03:35.673
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":346,"skipped":6348,"failed":0}
------------------------------
â€¢ [4.235 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 17:03:31.451
    Jul 29 17:03:31.452: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename projected 07/29/23 17:03:31.453
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 17:03:31.485
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 17:03:31.489
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:214
    STEP: Creating secret with name s-test-opt-del-b32197fd-07e2-408d-8b8d-b3a417ef840d 07/29/23 17:03:31.502
    STEP: Creating secret with name s-test-opt-upd-135886ad-42fe-4eb5-8ebf-c6c7976b51a9 07/29/23 17:03:31.509
    STEP: Creating the pod 07/29/23 17:03:31.516
    Jul 29 17:03:31.533: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6582ca03-8b8c-4eee-98f1-910042257f88" in namespace "projected-285" to be "running and ready"
    Jul 29 17:03:31.538: INFO: Pod "pod-projected-secrets-6582ca03-8b8c-4eee-98f1-910042257f88": Phase="Pending", Reason="", readiness=false. Elapsed: 5.257612ms
    Jul 29 17:03:31.539: INFO: The phase of Pod pod-projected-secrets-6582ca03-8b8c-4eee-98f1-910042257f88 is Pending, waiting for it to be Running (with Ready = true)
    Jul 29 17:03:33.547: INFO: Pod "pod-projected-secrets-6582ca03-8b8c-4eee-98f1-910042257f88": Phase="Running", Reason="", readiness=true. Elapsed: 2.013764623s
    Jul 29 17:03:33.547: INFO: The phase of Pod pod-projected-secrets-6582ca03-8b8c-4eee-98f1-910042257f88 is Running (Ready = true)
    Jul 29 17:03:33.547: INFO: Pod "pod-projected-secrets-6582ca03-8b8c-4eee-98f1-910042257f88" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-b32197fd-07e2-408d-8b8d-b3a417ef840d 07/29/23 17:03:33.581
    STEP: Updating secret s-test-opt-upd-135886ad-42fe-4eb5-8ebf-c6c7976b51a9 07/29/23 17:03:33.591
    STEP: Creating secret with name s-test-opt-create-04a09a16-ce0d-444f-967d-cddc8c41a54b 07/29/23 17:03:33.6
    STEP: waiting to observe update in volume 07/29/23 17:03:33.607
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Jul 29 17:03:35.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-285" for this suite. 07/29/23 17:03:35.673
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 17:03:35.691
Jul 29 17:03:35.692: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename webhook 07/29/23 17:03:35.695
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 17:03:35.724
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 17:03:35.729
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 07/29/23 17:03:35.755
STEP: Create role binding to let webhook read extension-apiserver-authentication 07/29/23 17:03:36.81
STEP: Deploying the webhook pod 07/29/23 17:03:36.822
STEP: Wait for the deployment to be ready 07/29/23 17:03:36.85
Jul 29 17:03:36.879: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jul 29 17:03:38.898: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 17, 3, 36, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 17, 3, 36, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 17, 3, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 17, 3, 36, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 07/29/23 17:03:40.906
STEP: Verifying the service has paired with the endpoint 07/29/23 17:03:40.929
Jul 29 17:03:41.930: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116
STEP: fetching the /apis discovery document 07/29/23 17:03:41.941
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 07/29/23 17:03:41.947
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 07/29/23 17:03:41.947
STEP: fetching the /apis/admissionregistration.k8s.io discovery document 07/29/23 17:03:41.947
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 07/29/23 17:03:41.949
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 07/29/23 17:03:41.958
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 07/29/23 17:03:41.961
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jul 29 17:03:41.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5534" for this suite. 07/29/23 17:03:41.979
STEP: Destroying namespace "webhook-5534-markers" for this suite. 07/29/23 17:03:41.996
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","completed":347,"skipped":6350,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.400 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 17:03:35.691
    Jul 29 17:03:35.692: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename webhook 07/29/23 17:03:35.695
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 17:03:35.724
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 17:03:35.729
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 07/29/23 17:03:35.755
    STEP: Create role binding to let webhook read extension-apiserver-authentication 07/29/23 17:03:36.81
    STEP: Deploying the webhook pod 07/29/23 17:03:36.822
    STEP: Wait for the deployment to be ready 07/29/23 17:03:36.85
    Jul 29 17:03:36.879: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jul 29 17:03:38.898: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 17, 3, 36, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 17, 3, 36, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 17, 3, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 17, 3, 36, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 07/29/23 17:03:40.906
    STEP: Verifying the service has paired with the endpoint 07/29/23 17:03:40.929
    Jul 29 17:03:41.930: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should include webhook resources in discovery documents [Conformance]
      test/e2e/apimachinery/webhook.go:116
    STEP: fetching the /apis discovery document 07/29/23 17:03:41.941
    STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 07/29/23 17:03:41.947
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 07/29/23 17:03:41.947
    STEP: fetching the /apis/admissionregistration.k8s.io discovery document 07/29/23 17:03:41.947
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 07/29/23 17:03:41.949
    STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 07/29/23 17:03:41.958
    STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 07/29/23 17:03:41.961
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jul 29 17:03:41.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-5534" for this suite. 07/29/23 17:03:41.979
    STEP: Destroying namespace "webhook-5534-markers" for this suite. 07/29/23 17:03:41.996
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 17:03:42.101
Jul 29 17:03:42.101: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename projected 07/29/23 17:03:42.104
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 17:03:42.139
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 17:03:42.145
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123
STEP: Creating projection with configMap that has name projected-configmap-test-upd-9cddc77b-6350-437f-90f8-83109233ab64 07/29/23 17:03:42.167
STEP: Creating the pod 07/29/23 17:03:42.183
Jul 29 17:03:42.203: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-8a984387-c43d-4409-8eb5-e894b2492b6d" in namespace "projected-2079" to be "running and ready"
Jul 29 17:03:42.212: INFO: Pod "pod-projected-configmaps-8a984387-c43d-4409-8eb5-e894b2492b6d": Phase="Pending", Reason="", readiness=false. Elapsed: 9.063487ms
Jul 29 17:03:42.212: INFO: The phase of Pod pod-projected-configmaps-8a984387-c43d-4409-8eb5-e894b2492b6d is Pending, waiting for it to be Running (with Ready = true)
Jul 29 17:03:44.220: INFO: Pod "pod-projected-configmaps-8a984387-c43d-4409-8eb5-e894b2492b6d": Phase="Running", Reason="", readiness=true. Elapsed: 2.017615805s
Jul 29 17:03:44.221: INFO: The phase of Pod pod-projected-configmaps-8a984387-c43d-4409-8eb5-e894b2492b6d is Running (Ready = true)
Jul 29 17:03:44.221: INFO: Pod "pod-projected-configmaps-8a984387-c43d-4409-8eb5-e894b2492b6d" satisfied condition "running and ready"
STEP: Updating configmap projected-configmap-test-upd-9cddc77b-6350-437f-90f8-83109233ab64 07/29/23 17:03:44.236
STEP: waiting to observe update in volume 07/29/23 17:03:44.247
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jul 29 17:03:46.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2079" for this suite. 07/29/23 17:03:46.278
{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","completed":348,"skipped":6369,"failed":0}
------------------------------
â€¢ [4.189 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 17:03:42.101
    Jul 29 17:03:42.101: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename projected 07/29/23 17:03:42.104
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 17:03:42.139
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 17:03:42.145
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:123
    STEP: Creating projection with configMap that has name projected-configmap-test-upd-9cddc77b-6350-437f-90f8-83109233ab64 07/29/23 17:03:42.167
    STEP: Creating the pod 07/29/23 17:03:42.183
    Jul 29 17:03:42.203: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-8a984387-c43d-4409-8eb5-e894b2492b6d" in namespace "projected-2079" to be "running and ready"
    Jul 29 17:03:42.212: INFO: Pod "pod-projected-configmaps-8a984387-c43d-4409-8eb5-e894b2492b6d": Phase="Pending", Reason="", readiness=false. Elapsed: 9.063487ms
    Jul 29 17:03:42.212: INFO: The phase of Pod pod-projected-configmaps-8a984387-c43d-4409-8eb5-e894b2492b6d is Pending, waiting for it to be Running (with Ready = true)
    Jul 29 17:03:44.220: INFO: Pod "pod-projected-configmaps-8a984387-c43d-4409-8eb5-e894b2492b6d": Phase="Running", Reason="", readiness=true. Elapsed: 2.017615805s
    Jul 29 17:03:44.221: INFO: The phase of Pod pod-projected-configmaps-8a984387-c43d-4409-8eb5-e894b2492b6d is Running (Ready = true)
    Jul 29 17:03:44.221: INFO: Pod "pod-projected-configmaps-8a984387-c43d-4409-8eb5-e894b2492b6d" satisfied condition "running and ready"
    STEP: Updating configmap projected-configmap-test-upd-9cddc77b-6350-437f-90f8-83109233ab64 07/29/23 17:03:44.236
    STEP: waiting to observe update in volume 07/29/23 17:03:44.247
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jul 29 17:03:46.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-2079" for this suite. 07/29/23 17:03:46.278
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 17:03:46.297
Jul 29 17:03:46.297: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename pod-network-test 07/29/23 17:03:46.299
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 17:03:46.326
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 17:03:46.33
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
STEP: Performing setup for networking test in namespace pod-network-test-2364 07/29/23 17:03:46.336
STEP: creating a selector 07/29/23 17:03:46.336
STEP: Creating the service pods in kubernetes 07/29/23 17:03:46.336
Jul 29 17:03:46.336: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jul 29 17:03:46.413: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-2364" to be "running and ready"
Jul 29 17:03:46.432: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 18.434193ms
Jul 29 17:03:46.432: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jul 29 17:03:48.438: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025104576s
Jul 29 17:03:48.438: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jul 29 17:03:50.440: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.026747852s
Jul 29 17:03:50.440: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jul 29 17:03:52.454: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.040763275s
Jul 29 17:03:52.454: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jul 29 17:03:54.440: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.027003857s
Jul 29 17:03:54.440: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jul 29 17:03:56.452: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.038785815s
Jul 29 17:03:56.452: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jul 29 17:03:58.443: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.030023168s
Jul 29 17:03:58.443: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jul 29 17:04:00.447: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.033994701s
Jul 29 17:04:00.447: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jul 29 17:04:02.443: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.03009556s
Jul 29 17:04:02.443: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jul 29 17:04:04.438: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.02497053s
Jul 29 17:04:04.438: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jul 29 17:04:06.439: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.025816979s
Jul 29 17:04:06.439: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jul 29 17:04:08.448: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.035222172s
Jul 29 17:04:08.449: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jul 29 17:04:08.449: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jul 29 17:04:08.464: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-2364" to be "running and ready"
Jul 29 17:04:08.469: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 4.43393ms
Jul 29 17:04:08.469: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jul 29 17:04:08.469: INFO: Pod "netserver-1" satisfied condition "running and ready"
Jul 29 17:04:08.475: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-2364" to be "running and ready"
Jul 29 17:04:08.487: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 11.285307ms
Jul 29 17:04:08.487: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Jul 29 17:04:08.487: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 07/29/23 17:04:08.492
Jul 29 17:04:08.511: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-2364" to be "running"
Jul 29 17:04:08.538: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 26.712332ms
Jul 29 17:04:10.549: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.037112036s
Jul 29 17:04:10.549: INFO: Pod "test-container-pod" satisfied condition "running"
Jul 29 17:04:10.555: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Jul 29 17:04:10.555: INFO: Breadth first check of 10.233.64.243 on host 192.168.121.28...
Jul 29 17:04:10.565: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.65.23:9080/dial?request=hostname&protocol=udp&host=10.233.64.243&port=8081&tries=1'] Namespace:pod-network-test-2364 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 29 17:04:10.566: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
Jul 29 17:04:10.569: INFO: ExecWithOptions: Clientset creation
Jul 29 17:04:10.570: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-2364/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.65.23%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.64.243%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jul 29 17:04:10.711: INFO: Waiting for responses: map[]
Jul 29 17:04:10.711: INFO: reached 10.233.64.243 after 0/1 tries
Jul 29 17:04:10.711: INFO: Breadth first check of 10.233.66.158 on host 192.168.121.206...
Jul 29 17:04:10.725: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.65.23:9080/dial?request=hostname&protocol=udp&host=10.233.66.158&port=8081&tries=1'] Namespace:pod-network-test-2364 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 29 17:04:10.725: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
Jul 29 17:04:10.726: INFO: ExecWithOptions: Clientset creation
Jul 29 17:04:10.726: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-2364/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.65.23%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.66.158%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jul 29 17:04:10.842: INFO: Waiting for responses: map[]
Jul 29 17:04:10.842: INFO: reached 10.233.66.158 after 0/1 tries
Jul 29 17:04:10.842: INFO: Breadth first check of 10.233.65.164 on host 192.168.121.234...
Jul 29 17:04:10.848: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.65.23:9080/dial?request=hostname&protocol=udp&host=10.233.65.164&port=8081&tries=1'] Namespace:pod-network-test-2364 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 29 17:04:10.848: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
Jul 29 17:04:10.849: INFO: ExecWithOptions: Clientset creation
Jul 29 17:04:10.849: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-2364/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.65.23%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.65.164%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jul 29 17:04:10.942: INFO: Waiting for responses: map[]
Jul 29 17:04:10.942: INFO: reached 10.233.65.164 after 0/1 tries
Jul 29 17:04:10.942: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Jul 29 17:04:10.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2364" for this suite. 07/29/23 17:04:10.952
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","completed":349,"skipped":6423,"failed":0}
------------------------------
â€¢ [SLOW TEST] [24.665 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 17:03:46.297
    Jul 29 17:03:46.297: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename pod-network-test 07/29/23 17:03:46.299
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 17:03:46.326
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 17:03:46.33
    [It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:93
    STEP: Performing setup for networking test in namespace pod-network-test-2364 07/29/23 17:03:46.336
    STEP: creating a selector 07/29/23 17:03:46.336
    STEP: Creating the service pods in kubernetes 07/29/23 17:03:46.336
    Jul 29 17:03:46.336: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jul 29 17:03:46.413: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-2364" to be "running and ready"
    Jul 29 17:03:46.432: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 18.434193ms
    Jul 29 17:03:46.432: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jul 29 17:03:48.438: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025104576s
    Jul 29 17:03:48.438: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jul 29 17:03:50.440: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.026747852s
    Jul 29 17:03:50.440: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jul 29 17:03:52.454: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.040763275s
    Jul 29 17:03:52.454: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jul 29 17:03:54.440: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.027003857s
    Jul 29 17:03:54.440: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jul 29 17:03:56.452: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.038785815s
    Jul 29 17:03:56.452: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jul 29 17:03:58.443: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.030023168s
    Jul 29 17:03:58.443: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jul 29 17:04:00.447: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.033994701s
    Jul 29 17:04:00.447: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jul 29 17:04:02.443: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.03009556s
    Jul 29 17:04:02.443: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jul 29 17:04:04.438: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.02497053s
    Jul 29 17:04:04.438: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jul 29 17:04:06.439: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.025816979s
    Jul 29 17:04:06.439: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jul 29 17:04:08.448: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.035222172s
    Jul 29 17:04:08.449: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jul 29 17:04:08.449: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jul 29 17:04:08.464: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-2364" to be "running and ready"
    Jul 29 17:04:08.469: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 4.43393ms
    Jul 29 17:04:08.469: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jul 29 17:04:08.469: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Jul 29 17:04:08.475: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-2364" to be "running and ready"
    Jul 29 17:04:08.487: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 11.285307ms
    Jul 29 17:04:08.487: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Jul 29 17:04:08.487: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 07/29/23 17:04:08.492
    Jul 29 17:04:08.511: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-2364" to be "running"
    Jul 29 17:04:08.538: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 26.712332ms
    Jul 29 17:04:10.549: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.037112036s
    Jul 29 17:04:10.549: INFO: Pod "test-container-pod" satisfied condition "running"
    Jul 29 17:04:10.555: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Jul 29 17:04:10.555: INFO: Breadth first check of 10.233.64.243 on host 192.168.121.28...
    Jul 29 17:04:10.565: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.65.23:9080/dial?request=hostname&protocol=udp&host=10.233.64.243&port=8081&tries=1'] Namespace:pod-network-test-2364 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jul 29 17:04:10.566: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    Jul 29 17:04:10.569: INFO: ExecWithOptions: Clientset creation
    Jul 29 17:04:10.570: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-2364/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.65.23%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.64.243%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jul 29 17:04:10.711: INFO: Waiting for responses: map[]
    Jul 29 17:04:10.711: INFO: reached 10.233.64.243 after 0/1 tries
    Jul 29 17:04:10.711: INFO: Breadth first check of 10.233.66.158 on host 192.168.121.206...
    Jul 29 17:04:10.725: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.65.23:9080/dial?request=hostname&protocol=udp&host=10.233.66.158&port=8081&tries=1'] Namespace:pod-network-test-2364 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jul 29 17:04:10.725: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    Jul 29 17:04:10.726: INFO: ExecWithOptions: Clientset creation
    Jul 29 17:04:10.726: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-2364/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.65.23%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.66.158%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jul 29 17:04:10.842: INFO: Waiting for responses: map[]
    Jul 29 17:04:10.842: INFO: reached 10.233.66.158 after 0/1 tries
    Jul 29 17:04:10.842: INFO: Breadth first check of 10.233.65.164 on host 192.168.121.234...
    Jul 29 17:04:10.848: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.65.23:9080/dial?request=hostname&protocol=udp&host=10.233.65.164&port=8081&tries=1'] Namespace:pod-network-test-2364 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jul 29 17:04:10.848: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    Jul 29 17:04:10.849: INFO: ExecWithOptions: Clientset creation
    Jul 29 17:04:10.849: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-2364/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.65.23%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.65.164%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jul 29 17:04:10.942: INFO: Waiting for responses: map[]
    Jul 29 17:04:10.942: INFO: reached 10.233.65.164 after 0/1 tries
    Jul 29 17:04:10.942: INFO: Going to retry 0 out of 3 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Jul 29 17:04:10.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-2364" for this suite. 07/29/23 17:04:10.952
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 17:04:10.964
Jul 29 17:04:10.964: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename projected 07/29/23 17:04:10.968
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 17:04:11.007
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 17:04:11.012
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192
STEP: Creating a pod to test downward API volume plugin 07/29/23 17:04:11.017
Jul 29 17:04:11.034: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1ece73d3-8ddc-4ad3-9304-d810c63b864d" in namespace "projected-1462" to be "Succeeded or Failed"
Jul 29 17:04:11.041: INFO: Pod "downwardapi-volume-1ece73d3-8ddc-4ad3-9304-d810c63b864d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.516912ms
Jul 29 17:04:13.052: INFO: Pod "downwardapi-volume-1ece73d3-8ddc-4ad3-9304-d810c63b864d": Phase="Running", Reason="", readiness=true. Elapsed: 2.016807602s
Jul 29 17:04:15.050: INFO: Pod "downwardapi-volume-1ece73d3-8ddc-4ad3-9304-d810c63b864d": Phase="Running", Reason="", readiness=false. Elapsed: 4.015338295s
Jul 29 17:04:17.053: INFO: Pod "downwardapi-volume-1ece73d3-8ddc-4ad3-9304-d810c63b864d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.018188781s
STEP: Saw pod success 07/29/23 17:04:17.053
Jul 29 17:04:17.053: INFO: Pod "downwardapi-volume-1ece73d3-8ddc-4ad3-9304-d810c63b864d" satisfied condition "Succeeded or Failed"
Jul 29 17:04:17.060: INFO: Trying to get logs from node wa4quivohpee-3 pod downwardapi-volume-1ece73d3-8ddc-4ad3-9304-d810c63b864d container client-container: <nil>
STEP: delete the pod 07/29/23 17:04:17.071
Jul 29 17:04:17.130: INFO: Waiting for pod downwardapi-volume-1ece73d3-8ddc-4ad3-9304-d810c63b864d to disappear
Jul 29 17:04:17.162: INFO: Pod downwardapi-volume-1ece73d3-8ddc-4ad3-9304-d810c63b864d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jul 29 17:04:17.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1462" for this suite. 07/29/23 17:04:17.203
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","completed":350,"skipped":6423,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.254 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 17:04:10.964
    Jul 29 17:04:10.964: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename projected 07/29/23 17:04:10.968
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 17:04:11.007
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 17:04:11.012
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:192
    STEP: Creating a pod to test downward API volume plugin 07/29/23 17:04:11.017
    Jul 29 17:04:11.034: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1ece73d3-8ddc-4ad3-9304-d810c63b864d" in namespace "projected-1462" to be "Succeeded or Failed"
    Jul 29 17:04:11.041: INFO: Pod "downwardapi-volume-1ece73d3-8ddc-4ad3-9304-d810c63b864d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.516912ms
    Jul 29 17:04:13.052: INFO: Pod "downwardapi-volume-1ece73d3-8ddc-4ad3-9304-d810c63b864d": Phase="Running", Reason="", readiness=true. Elapsed: 2.016807602s
    Jul 29 17:04:15.050: INFO: Pod "downwardapi-volume-1ece73d3-8ddc-4ad3-9304-d810c63b864d": Phase="Running", Reason="", readiness=false. Elapsed: 4.015338295s
    Jul 29 17:04:17.053: INFO: Pod "downwardapi-volume-1ece73d3-8ddc-4ad3-9304-d810c63b864d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.018188781s
    STEP: Saw pod success 07/29/23 17:04:17.053
    Jul 29 17:04:17.053: INFO: Pod "downwardapi-volume-1ece73d3-8ddc-4ad3-9304-d810c63b864d" satisfied condition "Succeeded or Failed"
    Jul 29 17:04:17.060: INFO: Trying to get logs from node wa4quivohpee-3 pod downwardapi-volume-1ece73d3-8ddc-4ad3-9304-d810c63b864d container client-container: <nil>
    STEP: delete the pod 07/29/23 17:04:17.071
    Jul 29 17:04:17.130: INFO: Waiting for pod downwardapi-volume-1ece73d3-8ddc-4ad3-9304-d810c63b864d to disappear
    Jul 29 17:04:17.162: INFO: Pod downwardapi-volume-1ece73d3-8ddc-4ad3-9304-d810c63b864d no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jul 29 17:04:17.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1462" for this suite. 07/29/23 17:04:17.203
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 17:04:17.226
Jul 29 17:04:17.227: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename deployment 07/29/23 17:04:17.229
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 17:04:17.28
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 17:04:17.285
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
Jul 29 17:04:17.289: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Jul 29 17:04:17.304: INFO: Pod name sample-pod: Found 0 pods out of 1
Jul 29 17:04:22.311: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 07/29/23 17:04:22.311
Jul 29 17:04:22.311: INFO: Creating deployment "test-rolling-update-deployment"
Jul 29 17:04:22.320: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Jul 29 17:04:22.329: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Jul 29 17:04:24.346: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Jul 29 17:04:24.355: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jul 29 17:04:24.369: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-9847  5f7f2935-9506-44e0-9e73-04881a0734e7 38454 1 2023-07-29 17:04:22 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-07-29 17:04:22 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-29 17:04:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002ee4cb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-07-29 17:04:22 +0000 UTC,LastTransitionTime:2023-07-29 17:04:22 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-78f575d8ff" has successfully progressed.,LastUpdateTime:2023-07-29 17:04:24 +0000 UTC,LastTransitionTime:2023-07-29 17:04:22 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jul 29 17:04:24.375: INFO: New ReplicaSet "test-rolling-update-deployment-78f575d8ff" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-78f575d8ff  deployment-9847  7f5a1be3-9505-4f5b-8cbb-24aa37d8b3e3 38444 1 2023-07-29 17:04:22 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 5f7f2935-9506-44e0-9e73-04881a0734e7 0xc0027b25b7 0xc0027b25b8}] [] [{kube-controller-manager Update apps/v1 2023-07-29 17:04:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5f7f2935-9506-44e0-9e73-04881a0734e7\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-29 17:04:24 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 78f575d8ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0027b2668 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jul 29 17:04:24.375: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Jul 29 17:04:24.375: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-9847  dc6be19a-9965-4394-a542-f6c8c7d33b4f 38453 2 2023-07-29 17:04:17 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 5f7f2935-9506-44e0-9e73-04881a0734e7 0xc0027b2487 0xc0027b2488}] [] [{e2e.test Update apps/v1 2023-07-29 17:04:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-29 17:04:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5f7f2935-9506-44e0-9e73-04881a0734e7\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-07-29 17:04:24 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0027b2548 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jul 29 17:04:24.380: INFO: Pod "test-rolling-update-deployment-78f575d8ff-cjwzp" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-78f575d8ff-cjwzp test-rolling-update-deployment-78f575d8ff- deployment-9847  09267e2b-fd92-4a68-ab1f-43ca0b0631c0 38443 0 2023-07-29 17:04:22 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-78f575d8ff 7f5a1be3-9505-4f5b-8cbb-24aa37d8b3e3 0xc002ee5097 0xc002ee5098}] [] [{kube-controller-manager Update v1 2023-07-29 17:04:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7f5a1be3-9505-4f5b-8cbb-24aa37d8b3e3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 17:04:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.65.244\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-754dz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-754dz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wa4quivohpee-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:04:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:04:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:04:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:04:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.234,PodIP:10.233.65.244,StartTime:2023-07-29 17:04:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-29 17:04:23 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:37dd9ad84b34913b4bc57e00b0a28e8b00f18ca6a5ecec69461aa57402e5c787,ContainerID:cri-o://e5d6d1613849d6db3f24ccb1c76a7850c67135765b971b501fbf040940b02c8b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.65.244,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jul 29 17:04:24.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9847" for this suite. 07/29/23 17:04:24.388
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","completed":351,"skipped":6469,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.172 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 17:04:17.226
    Jul 29 17:04:17.227: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename deployment 07/29/23 17:04:17.229
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 17:04:17.28
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 17:04:17.285
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:105
    Jul 29 17:04:17.289: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
    Jul 29 17:04:17.304: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jul 29 17:04:22.311: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 07/29/23 17:04:22.311
    Jul 29 17:04:22.311: INFO: Creating deployment "test-rolling-update-deployment"
    Jul 29 17:04:22.320: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
    Jul 29 17:04:22.329: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
    Jul 29 17:04:24.346: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
    Jul 29 17:04:24.355: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jul 29 17:04:24.369: INFO: Deployment "test-rolling-update-deployment":
    &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-9847  5f7f2935-9506-44e0-9e73-04881a0734e7 38454 1 2023-07-29 17:04:22 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-07-29 17:04:22 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-29 17:04:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002ee4cb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-07-29 17:04:22 +0000 UTC,LastTransitionTime:2023-07-29 17:04:22 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-78f575d8ff" has successfully progressed.,LastUpdateTime:2023-07-29 17:04:24 +0000 UTC,LastTransitionTime:2023-07-29 17:04:22 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jul 29 17:04:24.375: INFO: New ReplicaSet "test-rolling-update-deployment-78f575d8ff" of Deployment "test-rolling-update-deployment":
    &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-78f575d8ff  deployment-9847  7f5a1be3-9505-4f5b-8cbb-24aa37d8b3e3 38444 1 2023-07-29 17:04:22 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 5f7f2935-9506-44e0-9e73-04881a0734e7 0xc0027b25b7 0xc0027b25b8}] [] [{kube-controller-manager Update apps/v1 2023-07-29 17:04:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5f7f2935-9506-44e0-9e73-04881a0734e7\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-29 17:04:24 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 78f575d8ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0027b2668 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jul 29 17:04:24.375: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
    Jul 29 17:04:24.375: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-9847  dc6be19a-9965-4394-a542-f6c8c7d33b4f 38453 2 2023-07-29 17:04:17 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 5f7f2935-9506-44e0-9e73-04881a0734e7 0xc0027b2487 0xc0027b2488}] [] [{e2e.test Update apps/v1 2023-07-29 17:04:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-29 17:04:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5f7f2935-9506-44e0-9e73-04881a0734e7\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-07-29 17:04:24 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0027b2548 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jul 29 17:04:24.380: INFO: Pod "test-rolling-update-deployment-78f575d8ff-cjwzp" is available:
    &Pod{ObjectMeta:{test-rolling-update-deployment-78f575d8ff-cjwzp test-rolling-update-deployment-78f575d8ff- deployment-9847  09267e2b-fd92-4a68-ab1f-43ca0b0631c0 38443 0 2023-07-29 17:04:22 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-78f575d8ff 7f5a1be3-9505-4f5b-8cbb-24aa37d8b3e3 0xc002ee5097 0xc002ee5098}] [] [{kube-controller-manager Update v1 2023-07-29 17:04:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7f5a1be3-9505-4f5b-8cbb-24aa37d8b3e3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 17:04:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.65.244\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-754dz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-754dz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wa4quivohpee-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:04:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:04:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:04:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:04:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.234,PodIP:10.233.65.244,StartTime:2023-07-29 17:04:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-29 17:04:23 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:37dd9ad84b34913b4bc57e00b0a28e8b00f18ca6a5ecec69461aa57402e5c787,ContainerID:cri-o://e5d6d1613849d6db3f24ccb1c76a7850c67135765b971b501fbf040940b02c8b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.65.244,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jul 29 17:04:24.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-9847" for this suite. 07/29/23 17:04:24.388
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 17:04:24.408
Jul 29 17:04:24.408: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename cronjob 07/29/23 17:04:24.412
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 17:04:24.436
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 17:04:24.441
[It] should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
STEP: Creating a cronjob 07/29/23 17:04:24.444
STEP: creating 07/29/23 17:04:24.444
STEP: getting 07/29/23 17:04:24.454
STEP: listing 07/29/23 17:04:24.457
STEP: watching 07/29/23 17:04:24.462
Jul 29 17:04:24.462: INFO: starting watch
STEP: cluster-wide listing 07/29/23 17:04:24.463
STEP: cluster-wide watching 07/29/23 17:04:24.481
Jul 29 17:04:24.482: INFO: starting watch
STEP: patching 07/29/23 17:04:24.486
STEP: updating 07/29/23 17:04:24.508
Jul 29 17:04:24.548: INFO: waiting for watch events with expected annotations
Jul 29 17:04:24.549: INFO: saw patched and updated annotations
STEP: patching /status 07/29/23 17:04:24.549
STEP: updating /status 07/29/23 17:04:24.561
STEP: get /status 07/29/23 17:04:24.576
STEP: deleting 07/29/23 17:04:24.58
STEP: deleting a collection 07/29/23 17:04:24.604
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Jul 29 17:04:24.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-8316" for this suite. 07/29/23 17:04:24.661
{"msg":"PASSED [sig-apps] CronJob should support CronJob API operations [Conformance]","completed":352,"skipped":6501,"failed":0}
------------------------------
â€¢ [0.276 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 17:04:24.408
    Jul 29 17:04:24.408: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename cronjob 07/29/23 17:04:24.412
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 17:04:24.436
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 17:04:24.441
    [It] should support CronJob API operations [Conformance]
      test/e2e/apps/cronjob.go:319
    STEP: Creating a cronjob 07/29/23 17:04:24.444
    STEP: creating 07/29/23 17:04:24.444
    STEP: getting 07/29/23 17:04:24.454
    STEP: listing 07/29/23 17:04:24.457
    STEP: watching 07/29/23 17:04:24.462
    Jul 29 17:04:24.462: INFO: starting watch
    STEP: cluster-wide listing 07/29/23 17:04:24.463
    STEP: cluster-wide watching 07/29/23 17:04:24.481
    Jul 29 17:04:24.482: INFO: starting watch
    STEP: patching 07/29/23 17:04:24.486
    STEP: updating 07/29/23 17:04:24.508
    Jul 29 17:04:24.548: INFO: waiting for watch events with expected annotations
    Jul 29 17:04:24.549: INFO: saw patched and updated annotations
    STEP: patching /status 07/29/23 17:04:24.549
    STEP: updating /status 07/29/23 17:04:24.561
    STEP: get /status 07/29/23 17:04:24.576
    STEP: deleting 07/29/23 17:04:24.58
    STEP: deleting a collection 07/29/23 17:04:24.604
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Jul 29 17:04:24.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-8316" for this suite. 07/29/23 17:04:24.661
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 17:04:24.686
Jul 29 17:04:24.686: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename webhook 07/29/23 17:04:24.696
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 17:04:24.727
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 17:04:24.731
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 07/29/23 17:04:24.759
STEP: Create role binding to let webhook read extension-apiserver-authentication 07/29/23 17:04:26.193
STEP: Deploying the webhook pod 07/29/23 17:04:26.206
STEP: Wait for the deployment to be ready 07/29/23 17:04:26.222
Jul 29 17:04:26.234: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 07/29/23 17:04:28.253
STEP: Verifying the service has paired with the endpoint 07/29/23 17:04:28.269
Jul 29 17:04:29.269: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208
STEP: Registering the webhook via the AdmissionRegistration API 07/29/23 17:04:29.276
STEP: create a pod 07/29/23 17:04:29.304
Jul 29 17:04:29.319: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-1472" to be "running"
Jul 29 17:04:29.328: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 9.409828ms
Jul 29 17:04:31.335: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.016259035s
Jul 29 17:04:31.335: INFO: Pod "to-be-attached-pod" satisfied condition "running"
STEP: 'kubectl attach' the pod, should be denied by the webhook 07/29/23 17:04:31.335
Jul 29 17:04:31.336: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=webhook-1472 attach --namespace=webhook-1472 to-be-attached-pod -i -c=container1'
Jul 29 17:04:31.521: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jul 29 17:04:31.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1472" for this suite. 07/29/23 17:04:31.541
STEP: Destroying namespace "webhook-1472-markers" for this suite. 07/29/23 17:04:31.552
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","completed":353,"skipped":6502,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.963 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 17:04:24.686
    Jul 29 17:04:24.686: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename webhook 07/29/23 17:04:24.696
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 17:04:24.727
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 17:04:24.731
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 07/29/23 17:04:24.759
    STEP: Create role binding to let webhook read extension-apiserver-authentication 07/29/23 17:04:26.193
    STEP: Deploying the webhook pod 07/29/23 17:04:26.206
    STEP: Wait for the deployment to be ready 07/29/23 17:04:26.222
    Jul 29 17:04:26.234: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 07/29/23 17:04:28.253
    STEP: Verifying the service has paired with the endpoint 07/29/23 17:04:28.269
    Jul 29 17:04:29.269: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny attaching pod [Conformance]
      test/e2e/apimachinery/webhook.go:208
    STEP: Registering the webhook via the AdmissionRegistration API 07/29/23 17:04:29.276
    STEP: create a pod 07/29/23 17:04:29.304
    Jul 29 17:04:29.319: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-1472" to be "running"
    Jul 29 17:04:29.328: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 9.409828ms
    Jul 29 17:04:31.335: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.016259035s
    Jul 29 17:04:31.335: INFO: Pod "to-be-attached-pod" satisfied condition "running"
    STEP: 'kubectl attach' the pod, should be denied by the webhook 07/29/23 17:04:31.335
    Jul 29 17:04:31.336: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=webhook-1472 attach --namespace=webhook-1472 to-be-attached-pod -i -c=container1'
    Jul 29 17:04:31.521: INFO: rc: 1
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jul 29 17:04:31.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-1472" for this suite. 07/29/23 17:04:31.541
    STEP: Destroying namespace "webhook-1472-markers" for this suite. 07/29/23 17:04:31.552
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 17:04:31.651
Jul 29 17:04:31.651: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename watch 07/29/23 17:04:31.657
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 17:04:31.708
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 17:04:31.713
[It] should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
STEP: getting a starting resourceVersion 07/29/23 17:04:31.72
STEP: starting a background goroutine to produce watch events 07/29/23 17:04:31.727
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 07/29/23 17:04:31.727
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Jul 29 17:04:34.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9520" for this suite. 07/29/23 17:04:34.521
{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","completed":354,"skipped":6530,"failed":0}
------------------------------
â€¢ [2.925 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 17:04:31.651
    Jul 29 17:04:31.651: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename watch 07/29/23 17:04:31.657
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 17:04:31.708
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 17:04:31.713
    [It] should receive events on concurrent watches in same order [Conformance]
      test/e2e/apimachinery/watch.go:334
    STEP: getting a starting resourceVersion 07/29/23 17:04:31.72
    STEP: starting a background goroutine to produce watch events 07/29/23 17:04:31.727
    STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 07/29/23 17:04:31.727
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Jul 29 17:04:34.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-9520" for this suite. 07/29/23 17:04:34.521
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 17:04:34.585
Jul 29 17:04:34.585: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename cronjob 07/29/23 17:04:34.587
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 17:04:34.626
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 17:04:34.633
[It] should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
STEP: Creating a ReplaceConcurrent cronjob 07/29/23 17:04:34.641
STEP: Ensuring a job is scheduled 07/29/23 17:04:34.656
STEP: Ensuring exactly one is scheduled 07/29/23 17:05:00.664
STEP: Ensuring exactly one running job exists by listing jobs explicitly 07/29/23 17:05:00.682
STEP: Ensuring the job is replaced with a new one 07/29/23 17:05:00.696
STEP: Removing cronjob 07/29/23 17:06:00.709
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Jul 29 17:06:00.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-8060" for this suite. 07/29/23 17:06:00.754
{"msg":"PASSED [sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]","completed":355,"skipped":6578,"failed":0}
------------------------------
â€¢ [SLOW TEST] [86.186 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 17:04:34.585
    Jul 29 17:04:34.585: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename cronjob 07/29/23 17:04:34.587
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 17:04:34.626
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 17:04:34.633
    [It] should replace jobs when ReplaceConcurrent [Conformance]
      test/e2e/apps/cronjob.go:160
    STEP: Creating a ReplaceConcurrent cronjob 07/29/23 17:04:34.641
    STEP: Ensuring a job is scheduled 07/29/23 17:04:34.656
    STEP: Ensuring exactly one is scheduled 07/29/23 17:05:00.664
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 07/29/23 17:05:00.682
    STEP: Ensuring the job is replaced with a new one 07/29/23 17:05:00.696
    STEP: Removing cronjob 07/29/23 17:06:00.709
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Jul 29 17:06:00.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-8060" for this suite. 07/29/23 17:06:00.754
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 17:06:00.8
Jul 29 17:06:00.800: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename var-expansion 07/29/23 17:06:00.802
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 17:06:00.847
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 17:06:00.858
[It] should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111
STEP: Creating a pod to test substitution in volume subpath 07/29/23 17:06:00.868
Jul 29 17:06:00.888: INFO: Waiting up to 5m0s for pod "var-expansion-e606e1af-6a36-4d00-aab2-8c84797d6fdb" in namespace "var-expansion-7869" to be "Succeeded or Failed"
Jul 29 17:06:00.894: INFO: Pod "var-expansion-e606e1af-6a36-4d00-aab2-8c84797d6fdb": Phase="Pending", Reason="", readiness=false. Elapsed: 5.918537ms
Jul 29 17:06:02.903: INFO: Pod "var-expansion-e606e1af-6a36-4d00-aab2-8c84797d6fdb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01552867s
Jul 29 17:06:04.901: INFO: Pod "var-expansion-e606e1af-6a36-4d00-aab2-8c84797d6fdb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01327035s
STEP: Saw pod success 07/29/23 17:06:04.901
Jul 29 17:06:04.901: INFO: Pod "var-expansion-e606e1af-6a36-4d00-aab2-8c84797d6fdb" satisfied condition "Succeeded or Failed"
Jul 29 17:06:04.906: INFO: Trying to get logs from node wa4quivohpee-3 pod var-expansion-e606e1af-6a36-4d00-aab2-8c84797d6fdb container dapi-container: <nil>
STEP: delete the pod 07/29/23 17:06:04.933
Jul 29 17:06:04.952: INFO: Waiting for pod var-expansion-e606e1af-6a36-4d00-aab2-8c84797d6fdb to disappear
Jul 29 17:06:04.957: INFO: Pod var-expansion-e606e1af-6a36-4d00-aab2-8c84797d6fdb no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jul 29 17:06:04.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7869" for this suite. 07/29/23 17:06:04.967
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]","completed":356,"skipped":6604,"failed":0}
------------------------------
â€¢ [4.183 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 17:06:00.8
    Jul 29 17:06:00.800: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename var-expansion 07/29/23 17:06:00.802
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 17:06:00.847
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 17:06:00.858
    [It] should allow substituting values in a volume subpath [Conformance]
      test/e2e/common/node/expansion.go:111
    STEP: Creating a pod to test substitution in volume subpath 07/29/23 17:06:00.868
    Jul 29 17:06:00.888: INFO: Waiting up to 5m0s for pod "var-expansion-e606e1af-6a36-4d00-aab2-8c84797d6fdb" in namespace "var-expansion-7869" to be "Succeeded or Failed"
    Jul 29 17:06:00.894: INFO: Pod "var-expansion-e606e1af-6a36-4d00-aab2-8c84797d6fdb": Phase="Pending", Reason="", readiness=false. Elapsed: 5.918537ms
    Jul 29 17:06:02.903: INFO: Pod "var-expansion-e606e1af-6a36-4d00-aab2-8c84797d6fdb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01552867s
    Jul 29 17:06:04.901: INFO: Pod "var-expansion-e606e1af-6a36-4d00-aab2-8c84797d6fdb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01327035s
    STEP: Saw pod success 07/29/23 17:06:04.901
    Jul 29 17:06:04.901: INFO: Pod "var-expansion-e606e1af-6a36-4d00-aab2-8c84797d6fdb" satisfied condition "Succeeded or Failed"
    Jul 29 17:06:04.906: INFO: Trying to get logs from node wa4quivohpee-3 pod var-expansion-e606e1af-6a36-4d00-aab2-8c84797d6fdb container dapi-container: <nil>
    STEP: delete the pod 07/29/23 17:06:04.933
    Jul 29 17:06:04.952: INFO: Waiting for pod var-expansion-e606e1af-6a36-4d00-aab2-8c84797d6fdb to disappear
    Jul 29 17:06:04.957: INFO: Pod var-expansion-e606e1af-6a36-4d00-aab2-8c84797d6fdb no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jul 29 17:06:04.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-7869" for this suite. 07/29/23 17:06:04.967
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 17:06:04.985
Jul 29 17:06:04.985: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename runtimeclass 07/29/23 17:06:04.988
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 17:06:05.017
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 17:06:05.023
[It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
Jul 29 17:06:05.054: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-5702 to be scheduled
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Jul 29 17:06:05.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-5702" for this suite. 07/29/23 17:06:05.093
{"msg":"PASSED [sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]","completed":357,"skipped":6634,"failed":0}
------------------------------
â€¢ [0.136 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 17:06:04.985
    Jul 29 17:06:04.985: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename runtimeclass 07/29/23 17:06:04.988
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 17:06:05.017
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 17:06:05.023
    [It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:104
    Jul 29 17:06:05.054: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-5702 to be scheduled
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Jul 29 17:06:05.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-5702" for this suite. 07/29/23 17:06:05.093
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod
  should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1711
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 17:06:05.128
Jul 29 17:06:05.128: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename kubectl 07/29/23 17:06:05.13
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 17:06:05.193
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 17:06:05.197
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1698
[It] should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1711
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 07/29/23 17:06:05.201
Jul 29 17:06:05.201: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-5467 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2'
Jul 29 17:06:05.343: INFO: stderr: ""
Jul 29 17:06:05.344: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created 07/29/23 17:06:05.344
[AfterEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1702
Jul 29 17:06:05.358: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-5467 delete pods e2e-test-httpd-pod'
Jul 29 17:06:07.577: INFO: stderr: ""
Jul 29 17:06:07.577: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jul 29 17:06:07.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5467" for this suite. 07/29/23 17:06:07.587
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","completed":358,"skipped":6658,"failed":0}
------------------------------
â€¢ [2.470 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl run pod
  test/e2e/kubectl/kubectl.go:1695
    should create a pod from an image when restart is Never  [Conformance]
    test/e2e/kubectl/kubectl.go:1711

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 17:06:05.128
    Jul 29 17:06:05.128: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename kubectl 07/29/23 17:06:05.13
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 17:06:05.193
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 17:06:05.197
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1698
    [It] should create a pod from an image when restart is Never  [Conformance]
      test/e2e/kubectl/kubectl.go:1711
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 07/29/23 17:06:05.201
    Jul 29 17:06:05.201: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-5467 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2'
    Jul 29 17:06:05.343: INFO: stderr: ""
    Jul 29 17:06:05.344: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod was created 07/29/23 17:06:05.344
    [AfterEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1702
    Jul 29 17:06:05.358: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1985242600 --namespace=kubectl-5467 delete pods e2e-test-httpd-pod'
    Jul 29 17:06:07.577: INFO: stderr: ""
    Jul 29 17:06:07.577: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jul 29 17:06:07.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-5467" for this suite. 07/29/23 17:06:07.587
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 17:06:07.6
Jul 29 17:06:07.600: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename security-context 07/29/23 17:06:07.605
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 17:06:07.664
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 17:06:07.669
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 07/29/23 17:06:07.673
Jul 29 17:06:07.687: INFO: Waiting up to 5m0s for pod "security-context-823632e2-be87-4e84-8543-b684ee005319" in namespace "security-context-3856" to be "Succeeded or Failed"
Jul 29 17:06:07.693: INFO: Pod "security-context-823632e2-be87-4e84-8543-b684ee005319": Phase="Pending", Reason="", readiness=false. Elapsed: 5.782624ms
Jul 29 17:06:09.705: INFO: Pod "security-context-823632e2-be87-4e84-8543-b684ee005319": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017857235s
Jul 29 17:06:11.698: INFO: Pod "security-context-823632e2-be87-4e84-8543-b684ee005319": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011440278s
STEP: Saw pod success 07/29/23 17:06:11.699
Jul 29 17:06:11.701: INFO: Pod "security-context-823632e2-be87-4e84-8543-b684ee005319" satisfied condition "Succeeded or Failed"
Jul 29 17:06:11.706: INFO: Trying to get logs from node wa4quivohpee-3 pod security-context-823632e2-be87-4e84-8543-b684ee005319 container test-container: <nil>
STEP: delete the pod 07/29/23 17:06:11.728
Jul 29 17:06:11.748: INFO: Waiting for pod security-context-823632e2-be87-4e84-8543-b684ee005319 to disappear
Jul 29 17:06:11.752: INFO: Pod security-context-823632e2-be87-4e84-8543-b684ee005319 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Jul 29 17:06:11.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-3856" for this suite. 07/29/23 17:06:11.76
{"msg":"PASSED [sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","completed":359,"skipped":6672,"failed":0}
------------------------------
â€¢ [4.176 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 17:06:07.6
    Jul 29 17:06:07.600: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename security-context 07/29/23 17:06:07.605
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 17:06:07.664
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 17:06:07.669
    [It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:97
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 07/29/23 17:06:07.673
    Jul 29 17:06:07.687: INFO: Waiting up to 5m0s for pod "security-context-823632e2-be87-4e84-8543-b684ee005319" in namespace "security-context-3856" to be "Succeeded or Failed"
    Jul 29 17:06:07.693: INFO: Pod "security-context-823632e2-be87-4e84-8543-b684ee005319": Phase="Pending", Reason="", readiness=false. Elapsed: 5.782624ms
    Jul 29 17:06:09.705: INFO: Pod "security-context-823632e2-be87-4e84-8543-b684ee005319": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017857235s
    Jul 29 17:06:11.698: INFO: Pod "security-context-823632e2-be87-4e84-8543-b684ee005319": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011440278s
    STEP: Saw pod success 07/29/23 17:06:11.699
    Jul 29 17:06:11.701: INFO: Pod "security-context-823632e2-be87-4e84-8543-b684ee005319" satisfied condition "Succeeded or Failed"
    Jul 29 17:06:11.706: INFO: Trying to get logs from node wa4quivohpee-3 pod security-context-823632e2-be87-4e84-8543-b684ee005319 container test-container: <nil>
    STEP: delete the pod 07/29/23 17:06:11.728
    Jul 29 17:06:11.748: INFO: Waiting for pod security-context-823632e2-be87-4e84-8543-b684ee005319 to disappear
    Jul 29 17:06:11.752: INFO: Pod security-context-823632e2-be87-4e84-8543-b684ee005319 no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Jul 29 17:06:11.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-3856" for this suite. 07/29/23 17:06:11.76
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits
  should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:51
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 07/29/23 17:06:11.78
Jul 29 17:06:11.781: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
STEP: Building a namespace api object, basename container-runtime 07/29/23 17:06:11.785
STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 17:06:11.868
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 17:06:11.873
[It] should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:51
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 07/29/23 17:06:11.892
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 07/29/23 17:06:29.039
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 07/29/23 17:06:29.046
STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 07/29/23 17:06:29.058
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 07/29/23 17:06:29.058
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 07/29/23 17:06:29.112
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 07/29/23 17:06:32.146
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 07/29/23 17:06:34.168
STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 07/29/23 17:06:34.177
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 07/29/23 17:06:34.177
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 07/29/23 17:06:34.214
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 07/29/23 17:06:35.227
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 07/29/23 17:06:38.254
STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 07/29/23 17:06:38.265
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 07/29/23 17:06:38.265
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Jul 29 17:06:38.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6422" for this suite. 07/29/23 17:06:38.308
{"msg":"PASSED [sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","completed":360,"skipped":6675,"failed":0}
------------------------------
â€¢ [SLOW TEST] [26.538 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    when starting a container that exits
    test/e2e/common/node/runtime.go:44
      should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 07/29/23 17:06:11.78
    Jul 29 17:06:11.781: INFO: >>> kubeConfig: /tmp/kubeconfig-1985242600
    STEP: Building a namespace api object, basename container-runtime 07/29/23 17:06:11.785
    STEP: Waiting for a default service account to be provisioned in namespace 07/29/23 17:06:11.868
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/29/23 17:06:11.873
    [It] should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:51
    STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 07/29/23 17:06:11.892
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 07/29/23 17:06:29.039
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 07/29/23 17:06:29.046
    STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 07/29/23 17:06:29.058
    STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 07/29/23 17:06:29.058
    STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 07/29/23 17:06:29.112
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 07/29/23 17:06:32.146
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 07/29/23 17:06:34.168
    STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 07/29/23 17:06:34.177
    STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 07/29/23 17:06:34.177
    STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 07/29/23 17:06:34.214
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 07/29/23 17:06:35.227
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 07/29/23 17:06:38.254
    STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 07/29/23 17:06:38.265
    STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 07/29/23 17:06:38.265
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Jul 29 17:06:38.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-6422" for this suite. 07/29/23 17:06:38.308
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:87
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:87
{"msg":"Test Suite completed","completed":360,"skipped":6706,"failed":0}
Jul 29 17:06:38.327: INFO: Running AfterSuite actions on all nodes
Jul 29 17:06:38.327: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func20.2
Jul 29 17:06:38.327: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func10.2
Jul 29 17:06:38.327: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func9.2
Jul 29 17:06:38.327: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
Jul 29 17:06:38.327: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
Jul 29 17:06:38.327: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
Jul 29 17:06:38.327: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:87
Jul 29 17:06:38.327: INFO: Running AfterSuite actions on node 1
Jul 29 17:06:38.327: INFO: Skipping dumping logs from cluster
------------------------------
[SynchronizedAfterSuite] PASSED [0.001 seconds]
[SynchronizedAfterSuite] 
test/e2e/e2e.go:87

  Begin Captured GinkgoWriter Output >>
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:87
    Jul 29 17:06:38.327: INFO: Running AfterSuite actions on all nodes
    Jul 29 17:06:38.327: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func20.2
    Jul 29 17:06:38.327: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func10.2
    Jul 29 17:06:38.327: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func9.2
    Jul 29 17:06:38.327: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
    Jul 29 17:06:38.327: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
    Jul 29 17:06:38.327: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
    Jul 29 17:06:38.327: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:87
    Jul 29 17:06:38.327: INFO: Running AfterSuite actions on node 1
    Jul 29 17:06:38.327: INFO: Skipping dumping logs from cluster
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:146
[ReportAfterSuite] TOP-LEVEL
  test/e2e/e2e_test.go:146
------------------------------
[ReportAfterSuite] PASSED [0.000 seconds]
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:146

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/e2e_test.go:146
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:559
[ReportAfterSuite] TOP-LEVEL
  test/e2e/framework/test_context.go:559
------------------------------
[ReportAfterSuite] PASSED [0.139 seconds]
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:559

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/framework/test_context.go:559
  << End Captured GinkgoWriter Output
------------------------------

Ran 360 of 7066 Specs in 6065.689 seconds
SUCCESS! -- 360 Passed | 0 Failed | 0 Pending | 6706 Skipped
PASS

Ginkgo ran 1 suite in 1h41m6.589557262s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.1.6[0m

