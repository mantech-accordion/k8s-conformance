I0628 07:40:02.602087      18 e2e.go:116] Starting e2e run "2b4cfe99-b7cd-4bd1-9a61-43431200c020" on Ginkgo node 1
Jun 28 07:40:02.617: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1687938002 - will randomize all specs

Will run 360 of 7066 specs
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:76
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:76
{"msg":"Test Suite starting","completed":0,"skipped":0,"failed":0}
Jun 28 07:40:02.735: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
Jun 28 07:40:02.736: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Jun 28 07:40:02.767: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Jun 28 07:40:02.806: INFO: 19 / 19 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Jun 28 07:40:02.806: INFO: expected 7 pod replicas in namespace 'kube-system', 7 are Running and Ready.
Jun 28 07:40:02.806: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Jun 28 07:40:02.822: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Jun 28 07:40:02.822: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'calico-node-windows' (0 seconds elapsed)
Jun 28 07:40:02.822: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'csi-proxy' (0 seconds elapsed)
Jun 28 07:40:02.822: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'csi-smb-node' (0 seconds elapsed)
Jun 28 07:40:02.822: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'csi-smb-node-win' (0 seconds elapsed)
Jun 28 07:40:02.822: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'dcgm-exporter' (0 seconds elapsed)
Jun 28 07:40:02.822: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Jun 28 07:40:02.822: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'kube-proxy-windows' (0 seconds elapsed)
Jun 28 07:40:02.822: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'node-exporter' (0 seconds elapsed)
Jun 28 07:40:02.822: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'nvidia-device-plugin-daemonset' (0 seconds elapsed)
Jun 28 07:40:02.822: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'windows-exporter' (0 seconds elapsed)
Jun 28 07:40:02.822: INFO: e2e test version: v1.25.10
Jun 28 07:40:02.826: INFO: kube-apiserver version: v1.25.10-ske.p2
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:76
Jun 28 07:40:02.826: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
Jun 28 07:40:02.832: INFO: Cluster IP family: ipv4
------------------------------
[SynchronizedBeforeSuite] PASSED [0.100 seconds]
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:76

  Begin Captured GinkgoWriter Output >>
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:76
    Jun 28 07:40:02.735: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    Jun 28 07:40:02.736: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
    Jun 28 07:40:02.767: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
    Jun 28 07:40:02.806: INFO: 19 / 19 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
    Jun 28 07:40:02.806: INFO: expected 7 pod replicas in namespace 'kube-system', 7 are Running and Ready.
    Jun 28 07:40:02.806: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
    Jun 28 07:40:02.822: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
    Jun 28 07:40:02.822: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'calico-node-windows' (0 seconds elapsed)
    Jun 28 07:40:02.822: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'csi-proxy' (0 seconds elapsed)
    Jun 28 07:40:02.822: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'csi-smb-node' (0 seconds elapsed)
    Jun 28 07:40:02.822: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'csi-smb-node-win' (0 seconds elapsed)
    Jun 28 07:40:02.822: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'dcgm-exporter' (0 seconds elapsed)
    Jun 28 07:40:02.822: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
    Jun 28 07:40:02.822: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'kube-proxy-windows' (0 seconds elapsed)
    Jun 28 07:40:02.822: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'node-exporter' (0 seconds elapsed)
    Jun 28 07:40:02.822: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'nvidia-device-plugin-daemonset' (0 seconds elapsed)
    Jun 28 07:40:02.822: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'windows-exporter' (0 seconds elapsed)
    Jun 28 07:40:02.822: INFO: e2e test version: v1.25.10
    Jun 28 07:40:02.826: INFO: kube-apiserver version: v1.25.10-ske.p2
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:76
    Jun 28 07:40:02.826: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    Jun 28 07:40:02.832: INFO: Cluster IP family: ipv4
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath
  runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:543
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 07:40:02.855
Jun 28 07:40:02.855: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename sched-preemption 06/28/23 07:40:02.856
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:40:02.874
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:40:02.879
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Jun 28 07:40:02.906: INFO: Waiting up to 1m0s for all nodes to be ready
Jun 28 07:41:02.962: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 07:41:02.967
Jun 28 07:41:02.967: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename sched-preemption-path 06/28/23 07:41:02.968
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:41:02.981
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:41:02.986
[BeforeEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:496
STEP: Finding an available node 06/28/23 07:41:02.993
STEP: Trying to launch a pod without a label to get a node which can launch it. 06/28/23 07:41:02.993
Jun 28 07:41:03.002: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-6541" to be "running"
Jun 28 07:41:03.007: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 4.971685ms
Jun 28 07:41:05.013: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.011001399s
Jun 28 07:41:05.013: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 06/28/23 07:41:05.018
Jun 28 07:41:05.030: INFO: found a healthy node: ske-rhel-749f7d55c8xdd8b6-ct4cp
[It] runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:543
Jun 28 07:41:13.127: INFO: pods created so far: [1 1 1]
Jun 28 07:41:13.127: INFO: length of pods created so far: 3
Jun 28 07:41:15.140: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  test/e2e/framework/framework.go:187
Jun 28 07:41:22.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-6541" for this suite. 06/28/23 07:41:22.15
[AfterEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:470
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Jun 28 07:41:22.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-6848" for this suite. 06/28/23 07:41:22.203
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","completed":1,"skipped":1,"failed":0}
------------------------------
â€¢ [SLOW TEST] [79.418 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:458
    runs ReplicaSets to verify preemption running path [Conformance]
    test/e2e/scheduling/preemption.go:543

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 07:40:02.855
    Jun 28 07:40:02.855: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename sched-preemption 06/28/23 07:40:02.856
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:40:02.874
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:40:02.879
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Jun 28 07:40:02.906: INFO: Waiting up to 1m0s for all nodes to be ready
    Jun 28 07:41:02.962: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PreemptionExecutionPath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 07:41:02.967
    Jun 28 07:41:02.967: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename sched-preemption-path 06/28/23 07:41:02.968
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:41:02.981
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:41:02.986
    [BeforeEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:496
    STEP: Finding an available node 06/28/23 07:41:02.993
    STEP: Trying to launch a pod without a label to get a node which can launch it. 06/28/23 07:41:02.993
    Jun 28 07:41:03.002: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-6541" to be "running"
    Jun 28 07:41:03.007: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 4.971685ms
    Jun 28 07:41:05.013: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.011001399s
    Jun 28 07:41:05.013: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 06/28/23 07:41:05.018
    Jun 28 07:41:05.030: INFO: found a healthy node: ske-rhel-749f7d55c8xdd8b6-ct4cp
    [It] runs ReplicaSets to verify preemption running path [Conformance]
      test/e2e/scheduling/preemption.go:543
    Jun 28 07:41:13.127: INFO: pods created so far: [1 1 1]
    Jun 28 07:41:13.127: INFO: length of pods created so far: 3
    Jun 28 07:41:15.140: INFO: pods created so far: [2 2 1]
    [AfterEach] PreemptionExecutionPath
      test/e2e/framework/framework.go:187
    Jun 28 07:41:22.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-path-6541" for this suite. 06/28/23 07:41:22.15
    [AfterEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:470
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Jun 28 07:41:22.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-6848" for this suite. 06/28/23 07:41:22.203
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 07:41:22.276
Jun 28 07:41:22.276: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename watch 06/28/23 07:41:22.278
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:41:22.292
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:41:22.296
[It] should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
STEP: getting a starting resourceVersion 06/28/23 07:41:22.301
STEP: starting a background goroutine to produce watch events 06/28/23 07:41:22.305
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 06/28/23 07:41:22.306
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Jun 28 07:41:25.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3201" for this suite. 06/28/23 07:41:25.137
{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","completed":2,"skipped":67,"failed":0}
------------------------------
â€¢ [2.910 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 07:41:22.276
    Jun 28 07:41:22.276: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename watch 06/28/23 07:41:22.278
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:41:22.292
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:41:22.296
    [It] should receive events on concurrent watches in same order [Conformance]
      test/e2e/apimachinery/watch.go:334
    STEP: getting a starting resourceVersion 06/28/23 07:41:22.301
    STEP: starting a background goroutine to produce watch events 06/28/23 07:41:22.305
    STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 06/28/23 07:41:22.306
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Jun 28 07:41:25.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-3201" for this suite. 06/28/23 07:41:25.137
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 07:41:25.186
Jun 28 07:41:25.187: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename crd-publish-openapi 06/28/23 07:41:25.187
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:41:25.201
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:41:25.205
[It] works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 06/28/23 07:41:25.209
Jun 28 07:41:25.210: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
Jun 28 07:41:27.321: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jun 28 07:41:37.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7932" for this suite. 06/28/23 07:41:37.584
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","completed":3,"skipped":82,"failed":0}
------------------------------
â€¢ [SLOW TEST] [12.405 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 07:41:25.186
    Jun 28 07:41:25.187: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename crd-publish-openapi 06/28/23 07:41:25.187
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:41:25.201
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:41:25.205
    [It] works for multiple CRDs of different groups [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:275
    STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 06/28/23 07:41:25.209
    Jun 28 07:41:25.210: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    Jun 28 07:41:27.321: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jun 28 07:41:37.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-7932" for this suite. 06/28/23 07:41:37.584
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 07:41:37.593
Jun 28 07:41:37.593: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename projected 06/28/23 07:41:37.594
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:41:37.609
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:41:37.613
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220
STEP: Creating a pod to test downward API volume plugin 06/28/23 07:41:37.617
Jun 28 07:41:37.626: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ebfa74e4-57c5-4840-bacd-8aac3fe489c7" in namespace "projected-3471" to be "Succeeded or Failed"
Jun 28 07:41:37.631: INFO: Pod "downwardapi-volume-ebfa74e4-57c5-4840-bacd-8aac3fe489c7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.451619ms
Jun 28 07:41:39.637: INFO: Pod "downwardapi-volume-ebfa74e4-57c5-4840-bacd-8aac3fe489c7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010490977s
Jun 28 07:41:41.637: INFO: Pod "downwardapi-volume-ebfa74e4-57c5-4840-bacd-8aac3fe489c7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010599469s
STEP: Saw pod success 06/28/23 07:41:41.637
Jun 28 07:41:41.637: INFO: Pod "downwardapi-volume-ebfa74e4-57c5-4840-bacd-8aac3fe489c7" satisfied condition "Succeeded or Failed"
Jun 28 07:41:41.642: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod downwardapi-volume-ebfa74e4-57c5-4840-bacd-8aac3fe489c7 container client-container: <nil>
STEP: delete the pod 06/28/23 07:41:41.655
Jun 28 07:41:41.668: INFO: Waiting for pod downwardapi-volume-ebfa74e4-57c5-4840-bacd-8aac3fe489c7 to disappear
Jun 28 07:41:41.673: INFO: Pod downwardapi-volume-ebfa74e4-57c5-4840-bacd-8aac3fe489c7 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jun 28 07:41:41.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3471" for this suite. 06/28/23 07:41:41.681
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","completed":4,"skipped":100,"failed":0}
------------------------------
â€¢ [4.096 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 07:41:37.593
    Jun 28 07:41:37.593: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename projected 06/28/23 07:41:37.594
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:41:37.609
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:41:37.613
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:220
    STEP: Creating a pod to test downward API volume plugin 06/28/23 07:41:37.617
    Jun 28 07:41:37.626: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ebfa74e4-57c5-4840-bacd-8aac3fe489c7" in namespace "projected-3471" to be "Succeeded or Failed"
    Jun 28 07:41:37.631: INFO: Pod "downwardapi-volume-ebfa74e4-57c5-4840-bacd-8aac3fe489c7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.451619ms
    Jun 28 07:41:39.637: INFO: Pod "downwardapi-volume-ebfa74e4-57c5-4840-bacd-8aac3fe489c7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010490977s
    Jun 28 07:41:41.637: INFO: Pod "downwardapi-volume-ebfa74e4-57c5-4840-bacd-8aac3fe489c7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010599469s
    STEP: Saw pod success 06/28/23 07:41:41.637
    Jun 28 07:41:41.637: INFO: Pod "downwardapi-volume-ebfa74e4-57c5-4840-bacd-8aac3fe489c7" satisfied condition "Succeeded or Failed"
    Jun 28 07:41:41.642: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod downwardapi-volume-ebfa74e4-57c5-4840-bacd-8aac3fe489c7 container client-container: <nil>
    STEP: delete the pod 06/28/23 07:41:41.655
    Jun 28 07:41:41.668: INFO: Waiting for pod downwardapi-volume-ebfa74e4-57c5-4840-bacd-8aac3fe489c7 to disappear
    Jun 28 07:41:41.673: INFO: Pod downwardapi-volume-ebfa74e4-57c5-4840-bacd-8aac3fe489c7 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jun 28 07:41:41.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3471" for this suite. 06/28/23 07:41:41.681
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 07:41:41.689
Jun 28 07:41:41.690: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename dns 06/28/23 07:41:41.691
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:41:41.705
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:41:41.711
[It] should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
STEP: Creating a test externalName service 06/28/23 07:41:41.715
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7488.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-7488.svc.cluster.local; sleep 1; done
 06/28/23 07:41:41.721
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7488.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-7488.svc.cluster.local; sleep 1; done
 06/28/23 07:41:41.721
STEP: creating a pod to probe DNS 06/28/23 07:41:41.721
STEP: submitting the pod to kubernetes 06/28/23 07:41:41.721
Jun 28 07:41:41.730: INFO: Waiting up to 15m0s for pod "dns-test-b56eb71c-2ddf-451f-b3cf-f409ebb98ffa" in namespace "dns-7488" to be "running"
Jun 28 07:41:41.734: INFO: Pod "dns-test-b56eb71c-2ddf-451f-b3cf-f409ebb98ffa": Phase="Pending", Reason="", readiness=false. Elapsed: 3.960481ms
Jun 28 07:41:43.742: INFO: Pod "dns-test-b56eb71c-2ddf-451f-b3cf-f409ebb98ffa": Phase="Running", Reason="", readiness=true. Elapsed: 2.011026645s
Jun 28 07:41:43.742: INFO: Pod "dns-test-b56eb71c-2ddf-451f-b3cf-f409ebb98ffa" satisfied condition "running"
STEP: retrieving the pod 06/28/23 07:41:43.742
STEP: looking for the results for each expected name from probers 06/28/23 07:41:43.746
Jun 28 07:41:43.880: INFO: DNS probes using dns-test-b56eb71c-2ddf-451f-b3cf-f409ebb98ffa succeeded

STEP: deleting the pod 06/28/23 07:41:43.88
STEP: changing the externalName to bar.example.com 06/28/23 07:41:43.894
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7488.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-7488.svc.cluster.local; sleep 1; done
 06/28/23 07:41:43.906
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7488.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-7488.svc.cluster.local; sleep 1; done
 06/28/23 07:41:43.906
STEP: creating a second pod to probe DNS 06/28/23 07:41:43.906
STEP: submitting the pod to kubernetes 06/28/23 07:41:43.906
Jun 28 07:41:43.914: INFO: Waiting up to 15m0s for pod "dns-test-8eda39aa-4260-481e-92e2-e5651518d43d" in namespace "dns-7488" to be "running"
Jun 28 07:41:43.919: INFO: Pod "dns-test-8eda39aa-4260-481e-92e2-e5651518d43d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.096402ms
Jun 28 07:41:45.925: INFO: Pod "dns-test-8eda39aa-4260-481e-92e2-e5651518d43d": Phase="Running", Reason="", readiness=true. Elapsed: 2.010859032s
Jun 28 07:41:45.925: INFO: Pod "dns-test-8eda39aa-4260-481e-92e2-e5651518d43d" satisfied condition "running"
STEP: retrieving the pod 06/28/23 07:41:45.925
STEP: looking for the results for each expected name from probers 06/28/23 07:41:45.93
Jun 28 07:41:45.960: INFO: File wheezy_udp@dns-test-service-3.dns-7488.svc.cluster.local from pod  dns-7488/dns-test-8eda39aa-4260-481e-92e2-e5651518d43d contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun 28 07:41:45.968: INFO: File jessie_udp@dns-test-service-3.dns-7488.svc.cluster.local from pod  dns-7488/dns-test-8eda39aa-4260-481e-92e2-e5651518d43d contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun 28 07:41:45.968: INFO: Lookups using dns-7488/dns-test-8eda39aa-4260-481e-92e2-e5651518d43d failed for: [wheezy_udp@dns-test-service-3.dns-7488.svc.cluster.local jessie_udp@dns-test-service-3.dns-7488.svc.cluster.local]

Jun 28 07:41:50.979: INFO: File wheezy_udp@dns-test-service-3.dns-7488.svc.cluster.local from pod  dns-7488/dns-test-8eda39aa-4260-481e-92e2-e5651518d43d contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun 28 07:41:51.023: INFO: File jessie_udp@dns-test-service-3.dns-7488.svc.cluster.local from pod  dns-7488/dns-test-8eda39aa-4260-481e-92e2-e5651518d43d contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun 28 07:41:51.023: INFO: Lookups using dns-7488/dns-test-8eda39aa-4260-481e-92e2-e5651518d43d failed for: [wheezy_udp@dns-test-service-3.dns-7488.svc.cluster.local jessie_udp@dns-test-service-3.dns-7488.svc.cluster.local]

Jun 28 07:41:55.979: INFO: File wheezy_udp@dns-test-service-3.dns-7488.svc.cluster.local from pod  dns-7488/dns-test-8eda39aa-4260-481e-92e2-e5651518d43d contains '' instead of 'bar.example.com.'
Jun 28 07:41:56.023: INFO: File jessie_udp@dns-test-service-3.dns-7488.svc.cluster.local from pod  dns-7488/dns-test-8eda39aa-4260-481e-92e2-e5651518d43d contains '' instead of 'bar.example.com.'
Jun 28 07:41:56.023: INFO: Lookups using dns-7488/dns-test-8eda39aa-4260-481e-92e2-e5651518d43d failed for: [wheezy_udp@dns-test-service-3.dns-7488.svc.cluster.local jessie_udp@dns-test-service-3.dns-7488.svc.cluster.local]

Jun 28 07:42:00.982: INFO: File wheezy_udp@dns-test-service-3.dns-7488.svc.cluster.local from pod  dns-7488/dns-test-8eda39aa-4260-481e-92e2-e5651518d43d contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun 28 07:42:01.027: INFO: File jessie_udp@dns-test-service-3.dns-7488.svc.cluster.local from pod  dns-7488/dns-test-8eda39aa-4260-481e-92e2-e5651518d43d contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun 28 07:42:01.027: INFO: Lookups using dns-7488/dns-test-8eda39aa-4260-481e-92e2-e5651518d43d failed for: [wheezy_udp@dns-test-service-3.dns-7488.svc.cluster.local jessie_udp@dns-test-service-3.dns-7488.svc.cluster.local]

Jun 28 07:42:05.980: INFO: File wheezy_udp@dns-test-service-3.dns-7488.svc.cluster.local from pod  dns-7488/dns-test-8eda39aa-4260-481e-92e2-e5651518d43d contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun 28 07:42:06.024: INFO: File jessie_udp@dns-test-service-3.dns-7488.svc.cluster.local from pod  dns-7488/dns-test-8eda39aa-4260-481e-92e2-e5651518d43d contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun 28 07:42:06.024: INFO: Lookups using dns-7488/dns-test-8eda39aa-4260-481e-92e2-e5651518d43d failed for: [wheezy_udp@dns-test-service-3.dns-7488.svc.cluster.local jessie_udp@dns-test-service-3.dns-7488.svc.cluster.local]

Jun 28 07:42:10.976: INFO: File wheezy_udp@dns-test-service-3.dns-7488.svc.cluster.local from pod  dns-7488/dns-test-8eda39aa-4260-481e-92e2-e5651518d43d contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun 28 07:42:11.020: INFO: File jessie_udp@dns-test-service-3.dns-7488.svc.cluster.local from pod  dns-7488/dns-test-8eda39aa-4260-481e-92e2-e5651518d43d contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun 28 07:42:11.020: INFO: Lookups using dns-7488/dns-test-8eda39aa-4260-481e-92e2-e5651518d43d failed for: [wheezy_udp@dns-test-service-3.dns-7488.svc.cluster.local jessie_udp@dns-test-service-3.dns-7488.svc.cluster.local]

Jun 28 07:42:15.979: INFO: File wheezy_udp@dns-test-service-3.dns-7488.svc.cluster.local from pod  dns-7488/dns-test-8eda39aa-4260-481e-92e2-e5651518d43d contains '' instead of 'bar.example.com.'
Jun 28 07:42:15.990: INFO: Lookups using dns-7488/dns-test-8eda39aa-4260-481e-92e2-e5651518d43d failed for: [wheezy_udp@dns-test-service-3.dns-7488.svc.cluster.local]

Jun 28 07:42:21.023: INFO: DNS probes using dns-test-8eda39aa-4260-481e-92e2-e5651518d43d succeeded

STEP: deleting the pod 06/28/23 07:42:21.023
STEP: changing the service to type=ClusterIP 06/28/23 07:42:21.037
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7488.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-7488.svc.cluster.local; sleep 1; done
 06/28/23 07:42:21.056
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7488.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-7488.svc.cluster.local; sleep 1; done
 06/28/23 07:42:21.056
STEP: creating a third pod to probe DNS 06/28/23 07:42:21.056
STEP: submitting the pod to kubernetes 06/28/23 07:42:21.06
Jun 28 07:42:21.069: INFO: Waiting up to 15m0s for pod "dns-test-75521d78-8444-4725-b536-153c570bdaf2" in namespace "dns-7488" to be "running"
Jun 28 07:42:21.075: INFO: Pod "dns-test-75521d78-8444-4725-b536-153c570bdaf2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.432645ms
Jun 28 07:42:23.081: INFO: Pod "dns-test-75521d78-8444-4725-b536-153c570bdaf2": Phase="Running", Reason="", readiness=true. Elapsed: 2.011372912s
Jun 28 07:42:23.081: INFO: Pod "dns-test-75521d78-8444-4725-b536-153c570bdaf2" satisfied condition "running"
STEP: retrieving the pod 06/28/23 07:42:23.081
STEP: looking for the results for each expected name from probers 06/28/23 07:42:23.086
Jun 28 07:42:23.223: INFO: DNS probes using dns-test-75521d78-8444-4725-b536-153c570bdaf2 succeeded

STEP: deleting the pod 06/28/23 07:42:23.223
STEP: deleting the test externalName service 06/28/23 07:42:23.236
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jun 28 07:42:23.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7488" for this suite. 06/28/23 07:42:23.26
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","completed":5,"skipped":106,"failed":0}
------------------------------
â€¢ [SLOW TEST] [41.578 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 07:41:41.689
    Jun 28 07:41:41.690: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename dns 06/28/23 07:41:41.691
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:41:41.705
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:41:41.711
    [It] should provide DNS for ExternalName services [Conformance]
      test/e2e/network/dns.go:333
    STEP: Creating a test externalName service 06/28/23 07:41:41.715
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7488.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-7488.svc.cluster.local; sleep 1; done
     06/28/23 07:41:41.721
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7488.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-7488.svc.cluster.local; sleep 1; done
     06/28/23 07:41:41.721
    STEP: creating a pod to probe DNS 06/28/23 07:41:41.721
    STEP: submitting the pod to kubernetes 06/28/23 07:41:41.721
    Jun 28 07:41:41.730: INFO: Waiting up to 15m0s for pod "dns-test-b56eb71c-2ddf-451f-b3cf-f409ebb98ffa" in namespace "dns-7488" to be "running"
    Jun 28 07:41:41.734: INFO: Pod "dns-test-b56eb71c-2ddf-451f-b3cf-f409ebb98ffa": Phase="Pending", Reason="", readiness=false. Elapsed: 3.960481ms
    Jun 28 07:41:43.742: INFO: Pod "dns-test-b56eb71c-2ddf-451f-b3cf-f409ebb98ffa": Phase="Running", Reason="", readiness=true. Elapsed: 2.011026645s
    Jun 28 07:41:43.742: INFO: Pod "dns-test-b56eb71c-2ddf-451f-b3cf-f409ebb98ffa" satisfied condition "running"
    STEP: retrieving the pod 06/28/23 07:41:43.742
    STEP: looking for the results for each expected name from probers 06/28/23 07:41:43.746
    Jun 28 07:41:43.880: INFO: DNS probes using dns-test-b56eb71c-2ddf-451f-b3cf-f409ebb98ffa succeeded

    STEP: deleting the pod 06/28/23 07:41:43.88
    STEP: changing the externalName to bar.example.com 06/28/23 07:41:43.894
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7488.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-7488.svc.cluster.local; sleep 1; done
     06/28/23 07:41:43.906
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7488.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-7488.svc.cluster.local; sleep 1; done
     06/28/23 07:41:43.906
    STEP: creating a second pod to probe DNS 06/28/23 07:41:43.906
    STEP: submitting the pod to kubernetes 06/28/23 07:41:43.906
    Jun 28 07:41:43.914: INFO: Waiting up to 15m0s for pod "dns-test-8eda39aa-4260-481e-92e2-e5651518d43d" in namespace "dns-7488" to be "running"
    Jun 28 07:41:43.919: INFO: Pod "dns-test-8eda39aa-4260-481e-92e2-e5651518d43d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.096402ms
    Jun 28 07:41:45.925: INFO: Pod "dns-test-8eda39aa-4260-481e-92e2-e5651518d43d": Phase="Running", Reason="", readiness=true. Elapsed: 2.010859032s
    Jun 28 07:41:45.925: INFO: Pod "dns-test-8eda39aa-4260-481e-92e2-e5651518d43d" satisfied condition "running"
    STEP: retrieving the pod 06/28/23 07:41:45.925
    STEP: looking for the results for each expected name from probers 06/28/23 07:41:45.93
    Jun 28 07:41:45.960: INFO: File wheezy_udp@dns-test-service-3.dns-7488.svc.cluster.local from pod  dns-7488/dns-test-8eda39aa-4260-481e-92e2-e5651518d43d contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jun 28 07:41:45.968: INFO: File jessie_udp@dns-test-service-3.dns-7488.svc.cluster.local from pod  dns-7488/dns-test-8eda39aa-4260-481e-92e2-e5651518d43d contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jun 28 07:41:45.968: INFO: Lookups using dns-7488/dns-test-8eda39aa-4260-481e-92e2-e5651518d43d failed for: [wheezy_udp@dns-test-service-3.dns-7488.svc.cluster.local jessie_udp@dns-test-service-3.dns-7488.svc.cluster.local]

    Jun 28 07:41:50.979: INFO: File wheezy_udp@dns-test-service-3.dns-7488.svc.cluster.local from pod  dns-7488/dns-test-8eda39aa-4260-481e-92e2-e5651518d43d contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jun 28 07:41:51.023: INFO: File jessie_udp@dns-test-service-3.dns-7488.svc.cluster.local from pod  dns-7488/dns-test-8eda39aa-4260-481e-92e2-e5651518d43d contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jun 28 07:41:51.023: INFO: Lookups using dns-7488/dns-test-8eda39aa-4260-481e-92e2-e5651518d43d failed for: [wheezy_udp@dns-test-service-3.dns-7488.svc.cluster.local jessie_udp@dns-test-service-3.dns-7488.svc.cluster.local]

    Jun 28 07:41:55.979: INFO: File wheezy_udp@dns-test-service-3.dns-7488.svc.cluster.local from pod  dns-7488/dns-test-8eda39aa-4260-481e-92e2-e5651518d43d contains '' instead of 'bar.example.com.'
    Jun 28 07:41:56.023: INFO: File jessie_udp@dns-test-service-3.dns-7488.svc.cluster.local from pod  dns-7488/dns-test-8eda39aa-4260-481e-92e2-e5651518d43d contains '' instead of 'bar.example.com.'
    Jun 28 07:41:56.023: INFO: Lookups using dns-7488/dns-test-8eda39aa-4260-481e-92e2-e5651518d43d failed for: [wheezy_udp@dns-test-service-3.dns-7488.svc.cluster.local jessie_udp@dns-test-service-3.dns-7488.svc.cluster.local]

    Jun 28 07:42:00.982: INFO: File wheezy_udp@dns-test-service-3.dns-7488.svc.cluster.local from pod  dns-7488/dns-test-8eda39aa-4260-481e-92e2-e5651518d43d contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jun 28 07:42:01.027: INFO: File jessie_udp@dns-test-service-3.dns-7488.svc.cluster.local from pod  dns-7488/dns-test-8eda39aa-4260-481e-92e2-e5651518d43d contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jun 28 07:42:01.027: INFO: Lookups using dns-7488/dns-test-8eda39aa-4260-481e-92e2-e5651518d43d failed for: [wheezy_udp@dns-test-service-3.dns-7488.svc.cluster.local jessie_udp@dns-test-service-3.dns-7488.svc.cluster.local]

    Jun 28 07:42:05.980: INFO: File wheezy_udp@dns-test-service-3.dns-7488.svc.cluster.local from pod  dns-7488/dns-test-8eda39aa-4260-481e-92e2-e5651518d43d contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jun 28 07:42:06.024: INFO: File jessie_udp@dns-test-service-3.dns-7488.svc.cluster.local from pod  dns-7488/dns-test-8eda39aa-4260-481e-92e2-e5651518d43d contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jun 28 07:42:06.024: INFO: Lookups using dns-7488/dns-test-8eda39aa-4260-481e-92e2-e5651518d43d failed for: [wheezy_udp@dns-test-service-3.dns-7488.svc.cluster.local jessie_udp@dns-test-service-3.dns-7488.svc.cluster.local]

    Jun 28 07:42:10.976: INFO: File wheezy_udp@dns-test-service-3.dns-7488.svc.cluster.local from pod  dns-7488/dns-test-8eda39aa-4260-481e-92e2-e5651518d43d contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jun 28 07:42:11.020: INFO: File jessie_udp@dns-test-service-3.dns-7488.svc.cluster.local from pod  dns-7488/dns-test-8eda39aa-4260-481e-92e2-e5651518d43d contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jun 28 07:42:11.020: INFO: Lookups using dns-7488/dns-test-8eda39aa-4260-481e-92e2-e5651518d43d failed for: [wheezy_udp@dns-test-service-3.dns-7488.svc.cluster.local jessie_udp@dns-test-service-3.dns-7488.svc.cluster.local]

    Jun 28 07:42:15.979: INFO: File wheezy_udp@dns-test-service-3.dns-7488.svc.cluster.local from pod  dns-7488/dns-test-8eda39aa-4260-481e-92e2-e5651518d43d contains '' instead of 'bar.example.com.'
    Jun 28 07:42:15.990: INFO: Lookups using dns-7488/dns-test-8eda39aa-4260-481e-92e2-e5651518d43d failed for: [wheezy_udp@dns-test-service-3.dns-7488.svc.cluster.local]

    Jun 28 07:42:21.023: INFO: DNS probes using dns-test-8eda39aa-4260-481e-92e2-e5651518d43d succeeded

    STEP: deleting the pod 06/28/23 07:42:21.023
    STEP: changing the service to type=ClusterIP 06/28/23 07:42:21.037
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7488.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-7488.svc.cluster.local; sleep 1; done
     06/28/23 07:42:21.056
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7488.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-7488.svc.cluster.local; sleep 1; done
     06/28/23 07:42:21.056
    STEP: creating a third pod to probe DNS 06/28/23 07:42:21.056
    STEP: submitting the pod to kubernetes 06/28/23 07:42:21.06
    Jun 28 07:42:21.069: INFO: Waiting up to 15m0s for pod "dns-test-75521d78-8444-4725-b536-153c570bdaf2" in namespace "dns-7488" to be "running"
    Jun 28 07:42:21.075: INFO: Pod "dns-test-75521d78-8444-4725-b536-153c570bdaf2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.432645ms
    Jun 28 07:42:23.081: INFO: Pod "dns-test-75521d78-8444-4725-b536-153c570bdaf2": Phase="Running", Reason="", readiness=true. Elapsed: 2.011372912s
    Jun 28 07:42:23.081: INFO: Pod "dns-test-75521d78-8444-4725-b536-153c570bdaf2" satisfied condition "running"
    STEP: retrieving the pod 06/28/23 07:42:23.081
    STEP: looking for the results for each expected name from probers 06/28/23 07:42:23.086
    Jun 28 07:42:23.223: INFO: DNS probes using dns-test-75521d78-8444-4725-b536-153c570bdaf2 succeeded

    STEP: deleting the pod 06/28/23 07:42:23.223
    STEP: deleting the test externalName service 06/28/23 07:42:23.236
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jun 28 07:42:23.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-7488" for this suite. 06/28/23 07:42:23.26
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Probing container
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 07:42:23.268
Jun 28 07:42:23.268: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename container-probe 06/28/23 07:42:23.269
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:42:23.286
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:42:23.292
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131
STEP: Creating pod busybox-fc9227ff-fed3-4850-bba4-110e7ecd8b85 in namespace container-probe-9228 06/28/23 07:42:23.298
Jun 28 07:42:23.310: INFO: Waiting up to 5m0s for pod "busybox-fc9227ff-fed3-4850-bba4-110e7ecd8b85" in namespace "container-probe-9228" to be "not pending"
Jun 28 07:42:23.315: INFO: Pod "busybox-fc9227ff-fed3-4850-bba4-110e7ecd8b85": Phase="Pending", Reason="", readiness=false. Elapsed: 5.658315ms
Jun 28 07:42:25.323: INFO: Pod "busybox-fc9227ff-fed3-4850-bba4-110e7ecd8b85": Phase="Running", Reason="", readiness=true. Elapsed: 2.013135693s
Jun 28 07:42:25.323: INFO: Pod "busybox-fc9227ff-fed3-4850-bba4-110e7ecd8b85" satisfied condition "not pending"
Jun 28 07:42:25.323: INFO: Started pod busybox-fc9227ff-fed3-4850-bba4-110e7ecd8b85 in namespace container-probe-9228
STEP: checking the pod's current state and verifying that restartCount is present 06/28/23 07:42:25.323
Jun 28 07:42:25.328: INFO: Initial restart count of pod busybox-fc9227ff-fed3-4850-bba4-110e7ecd8b85 is 0
Jun 28 07:43:15.495: INFO: Restart count of pod container-probe-9228/busybox-fc9227ff-fed3-4850-bba4-110e7ecd8b85 is now 1 (50.166363904s elapsed)
STEP: deleting the pod 06/28/23 07:43:15.495
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jun 28 07:43:15.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9228" for this suite. 06/28/23 07:43:15.516
{"msg":"PASSED [sig-node] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","completed":6,"skipped":107,"failed":0}
------------------------------
â€¢ [SLOW TEST] [52.255 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 07:42:23.268
    Jun 28 07:42:23.268: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename container-probe 06/28/23 07:42:23.269
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:42:23.286
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:42:23.292
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:131
    STEP: Creating pod busybox-fc9227ff-fed3-4850-bba4-110e7ecd8b85 in namespace container-probe-9228 06/28/23 07:42:23.298
    Jun 28 07:42:23.310: INFO: Waiting up to 5m0s for pod "busybox-fc9227ff-fed3-4850-bba4-110e7ecd8b85" in namespace "container-probe-9228" to be "not pending"
    Jun 28 07:42:23.315: INFO: Pod "busybox-fc9227ff-fed3-4850-bba4-110e7ecd8b85": Phase="Pending", Reason="", readiness=false. Elapsed: 5.658315ms
    Jun 28 07:42:25.323: INFO: Pod "busybox-fc9227ff-fed3-4850-bba4-110e7ecd8b85": Phase="Running", Reason="", readiness=true. Elapsed: 2.013135693s
    Jun 28 07:42:25.323: INFO: Pod "busybox-fc9227ff-fed3-4850-bba4-110e7ecd8b85" satisfied condition "not pending"
    Jun 28 07:42:25.323: INFO: Started pod busybox-fc9227ff-fed3-4850-bba4-110e7ecd8b85 in namespace container-probe-9228
    STEP: checking the pod's current state and verifying that restartCount is present 06/28/23 07:42:25.323
    Jun 28 07:42:25.328: INFO: Initial restart count of pod busybox-fc9227ff-fed3-4850-bba4-110e7ecd8b85 is 0
    Jun 28 07:43:15.495: INFO: Restart count of pod container-probe-9228/busybox-fc9227ff-fed3-4850-bba4-110e7ecd8b85 is now 1 (50.166363904s elapsed)
    STEP: deleting the pod 06/28/23 07:43:15.495
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jun 28 07:43:15.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-9228" for this suite. 06/28/23 07:43:15.516
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 07:43:15.523
Jun 28 07:43:15.524: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename emptydir 06/28/23 07:43:15.524
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:43:15.538
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:43:15.543
[It] pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226
STEP: Creating Pod 06/28/23 07:43:15.547
Jun 28 07:43:15.557: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-b07e2ec4-507d-4c97-80d8-072c75c16a42" in namespace "emptydir-5465" to be "running"
Jun 28 07:43:15.561: INFO: Pod "pod-sharedvolume-b07e2ec4-507d-4c97-80d8-072c75c16a42": Phase="Pending", Reason="", readiness=false. Elapsed: 4.675301ms
Jun 28 07:43:17.567: INFO: Pod "pod-sharedvolume-b07e2ec4-507d-4c97-80d8-072c75c16a42": Phase="Running", Reason="", readiness=false. Elapsed: 2.010057907s
Jun 28 07:43:17.567: INFO: Pod "pod-sharedvolume-b07e2ec4-507d-4c97-80d8-072c75c16a42" satisfied condition "running"
STEP: Reading file content from the nginx-container 06/28/23 07:43:17.567
Jun 28 07:43:17.567: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-5465 PodName:pod-sharedvolume-b07e2ec4-507d-4c97-80d8-072c75c16a42 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 28 07:43:17.567: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
Jun 28 07:43:17.568: INFO: ExecWithOptions: Clientset creation
Jun 28 07:43:17.569: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/emptydir-5465/pods/pod-sharedvolume-b07e2ec4-507d-4c97-80d8-072c75c16a42/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
Jun 28 07:43:17.931: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jun 28 07:43:17.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5465" for this suite. 06/28/23 07:43:17.943
{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","completed":7,"skipped":121,"failed":0}
------------------------------
â€¢ [2.427 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 07:43:15.523
    Jun 28 07:43:15.524: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename emptydir 06/28/23 07:43:15.524
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:43:15.538
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:43:15.543
    [It] pod should support shared volumes between containers [Conformance]
      test/e2e/common/storage/empty_dir.go:226
    STEP: Creating Pod 06/28/23 07:43:15.547
    Jun 28 07:43:15.557: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-b07e2ec4-507d-4c97-80d8-072c75c16a42" in namespace "emptydir-5465" to be "running"
    Jun 28 07:43:15.561: INFO: Pod "pod-sharedvolume-b07e2ec4-507d-4c97-80d8-072c75c16a42": Phase="Pending", Reason="", readiness=false. Elapsed: 4.675301ms
    Jun 28 07:43:17.567: INFO: Pod "pod-sharedvolume-b07e2ec4-507d-4c97-80d8-072c75c16a42": Phase="Running", Reason="", readiness=false. Elapsed: 2.010057907s
    Jun 28 07:43:17.567: INFO: Pod "pod-sharedvolume-b07e2ec4-507d-4c97-80d8-072c75c16a42" satisfied condition "running"
    STEP: Reading file content from the nginx-container 06/28/23 07:43:17.567
    Jun 28 07:43:17.567: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-5465 PodName:pod-sharedvolume-b07e2ec4-507d-4c97-80d8-072c75c16a42 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 28 07:43:17.567: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    Jun 28 07:43:17.568: INFO: ExecWithOptions: Clientset creation
    Jun 28 07:43:17.569: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/emptydir-5465/pods/pod-sharedvolume-b07e2ec4-507d-4c97-80d8-072c75c16a42/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
    Jun 28 07:43:17.931: INFO: Exec stderr: ""
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jun 28 07:43:17.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-5465" for this suite. 06/28/23 07:43:17.943
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 07:43:17.952
Jun 28 07:43:17.952: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename custom-resource-definition 06/28/23 07:43:17.952
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:43:17.968
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:43:17.972
[It] custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
Jun 28 07:43:17.977: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jun 28 07:43:21.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-5440" for this suite. 06/28/23 07:43:21.124
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","completed":8,"skipped":128,"failed":0}
------------------------------
â€¢ [3.180 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 07:43:17.952
    Jun 28 07:43:17.952: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename custom-resource-definition 06/28/23 07:43:17.952
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:43:17.968
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:43:17.972
    [It] custom resource defaulting for requests and from storage works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:269
    Jun 28 07:43:17.977: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jun 28 07:43:21.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-5440" for this suite. 06/28/23 07:43:21.124
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 07:43:21.133
Jun 28 07:43:21.133: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename var-expansion 06/28/23 07:43:21.134
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:43:21.15
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:43:21.154
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296
STEP: creating the pod 06/28/23 07:43:21.159
STEP: waiting for pod running 06/28/23 07:43:21.169
Jun 28 07:43:21.169: INFO: Waiting up to 2m0s for pod "var-expansion-6c42f19b-49f7-4188-a9fb-6140aef247b7" in namespace "var-expansion-8695" to be "running"
Jun 28 07:43:21.174: INFO: Pod "var-expansion-6c42f19b-49f7-4188-a9fb-6140aef247b7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.851646ms
Jun 28 07:43:23.179: INFO: Pod "var-expansion-6c42f19b-49f7-4188-a9fb-6140aef247b7": Phase="Running", Reason="", readiness=true. Elapsed: 2.010074626s
Jun 28 07:43:23.179: INFO: Pod "var-expansion-6c42f19b-49f7-4188-a9fb-6140aef247b7" satisfied condition "running"
STEP: creating a file in subpath 06/28/23 07:43:23.179
Jun 28 07:43:23.183: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-8695 PodName:var-expansion-6c42f19b-49f7-4188-a9fb-6140aef247b7 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 28 07:43:23.183: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
Jun 28 07:43:23.183: INFO: ExecWithOptions: Clientset creation
Jun 28 07:43:23.184: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/var-expansion-8695/pods/var-expansion-6c42f19b-49f7-4188-a9fb-6140aef247b7/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: test for file in mounted path 06/28/23 07:43:23.529
Jun 28 07:43:23.534: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-8695 PodName:var-expansion-6c42f19b-49f7-4188-a9fb-6140aef247b7 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 28 07:43:23.534: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
Jun 28 07:43:23.534: INFO: ExecWithOptions: Clientset creation
Jun 28 07:43:23.534: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/var-expansion-8695/pods/var-expansion-6c42f19b-49f7-4188-a9fb-6140aef247b7/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: updating the annotation value 06/28/23 07:43:23.964
Jun 28 07:43:24.478: INFO: Successfully updated pod "var-expansion-6c42f19b-49f7-4188-a9fb-6140aef247b7"
STEP: waiting for annotated pod running 06/28/23 07:43:24.478
Jun 28 07:43:24.478: INFO: Waiting up to 2m0s for pod "var-expansion-6c42f19b-49f7-4188-a9fb-6140aef247b7" in namespace "var-expansion-8695" to be "running"
Jun 28 07:43:24.483: INFO: Pod "var-expansion-6c42f19b-49f7-4188-a9fb-6140aef247b7": Phase="Running", Reason="", readiness=true. Elapsed: 4.670108ms
Jun 28 07:43:24.483: INFO: Pod "var-expansion-6c42f19b-49f7-4188-a9fb-6140aef247b7" satisfied condition "running"
STEP: deleting the pod gracefully 06/28/23 07:43:24.483
Jun 28 07:43:24.483: INFO: Deleting pod "var-expansion-6c42f19b-49f7-4188-a9fb-6140aef247b7" in namespace "var-expansion-8695"
Jun 28 07:43:24.491: INFO: Wait up to 5m0s for pod "var-expansion-6c42f19b-49f7-4188-a9fb-6140aef247b7" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jun 28 07:43:58.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8695" for this suite. 06/28/23 07:43:58.512
{"msg":"PASSED [sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]","completed":9,"skipped":179,"failed":0}
------------------------------
â€¢ [SLOW TEST] [37.387 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 07:43:21.133
    Jun 28 07:43:21.133: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename var-expansion 06/28/23 07:43:21.134
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:43:21.15
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:43:21.154
    [It] should succeed in writing subpaths in container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:296
    STEP: creating the pod 06/28/23 07:43:21.159
    STEP: waiting for pod running 06/28/23 07:43:21.169
    Jun 28 07:43:21.169: INFO: Waiting up to 2m0s for pod "var-expansion-6c42f19b-49f7-4188-a9fb-6140aef247b7" in namespace "var-expansion-8695" to be "running"
    Jun 28 07:43:21.174: INFO: Pod "var-expansion-6c42f19b-49f7-4188-a9fb-6140aef247b7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.851646ms
    Jun 28 07:43:23.179: INFO: Pod "var-expansion-6c42f19b-49f7-4188-a9fb-6140aef247b7": Phase="Running", Reason="", readiness=true. Elapsed: 2.010074626s
    Jun 28 07:43:23.179: INFO: Pod "var-expansion-6c42f19b-49f7-4188-a9fb-6140aef247b7" satisfied condition "running"
    STEP: creating a file in subpath 06/28/23 07:43:23.179
    Jun 28 07:43:23.183: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-8695 PodName:var-expansion-6c42f19b-49f7-4188-a9fb-6140aef247b7 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 28 07:43:23.183: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    Jun 28 07:43:23.183: INFO: ExecWithOptions: Clientset creation
    Jun 28 07:43:23.184: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/var-expansion-8695/pods/var-expansion-6c42f19b-49f7-4188-a9fb-6140aef247b7/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: test for file in mounted path 06/28/23 07:43:23.529
    Jun 28 07:43:23.534: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-8695 PodName:var-expansion-6c42f19b-49f7-4188-a9fb-6140aef247b7 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 28 07:43:23.534: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    Jun 28 07:43:23.534: INFO: ExecWithOptions: Clientset creation
    Jun 28 07:43:23.534: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/var-expansion-8695/pods/var-expansion-6c42f19b-49f7-4188-a9fb-6140aef247b7/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: updating the annotation value 06/28/23 07:43:23.964
    Jun 28 07:43:24.478: INFO: Successfully updated pod "var-expansion-6c42f19b-49f7-4188-a9fb-6140aef247b7"
    STEP: waiting for annotated pod running 06/28/23 07:43:24.478
    Jun 28 07:43:24.478: INFO: Waiting up to 2m0s for pod "var-expansion-6c42f19b-49f7-4188-a9fb-6140aef247b7" in namespace "var-expansion-8695" to be "running"
    Jun 28 07:43:24.483: INFO: Pod "var-expansion-6c42f19b-49f7-4188-a9fb-6140aef247b7": Phase="Running", Reason="", readiness=true. Elapsed: 4.670108ms
    Jun 28 07:43:24.483: INFO: Pod "var-expansion-6c42f19b-49f7-4188-a9fb-6140aef247b7" satisfied condition "running"
    STEP: deleting the pod gracefully 06/28/23 07:43:24.483
    Jun 28 07:43:24.483: INFO: Deleting pod "var-expansion-6c42f19b-49f7-4188-a9fb-6140aef247b7" in namespace "var-expansion-8695"
    Jun 28 07:43:24.491: INFO: Wait up to 5m0s for pod "var-expansion-6c42f19b-49f7-4188-a9fb-6140aef247b7" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jun 28 07:43:58.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-8695" for this suite. 06/28/23 07:43:58.512
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 07:43:58.522
Jun 28 07:43:58.522: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename gc 06/28/23 07:43:58.523
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:43:58.538
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:43:58.545
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
STEP: create the deployment 06/28/23 07:43:58.553
STEP: Wait for the Deployment to create new ReplicaSet 06/28/23 07:43:58.562
STEP: delete the deployment 06/28/23 07:43:59.079
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 06/28/23 07:43:59.085
STEP: Gathering metrics 06/28/23 07:43:59.614
W0628 07:43:59.626912      18 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jun 28 07:43:59.626: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Jun 28 07:43:59.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8955" for this suite. 06/28/23 07:43:59.635
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","completed":10,"skipped":232,"failed":0}
------------------------------
â€¢ [1.121 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 07:43:58.522
    Jun 28 07:43:58.522: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename gc 06/28/23 07:43:58.523
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:43:58.538
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:43:58.545
    [It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
      test/e2e/apimachinery/garbage_collector.go:550
    STEP: create the deployment 06/28/23 07:43:58.553
    STEP: Wait for the Deployment to create new ReplicaSet 06/28/23 07:43:58.562
    STEP: delete the deployment 06/28/23 07:43:59.079
    STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 06/28/23 07:43:59.085
    STEP: Gathering metrics 06/28/23 07:43:59.614
    W0628 07:43:59.626912      18 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Jun 28 07:43:59.626: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Jun 28 07:43:59.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-8955" for this suite. 06/28/23 07:43:59.635
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 07:43:59.643
Jun 28 07:43:59.643: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename replicaset 06/28/23 07:43:59.644
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:43:59.658
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:43:59.663
[It] should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
STEP: Create a ReplicaSet 06/28/23 07:43:59.668
STEP: Verify that the required pods have come up 06/28/23 07:43:59.675
Jun 28 07:43:59.679: INFO: Pod name sample-pod: Found 0 pods out of 3
Jun 28 07:44:04.686: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running 06/28/23 07:44:04.686
Jun 28 07:44:04.691: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets 06/28/23 07:44:04.691
STEP: DeleteCollection of the ReplicaSets 06/28/23 07:44:04.696
STEP: After DeleteCollection verify that ReplicaSets have been deleted 06/28/23 07:44:04.706
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Jun 28 07:44:04.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-8009" for this suite. 06/28/23 07:44:04.736
{"msg":"PASSED [sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]","completed":11,"skipped":243,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.100 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 07:43:59.643
    Jun 28 07:43:59.643: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename replicaset 06/28/23 07:43:59.644
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:43:59.658
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:43:59.663
    [It] should list and delete a collection of ReplicaSets [Conformance]
      test/e2e/apps/replica_set.go:165
    STEP: Create a ReplicaSet 06/28/23 07:43:59.668
    STEP: Verify that the required pods have come up 06/28/23 07:43:59.675
    Jun 28 07:43:59.679: INFO: Pod name sample-pod: Found 0 pods out of 3
    Jun 28 07:44:04.686: INFO: Pod name sample-pod: Found 3 pods out of 3
    STEP: ensuring each pod is running 06/28/23 07:44:04.686
    Jun 28 07:44:04.691: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
    STEP: Listing all ReplicaSets 06/28/23 07:44:04.691
    STEP: DeleteCollection of the ReplicaSets 06/28/23 07:44:04.696
    STEP: After DeleteCollection verify that ReplicaSets have been deleted 06/28/23 07:44:04.706
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Jun 28 07:44:04.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-8009" for this suite. 06/28/23 07:44:04.736
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin]
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 07:44:04.743
Jun 28 07:44:04.743: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename certificates 06/28/23 07:44:04.744
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:44:04.757
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:44:04.778
[It] should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
STEP: getting /apis 06/28/23 07:44:05.336
STEP: getting /apis/certificates.k8s.io 06/28/23 07:44:05.342
STEP: getting /apis/certificates.k8s.io/v1 06/28/23 07:44:05.344
STEP: creating 06/28/23 07:44:05.346
STEP: getting 06/28/23 07:44:05.365
STEP: listing 06/28/23 07:44:05.37
STEP: watching 06/28/23 07:44:05.375
Jun 28 07:44:05.375: INFO: starting watch
STEP: patching 06/28/23 07:44:05.377
STEP: updating 06/28/23 07:44:05.384
Jun 28 07:44:05.391: INFO: waiting for watch events with expected annotations
Jun 28 07:44:05.391: INFO: saw patched and updated annotations
STEP: getting /approval 06/28/23 07:44:05.391
STEP: patching /approval 06/28/23 07:44:05.396
STEP: updating /approval 06/28/23 07:44:05.404
STEP: getting /status 06/28/23 07:44:05.411
STEP: patching /status 06/28/23 07:44:05.416
STEP: updating /status 06/28/23 07:44:05.425
STEP: deleting 06/28/23 07:44:05.434
STEP: deleting a collection 06/28/23 07:44:05.452
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jun 28 07:44:05.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-330" for this suite. 06/28/23 07:44:05.481
{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","completed":12,"skipped":251,"failed":0}
------------------------------
â€¢ [0.746 seconds]
[sig-auth] Certificates API [Privileged:ClusterAdmin]
test/e2e/auth/framework.go:23
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 07:44:04.743
    Jun 28 07:44:04.743: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename certificates 06/28/23 07:44:04.744
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:44:04.757
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:44:04.778
    [It] should support CSR API operations [Conformance]
      test/e2e/auth/certificates.go:200
    STEP: getting /apis 06/28/23 07:44:05.336
    STEP: getting /apis/certificates.k8s.io 06/28/23 07:44:05.342
    STEP: getting /apis/certificates.k8s.io/v1 06/28/23 07:44:05.344
    STEP: creating 06/28/23 07:44:05.346
    STEP: getting 06/28/23 07:44:05.365
    STEP: listing 06/28/23 07:44:05.37
    STEP: watching 06/28/23 07:44:05.375
    Jun 28 07:44:05.375: INFO: starting watch
    STEP: patching 06/28/23 07:44:05.377
    STEP: updating 06/28/23 07:44:05.384
    Jun 28 07:44:05.391: INFO: waiting for watch events with expected annotations
    Jun 28 07:44:05.391: INFO: saw patched and updated annotations
    STEP: getting /approval 06/28/23 07:44:05.391
    STEP: patching /approval 06/28/23 07:44:05.396
    STEP: updating /approval 06/28/23 07:44:05.404
    STEP: getting /status 06/28/23 07:44:05.411
    STEP: patching /status 06/28/23 07:44:05.416
    STEP: updating /status 06/28/23 07:44:05.425
    STEP: deleting 06/28/23 07:44:05.434
    STEP: deleting a collection 06/28/23 07:44:05.452
    [AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jun 28 07:44:05.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "certificates-330" for this suite. 06/28/23 07:44:05.481
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:846
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 07:44:05.489
Jun 28 07:44:05.489: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename statefulset 06/28/23 07:44:05.49
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:44:05.504
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:44:05.508
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-1781 06/28/23 07:44:05.513
[It] should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:846
STEP: Creating statefulset ss in namespace statefulset-1781 06/28/23 07:44:05.52
Jun 28 07:44:05.531: INFO: Found 0 stateful pods, waiting for 1
Jun 28 07:44:15.537: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource 06/28/23 07:44:15.547
STEP: updating a scale subresource 06/28/23 07:44:15.552
STEP: verifying the statefulset Spec.Replicas was modified 06/28/23 07:44:15.562
STEP: Patch a scale subresource 06/28/23 07:44:15.567
STEP: verifying the statefulset Spec.Replicas was modified 06/28/23 07:44:15.582
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jun 28 07:44:15.587: INFO: Deleting all statefulset in ns statefulset-1781
Jun 28 07:44:15.591: INFO: Scaling statefulset ss to 0
Jun 28 07:44:25.617: INFO: Waiting for statefulset status.replicas updated to 0
Jun 28 07:44:25.622: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jun 28 07:44:25.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1781" for this suite. 06/28/23 07:44:25.646
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","completed":13,"skipped":254,"failed":0}
------------------------------
â€¢ [SLOW TEST] [20.164 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should have a working scale subresource [Conformance]
    test/e2e/apps/statefulset.go:846

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 07:44:05.489
    Jun 28 07:44:05.489: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename statefulset 06/28/23 07:44:05.49
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:44:05.504
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:44:05.508
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-1781 06/28/23 07:44:05.513
    [It] should have a working scale subresource [Conformance]
      test/e2e/apps/statefulset.go:846
    STEP: Creating statefulset ss in namespace statefulset-1781 06/28/23 07:44:05.52
    Jun 28 07:44:05.531: INFO: Found 0 stateful pods, waiting for 1
    Jun 28 07:44:15.537: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: getting scale subresource 06/28/23 07:44:15.547
    STEP: updating a scale subresource 06/28/23 07:44:15.552
    STEP: verifying the statefulset Spec.Replicas was modified 06/28/23 07:44:15.562
    STEP: Patch a scale subresource 06/28/23 07:44:15.567
    STEP: verifying the statefulset Spec.Replicas was modified 06/28/23 07:44:15.582
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jun 28 07:44:15.587: INFO: Deleting all statefulset in ns statefulset-1781
    Jun 28 07:44:15.591: INFO: Scaling statefulset ss to 0
    Jun 28 07:44:25.617: INFO: Waiting for statefulset status.replicas updated to 0
    Jun 28 07:44:25.622: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jun 28 07:44:25.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-1781" for this suite. 06/28/23 07:44:25.646
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 07:44:25.654
Jun 28 07:44:25.654: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename aggregator 06/28/23 07:44:25.655
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:44:25.668
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:44:25.675
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:78
Jun 28 07:44:25.680: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
STEP: Registering the sample API server. 06/28/23 07:44:25.681
Jun 28 07:44:26.084: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Jun 28 07:44:28.138: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 28, 7, 44, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 28, 7, 44, 26, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 28, 7, 44, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 28, 7, 44, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 28 07:44:30.145: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 28, 7, 44, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 28, 7, 44, 26, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 28, 7, 44, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 28, 7, 44, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 28 07:44:32.145: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 28, 7, 44, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 28, 7, 44, 26, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 28, 7, 44, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 28, 7, 44, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 28 07:44:34.173: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 28, 7, 44, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 28, 7, 44, 26, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 28, 7, 44, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 28, 7, 44, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 28 07:44:36.144: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 28, 7, 44, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 28, 7, 44, 26, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 28, 7, 44, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 28, 7, 44, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 28 07:44:38.145: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 28, 7, 44, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 28, 7, 44, 26, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 28, 7, 44, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 28, 7, 44, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 28 07:44:40.145: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 28, 7, 44, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 28, 7, 44, 26, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 28, 7, 44, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 28, 7, 44, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 28 07:44:42.144: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 28, 7, 44, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 28, 7, 44, 26, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 28, 7, 44, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 28, 7, 44, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 28 07:44:44.144: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 28, 7, 44, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 28, 7, 44, 26, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 28, 7, 44, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 28, 7, 44, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 28 07:44:46.164: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 28, 7, 44, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 28, 7, 44, 26, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 28, 7, 44, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 28, 7, 44, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 28 07:44:48.184: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 28, 7, 44, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 28, 7, 44, 26, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 28, 7, 44, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 28, 7, 44, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 28 07:44:50.145: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 28, 7, 44, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 28, 7, 44, 26, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 28, 7, 44, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 28, 7, 44, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 28 07:44:52.451: INFO: Waited 292.449435ms for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com 06/28/23 07:44:53.057
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 06/28/23 07:44:53.062
STEP: List APIServices 06/28/23 07:44:53.071
Jun 28 07:44:53.093: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/framework/framework.go:187
Jun 28 07:44:53.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-4042" for this suite. 06/28/23 07:44:53.462
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","completed":14,"skipped":281,"failed":0}
------------------------------
â€¢ [SLOW TEST] [27.817 seconds]
[sig-api-machinery] Aggregator
test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 07:44:25.654
    Jun 28 07:44:25.654: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename aggregator 06/28/23 07:44:25.655
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:44:25.668
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:44:25.675
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:78
    Jun 28 07:44:25.680: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    [It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
      test/e2e/apimachinery/aggregator.go:100
    STEP: Registering the sample API server. 06/28/23 07:44:25.681
    Jun 28 07:44:26.084: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
    Jun 28 07:44:28.138: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 28, 7, 44, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 28, 7, 44, 26, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 28, 7, 44, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 28, 7, 44, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun 28 07:44:30.145: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 28, 7, 44, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 28, 7, 44, 26, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 28, 7, 44, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 28, 7, 44, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun 28 07:44:32.145: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 28, 7, 44, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 28, 7, 44, 26, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 28, 7, 44, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 28, 7, 44, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun 28 07:44:34.173: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 28, 7, 44, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 28, 7, 44, 26, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 28, 7, 44, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 28, 7, 44, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun 28 07:44:36.144: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 28, 7, 44, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 28, 7, 44, 26, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 28, 7, 44, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 28, 7, 44, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun 28 07:44:38.145: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 28, 7, 44, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 28, 7, 44, 26, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 28, 7, 44, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 28, 7, 44, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun 28 07:44:40.145: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 28, 7, 44, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 28, 7, 44, 26, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 28, 7, 44, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 28, 7, 44, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun 28 07:44:42.144: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 28, 7, 44, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 28, 7, 44, 26, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 28, 7, 44, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 28, 7, 44, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun 28 07:44:44.144: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 28, 7, 44, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 28, 7, 44, 26, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 28, 7, 44, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 28, 7, 44, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun 28 07:44:46.164: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 28, 7, 44, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 28, 7, 44, 26, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 28, 7, 44, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 28, 7, 44, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun 28 07:44:48.184: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 28, 7, 44, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 28, 7, 44, 26, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 28, 7, 44, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 28, 7, 44, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun 28 07:44:50.145: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 28, 7, 44, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 28, 7, 44, 26, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 28, 7, 44, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 28, 7, 44, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun 28 07:44:52.451: INFO: Waited 292.449435ms for the sample-apiserver to be ready to handle requests.
    STEP: Read Status for v1alpha1.wardle.example.com 06/28/23 07:44:53.057
    STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 06/28/23 07:44:53.062
    STEP: List APIServices 06/28/23 07:44:53.071
    Jun 28 07:44:53.093: INFO: Found v1alpha1.wardle.example.com in APIServiceList
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:68
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/framework/framework.go:187
    Jun 28 07:44:53.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "aggregator-4042" for this suite. 06/28/23 07:44:53.462
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:304
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 07:44:53.473
Jun 28 07:44:53.473: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename statefulset 06/28/23 07:44:53.474
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:44:53.499
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:44:53.512
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-1875 06/28/23 07:44:53.526
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:304
STEP: Creating a new StatefulSet 06/28/23 07:44:53.534
Jun 28 07:44:53.554: INFO: Found 0 stateful pods, waiting for 3
Jun 28 07:45:03.563: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 28 07:45:03.563: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 28 07:45:03.563: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Jun 28 07:45:03.583: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=statefulset-1875 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 28 07:45:04.105: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 28 07:45:04.105: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 28 07:45:04.105: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 06/28/23 07:45:14.131
Jun 28 07:45:14.161: INFO: Updating stateful set ss2
STEP: Creating a new revision 06/28/23 07:45:14.161
STEP: Updating Pods in reverse ordinal order 06/28/23 07:45:24.195
Jun 28 07:45:24.202: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=statefulset-1875 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 07:45:24.629: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun 28 07:45:24.629: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 28 07:45:24.629: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 28 07:45:44.666: INFO: Waiting for StatefulSet statefulset-1875/ss2 to complete update
STEP: Rolling back to a previous revision 06/28/23 07:45:54.679
Jun 28 07:45:54.679: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=statefulset-1875 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 28 07:45:55.149: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 28 07:45:55.149: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 28 07:45:55.149: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 28 07:46:05.194: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order 06/28/23 07:46:15.218
Jun 28 07:46:15.224: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=statefulset-1875 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 07:46:15.575: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun 28 07:46:15.575: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 28 07:46:15.575: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jun 28 07:46:25.611: INFO: Deleting all statefulset in ns statefulset-1875
Jun 28 07:46:25.616: INFO: Scaling statefulset ss2 to 0
Jun 28 07:46:35.643: INFO: Waiting for statefulset status.replicas updated to 0
Jun 28 07:46:35.647: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jun 28 07:46:35.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1875" for this suite. 06/28/23 07:46:35.675
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","completed":15,"skipped":326,"failed":0}
------------------------------
â€¢ [SLOW TEST] [102.209 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should perform rolling updates and roll backs of template modifications [Conformance]
    test/e2e/apps/statefulset.go:304

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 07:44:53.473
    Jun 28 07:44:53.473: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename statefulset 06/28/23 07:44:53.474
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:44:53.499
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:44:53.512
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-1875 06/28/23 07:44:53.526
    [It] should perform rolling updates and roll backs of template modifications [Conformance]
      test/e2e/apps/statefulset.go:304
    STEP: Creating a new StatefulSet 06/28/23 07:44:53.534
    Jun 28 07:44:53.554: INFO: Found 0 stateful pods, waiting for 3
    Jun 28 07:45:03.563: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Jun 28 07:45:03.563: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Jun 28 07:45:03.563: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    Jun 28 07:45:03.583: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=statefulset-1875 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jun 28 07:45:04.105: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jun 28 07:45:04.105: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jun 28 07:45:04.105: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 06/28/23 07:45:14.131
    Jun 28 07:45:14.161: INFO: Updating stateful set ss2
    STEP: Creating a new revision 06/28/23 07:45:14.161
    STEP: Updating Pods in reverse ordinal order 06/28/23 07:45:24.195
    Jun 28 07:45:24.202: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=statefulset-1875 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jun 28 07:45:24.629: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jun 28 07:45:24.629: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jun 28 07:45:24.629: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jun 28 07:45:44.666: INFO: Waiting for StatefulSet statefulset-1875/ss2 to complete update
    STEP: Rolling back to a previous revision 06/28/23 07:45:54.679
    Jun 28 07:45:54.679: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=statefulset-1875 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jun 28 07:45:55.149: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jun 28 07:45:55.149: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jun 28 07:45:55.149: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jun 28 07:46:05.194: INFO: Updating stateful set ss2
    STEP: Rolling back update in reverse ordinal order 06/28/23 07:46:15.218
    Jun 28 07:46:15.224: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=statefulset-1875 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jun 28 07:46:15.575: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jun 28 07:46:15.575: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jun 28 07:46:15.575: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jun 28 07:46:25.611: INFO: Deleting all statefulset in ns statefulset-1875
    Jun 28 07:46:25.616: INFO: Scaling statefulset ss2 to 0
    Jun 28 07:46:35.643: INFO: Waiting for statefulset status.replicas updated to 0
    Jun 28 07:46:35.647: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jun 28 07:46:35.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-1875" for this suite. 06/28/23 07:46:35.675
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] DNS
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 07:46:35.682
Jun 28 07:46:35.682: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename dns 06/28/23 07:46:35.683
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:46:35.699
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:46:35.703
[It] should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 06/28/23 07:46:35.709
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 06/28/23 07:46:35.709
STEP: creating a pod to probe DNS 06/28/23 07:46:35.709
STEP: submitting the pod to kubernetes 06/28/23 07:46:35.709
Jun 28 07:46:35.719: INFO: Waiting up to 15m0s for pod "dns-test-c5cfb949-0566-40a2-a598-96ff590d2697" in namespace "dns-8430" to be "running"
Jun 28 07:46:35.724: INFO: Pod "dns-test-c5cfb949-0566-40a2-a598-96ff590d2697": Phase="Pending", Reason="", readiness=false. Elapsed: 5.203379ms
Jun 28 07:46:37.731: INFO: Pod "dns-test-c5cfb949-0566-40a2-a598-96ff590d2697": Phase="Running", Reason="", readiness=true. Elapsed: 2.011972407s
Jun 28 07:46:37.731: INFO: Pod "dns-test-c5cfb949-0566-40a2-a598-96ff590d2697" satisfied condition "running"
STEP: retrieving the pod 06/28/23 07:46:37.731
STEP: looking for the results for each expected name from probers 06/28/23 07:46:37.735
Jun 28 07:46:37.887: INFO: DNS probes using dns-8430/dns-test-c5cfb949-0566-40a2-a598-96ff590d2697 succeeded

STEP: deleting the pod 06/28/23 07:46:37.887
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jun 28 07:46:37.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8430" for this suite. 06/28/23 07:46:37.908
{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","completed":16,"skipped":331,"failed":0}
------------------------------
â€¢ [2.234 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 07:46:35.682
    Jun 28 07:46:35.682: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename dns 06/28/23 07:46:35.683
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:46:35.699
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:46:35.703
    [It] should provide DNS for the cluster  [Conformance]
      test/e2e/network/dns.go:50
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     06/28/23 07:46:35.709
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     06/28/23 07:46:35.709
    STEP: creating a pod to probe DNS 06/28/23 07:46:35.709
    STEP: submitting the pod to kubernetes 06/28/23 07:46:35.709
    Jun 28 07:46:35.719: INFO: Waiting up to 15m0s for pod "dns-test-c5cfb949-0566-40a2-a598-96ff590d2697" in namespace "dns-8430" to be "running"
    Jun 28 07:46:35.724: INFO: Pod "dns-test-c5cfb949-0566-40a2-a598-96ff590d2697": Phase="Pending", Reason="", readiness=false. Elapsed: 5.203379ms
    Jun 28 07:46:37.731: INFO: Pod "dns-test-c5cfb949-0566-40a2-a598-96ff590d2697": Phase="Running", Reason="", readiness=true. Elapsed: 2.011972407s
    Jun 28 07:46:37.731: INFO: Pod "dns-test-c5cfb949-0566-40a2-a598-96ff590d2697" satisfied condition "running"
    STEP: retrieving the pod 06/28/23 07:46:37.731
    STEP: looking for the results for each expected name from probers 06/28/23 07:46:37.735
    Jun 28 07:46:37.887: INFO: DNS probes using dns-8430/dns-test-c5cfb949-0566-40a2-a598-96ff590d2697 succeeded

    STEP: deleting the pod 06/28/23 07:46:37.887
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jun 28 07:46:37.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-8430" for this suite. 06/28/23 07:46:37.908
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:608
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 07:46:37.918
Jun 28 07:46:37.918: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename security-context-test 06/28/23 07:46:37.919
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:46:37.934
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:46:37.94
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:608
Jun 28 07:46:37.954: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-5c4f6785-048a-409f-8e97-7c98421219b3" in namespace "security-context-test-8238" to be "Succeeded or Failed"
Jun 28 07:46:37.959: INFO: Pod "alpine-nnp-false-5c4f6785-048a-409f-8e97-7c98421219b3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.650629ms
Jun 28 07:46:39.965: INFO: Pod "alpine-nnp-false-5c4f6785-048a-409f-8e97-7c98421219b3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010743815s
Jun 28 07:46:41.966: INFO: Pod "alpine-nnp-false-5c4f6785-048a-409f-8e97-7c98421219b3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011937109s
Jun 28 07:46:43.963: INFO: Pod "alpine-nnp-false-5c4f6785-048a-409f-8e97-7c98421219b3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009282486s
Jun 28 07:46:43.963: INFO: Pod "alpine-nnp-false-5c4f6785-048a-409f-8e97-7c98421219b3" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Jun 28 07:46:43.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-8238" for this suite. 06/28/23 07:46:43.987
{"msg":"PASSED [sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","completed":17,"skipped":360,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.076 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  test/e2e/common/node/security_context.go:554
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:608

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 07:46:37.918
    Jun 28 07:46:37.918: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename security-context-test 06/28/23 07:46:37.919
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:46:37.934
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:46:37.94
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:608
    Jun 28 07:46:37.954: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-5c4f6785-048a-409f-8e97-7c98421219b3" in namespace "security-context-test-8238" to be "Succeeded or Failed"
    Jun 28 07:46:37.959: INFO: Pod "alpine-nnp-false-5c4f6785-048a-409f-8e97-7c98421219b3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.650629ms
    Jun 28 07:46:39.965: INFO: Pod "alpine-nnp-false-5c4f6785-048a-409f-8e97-7c98421219b3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010743815s
    Jun 28 07:46:41.966: INFO: Pod "alpine-nnp-false-5c4f6785-048a-409f-8e97-7c98421219b3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011937109s
    Jun 28 07:46:43.963: INFO: Pod "alpine-nnp-false-5c4f6785-048a-409f-8e97-7c98421219b3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009282486s
    Jun 28 07:46:43.963: INFO: Pod "alpine-nnp-false-5c4f6785-048a-409f-8e97-7c98421219b3" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Jun 28 07:46:43.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-8238" for this suite. 06/28/23 07:46:43.987
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:215
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 07:46:43.995
Jun 28 07:46:43.995: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename container-runtime 06/28/23 07:46:43.996
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:46:44.009
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:46:44.014
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:215
STEP: create the container 06/28/23 07:46:44.019
STEP: wait for the container to reach Failed 06/28/23 07:46:44.029
STEP: get the container status 06/28/23 07:46:48.056
STEP: the container should be terminated 06/28/23 07:46:48.06
STEP: the termination message should be set 06/28/23 07:46:48.06
Jun 28 07:46:48.060: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 06/28/23 07:46:48.06
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Jun 28 07:46:48.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9488" for this suite. 06/28/23 07:46:48.085
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":18,"skipped":393,"failed":0}
------------------------------
â€¢ [4.097 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 07:46:43.995
    Jun 28 07:46:43.995: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename container-runtime 06/28/23 07:46:43.996
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:46:44.009
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:46:44.014
    [It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:215
    STEP: create the container 06/28/23 07:46:44.019
    STEP: wait for the container to reach Failed 06/28/23 07:46:44.029
    STEP: get the container status 06/28/23 07:46:48.056
    STEP: the container should be terminated 06/28/23 07:46:48.06
    STEP: the termination message should be set 06/28/23 07:46:48.06
    Jun 28 07:46:48.060: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 06/28/23 07:46:48.06
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Jun 28 07:46:48.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-9488" for this suite. 06/28/23 07:46:48.085
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 07:46:48.092
Jun 28 07:46:48.092: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename emptydir 06/28/23 07:46:48.093
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:46:48.106
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:46:48.11
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166
STEP: Creating a pod to test emptydir 0644 on node default medium 06/28/23 07:46:48.114
Jun 28 07:46:48.124: INFO: Waiting up to 5m0s for pod "pod-724fe94f-0a43-46e4-b5a7-54641aef0c61" in namespace "emptydir-7960" to be "Succeeded or Failed"
Jun 28 07:46:48.129: INFO: Pod "pod-724fe94f-0a43-46e4-b5a7-54641aef0c61": Phase="Pending", Reason="", readiness=false. Elapsed: 5.217977ms
Jun 28 07:46:50.135: INFO: Pod "pod-724fe94f-0a43-46e4-b5a7-54641aef0c61": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011491548s
Jun 28 07:46:52.135: INFO: Pod "pod-724fe94f-0a43-46e4-b5a7-54641aef0c61": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011771726s
STEP: Saw pod success 06/28/23 07:46:52.135
Jun 28 07:46:52.135: INFO: Pod "pod-724fe94f-0a43-46e4-b5a7-54641aef0c61" satisfied condition "Succeeded or Failed"
Jun 28 07:46:52.140: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod pod-724fe94f-0a43-46e4-b5a7-54641aef0c61 container test-container: <nil>
STEP: delete the pod 06/28/23 07:46:52.15
Jun 28 07:46:52.162: INFO: Waiting for pod pod-724fe94f-0a43-46e4-b5a7-54641aef0c61 to disappear
Jun 28 07:46:52.166: INFO: Pod pod-724fe94f-0a43-46e4-b5a7-54641aef0c61 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jun 28 07:46:52.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7960" for this suite. 06/28/23 07:46:52.175
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":19,"skipped":396,"failed":0}
------------------------------
â€¢ [4.090 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 07:46:48.092
    Jun 28 07:46:48.092: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename emptydir 06/28/23 07:46:48.093
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:46:48.106
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:46:48.11
    [It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:166
    STEP: Creating a pod to test emptydir 0644 on node default medium 06/28/23 07:46:48.114
    Jun 28 07:46:48.124: INFO: Waiting up to 5m0s for pod "pod-724fe94f-0a43-46e4-b5a7-54641aef0c61" in namespace "emptydir-7960" to be "Succeeded or Failed"
    Jun 28 07:46:48.129: INFO: Pod "pod-724fe94f-0a43-46e4-b5a7-54641aef0c61": Phase="Pending", Reason="", readiness=false. Elapsed: 5.217977ms
    Jun 28 07:46:50.135: INFO: Pod "pod-724fe94f-0a43-46e4-b5a7-54641aef0c61": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011491548s
    Jun 28 07:46:52.135: INFO: Pod "pod-724fe94f-0a43-46e4-b5a7-54641aef0c61": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011771726s
    STEP: Saw pod success 06/28/23 07:46:52.135
    Jun 28 07:46:52.135: INFO: Pod "pod-724fe94f-0a43-46e4-b5a7-54641aef0c61" satisfied condition "Succeeded or Failed"
    Jun 28 07:46:52.140: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod pod-724fe94f-0a43-46e4-b5a7-54641aef0c61 container test-container: <nil>
    STEP: delete the pod 06/28/23 07:46:52.15
    Jun 28 07:46:52.162: INFO: Waiting for pod pod-724fe94f-0a43-46e4-b5a7-54641aef0c61 to disappear
    Jun 28 07:46:52.166: INFO: Pod pod-724fe94f-0a43-46e4-b5a7-54641aef0c61 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jun 28 07:46:52.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-7960" for this suite. 06/28/23 07:46:52.175
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] Watchers
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 07:46:52.182
Jun 28 07:46:52.182: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename watch 06/28/23 07:46:52.183
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:46:52.196
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:46:52.201
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
STEP: creating a watch on configmaps with a certain label 06/28/23 07:46:52.206
STEP: creating a new configmap 06/28/23 07:46:52.21
STEP: modifying the configmap once 06/28/23 07:46:52.216
STEP: changing the label value of the configmap 06/28/23 07:46:52.227
STEP: Expecting to observe a delete notification for the watched object 06/28/23 07:46:52.238
Jun 28 07:46:52.238: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5705  a0c81caf-aa24-4360-a729-da8638f21001 47911 0 2023-06-28 07:46:52 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-06-28 07:46:52 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 28 07:46:52.241: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5705  a0c81caf-aa24-4360-a729-da8638f21001 47912 0 2023-06-28 07:46:52 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-06-28 07:46:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 28 07:46:52.241: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5705  a0c81caf-aa24-4360-a729-da8638f21001 47913 0 2023-06-28 07:46:52 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-06-28 07:46:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time 06/28/23 07:46:52.241
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 06/28/23 07:46:52.252
STEP: changing the label value of the configmap back 06/28/23 07:47:02.253
STEP: modifying the configmap a third time 06/28/23 07:47:02.267
STEP: deleting the configmap 06/28/23 07:47:02.28
STEP: Expecting to observe an add notification for the watched object when the label value was restored 06/28/23 07:47:02.289
Jun 28 07:47:02.289: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5705  a0c81caf-aa24-4360-a729-da8638f21001 47966 0 2023-06-28 07:46:52 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-06-28 07:47:02 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 28 07:47:02.289: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5705  a0c81caf-aa24-4360-a729-da8638f21001 47967 0 2023-06-28 07:46:52 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-06-28 07:47:02 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 28 07:47:02.289: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5705  a0c81caf-aa24-4360-a729-da8638f21001 47968 0 2023-06-28 07:46:52 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-06-28 07:47:02 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Jun 28 07:47:02.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5705" for this suite. 06/28/23 07:47:02.299
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","completed":20,"skipped":396,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.125 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 07:46:52.182
    Jun 28 07:46:52.182: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename watch 06/28/23 07:46:52.183
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:46:52.196
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:46:52.201
    [It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
      test/e2e/apimachinery/watch.go:257
    STEP: creating a watch on configmaps with a certain label 06/28/23 07:46:52.206
    STEP: creating a new configmap 06/28/23 07:46:52.21
    STEP: modifying the configmap once 06/28/23 07:46:52.216
    STEP: changing the label value of the configmap 06/28/23 07:46:52.227
    STEP: Expecting to observe a delete notification for the watched object 06/28/23 07:46:52.238
    Jun 28 07:46:52.238: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5705  a0c81caf-aa24-4360-a729-da8638f21001 47911 0 2023-06-28 07:46:52 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-06-28 07:46:52 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jun 28 07:46:52.241: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5705  a0c81caf-aa24-4360-a729-da8638f21001 47912 0 2023-06-28 07:46:52 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-06-28 07:46:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jun 28 07:46:52.241: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5705  a0c81caf-aa24-4360-a729-da8638f21001 47913 0 2023-06-28 07:46:52 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-06-28 07:46:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time 06/28/23 07:46:52.241
    STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 06/28/23 07:46:52.252
    STEP: changing the label value of the configmap back 06/28/23 07:47:02.253
    STEP: modifying the configmap a third time 06/28/23 07:47:02.267
    STEP: deleting the configmap 06/28/23 07:47:02.28
    STEP: Expecting to observe an add notification for the watched object when the label value was restored 06/28/23 07:47:02.289
    Jun 28 07:47:02.289: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5705  a0c81caf-aa24-4360-a729-da8638f21001 47966 0 2023-06-28 07:46:52 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-06-28 07:47:02 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jun 28 07:47:02.289: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5705  a0c81caf-aa24-4360-a729-da8638f21001 47967 0 2023-06-28 07:46:52 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-06-28 07:47:02 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jun 28 07:47:02.289: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5705  a0c81caf-aa24-4360-a729-da8638f21001 47968 0 2023-06-28 07:46:52 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-06-28 07:47:02 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Jun 28 07:47:02.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-5705" for this suite. 06/28/23 07:47:02.299
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 07:47:02.307
Jun 28 07:47:02.308: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename services 06/28/23 07:47:02.308
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:47:02.326
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:47:02.332
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523
STEP: creating a service nodeport-service with the type=NodePort in namespace services-3773 06/28/23 07:47:02.339
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 06/28/23 07:47:02.36
STEP: creating service externalsvc in namespace services-3773 06/28/23 07:47:02.361
STEP: creating replication controller externalsvc in namespace services-3773 06/28/23 07:47:02.372
I0628 07:47:02.392197      18 runners.go:193] Created replication controller with name: externalsvc, namespace: services-3773, replica count: 2
I0628 07:47:05.443144      18 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName 06/28/23 07:47:05.447
Jun 28 07:47:05.470: INFO: Creating new exec pod
Jun 28 07:47:05.476: INFO: Waiting up to 5m0s for pod "execpodzjxjm" in namespace "services-3773" to be "running"
Jun 28 07:47:05.481: INFO: Pod "execpodzjxjm": Phase="Pending", Reason="", readiness=false. Elapsed: 5.621622ms
Jun 28 07:47:07.486: INFO: Pod "execpodzjxjm": Phase="Running", Reason="", readiness=true. Elapsed: 2.009929754s
Jun 28 07:47:07.486: INFO: Pod "execpodzjxjm" satisfied condition "running"
Jun 28 07:47:07.486: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-3773 exec execpodzjxjm -- /bin/sh -x -c nslookup nodeport-service.services-3773.svc.cluster.local'
Jun 28 07:47:07.909: INFO: stderr: "+ nslookup nodeport-service.services-3773.svc.cluster.local\n"
Jun 28 07:47:07.909: INFO: stdout: "Server:\t\t172.20.0.10\nAddress:\t172.20.0.10#53\n\nnodeport-service.services-3773.svc.cluster.local\tcanonical name = externalsvc.services-3773.svc.cluster.local.\nName:\texternalsvc.services-3773.svc.cluster.local\nAddress: 172.20.92.107\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-3773, will wait for the garbage collector to delete the pods 06/28/23 07:47:07.909
Jun 28 07:47:07.973: INFO: Deleting ReplicationController externalsvc took: 8.084404ms
Jun 28 07:47:08.074: INFO: Terminating ReplicationController externalsvc pods took: 100.956556ms
Jun 28 07:47:09.791: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jun 28 07:47:09.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3773" for this suite. 06/28/23 07:47:09.813
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","completed":21,"skipped":411,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.513 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 07:47:02.307
    Jun 28 07:47:02.308: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename services 06/28/23 07:47:02.308
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:47:02.326
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:47:02.332
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from NodePort to ExternalName [Conformance]
      test/e2e/network/service.go:1523
    STEP: creating a service nodeport-service with the type=NodePort in namespace services-3773 06/28/23 07:47:02.339
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 06/28/23 07:47:02.36
    STEP: creating service externalsvc in namespace services-3773 06/28/23 07:47:02.361
    STEP: creating replication controller externalsvc in namespace services-3773 06/28/23 07:47:02.372
    I0628 07:47:02.392197      18 runners.go:193] Created replication controller with name: externalsvc, namespace: services-3773, replica count: 2
    I0628 07:47:05.443144      18 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the NodePort service to type=ExternalName 06/28/23 07:47:05.447
    Jun 28 07:47:05.470: INFO: Creating new exec pod
    Jun 28 07:47:05.476: INFO: Waiting up to 5m0s for pod "execpodzjxjm" in namespace "services-3773" to be "running"
    Jun 28 07:47:05.481: INFO: Pod "execpodzjxjm": Phase="Pending", Reason="", readiness=false. Elapsed: 5.621622ms
    Jun 28 07:47:07.486: INFO: Pod "execpodzjxjm": Phase="Running", Reason="", readiness=true. Elapsed: 2.009929754s
    Jun 28 07:47:07.486: INFO: Pod "execpodzjxjm" satisfied condition "running"
    Jun 28 07:47:07.486: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-3773 exec execpodzjxjm -- /bin/sh -x -c nslookup nodeport-service.services-3773.svc.cluster.local'
    Jun 28 07:47:07.909: INFO: stderr: "+ nslookup nodeport-service.services-3773.svc.cluster.local\n"
    Jun 28 07:47:07.909: INFO: stdout: "Server:\t\t172.20.0.10\nAddress:\t172.20.0.10#53\n\nnodeport-service.services-3773.svc.cluster.local\tcanonical name = externalsvc.services-3773.svc.cluster.local.\nName:\texternalsvc.services-3773.svc.cluster.local\nAddress: 172.20.92.107\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-3773, will wait for the garbage collector to delete the pods 06/28/23 07:47:07.909
    Jun 28 07:47:07.973: INFO: Deleting ReplicationController externalsvc took: 8.084404ms
    Jun 28 07:47:08.074: INFO: Terminating ReplicationController externalsvc pods took: 100.956556ms
    Jun 28 07:47:09.791: INFO: Cleaning up the NodePort to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jun 28 07:47:09.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-3773" for this suite. 06/28/23 07:47:09.813
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-architecture] Conformance Tests
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
[BeforeEach] [sig-architecture] Conformance Tests
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 07:47:09.821
Jun 28 07:47:09.821: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename conformance-tests 06/28/23 07:47:09.822
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:47:09.836
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:47:09.845
[It] should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
STEP: Getting node addresses 06/28/23 07:47:09.85
Jun 28 07:47:09.850: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
[AfterEach] [sig-architecture] Conformance Tests
  test/e2e/framework/framework.go:187
Jun 28 07:47:09.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "conformance-tests-4721" for this suite. 06/28/23 07:47:09.876
{"msg":"PASSED [sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]","completed":22,"skipped":426,"failed":0}
------------------------------
â€¢ [0.062 seconds]
[sig-architecture] Conformance Tests
test/e2e/architecture/framework.go:23
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-architecture] Conformance Tests
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 07:47:09.821
    Jun 28 07:47:09.821: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename conformance-tests 06/28/23 07:47:09.822
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:47:09.836
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:47:09.845
    [It] should have at least two untainted nodes [Conformance]
      test/e2e/architecture/conformance.go:38
    STEP: Getting node addresses 06/28/23 07:47:09.85
    Jun 28 07:47:09.850: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    [AfterEach] [sig-architecture] Conformance Tests
      test/e2e/framework/framework.go:187
    Jun 28 07:47:09.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "conformance-tests-4721" for this suite. 06/28/23 07:47:09.876
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 07:47:09.884
Jun 28 07:47:09.884: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename sched-pred 06/28/23 07:47:09.885
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:47:09.9
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:47:09.905
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Jun 28 07:47:09.909: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun 28 07:47:09.926: INFO: Waiting for terminating namespaces to be deleted...
Jun 28 07:47:09.931: INFO: 
Logging pods the apiserver thinks is on node ske-rhel-749f7d55c8xdd8b6-ct4cp before test
Jun 28 07:47:09.944: INFO: calico-node-h6www from kube-system started at 2023-06-28 05:12:45 +0000 UTC (1 container statuses recorded)
Jun 28 07:47:09.945: INFO: 	Container calico-node ready: true, restart count 0
Jun 28 07:47:09.945: INFO: csi-smb-node-7hhpt from kube-system started at 2023-06-28 05:12:45 +0000 UTC (3 container statuses recorded)
Jun 28 07:47:09.945: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 28 07:47:09.945: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jun 28 07:47:09.945: INFO: 	Container smb ready: true, restart count 0
Jun 28 07:47:09.945: INFO: kube-proxy-78kbb from kube-system started at 2023-06-28 05:12:45 +0000 UTC (1 container statuses recorded)
Jun 28 07:47:09.945: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 28 07:47:09.945: INFO: node-exporter-fkq2r from kube-system started at 2023-06-28 05:12:45 +0000 UTC (1 container statuses recorded)
Jun 28 07:47:09.945: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Jun 28 07:47:09.945: INFO: execpodzjxjm from services-3773 started at 2023-06-28 07:47:05 +0000 UTC (1 container statuses recorded)
Jun 28 07:47:09.945: INFO: 	Container agnhost-container ready: true, restart count 0
Jun 28 07:47:09.945: INFO: sonobuoy from sonobuoy started at 2023-06-28 07:40:00 +0000 UTC (1 container statuses recorded)
Jun 28 07:47:09.945: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun 28 07:47:09.945: INFO: sonobuoy-systemd-logs-daemon-set-747dcc3d7bcc492c-v77g9 from sonobuoy started at 2023-06-28 07:40:01 +0000 UTC (2 container statuses recorded)
Jun 28 07:47:09.945: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 28 07:47:09.945: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 28 07:47:09.945: INFO: 
Logging pods the apiserver thinks is on node ske-rhel-749f7d55c8xdd8b6-srshq before test
Jun 28 07:47:09.961: INFO: calico-kube-controllers-77cc457ff7-jfwrp from kube-system started at 2023-06-28 05:13:50 +0000 UTC (1 container statuses recorded)
Jun 28 07:47:09.961: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jun 28 07:47:09.961: INFO: calico-node-xkhs5 from kube-system started at 2023-06-28 05:12:45 +0000 UTC (1 container statuses recorded)
Jun 28 07:47:09.961: INFO: 	Container calico-node ready: true, restart count 0
Jun 28 07:47:09.961: INFO: coredns-7b4f76cbb6-gkxll from kube-system started at 2023-06-28 05:13:50 +0000 UTC (1 container statuses recorded)
Jun 28 07:47:09.961: INFO: 	Container coredns ready: true, restart count 0
Jun 28 07:47:09.961: INFO: coredns-7b4f76cbb6-ztck5 from kube-system started at 2023-06-28 05:13:50 +0000 UTC (1 container statuses recorded)
Jun 28 07:47:09.961: INFO: 	Container coredns ready: true, restart count 0
Jun 28 07:47:09.961: INFO: csi-smb-controller-7ffdfbf4b6-pfh4n from kube-system started at 2023-06-28 05:13:50 +0000 UTC (3 container statuses recorded)
Jun 28 07:47:09.961: INFO: 	Container csi-provisioner ready: true, restart count 1
Jun 28 07:47:09.961: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 28 07:47:09.961: INFO: 	Container smb ready: true, restart count 0
Jun 28 07:47:09.961: INFO: csi-smb-node-v64b5 from kube-system started at 2023-06-28 05:12:45 +0000 UTC (3 container statuses recorded)
Jun 28 07:47:09.961: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 28 07:47:09.961: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jun 28 07:47:09.961: INFO: 	Container smb ready: true, restart count 0
Jun 28 07:47:09.961: INFO: kube-proxy-l8rq8 from kube-system started at 2023-06-28 05:12:45 +0000 UTC (1 container statuses recorded)
Jun 28 07:47:09.961: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 28 07:47:09.961: INFO: metrics-server-54cd6dc7f5-8j8ss from kube-system started at 2023-06-28 05:13:50 +0000 UTC (1 container statuses recorded)
Jun 28 07:47:09.961: INFO: 	Container metrics-server ready: true, restart count 0
Jun 28 07:47:09.961: INFO: nfs-subdir-external-provisioner-548dcf4dc4-z87tg from kube-system started at 2023-06-28 05:13:50 +0000 UTC (1 container statuses recorded)
Jun 28 07:47:09.961: INFO: 	Container nfs-subdir-external-provisioner ready: true, restart count 1
Jun 28 07:47:09.961: INFO: node-exporter-hcfhb from kube-system started at 2023-06-28 05:12:45 +0000 UTC (1 container statuses recorded)
Jun 28 07:47:09.961: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Jun 28 07:47:09.961: INFO: vpn-target-bcf545797-bfhjg from kube-system started at 2023-06-28 05:13:50 +0000 UTC (1 container statuses recorded)
Jun 28 07:47:09.961: INFO: 	Container vpn-target ready: true, restart count 0
Jun 28 07:47:09.961: INFO: sonobuoy-systemd-logs-daemon-set-747dcc3d7bcc492c-gmpnw from sonobuoy started at 2023-06-28 07:40:01 +0000 UTC (2 container statuses recorded)
Jun 28 07:47:09.961: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 28 07:47:09.961: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 28 07:47:09.961: INFO: 
Logging pods the apiserver thinks is on node ske-rhel-749f7d55c8xdd8b6-zxlfv before test
Jun 28 07:47:09.974: INFO: calico-node-6xpbr from kube-system started at 2023-06-28 05:12:49 +0000 UTC (1 container statuses recorded)
Jun 28 07:47:09.974: INFO: 	Container calico-node ready: true, restart count 0
Jun 28 07:47:09.974: INFO: csi-smb-node-t6cfj from kube-system started at 2023-06-28 05:12:49 +0000 UTC (3 container statuses recorded)
Jun 28 07:47:09.974: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 28 07:47:09.974: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jun 28 07:47:09.974: INFO: 	Container smb ready: true, restart count 0
Jun 28 07:47:09.974: INFO: kube-proxy-l6xjw from kube-system started at 2023-06-28 05:12:49 +0000 UTC (1 container statuses recorded)
Jun 28 07:47:09.974: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 28 07:47:09.974: INFO: node-exporter-w4bjx from kube-system started at 2023-06-28 05:12:49 +0000 UTC (1 container statuses recorded)
Jun 28 07:47:09.974: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Jun 28 07:47:09.974: INFO: sonobuoy-e2e-job-4078a53074bd4828 from sonobuoy started at 2023-06-28 07:40:01 +0000 UTC (2 container statuses recorded)
Jun 28 07:47:09.974: INFO: 	Container e2e ready: true, restart count 0
Jun 28 07:47:09.974: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 28 07:47:09.974: INFO: sonobuoy-systemd-logs-daemon-set-747dcc3d7bcc492c-wmgvv from sonobuoy started at 2023-06-28 07:40:01 +0000 UTC (2 container statuses recorded)
Jun 28 07:47:09.974: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 28 07:47:09.974: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438
STEP: Trying to schedule Pod with nonempty NodeSelector. 06/28/23 07:47:09.974
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.176cc30f1b515316], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match Pod's node affinity/selector. preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling.] 06/28/23 07:47:10.011
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Jun 28 07:47:11.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5105" for this suite. 06/28/23 07:47:11.021
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","completed":23,"skipped":436,"failed":0}
------------------------------
â€¢ [1.144 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 07:47:09.884
    Jun 28 07:47:09.884: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename sched-pred 06/28/23 07:47:09.885
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:47:09.9
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:47:09.905
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Jun 28 07:47:09.909: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jun 28 07:47:09.926: INFO: Waiting for terminating namespaces to be deleted...
    Jun 28 07:47:09.931: INFO: 
    Logging pods the apiserver thinks is on node ske-rhel-749f7d55c8xdd8b6-ct4cp before test
    Jun 28 07:47:09.944: INFO: calico-node-h6www from kube-system started at 2023-06-28 05:12:45 +0000 UTC (1 container statuses recorded)
    Jun 28 07:47:09.945: INFO: 	Container calico-node ready: true, restart count 0
    Jun 28 07:47:09.945: INFO: csi-smb-node-7hhpt from kube-system started at 2023-06-28 05:12:45 +0000 UTC (3 container statuses recorded)
    Jun 28 07:47:09.945: INFO: 	Container liveness-probe ready: true, restart count 0
    Jun 28 07:47:09.945: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Jun 28 07:47:09.945: INFO: 	Container smb ready: true, restart count 0
    Jun 28 07:47:09.945: INFO: kube-proxy-78kbb from kube-system started at 2023-06-28 05:12:45 +0000 UTC (1 container statuses recorded)
    Jun 28 07:47:09.945: INFO: 	Container kube-proxy ready: true, restart count 0
    Jun 28 07:47:09.945: INFO: node-exporter-fkq2r from kube-system started at 2023-06-28 05:12:45 +0000 UTC (1 container statuses recorded)
    Jun 28 07:47:09.945: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
    Jun 28 07:47:09.945: INFO: execpodzjxjm from services-3773 started at 2023-06-28 07:47:05 +0000 UTC (1 container statuses recorded)
    Jun 28 07:47:09.945: INFO: 	Container agnhost-container ready: true, restart count 0
    Jun 28 07:47:09.945: INFO: sonobuoy from sonobuoy started at 2023-06-28 07:40:00 +0000 UTC (1 container statuses recorded)
    Jun 28 07:47:09.945: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jun 28 07:47:09.945: INFO: sonobuoy-systemd-logs-daemon-set-747dcc3d7bcc492c-v77g9 from sonobuoy started at 2023-06-28 07:40:01 +0000 UTC (2 container statuses recorded)
    Jun 28 07:47:09.945: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jun 28 07:47:09.945: INFO: 	Container systemd-logs ready: true, restart count 0
    Jun 28 07:47:09.945: INFO: 
    Logging pods the apiserver thinks is on node ske-rhel-749f7d55c8xdd8b6-srshq before test
    Jun 28 07:47:09.961: INFO: calico-kube-controllers-77cc457ff7-jfwrp from kube-system started at 2023-06-28 05:13:50 +0000 UTC (1 container statuses recorded)
    Jun 28 07:47:09.961: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Jun 28 07:47:09.961: INFO: calico-node-xkhs5 from kube-system started at 2023-06-28 05:12:45 +0000 UTC (1 container statuses recorded)
    Jun 28 07:47:09.961: INFO: 	Container calico-node ready: true, restart count 0
    Jun 28 07:47:09.961: INFO: coredns-7b4f76cbb6-gkxll from kube-system started at 2023-06-28 05:13:50 +0000 UTC (1 container statuses recorded)
    Jun 28 07:47:09.961: INFO: 	Container coredns ready: true, restart count 0
    Jun 28 07:47:09.961: INFO: coredns-7b4f76cbb6-ztck5 from kube-system started at 2023-06-28 05:13:50 +0000 UTC (1 container statuses recorded)
    Jun 28 07:47:09.961: INFO: 	Container coredns ready: true, restart count 0
    Jun 28 07:47:09.961: INFO: csi-smb-controller-7ffdfbf4b6-pfh4n from kube-system started at 2023-06-28 05:13:50 +0000 UTC (3 container statuses recorded)
    Jun 28 07:47:09.961: INFO: 	Container csi-provisioner ready: true, restart count 1
    Jun 28 07:47:09.961: INFO: 	Container liveness-probe ready: true, restart count 0
    Jun 28 07:47:09.961: INFO: 	Container smb ready: true, restart count 0
    Jun 28 07:47:09.961: INFO: csi-smb-node-v64b5 from kube-system started at 2023-06-28 05:12:45 +0000 UTC (3 container statuses recorded)
    Jun 28 07:47:09.961: INFO: 	Container liveness-probe ready: true, restart count 0
    Jun 28 07:47:09.961: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Jun 28 07:47:09.961: INFO: 	Container smb ready: true, restart count 0
    Jun 28 07:47:09.961: INFO: kube-proxy-l8rq8 from kube-system started at 2023-06-28 05:12:45 +0000 UTC (1 container statuses recorded)
    Jun 28 07:47:09.961: INFO: 	Container kube-proxy ready: true, restart count 0
    Jun 28 07:47:09.961: INFO: metrics-server-54cd6dc7f5-8j8ss from kube-system started at 2023-06-28 05:13:50 +0000 UTC (1 container statuses recorded)
    Jun 28 07:47:09.961: INFO: 	Container metrics-server ready: true, restart count 0
    Jun 28 07:47:09.961: INFO: nfs-subdir-external-provisioner-548dcf4dc4-z87tg from kube-system started at 2023-06-28 05:13:50 +0000 UTC (1 container statuses recorded)
    Jun 28 07:47:09.961: INFO: 	Container nfs-subdir-external-provisioner ready: true, restart count 1
    Jun 28 07:47:09.961: INFO: node-exporter-hcfhb from kube-system started at 2023-06-28 05:12:45 +0000 UTC (1 container statuses recorded)
    Jun 28 07:47:09.961: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
    Jun 28 07:47:09.961: INFO: vpn-target-bcf545797-bfhjg from kube-system started at 2023-06-28 05:13:50 +0000 UTC (1 container statuses recorded)
    Jun 28 07:47:09.961: INFO: 	Container vpn-target ready: true, restart count 0
    Jun 28 07:47:09.961: INFO: sonobuoy-systemd-logs-daemon-set-747dcc3d7bcc492c-gmpnw from sonobuoy started at 2023-06-28 07:40:01 +0000 UTC (2 container statuses recorded)
    Jun 28 07:47:09.961: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jun 28 07:47:09.961: INFO: 	Container systemd-logs ready: true, restart count 0
    Jun 28 07:47:09.961: INFO: 
    Logging pods the apiserver thinks is on node ske-rhel-749f7d55c8xdd8b6-zxlfv before test
    Jun 28 07:47:09.974: INFO: calico-node-6xpbr from kube-system started at 2023-06-28 05:12:49 +0000 UTC (1 container statuses recorded)
    Jun 28 07:47:09.974: INFO: 	Container calico-node ready: true, restart count 0
    Jun 28 07:47:09.974: INFO: csi-smb-node-t6cfj from kube-system started at 2023-06-28 05:12:49 +0000 UTC (3 container statuses recorded)
    Jun 28 07:47:09.974: INFO: 	Container liveness-probe ready: true, restart count 0
    Jun 28 07:47:09.974: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Jun 28 07:47:09.974: INFO: 	Container smb ready: true, restart count 0
    Jun 28 07:47:09.974: INFO: kube-proxy-l6xjw from kube-system started at 2023-06-28 05:12:49 +0000 UTC (1 container statuses recorded)
    Jun 28 07:47:09.974: INFO: 	Container kube-proxy ready: true, restart count 0
    Jun 28 07:47:09.974: INFO: node-exporter-w4bjx from kube-system started at 2023-06-28 05:12:49 +0000 UTC (1 container statuses recorded)
    Jun 28 07:47:09.974: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
    Jun 28 07:47:09.974: INFO: sonobuoy-e2e-job-4078a53074bd4828 from sonobuoy started at 2023-06-28 07:40:01 +0000 UTC (2 container statuses recorded)
    Jun 28 07:47:09.974: INFO: 	Container e2e ready: true, restart count 0
    Jun 28 07:47:09.974: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jun 28 07:47:09.974: INFO: sonobuoy-systemd-logs-daemon-set-747dcc3d7bcc492c-wmgvv from sonobuoy started at 2023-06-28 07:40:01 +0000 UTC (2 container statuses recorded)
    Jun 28 07:47:09.974: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jun 28 07:47:09.974: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if not matching  [Conformance]
      test/e2e/scheduling/predicates.go:438
    STEP: Trying to schedule Pod with nonempty NodeSelector. 06/28/23 07:47:09.974
    STEP: Considering event: 
    Type = [Warning], Name = [restricted-pod.176cc30f1b515316], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match Pod's node affinity/selector. preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling.] 06/28/23 07:47:10.011
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Jun 28 07:47:11.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-5105" for this suite. 06/28/23 07:47:11.021
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 07:47:11.029
Jun 28 07:47:11.029: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename crd-publish-openapi 06/28/23 07:47:11.03
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:47:11.044
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:47:11.048
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235
Jun 28 07:47:11.052: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 06/28/23 07:47:13.178
Jun 28 07:47:13.178: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=crd-publish-openapi-1377 --namespace=crd-publish-openapi-1377 create -f -'
Jun 28 07:47:13.802: INFO: stderr: ""
Jun 28 07:47:13.802: INFO: stdout: "e2e-test-crd-publish-openapi-5694-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jun 28 07:47:13.802: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=crd-publish-openapi-1377 --namespace=crd-publish-openapi-1377 delete e2e-test-crd-publish-openapi-5694-crds test-cr'
Jun 28 07:47:13.880: INFO: stderr: ""
Jun 28 07:47:13.880: INFO: stdout: "e2e-test-crd-publish-openapi-5694-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Jun 28 07:47:13.880: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=crd-publish-openapi-1377 --namespace=crd-publish-openapi-1377 apply -f -'
Jun 28 07:47:14.084: INFO: stderr: ""
Jun 28 07:47:14.085: INFO: stdout: "e2e-test-crd-publish-openapi-5694-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jun 28 07:47:14.085: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=crd-publish-openapi-1377 --namespace=crd-publish-openapi-1377 delete e2e-test-crd-publish-openapi-5694-crds test-cr'
Jun 28 07:47:14.160: INFO: stderr: ""
Jun 28 07:47:14.160: INFO: stdout: "e2e-test-crd-publish-openapi-5694-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 06/28/23 07:47:14.16
Jun 28 07:47:14.160: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=crd-publish-openapi-1377 explain e2e-test-crd-publish-openapi-5694-crds'
Jun 28 07:47:14.361: INFO: stderr: ""
Jun 28 07:47:14.361: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5694-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jun 28 07:47:16.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1377" for this suite. 06/28/23 07:47:16.49
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","completed":24,"skipped":446,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.469 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 07:47:11.029
    Jun 28 07:47:11.029: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename crd-publish-openapi 06/28/23 07:47:11.03
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:47:11.044
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:47:11.048
    [It] works for CRD preserving unknown fields in an embedded object [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:235
    Jun 28 07:47:11.052: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 06/28/23 07:47:13.178
    Jun 28 07:47:13.178: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=crd-publish-openapi-1377 --namespace=crd-publish-openapi-1377 create -f -'
    Jun 28 07:47:13.802: INFO: stderr: ""
    Jun 28 07:47:13.802: INFO: stdout: "e2e-test-crd-publish-openapi-5694-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Jun 28 07:47:13.802: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=crd-publish-openapi-1377 --namespace=crd-publish-openapi-1377 delete e2e-test-crd-publish-openapi-5694-crds test-cr'
    Jun 28 07:47:13.880: INFO: stderr: ""
    Jun 28 07:47:13.880: INFO: stdout: "e2e-test-crd-publish-openapi-5694-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    Jun 28 07:47:13.880: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=crd-publish-openapi-1377 --namespace=crd-publish-openapi-1377 apply -f -'
    Jun 28 07:47:14.084: INFO: stderr: ""
    Jun 28 07:47:14.085: INFO: stdout: "e2e-test-crd-publish-openapi-5694-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Jun 28 07:47:14.085: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=crd-publish-openapi-1377 --namespace=crd-publish-openapi-1377 delete e2e-test-crd-publish-openapi-5694-crds test-cr'
    Jun 28 07:47:14.160: INFO: stderr: ""
    Jun 28 07:47:14.160: INFO: stdout: "e2e-test-crd-publish-openapi-5694-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 06/28/23 07:47:14.16
    Jun 28 07:47:14.160: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=crd-publish-openapi-1377 explain e2e-test-crd-publish-openapi-5694-crds'
    Jun 28 07:47:14.361: INFO: stderr: ""
    Jun 28 07:47:14.361: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5694-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jun 28 07:47:16.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-1377" for this suite. 06/28/23 07:47:16.49
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 07:47:16.5
Jun 28 07:47:16.500: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename containers 06/28/23 07:47:16.501
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:47:16.515
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:47:16.52
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86
STEP: Creating a pod to test override all 06/28/23 07:47:16.525
Jun 28 07:47:16.534: INFO: Waiting up to 5m0s for pod "client-containers-9b0fd3a6-9ca9-4095-ba19-b9ece336a223" in namespace "containers-3883" to be "Succeeded or Failed"
Jun 28 07:47:16.540: INFO: Pod "client-containers-9b0fd3a6-9ca9-4095-ba19-b9ece336a223": Phase="Pending", Reason="", readiness=false. Elapsed: 5.371032ms
Jun 28 07:47:18.546: INFO: Pod "client-containers-9b0fd3a6-9ca9-4095-ba19-b9ece336a223": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011496384s
Jun 28 07:47:20.546: INFO: Pod "client-containers-9b0fd3a6-9ca9-4095-ba19-b9ece336a223": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011684498s
STEP: Saw pod success 06/28/23 07:47:20.546
Jun 28 07:47:20.546: INFO: Pod "client-containers-9b0fd3a6-9ca9-4095-ba19-b9ece336a223" satisfied condition "Succeeded or Failed"
Jun 28 07:47:20.551: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod client-containers-9b0fd3a6-9ca9-4095-ba19-b9ece336a223 container agnhost-container: <nil>
STEP: delete the pod 06/28/23 07:47:20.603
Jun 28 07:47:20.616: INFO: Waiting for pod client-containers-9b0fd3a6-9ca9-4095-ba19-b9ece336a223 to disappear
Jun 28 07:47:20.621: INFO: Pod client-containers-9b0fd3a6-9ca9-4095-ba19-b9ece336a223 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Jun 28 07:47:20.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-3883" for this suite. 06/28/23 07:47:20.628
{"msg":"PASSED [sig-node] Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","completed":25,"skipped":500,"failed":0}
------------------------------
â€¢ [4.139 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 07:47:16.5
    Jun 28 07:47:16.500: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename containers 06/28/23 07:47:16.501
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:47:16.515
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:47:16.52
    [It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:86
    STEP: Creating a pod to test override all 06/28/23 07:47:16.525
    Jun 28 07:47:16.534: INFO: Waiting up to 5m0s for pod "client-containers-9b0fd3a6-9ca9-4095-ba19-b9ece336a223" in namespace "containers-3883" to be "Succeeded or Failed"
    Jun 28 07:47:16.540: INFO: Pod "client-containers-9b0fd3a6-9ca9-4095-ba19-b9ece336a223": Phase="Pending", Reason="", readiness=false. Elapsed: 5.371032ms
    Jun 28 07:47:18.546: INFO: Pod "client-containers-9b0fd3a6-9ca9-4095-ba19-b9ece336a223": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011496384s
    Jun 28 07:47:20.546: INFO: Pod "client-containers-9b0fd3a6-9ca9-4095-ba19-b9ece336a223": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011684498s
    STEP: Saw pod success 06/28/23 07:47:20.546
    Jun 28 07:47:20.546: INFO: Pod "client-containers-9b0fd3a6-9ca9-4095-ba19-b9ece336a223" satisfied condition "Succeeded or Failed"
    Jun 28 07:47:20.551: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod client-containers-9b0fd3a6-9ca9-4095-ba19-b9ece336a223 container agnhost-container: <nil>
    STEP: delete the pod 06/28/23 07:47:20.603
    Jun 28 07:47:20.616: INFO: Waiting for pod client-containers-9b0fd3a6-9ca9-4095-ba19-b9ece336a223 to disappear
    Jun 28 07:47:20.621: INFO: Pod client-containers-9b0fd3a6-9ca9-4095-ba19-b9ece336a223 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Jun 28 07:47:20.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-3883" for this suite. 06/28/23 07:47:20.628
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 07:47:20.642
Jun 28 07:47:20.642: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename svcaccounts 06/28/23 07:47:20.643
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:47:20.66
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:47:20.666
[It] should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272
STEP: Creating a pod to test service account token:  06/28/23 07:47:20.673
Jun 28 07:47:20.684: INFO: Waiting up to 5m0s for pod "test-pod-4b43175e-2352-4094-b414-e6e797f9e1a7" in namespace "svcaccounts-1529" to be "Succeeded or Failed"
Jun 28 07:47:20.690: INFO: Pod "test-pod-4b43175e-2352-4094-b414-e6e797f9e1a7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.548925ms
Jun 28 07:47:22.696: INFO: Pod "test-pod-4b43175e-2352-4094-b414-e6e797f9e1a7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011283721s
Jun 28 07:47:24.697: INFO: Pod "test-pod-4b43175e-2352-4094-b414-e6e797f9e1a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012280111s
STEP: Saw pod success 06/28/23 07:47:24.697
Jun 28 07:47:24.697: INFO: Pod "test-pod-4b43175e-2352-4094-b414-e6e797f9e1a7" satisfied condition "Succeeded or Failed"
Jun 28 07:47:24.701: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod test-pod-4b43175e-2352-4094-b414-e6e797f9e1a7 container agnhost-container: <nil>
STEP: delete the pod 06/28/23 07:47:24.711
Jun 28 07:47:24.724: INFO: Waiting for pod test-pod-4b43175e-2352-4094-b414-e6e797f9e1a7 to disappear
Jun 28 07:47:24.728: INFO: Pod test-pod-4b43175e-2352-4094-b414-e6e797f9e1a7 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Jun 28 07:47:24.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-1529" for this suite. 06/28/23 07:47:24.736
{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","completed":26,"skipped":523,"failed":0}
------------------------------
â€¢ [4.101 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 07:47:20.642
    Jun 28 07:47:20.642: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename svcaccounts 06/28/23 07:47:20.643
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:47:20.66
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:47:20.666
    [It] should mount projected service account token [Conformance]
      test/e2e/auth/service_accounts.go:272
    STEP: Creating a pod to test service account token:  06/28/23 07:47:20.673
    Jun 28 07:47:20.684: INFO: Waiting up to 5m0s for pod "test-pod-4b43175e-2352-4094-b414-e6e797f9e1a7" in namespace "svcaccounts-1529" to be "Succeeded or Failed"
    Jun 28 07:47:20.690: INFO: Pod "test-pod-4b43175e-2352-4094-b414-e6e797f9e1a7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.548925ms
    Jun 28 07:47:22.696: INFO: Pod "test-pod-4b43175e-2352-4094-b414-e6e797f9e1a7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011283721s
    Jun 28 07:47:24.697: INFO: Pod "test-pod-4b43175e-2352-4094-b414-e6e797f9e1a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012280111s
    STEP: Saw pod success 06/28/23 07:47:24.697
    Jun 28 07:47:24.697: INFO: Pod "test-pod-4b43175e-2352-4094-b414-e6e797f9e1a7" satisfied condition "Succeeded or Failed"
    Jun 28 07:47:24.701: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod test-pod-4b43175e-2352-4094-b414-e6e797f9e1a7 container agnhost-container: <nil>
    STEP: delete the pod 06/28/23 07:47:24.711
    Jun 28 07:47:24.724: INFO: Waiting for pod test-pod-4b43175e-2352-4094-b414-e6e797f9e1a7 to disappear
    Jun 28 07:47:24.728: INFO: Pod test-pod-4b43175e-2352-4094-b414-e6e797f9e1a7 no longer exists
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Jun 28 07:47:24.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-1529" for this suite. 06/28/23 07:47:24.736
  << End Captured GinkgoWriter Output
------------------------------
[sig-cli] Kubectl client Kubectl logs
  should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1590
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 07:47:24.743
Jun 28 07:47:24.743: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename kubectl 06/28/23 07:47:24.744
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:47:24.757
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:47:24.762
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1570
STEP: creating an pod 06/28/23 07:47:24.767
Jun 28 07:47:24.767: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-4061 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.40 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Jun 28 07:47:24.838: INFO: stderr: ""
Jun 28 07:47:24.838: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1590
STEP: Waiting for log generator to start. 06/28/23 07:47:24.838
Jun 28 07:47:24.839: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Jun 28 07:47:24.839: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-4061" to be "running and ready, or succeeded"
Jun 28 07:47:24.844: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 5.257091ms
Jun 28 07:47:24.844: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'ske-rhel-749f7d55c8xdd8b6-ct4cp' to be 'Running' but was 'Pending'
Jun 28 07:47:26.850: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.011448416s
Jun 28 07:47:26.850: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Jun 28 07:47:26.850: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings 06/28/23 07:47:26.85
Jun 28 07:47:26.850: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-4061 logs logs-generator logs-generator'
Jun 28 07:47:26.929: INFO: stderr: ""
Jun 28 07:47:26.929: INFO: stdout: "I0628 07:47:25.602391       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/kube-system/pods/84bv 344\nI0628 07:47:25.802586       1 logs_generator.go:76] 1 GET /api/v1/namespaces/ns/pods/lh5g 226\nI0628 07:47:26.003402       1 logs_generator.go:76] 2 POST /api/v1/namespaces/ns/pods/dc89 405\nI0628 07:47:26.202809       1 logs_generator.go:76] 3 POST /api/v1/namespaces/kube-system/pods/5hs 296\nI0628 07:47:26.403215       1 logs_generator.go:76] 4 POST /api/v1/namespaces/kube-system/pods/8694 463\nI0628 07:47:26.602516       1 logs_generator.go:76] 5 POST /api/v1/namespaces/default/pods/rpj9 339\nI0628 07:47:26.802988       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/bqp6 496\n"
STEP: limiting log lines 06/28/23 07:47:26.929
Jun 28 07:47:26.929: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-4061 logs logs-generator logs-generator --tail=1'
Jun 28 07:47:27.013: INFO: stderr: ""
Jun 28 07:47:27.013: INFO: stdout: "I0628 07:47:27.003312       1 logs_generator.go:76] 7 GET /api/v1/namespaces/kube-system/pods/2z7k 537\n"
Jun 28 07:47:27.013: INFO: got output "I0628 07:47:27.003312       1 logs_generator.go:76] 7 GET /api/v1/namespaces/kube-system/pods/2z7k 537\n"
STEP: limiting log bytes 06/28/23 07:47:27.013
Jun 28 07:47:27.014: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-4061 logs logs-generator logs-generator --limit-bytes=1'
Jun 28 07:47:27.097: INFO: stderr: ""
Jun 28 07:47:27.097: INFO: stdout: "I"
Jun 28 07:47:27.097: INFO: got output "I"
STEP: exposing timestamps 06/28/23 07:47:27.097
Jun 28 07:47:27.097: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-4061 logs logs-generator logs-generator --tail=1 --timestamps'
Jun 28 07:47:27.173: INFO: stderr: ""
Jun 28 07:47:27.173: INFO: stdout: "2023-06-28T16:47:27.003440930+09:00 I0628 07:47:27.003312       1 logs_generator.go:76] 7 GET /api/v1/namespaces/kube-system/pods/2z7k 537\n"
Jun 28 07:47:27.173: INFO: got output "2023-06-28T16:47:27.003440930+09:00 I0628 07:47:27.003312       1 logs_generator.go:76] 7 GET /api/v1/namespaces/kube-system/pods/2z7k 537\n"
STEP: restricting to a time range 06/28/23 07:47:27.173
Jun 28 07:47:29.674: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-4061 logs logs-generator logs-generator --since=1s'
Jun 28 07:47:29.756: INFO: stderr: ""
Jun 28 07:47:29.756: INFO: stdout: "I0628 07:47:28.803376       1 logs_generator.go:76] 16 POST /api/v1/namespaces/default/pods/6lt7 475\nI0628 07:47:29.002631       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/kube-system/pods/ns8 239\nI0628 07:47:29.203282       1 logs_generator.go:76] 18 GET /api/v1/namespaces/ns/pods/l25 366\nI0628 07:47:29.402606       1 logs_generator.go:76] 19 GET /api/v1/namespaces/ns/pods/7sj 305\nI0628 07:47:29.602895       1 logs_generator.go:76] 20 POST /api/v1/namespaces/kube-system/pods/7ptk 430\n"
Jun 28 07:47:29.756: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-4061 logs logs-generator logs-generator --since=24h'
Jun 28 07:47:29.843: INFO: stderr: ""
Jun 28 07:47:29.843: INFO: stdout: "I0628 07:47:25.602391       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/kube-system/pods/84bv 344\nI0628 07:47:25.802586       1 logs_generator.go:76] 1 GET /api/v1/namespaces/ns/pods/lh5g 226\nI0628 07:47:26.003402       1 logs_generator.go:76] 2 POST /api/v1/namespaces/ns/pods/dc89 405\nI0628 07:47:26.202809       1 logs_generator.go:76] 3 POST /api/v1/namespaces/kube-system/pods/5hs 296\nI0628 07:47:26.403215       1 logs_generator.go:76] 4 POST /api/v1/namespaces/kube-system/pods/8694 463\nI0628 07:47:26.602516       1 logs_generator.go:76] 5 POST /api/v1/namespaces/default/pods/rpj9 339\nI0628 07:47:26.802988       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/bqp6 496\nI0628 07:47:27.003312       1 logs_generator.go:76] 7 GET /api/v1/namespaces/kube-system/pods/2z7k 537\nI0628 07:47:27.202555       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/kube-system/pods/fz9 265\nI0628 07:47:27.402915       1 logs_generator.go:76] 9 POST /api/v1/namespaces/kube-system/pods/2g6 270\nI0628 07:47:27.603259       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/default/pods/hm4b 409\nI0628 07:47:27.802525       1 logs_generator.go:76] 11 POST /api/v1/namespaces/default/pods/q8d 259\nI0628 07:47:28.002970       1 logs_generator.go:76] 12 POST /api/v1/namespaces/kube-system/pods/wxx 517\nI0628 07:47:28.203349       1 logs_generator.go:76] 13 GET /api/v1/namespaces/kube-system/pods/hm7 219\nI0628 07:47:28.402515       1 logs_generator.go:76] 14 GET /api/v1/namespaces/kube-system/pods/4b7r 533\nI0628 07:47:28.602997       1 logs_generator.go:76] 15 GET /api/v1/namespaces/kube-system/pods/gl54 599\nI0628 07:47:28.803376       1 logs_generator.go:76] 16 POST /api/v1/namespaces/default/pods/6lt7 475\nI0628 07:47:29.002631       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/kube-system/pods/ns8 239\nI0628 07:47:29.203282       1 logs_generator.go:76] 18 GET /api/v1/namespaces/ns/pods/l25 366\nI0628 07:47:29.402606       1 logs_generator.go:76] 19 GET /api/v1/namespaces/ns/pods/7sj 305\nI0628 07:47:29.602895       1 logs_generator.go:76] 20 POST /api/v1/namespaces/kube-system/pods/7ptk 430\nI0628 07:47:29.803276       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/kube-system/pods/7rl 272\n"
[AfterEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1575
Jun 28 07:47:29.843: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-4061 delete pod logs-generator'
Jun 28 07:47:30.729: INFO: stderr: ""
Jun 28 07:47:30.729: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jun 28 07:47:30.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4061" for this suite. 06/28/23 07:47:30.74
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","completed":27,"skipped":523,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.005 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl logs
  test/e2e/kubectl/kubectl.go:1567
    should be able to retrieve and filter logs  [Conformance]
    test/e2e/kubectl/kubectl.go:1590

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 07:47:24.743
    Jun 28 07:47:24.743: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename kubectl 06/28/23 07:47:24.744
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:47:24.757
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:47:24.762
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1570
    STEP: creating an pod 06/28/23 07:47:24.767
    Jun 28 07:47:24.767: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-4061 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.40 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
    Jun 28 07:47:24.838: INFO: stderr: ""
    Jun 28 07:47:24.838: INFO: stdout: "pod/logs-generator created\n"
    [It] should be able to retrieve and filter logs  [Conformance]
      test/e2e/kubectl/kubectl.go:1590
    STEP: Waiting for log generator to start. 06/28/23 07:47:24.838
    Jun 28 07:47:24.839: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
    Jun 28 07:47:24.839: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-4061" to be "running and ready, or succeeded"
    Jun 28 07:47:24.844: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 5.257091ms
    Jun 28 07:47:24.844: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'ske-rhel-749f7d55c8xdd8b6-ct4cp' to be 'Running' but was 'Pending'
    Jun 28 07:47:26.850: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.011448416s
    Jun 28 07:47:26.850: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
    Jun 28 07:47:26.850: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
    STEP: checking for a matching strings 06/28/23 07:47:26.85
    Jun 28 07:47:26.850: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-4061 logs logs-generator logs-generator'
    Jun 28 07:47:26.929: INFO: stderr: ""
    Jun 28 07:47:26.929: INFO: stdout: "I0628 07:47:25.602391       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/kube-system/pods/84bv 344\nI0628 07:47:25.802586       1 logs_generator.go:76] 1 GET /api/v1/namespaces/ns/pods/lh5g 226\nI0628 07:47:26.003402       1 logs_generator.go:76] 2 POST /api/v1/namespaces/ns/pods/dc89 405\nI0628 07:47:26.202809       1 logs_generator.go:76] 3 POST /api/v1/namespaces/kube-system/pods/5hs 296\nI0628 07:47:26.403215       1 logs_generator.go:76] 4 POST /api/v1/namespaces/kube-system/pods/8694 463\nI0628 07:47:26.602516       1 logs_generator.go:76] 5 POST /api/v1/namespaces/default/pods/rpj9 339\nI0628 07:47:26.802988       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/bqp6 496\n"
    STEP: limiting log lines 06/28/23 07:47:26.929
    Jun 28 07:47:26.929: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-4061 logs logs-generator logs-generator --tail=1'
    Jun 28 07:47:27.013: INFO: stderr: ""
    Jun 28 07:47:27.013: INFO: stdout: "I0628 07:47:27.003312       1 logs_generator.go:76] 7 GET /api/v1/namespaces/kube-system/pods/2z7k 537\n"
    Jun 28 07:47:27.013: INFO: got output "I0628 07:47:27.003312       1 logs_generator.go:76] 7 GET /api/v1/namespaces/kube-system/pods/2z7k 537\n"
    STEP: limiting log bytes 06/28/23 07:47:27.013
    Jun 28 07:47:27.014: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-4061 logs logs-generator logs-generator --limit-bytes=1'
    Jun 28 07:47:27.097: INFO: stderr: ""
    Jun 28 07:47:27.097: INFO: stdout: "I"
    Jun 28 07:47:27.097: INFO: got output "I"
    STEP: exposing timestamps 06/28/23 07:47:27.097
    Jun 28 07:47:27.097: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-4061 logs logs-generator logs-generator --tail=1 --timestamps'
    Jun 28 07:47:27.173: INFO: stderr: ""
    Jun 28 07:47:27.173: INFO: stdout: "2023-06-28T16:47:27.003440930+09:00 I0628 07:47:27.003312       1 logs_generator.go:76] 7 GET /api/v1/namespaces/kube-system/pods/2z7k 537\n"
    Jun 28 07:47:27.173: INFO: got output "2023-06-28T16:47:27.003440930+09:00 I0628 07:47:27.003312       1 logs_generator.go:76] 7 GET /api/v1/namespaces/kube-system/pods/2z7k 537\n"
    STEP: restricting to a time range 06/28/23 07:47:27.173
    Jun 28 07:47:29.674: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-4061 logs logs-generator logs-generator --since=1s'
    Jun 28 07:47:29.756: INFO: stderr: ""
    Jun 28 07:47:29.756: INFO: stdout: "I0628 07:47:28.803376       1 logs_generator.go:76] 16 POST /api/v1/namespaces/default/pods/6lt7 475\nI0628 07:47:29.002631       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/kube-system/pods/ns8 239\nI0628 07:47:29.203282       1 logs_generator.go:76] 18 GET /api/v1/namespaces/ns/pods/l25 366\nI0628 07:47:29.402606       1 logs_generator.go:76] 19 GET /api/v1/namespaces/ns/pods/7sj 305\nI0628 07:47:29.602895       1 logs_generator.go:76] 20 POST /api/v1/namespaces/kube-system/pods/7ptk 430\n"
    Jun 28 07:47:29.756: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-4061 logs logs-generator logs-generator --since=24h'
    Jun 28 07:47:29.843: INFO: stderr: ""
    Jun 28 07:47:29.843: INFO: stdout: "I0628 07:47:25.602391       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/kube-system/pods/84bv 344\nI0628 07:47:25.802586       1 logs_generator.go:76] 1 GET /api/v1/namespaces/ns/pods/lh5g 226\nI0628 07:47:26.003402       1 logs_generator.go:76] 2 POST /api/v1/namespaces/ns/pods/dc89 405\nI0628 07:47:26.202809       1 logs_generator.go:76] 3 POST /api/v1/namespaces/kube-system/pods/5hs 296\nI0628 07:47:26.403215       1 logs_generator.go:76] 4 POST /api/v1/namespaces/kube-system/pods/8694 463\nI0628 07:47:26.602516       1 logs_generator.go:76] 5 POST /api/v1/namespaces/default/pods/rpj9 339\nI0628 07:47:26.802988       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/bqp6 496\nI0628 07:47:27.003312       1 logs_generator.go:76] 7 GET /api/v1/namespaces/kube-system/pods/2z7k 537\nI0628 07:47:27.202555       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/kube-system/pods/fz9 265\nI0628 07:47:27.402915       1 logs_generator.go:76] 9 POST /api/v1/namespaces/kube-system/pods/2g6 270\nI0628 07:47:27.603259       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/default/pods/hm4b 409\nI0628 07:47:27.802525       1 logs_generator.go:76] 11 POST /api/v1/namespaces/default/pods/q8d 259\nI0628 07:47:28.002970       1 logs_generator.go:76] 12 POST /api/v1/namespaces/kube-system/pods/wxx 517\nI0628 07:47:28.203349       1 logs_generator.go:76] 13 GET /api/v1/namespaces/kube-system/pods/hm7 219\nI0628 07:47:28.402515       1 logs_generator.go:76] 14 GET /api/v1/namespaces/kube-system/pods/4b7r 533\nI0628 07:47:28.602997       1 logs_generator.go:76] 15 GET /api/v1/namespaces/kube-system/pods/gl54 599\nI0628 07:47:28.803376       1 logs_generator.go:76] 16 POST /api/v1/namespaces/default/pods/6lt7 475\nI0628 07:47:29.002631       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/kube-system/pods/ns8 239\nI0628 07:47:29.203282       1 logs_generator.go:76] 18 GET /api/v1/namespaces/ns/pods/l25 366\nI0628 07:47:29.402606       1 logs_generator.go:76] 19 GET /api/v1/namespaces/ns/pods/7sj 305\nI0628 07:47:29.602895       1 logs_generator.go:76] 20 POST /api/v1/namespaces/kube-system/pods/7ptk 430\nI0628 07:47:29.803276       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/kube-system/pods/7rl 272\n"
    [AfterEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1575
    Jun 28 07:47:29.843: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-4061 delete pod logs-generator'
    Jun 28 07:47:30.729: INFO: stderr: ""
    Jun 28 07:47:30.729: INFO: stdout: "pod \"logs-generator\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jun 28 07:47:30.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-4061" for this suite. 06/28/23 07:47:30.74
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 07:47:30.749
Jun 28 07:47:30.749: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename emptydir-wrapper 06/28/23 07:47:30.75
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:47:30.763
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:47:30.768
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
STEP: Creating 50 configmaps 06/28/23 07:47:30.773
STEP: Creating RC which spawns configmap-volume pods 06/28/23 07:47:31.087
Jun 28 07:47:31.105: INFO: Pod name wrapped-volume-race-0b12aa43-aa1d-43ea-bc05-bbabbd7066d1: Found 0 pods out of 5
Jun 28 07:47:36.118: INFO: Pod name wrapped-volume-race-0b12aa43-aa1d-43ea-bc05-bbabbd7066d1: Found 5 pods out of 5
STEP: Ensuring each pod is running 06/28/23 07:47:36.118
Jun 28 07:47:36.119: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-0b12aa43-aa1d-43ea-bc05-bbabbd7066d1-2v8jz" in namespace "emptydir-wrapper-1812" to be "running"
Jun 28 07:47:36.124: INFO: Pod "wrapped-volume-race-0b12aa43-aa1d-43ea-bc05-bbabbd7066d1-2v8jz": Phase="Pending", Reason="", readiness=false. Elapsed: 5.823852ms
Jun 28 07:47:38.133: INFO: Pod "wrapped-volume-race-0b12aa43-aa1d-43ea-bc05-bbabbd7066d1-2v8jz": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01399959s
Jun 28 07:47:40.132: INFO: Pod "wrapped-volume-race-0b12aa43-aa1d-43ea-bc05-bbabbd7066d1-2v8jz": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013527431s
Jun 28 07:47:42.131: INFO: Pod "wrapped-volume-race-0b12aa43-aa1d-43ea-bc05-bbabbd7066d1-2v8jz": Phase="Pending", Reason="", readiness=false. Elapsed: 6.012769244s
Jun 28 07:47:44.133: INFO: Pod "wrapped-volume-race-0b12aa43-aa1d-43ea-bc05-bbabbd7066d1-2v8jz": Phase="Pending", Reason="", readiness=false. Elapsed: 8.013969343s
Jun 28 07:47:46.132: INFO: Pod "wrapped-volume-race-0b12aa43-aa1d-43ea-bc05-bbabbd7066d1-2v8jz": Phase="Running", Reason="", readiness=true. Elapsed: 10.013749425s
Jun 28 07:47:46.132: INFO: Pod "wrapped-volume-race-0b12aa43-aa1d-43ea-bc05-bbabbd7066d1-2v8jz" satisfied condition "running"
Jun 28 07:47:46.132: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-0b12aa43-aa1d-43ea-bc05-bbabbd7066d1-4ttq7" in namespace "emptydir-wrapper-1812" to be "running"
Jun 28 07:47:46.138: INFO: Pod "wrapped-volume-race-0b12aa43-aa1d-43ea-bc05-bbabbd7066d1-4ttq7": Phase="Running", Reason="", readiness=true. Elapsed: 5.970665ms
Jun 28 07:47:46.138: INFO: Pod "wrapped-volume-race-0b12aa43-aa1d-43ea-bc05-bbabbd7066d1-4ttq7" satisfied condition "running"
Jun 28 07:47:46.138: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-0b12aa43-aa1d-43ea-bc05-bbabbd7066d1-68pbp" in namespace "emptydir-wrapper-1812" to be "running"
Jun 28 07:47:46.145: INFO: Pod "wrapped-volume-race-0b12aa43-aa1d-43ea-bc05-bbabbd7066d1-68pbp": Phase="Running", Reason="", readiness=true. Elapsed: 6.588023ms
Jun 28 07:47:46.145: INFO: Pod "wrapped-volume-race-0b12aa43-aa1d-43ea-bc05-bbabbd7066d1-68pbp" satisfied condition "running"
Jun 28 07:47:46.145: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-0b12aa43-aa1d-43ea-bc05-bbabbd7066d1-pkkjc" in namespace "emptydir-wrapper-1812" to be "running"
Jun 28 07:47:46.151: INFO: Pod "wrapped-volume-race-0b12aa43-aa1d-43ea-bc05-bbabbd7066d1-pkkjc": Phase="Running", Reason="", readiness=true. Elapsed: 5.917858ms
Jun 28 07:47:46.151: INFO: Pod "wrapped-volume-race-0b12aa43-aa1d-43ea-bc05-bbabbd7066d1-pkkjc" satisfied condition "running"
Jun 28 07:47:46.151: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-0b12aa43-aa1d-43ea-bc05-bbabbd7066d1-sv7n5" in namespace "emptydir-wrapper-1812" to be "running"
Jun 28 07:47:46.158: INFO: Pod "wrapped-volume-race-0b12aa43-aa1d-43ea-bc05-bbabbd7066d1-sv7n5": Phase="Running", Reason="", readiness=true. Elapsed: 6.53939ms
Jun 28 07:47:46.158: INFO: Pod "wrapped-volume-race-0b12aa43-aa1d-43ea-bc05-bbabbd7066d1-sv7n5" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-0b12aa43-aa1d-43ea-bc05-bbabbd7066d1 in namespace emptydir-wrapper-1812, will wait for the garbage collector to delete the pods 06/28/23 07:47:46.158
Jun 28 07:47:46.223: INFO: Deleting ReplicationController wrapped-volume-race-0b12aa43-aa1d-43ea-bc05-bbabbd7066d1 took: 8.191854ms
Jun 28 07:47:46.424: INFO: Terminating ReplicationController wrapped-volume-race-0b12aa43-aa1d-43ea-bc05-bbabbd7066d1 pods took: 201.021009ms
STEP: Creating RC which spawns configmap-volume pods 06/28/23 07:47:49.234
Jun 28 07:47:49.249: INFO: Pod name wrapped-volume-race-9f043f87-46ef-495b-a546-ce2f5dc703b5: Found 0 pods out of 5
Jun 28 07:47:54.266: INFO: Pod name wrapped-volume-race-9f043f87-46ef-495b-a546-ce2f5dc703b5: Found 5 pods out of 5
STEP: Ensuring each pod is running 06/28/23 07:47:54.266
Jun 28 07:47:54.267: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-9f043f87-46ef-495b-a546-ce2f5dc703b5-45gck" in namespace "emptydir-wrapper-1812" to be "running"
Jun 28 07:47:54.272: INFO: Pod "wrapped-volume-race-9f043f87-46ef-495b-a546-ce2f5dc703b5-45gck": Phase="Pending", Reason="", readiness=false. Elapsed: 5.888334ms
Jun 28 07:47:56.281: INFO: Pod "wrapped-volume-race-9f043f87-46ef-495b-a546-ce2f5dc703b5-45gck": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014681279s
Jun 28 07:47:58.282: INFO: Pod "wrapped-volume-race-9f043f87-46ef-495b-a546-ce2f5dc703b5-45gck": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015142612s
Jun 28 07:48:00.280: INFO: Pod "wrapped-volume-race-9f043f87-46ef-495b-a546-ce2f5dc703b5-45gck": Phase="Pending", Reason="", readiness=false. Elapsed: 6.013859684s
Jun 28 07:48:02.288: INFO: Pod "wrapped-volume-race-9f043f87-46ef-495b-a546-ce2f5dc703b5-45gck": Phase="Pending", Reason="", readiness=false. Elapsed: 8.021250697s
Jun 28 07:48:04.282: INFO: Pod "wrapped-volume-race-9f043f87-46ef-495b-a546-ce2f5dc703b5-45gck": Phase="Running", Reason="", readiness=true. Elapsed: 10.015517516s
Jun 28 07:48:04.282: INFO: Pod "wrapped-volume-race-9f043f87-46ef-495b-a546-ce2f5dc703b5-45gck" satisfied condition "running"
Jun 28 07:48:04.282: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-9f043f87-46ef-495b-a546-ce2f5dc703b5-lv6nm" in namespace "emptydir-wrapper-1812" to be "running"
Jun 28 07:48:04.289: INFO: Pod "wrapped-volume-race-9f043f87-46ef-495b-a546-ce2f5dc703b5-lv6nm": Phase="Running", Reason="", readiness=true. Elapsed: 7.153868ms
Jun 28 07:48:04.289: INFO: Pod "wrapped-volume-race-9f043f87-46ef-495b-a546-ce2f5dc703b5-lv6nm" satisfied condition "running"
Jun 28 07:48:04.289: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-9f043f87-46ef-495b-a546-ce2f5dc703b5-ncmqd" in namespace "emptydir-wrapper-1812" to be "running"
Jun 28 07:48:04.297: INFO: Pod "wrapped-volume-race-9f043f87-46ef-495b-a546-ce2f5dc703b5-ncmqd": Phase="Pending", Reason="", readiness=false. Elapsed: 7.839922ms
Jun 28 07:48:06.308: INFO: Pod "wrapped-volume-race-9f043f87-46ef-495b-a546-ce2f5dc703b5-ncmqd": Phase="Running", Reason="", readiness=true. Elapsed: 2.018862747s
Jun 28 07:48:06.308: INFO: Pod "wrapped-volume-race-9f043f87-46ef-495b-a546-ce2f5dc703b5-ncmqd" satisfied condition "running"
Jun 28 07:48:06.308: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-9f043f87-46ef-495b-a546-ce2f5dc703b5-qcjbv" in namespace "emptydir-wrapper-1812" to be "running"
Jun 28 07:48:06.315: INFO: Pod "wrapped-volume-race-9f043f87-46ef-495b-a546-ce2f5dc703b5-qcjbv": Phase="Running", Reason="", readiness=true. Elapsed: 6.984537ms
Jun 28 07:48:06.315: INFO: Pod "wrapped-volume-race-9f043f87-46ef-495b-a546-ce2f5dc703b5-qcjbv" satisfied condition "running"
Jun 28 07:48:06.315: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-9f043f87-46ef-495b-a546-ce2f5dc703b5-qm2jx" in namespace "emptydir-wrapper-1812" to be "running"
Jun 28 07:48:06.322: INFO: Pod "wrapped-volume-race-9f043f87-46ef-495b-a546-ce2f5dc703b5-qm2jx": Phase="Running", Reason="", readiness=true. Elapsed: 6.793519ms
Jun 28 07:48:06.322: INFO: Pod "wrapped-volume-race-9f043f87-46ef-495b-a546-ce2f5dc703b5-qm2jx" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-9f043f87-46ef-495b-a546-ce2f5dc703b5 in namespace emptydir-wrapper-1812, will wait for the garbage collector to delete the pods 06/28/23 07:48:06.322
Jun 28 07:48:06.389: INFO: Deleting ReplicationController wrapped-volume-race-9f043f87-46ef-495b-a546-ce2f5dc703b5 took: 8.91786ms
Jun 28 07:48:06.590: INFO: Terminating ReplicationController wrapped-volume-race-9f043f87-46ef-495b-a546-ce2f5dc703b5 pods took: 201.363227ms
STEP: Creating RC which spawns configmap-volume pods 06/28/23 07:48:09.6
Jun 28 07:48:09.621: INFO: Pod name wrapped-volume-race-ba93408c-2251-458b-b6dc-6f65ebbbe002: Found 0 pods out of 5
Jun 28 07:48:14.640: INFO: Pod name wrapped-volume-race-ba93408c-2251-458b-b6dc-6f65ebbbe002: Found 5 pods out of 5
STEP: Ensuring each pod is running 06/28/23 07:48:14.64
Jun 28 07:48:14.640: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-ba93408c-2251-458b-b6dc-6f65ebbbe002-2dln5" in namespace "emptydir-wrapper-1812" to be "running"
Jun 28 07:48:14.647: INFO: Pod "wrapped-volume-race-ba93408c-2251-458b-b6dc-6f65ebbbe002-2dln5": Phase="Pending", Reason="", readiness=false. Elapsed: 7.17678ms
Jun 28 07:48:16.655: INFO: Pod "wrapped-volume-race-ba93408c-2251-458b-b6dc-6f65ebbbe002-2dln5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014952652s
Jun 28 07:48:18.655: INFO: Pod "wrapped-volume-race-ba93408c-2251-458b-b6dc-6f65ebbbe002-2dln5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015264217s
Jun 28 07:48:20.658: INFO: Pod "wrapped-volume-race-ba93408c-2251-458b-b6dc-6f65ebbbe002-2dln5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.017642567s
Jun 28 07:48:22.655: INFO: Pod "wrapped-volume-race-ba93408c-2251-458b-b6dc-6f65ebbbe002-2dln5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.0152257s
Jun 28 07:48:24.656: INFO: Pod "wrapped-volume-race-ba93408c-2251-458b-b6dc-6f65ebbbe002-2dln5": Phase="Running", Reason="", readiness=true. Elapsed: 10.015413307s
Jun 28 07:48:24.656: INFO: Pod "wrapped-volume-race-ba93408c-2251-458b-b6dc-6f65ebbbe002-2dln5" satisfied condition "running"
Jun 28 07:48:24.656: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-ba93408c-2251-458b-b6dc-6f65ebbbe002-5f5hj" in namespace "emptydir-wrapper-1812" to be "running"
Jun 28 07:48:24.663: INFO: Pod "wrapped-volume-race-ba93408c-2251-458b-b6dc-6f65ebbbe002-5f5hj": Phase="Running", Reason="", readiness=true. Elapsed: 7.662783ms
Jun 28 07:48:24.663: INFO: Pod "wrapped-volume-race-ba93408c-2251-458b-b6dc-6f65ebbbe002-5f5hj" satisfied condition "running"
Jun 28 07:48:24.663: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-ba93408c-2251-458b-b6dc-6f65ebbbe002-9trfq" in namespace "emptydir-wrapper-1812" to be "running"
Jun 28 07:48:24.670: INFO: Pod "wrapped-volume-race-ba93408c-2251-458b-b6dc-6f65ebbbe002-9trfq": Phase="Running", Reason="", readiness=true. Elapsed: 6.40136ms
Jun 28 07:48:24.670: INFO: Pod "wrapped-volume-race-ba93408c-2251-458b-b6dc-6f65ebbbe002-9trfq" satisfied condition "running"
Jun 28 07:48:24.670: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-ba93408c-2251-458b-b6dc-6f65ebbbe002-h9lg7" in namespace "emptydir-wrapper-1812" to be "running"
Jun 28 07:48:24.675: INFO: Pod "wrapped-volume-race-ba93408c-2251-458b-b6dc-6f65ebbbe002-h9lg7": Phase="Running", Reason="", readiness=true. Elapsed: 5.589651ms
Jun 28 07:48:24.675: INFO: Pod "wrapped-volume-race-ba93408c-2251-458b-b6dc-6f65ebbbe002-h9lg7" satisfied condition "running"
Jun 28 07:48:24.675: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-ba93408c-2251-458b-b6dc-6f65ebbbe002-jzm25" in namespace "emptydir-wrapper-1812" to be "running"
Jun 28 07:48:24.681: INFO: Pod "wrapped-volume-race-ba93408c-2251-458b-b6dc-6f65ebbbe002-jzm25": Phase="Running", Reason="", readiness=true. Elapsed: 5.301529ms
Jun 28 07:48:24.681: INFO: Pod "wrapped-volume-race-ba93408c-2251-458b-b6dc-6f65ebbbe002-jzm25" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-ba93408c-2251-458b-b6dc-6f65ebbbe002 in namespace emptydir-wrapper-1812, will wait for the garbage collector to delete the pods 06/28/23 07:48:24.681
Jun 28 07:48:24.745: INFO: Deleting ReplicationController wrapped-volume-race-ba93408c-2251-458b-b6dc-6f65ebbbe002 took: 7.809997ms
Jun 28 07:48:24.945: INFO: Terminating ReplicationController wrapped-volume-race-ba93408c-2251-458b-b6dc-6f65ebbbe002 pods took: 200.208141ms
STEP: Cleaning up the configMaps 06/28/23 07:48:27.846
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:187
Jun 28 07:48:28.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-1812" for this suite. 06/28/23 07:48:28.214
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","completed":28,"skipped":536,"failed":0}
------------------------------
â€¢ [SLOW TEST] [57.473 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 07:47:30.749
    Jun 28 07:47:30.749: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename emptydir-wrapper 06/28/23 07:47:30.75
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:47:30.763
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:47:30.768
    [It] should not cause race condition when used for configmaps [Serial] [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:189
    STEP: Creating 50 configmaps 06/28/23 07:47:30.773
    STEP: Creating RC which spawns configmap-volume pods 06/28/23 07:47:31.087
    Jun 28 07:47:31.105: INFO: Pod name wrapped-volume-race-0b12aa43-aa1d-43ea-bc05-bbabbd7066d1: Found 0 pods out of 5
    Jun 28 07:47:36.118: INFO: Pod name wrapped-volume-race-0b12aa43-aa1d-43ea-bc05-bbabbd7066d1: Found 5 pods out of 5
    STEP: Ensuring each pod is running 06/28/23 07:47:36.118
    Jun 28 07:47:36.119: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-0b12aa43-aa1d-43ea-bc05-bbabbd7066d1-2v8jz" in namespace "emptydir-wrapper-1812" to be "running"
    Jun 28 07:47:36.124: INFO: Pod "wrapped-volume-race-0b12aa43-aa1d-43ea-bc05-bbabbd7066d1-2v8jz": Phase="Pending", Reason="", readiness=false. Elapsed: 5.823852ms
    Jun 28 07:47:38.133: INFO: Pod "wrapped-volume-race-0b12aa43-aa1d-43ea-bc05-bbabbd7066d1-2v8jz": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01399959s
    Jun 28 07:47:40.132: INFO: Pod "wrapped-volume-race-0b12aa43-aa1d-43ea-bc05-bbabbd7066d1-2v8jz": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013527431s
    Jun 28 07:47:42.131: INFO: Pod "wrapped-volume-race-0b12aa43-aa1d-43ea-bc05-bbabbd7066d1-2v8jz": Phase="Pending", Reason="", readiness=false. Elapsed: 6.012769244s
    Jun 28 07:47:44.133: INFO: Pod "wrapped-volume-race-0b12aa43-aa1d-43ea-bc05-bbabbd7066d1-2v8jz": Phase="Pending", Reason="", readiness=false. Elapsed: 8.013969343s
    Jun 28 07:47:46.132: INFO: Pod "wrapped-volume-race-0b12aa43-aa1d-43ea-bc05-bbabbd7066d1-2v8jz": Phase="Running", Reason="", readiness=true. Elapsed: 10.013749425s
    Jun 28 07:47:46.132: INFO: Pod "wrapped-volume-race-0b12aa43-aa1d-43ea-bc05-bbabbd7066d1-2v8jz" satisfied condition "running"
    Jun 28 07:47:46.132: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-0b12aa43-aa1d-43ea-bc05-bbabbd7066d1-4ttq7" in namespace "emptydir-wrapper-1812" to be "running"
    Jun 28 07:47:46.138: INFO: Pod "wrapped-volume-race-0b12aa43-aa1d-43ea-bc05-bbabbd7066d1-4ttq7": Phase="Running", Reason="", readiness=true. Elapsed: 5.970665ms
    Jun 28 07:47:46.138: INFO: Pod "wrapped-volume-race-0b12aa43-aa1d-43ea-bc05-bbabbd7066d1-4ttq7" satisfied condition "running"
    Jun 28 07:47:46.138: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-0b12aa43-aa1d-43ea-bc05-bbabbd7066d1-68pbp" in namespace "emptydir-wrapper-1812" to be "running"
    Jun 28 07:47:46.145: INFO: Pod "wrapped-volume-race-0b12aa43-aa1d-43ea-bc05-bbabbd7066d1-68pbp": Phase="Running", Reason="", readiness=true. Elapsed: 6.588023ms
    Jun 28 07:47:46.145: INFO: Pod "wrapped-volume-race-0b12aa43-aa1d-43ea-bc05-bbabbd7066d1-68pbp" satisfied condition "running"
    Jun 28 07:47:46.145: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-0b12aa43-aa1d-43ea-bc05-bbabbd7066d1-pkkjc" in namespace "emptydir-wrapper-1812" to be "running"
    Jun 28 07:47:46.151: INFO: Pod "wrapped-volume-race-0b12aa43-aa1d-43ea-bc05-bbabbd7066d1-pkkjc": Phase="Running", Reason="", readiness=true. Elapsed: 5.917858ms
    Jun 28 07:47:46.151: INFO: Pod "wrapped-volume-race-0b12aa43-aa1d-43ea-bc05-bbabbd7066d1-pkkjc" satisfied condition "running"
    Jun 28 07:47:46.151: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-0b12aa43-aa1d-43ea-bc05-bbabbd7066d1-sv7n5" in namespace "emptydir-wrapper-1812" to be "running"
    Jun 28 07:47:46.158: INFO: Pod "wrapped-volume-race-0b12aa43-aa1d-43ea-bc05-bbabbd7066d1-sv7n5": Phase="Running", Reason="", readiness=true. Elapsed: 6.53939ms
    Jun 28 07:47:46.158: INFO: Pod "wrapped-volume-race-0b12aa43-aa1d-43ea-bc05-bbabbd7066d1-sv7n5" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-0b12aa43-aa1d-43ea-bc05-bbabbd7066d1 in namespace emptydir-wrapper-1812, will wait for the garbage collector to delete the pods 06/28/23 07:47:46.158
    Jun 28 07:47:46.223: INFO: Deleting ReplicationController wrapped-volume-race-0b12aa43-aa1d-43ea-bc05-bbabbd7066d1 took: 8.191854ms
    Jun 28 07:47:46.424: INFO: Terminating ReplicationController wrapped-volume-race-0b12aa43-aa1d-43ea-bc05-bbabbd7066d1 pods took: 201.021009ms
    STEP: Creating RC which spawns configmap-volume pods 06/28/23 07:47:49.234
    Jun 28 07:47:49.249: INFO: Pod name wrapped-volume-race-9f043f87-46ef-495b-a546-ce2f5dc703b5: Found 0 pods out of 5
    Jun 28 07:47:54.266: INFO: Pod name wrapped-volume-race-9f043f87-46ef-495b-a546-ce2f5dc703b5: Found 5 pods out of 5
    STEP: Ensuring each pod is running 06/28/23 07:47:54.266
    Jun 28 07:47:54.267: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-9f043f87-46ef-495b-a546-ce2f5dc703b5-45gck" in namespace "emptydir-wrapper-1812" to be "running"
    Jun 28 07:47:54.272: INFO: Pod "wrapped-volume-race-9f043f87-46ef-495b-a546-ce2f5dc703b5-45gck": Phase="Pending", Reason="", readiness=false. Elapsed: 5.888334ms
    Jun 28 07:47:56.281: INFO: Pod "wrapped-volume-race-9f043f87-46ef-495b-a546-ce2f5dc703b5-45gck": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014681279s
    Jun 28 07:47:58.282: INFO: Pod "wrapped-volume-race-9f043f87-46ef-495b-a546-ce2f5dc703b5-45gck": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015142612s
    Jun 28 07:48:00.280: INFO: Pod "wrapped-volume-race-9f043f87-46ef-495b-a546-ce2f5dc703b5-45gck": Phase="Pending", Reason="", readiness=false. Elapsed: 6.013859684s
    Jun 28 07:48:02.288: INFO: Pod "wrapped-volume-race-9f043f87-46ef-495b-a546-ce2f5dc703b5-45gck": Phase="Pending", Reason="", readiness=false. Elapsed: 8.021250697s
    Jun 28 07:48:04.282: INFO: Pod "wrapped-volume-race-9f043f87-46ef-495b-a546-ce2f5dc703b5-45gck": Phase="Running", Reason="", readiness=true. Elapsed: 10.015517516s
    Jun 28 07:48:04.282: INFO: Pod "wrapped-volume-race-9f043f87-46ef-495b-a546-ce2f5dc703b5-45gck" satisfied condition "running"
    Jun 28 07:48:04.282: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-9f043f87-46ef-495b-a546-ce2f5dc703b5-lv6nm" in namespace "emptydir-wrapper-1812" to be "running"
    Jun 28 07:48:04.289: INFO: Pod "wrapped-volume-race-9f043f87-46ef-495b-a546-ce2f5dc703b5-lv6nm": Phase="Running", Reason="", readiness=true. Elapsed: 7.153868ms
    Jun 28 07:48:04.289: INFO: Pod "wrapped-volume-race-9f043f87-46ef-495b-a546-ce2f5dc703b5-lv6nm" satisfied condition "running"
    Jun 28 07:48:04.289: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-9f043f87-46ef-495b-a546-ce2f5dc703b5-ncmqd" in namespace "emptydir-wrapper-1812" to be "running"
    Jun 28 07:48:04.297: INFO: Pod "wrapped-volume-race-9f043f87-46ef-495b-a546-ce2f5dc703b5-ncmqd": Phase="Pending", Reason="", readiness=false. Elapsed: 7.839922ms
    Jun 28 07:48:06.308: INFO: Pod "wrapped-volume-race-9f043f87-46ef-495b-a546-ce2f5dc703b5-ncmqd": Phase="Running", Reason="", readiness=true. Elapsed: 2.018862747s
    Jun 28 07:48:06.308: INFO: Pod "wrapped-volume-race-9f043f87-46ef-495b-a546-ce2f5dc703b5-ncmqd" satisfied condition "running"
    Jun 28 07:48:06.308: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-9f043f87-46ef-495b-a546-ce2f5dc703b5-qcjbv" in namespace "emptydir-wrapper-1812" to be "running"
    Jun 28 07:48:06.315: INFO: Pod "wrapped-volume-race-9f043f87-46ef-495b-a546-ce2f5dc703b5-qcjbv": Phase="Running", Reason="", readiness=true. Elapsed: 6.984537ms
    Jun 28 07:48:06.315: INFO: Pod "wrapped-volume-race-9f043f87-46ef-495b-a546-ce2f5dc703b5-qcjbv" satisfied condition "running"
    Jun 28 07:48:06.315: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-9f043f87-46ef-495b-a546-ce2f5dc703b5-qm2jx" in namespace "emptydir-wrapper-1812" to be "running"
    Jun 28 07:48:06.322: INFO: Pod "wrapped-volume-race-9f043f87-46ef-495b-a546-ce2f5dc703b5-qm2jx": Phase="Running", Reason="", readiness=true. Elapsed: 6.793519ms
    Jun 28 07:48:06.322: INFO: Pod "wrapped-volume-race-9f043f87-46ef-495b-a546-ce2f5dc703b5-qm2jx" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-9f043f87-46ef-495b-a546-ce2f5dc703b5 in namespace emptydir-wrapper-1812, will wait for the garbage collector to delete the pods 06/28/23 07:48:06.322
    Jun 28 07:48:06.389: INFO: Deleting ReplicationController wrapped-volume-race-9f043f87-46ef-495b-a546-ce2f5dc703b5 took: 8.91786ms
    Jun 28 07:48:06.590: INFO: Terminating ReplicationController wrapped-volume-race-9f043f87-46ef-495b-a546-ce2f5dc703b5 pods took: 201.363227ms
    STEP: Creating RC which spawns configmap-volume pods 06/28/23 07:48:09.6
    Jun 28 07:48:09.621: INFO: Pod name wrapped-volume-race-ba93408c-2251-458b-b6dc-6f65ebbbe002: Found 0 pods out of 5
    Jun 28 07:48:14.640: INFO: Pod name wrapped-volume-race-ba93408c-2251-458b-b6dc-6f65ebbbe002: Found 5 pods out of 5
    STEP: Ensuring each pod is running 06/28/23 07:48:14.64
    Jun 28 07:48:14.640: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-ba93408c-2251-458b-b6dc-6f65ebbbe002-2dln5" in namespace "emptydir-wrapper-1812" to be "running"
    Jun 28 07:48:14.647: INFO: Pod "wrapped-volume-race-ba93408c-2251-458b-b6dc-6f65ebbbe002-2dln5": Phase="Pending", Reason="", readiness=false. Elapsed: 7.17678ms
    Jun 28 07:48:16.655: INFO: Pod "wrapped-volume-race-ba93408c-2251-458b-b6dc-6f65ebbbe002-2dln5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014952652s
    Jun 28 07:48:18.655: INFO: Pod "wrapped-volume-race-ba93408c-2251-458b-b6dc-6f65ebbbe002-2dln5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015264217s
    Jun 28 07:48:20.658: INFO: Pod "wrapped-volume-race-ba93408c-2251-458b-b6dc-6f65ebbbe002-2dln5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.017642567s
    Jun 28 07:48:22.655: INFO: Pod "wrapped-volume-race-ba93408c-2251-458b-b6dc-6f65ebbbe002-2dln5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.0152257s
    Jun 28 07:48:24.656: INFO: Pod "wrapped-volume-race-ba93408c-2251-458b-b6dc-6f65ebbbe002-2dln5": Phase="Running", Reason="", readiness=true. Elapsed: 10.015413307s
    Jun 28 07:48:24.656: INFO: Pod "wrapped-volume-race-ba93408c-2251-458b-b6dc-6f65ebbbe002-2dln5" satisfied condition "running"
    Jun 28 07:48:24.656: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-ba93408c-2251-458b-b6dc-6f65ebbbe002-5f5hj" in namespace "emptydir-wrapper-1812" to be "running"
    Jun 28 07:48:24.663: INFO: Pod "wrapped-volume-race-ba93408c-2251-458b-b6dc-6f65ebbbe002-5f5hj": Phase="Running", Reason="", readiness=true. Elapsed: 7.662783ms
    Jun 28 07:48:24.663: INFO: Pod "wrapped-volume-race-ba93408c-2251-458b-b6dc-6f65ebbbe002-5f5hj" satisfied condition "running"
    Jun 28 07:48:24.663: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-ba93408c-2251-458b-b6dc-6f65ebbbe002-9trfq" in namespace "emptydir-wrapper-1812" to be "running"
    Jun 28 07:48:24.670: INFO: Pod "wrapped-volume-race-ba93408c-2251-458b-b6dc-6f65ebbbe002-9trfq": Phase="Running", Reason="", readiness=true. Elapsed: 6.40136ms
    Jun 28 07:48:24.670: INFO: Pod "wrapped-volume-race-ba93408c-2251-458b-b6dc-6f65ebbbe002-9trfq" satisfied condition "running"
    Jun 28 07:48:24.670: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-ba93408c-2251-458b-b6dc-6f65ebbbe002-h9lg7" in namespace "emptydir-wrapper-1812" to be "running"
    Jun 28 07:48:24.675: INFO: Pod "wrapped-volume-race-ba93408c-2251-458b-b6dc-6f65ebbbe002-h9lg7": Phase="Running", Reason="", readiness=true. Elapsed: 5.589651ms
    Jun 28 07:48:24.675: INFO: Pod "wrapped-volume-race-ba93408c-2251-458b-b6dc-6f65ebbbe002-h9lg7" satisfied condition "running"
    Jun 28 07:48:24.675: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-ba93408c-2251-458b-b6dc-6f65ebbbe002-jzm25" in namespace "emptydir-wrapper-1812" to be "running"
    Jun 28 07:48:24.681: INFO: Pod "wrapped-volume-race-ba93408c-2251-458b-b6dc-6f65ebbbe002-jzm25": Phase="Running", Reason="", readiness=true. Elapsed: 5.301529ms
    Jun 28 07:48:24.681: INFO: Pod "wrapped-volume-race-ba93408c-2251-458b-b6dc-6f65ebbbe002-jzm25" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-ba93408c-2251-458b-b6dc-6f65ebbbe002 in namespace emptydir-wrapper-1812, will wait for the garbage collector to delete the pods 06/28/23 07:48:24.681
    Jun 28 07:48:24.745: INFO: Deleting ReplicationController wrapped-volume-race-ba93408c-2251-458b-b6dc-6f65ebbbe002 took: 7.809997ms
    Jun 28 07:48:24.945: INFO: Terminating ReplicationController wrapped-volume-race-ba93408c-2251-458b-b6dc-6f65ebbbe002 pods took: 200.208141ms
    STEP: Cleaning up the configMaps 06/28/23 07:48:27.846
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:187
    Jun 28 07:48:28.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-wrapper-1812" for this suite. 06/28/23 07:48:28.214
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-instrumentation] Events
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 07:48:28.222
Jun 28 07:48:28.222: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename events 06/28/23 07:48:28.223
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:48:28.238
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:48:28.243
[It] should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
STEP: creating a test event 06/28/23 07:48:28.247
STEP: listing all events in all namespaces 06/28/23 07:48:28.252
STEP: patching the test event 06/28/23 07:48:28.262
STEP: fetching the test event 06/28/23 07:48:28.27
STEP: updating the test event 06/28/23 07:48:28.273
STEP: getting the test event 06/28/23 07:48:28.283
STEP: deleting the test event 06/28/23 07:48:28.287
STEP: listing all events in all namespaces 06/28/23 07:48:28.294
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:187
Jun 28 07:48:28.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-3643" for this suite. 06/28/23 07:48:28.31
{"msg":"PASSED [sig-instrumentation] Events should manage the lifecycle of an event [Conformance]","completed":29,"skipped":537,"failed":0}
------------------------------
â€¢ [0.095 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 07:48:28.222
    Jun 28 07:48:28.222: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename events 06/28/23 07:48:28.223
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:48:28.238
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:48:28.243
    [It] should manage the lifecycle of an event [Conformance]
      test/e2e/instrumentation/core_events.go:57
    STEP: creating a test event 06/28/23 07:48:28.247
    STEP: listing all events in all namespaces 06/28/23 07:48:28.252
    STEP: patching the test event 06/28/23 07:48:28.262
    STEP: fetching the test event 06/28/23 07:48:28.27
    STEP: updating the test event 06/28/23 07:48:28.273
    STEP: getting the test event 06/28/23 07:48:28.283
    STEP: deleting the test event 06/28/23 07:48:28.287
    STEP: listing all events in all namespaces 06/28/23 07:48:28.294
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:187
    Jun 28 07:48:28.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-3643" for this suite. 06/28/23 07:48:28.31
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Secrets
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 07:48:28.318
Jun 28 07:48:28.318: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename secrets 06/28/23 07:48:28.319
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:48:28.334
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:48:28.338
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98
STEP: Creating secret with name secret-test-88233c16-56cc-4dd4-b88d-867050cfb7b3 06/28/23 07:48:28.362
STEP: Creating a pod to test consume secrets 06/28/23 07:48:28.367
Jun 28 07:48:28.377: INFO: Waiting up to 5m0s for pod "pod-secrets-1a021292-a186-4d95-a8cf-1d12fe724a76" in namespace "secrets-216" to be "Succeeded or Failed"
Jun 28 07:48:28.381: INFO: Pod "pod-secrets-1a021292-a186-4d95-a8cf-1d12fe724a76": Phase="Pending", Reason="", readiness=false. Elapsed: 4.636368ms
Jun 28 07:48:30.387: INFO: Pod "pod-secrets-1a021292-a186-4d95-a8cf-1d12fe724a76": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009843727s
Jun 28 07:48:32.388: INFO: Pod "pod-secrets-1a021292-a186-4d95-a8cf-1d12fe724a76": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011261164s
STEP: Saw pod success 06/28/23 07:48:32.388
Jun 28 07:48:32.388: INFO: Pod "pod-secrets-1a021292-a186-4d95-a8cf-1d12fe724a76" satisfied condition "Succeeded or Failed"
Jun 28 07:48:32.394: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod pod-secrets-1a021292-a186-4d95-a8cf-1d12fe724a76 container secret-volume-test: <nil>
STEP: delete the pod 06/28/23 07:48:32.443
Jun 28 07:48:32.456: INFO: Waiting for pod pod-secrets-1a021292-a186-4d95-a8cf-1d12fe724a76 to disappear
Jun 28 07:48:32.460: INFO: Pod pod-secrets-1a021292-a186-4d95-a8cf-1d12fe724a76 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jun 28 07:48:32.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-216" for this suite. 06/28/23 07:48:32.467
STEP: Destroying namespace "secret-namespace-5218" for this suite. 06/28/23 07:48:32.475
{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","completed":30,"skipped":541,"failed":0}
------------------------------
â€¢ [4.164 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 07:48:28.318
    Jun 28 07:48:28.318: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename secrets 06/28/23 07:48:28.319
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:48:28.334
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:48:28.338
    [It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:98
    STEP: Creating secret with name secret-test-88233c16-56cc-4dd4-b88d-867050cfb7b3 06/28/23 07:48:28.362
    STEP: Creating a pod to test consume secrets 06/28/23 07:48:28.367
    Jun 28 07:48:28.377: INFO: Waiting up to 5m0s for pod "pod-secrets-1a021292-a186-4d95-a8cf-1d12fe724a76" in namespace "secrets-216" to be "Succeeded or Failed"
    Jun 28 07:48:28.381: INFO: Pod "pod-secrets-1a021292-a186-4d95-a8cf-1d12fe724a76": Phase="Pending", Reason="", readiness=false. Elapsed: 4.636368ms
    Jun 28 07:48:30.387: INFO: Pod "pod-secrets-1a021292-a186-4d95-a8cf-1d12fe724a76": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009843727s
    Jun 28 07:48:32.388: INFO: Pod "pod-secrets-1a021292-a186-4d95-a8cf-1d12fe724a76": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011261164s
    STEP: Saw pod success 06/28/23 07:48:32.388
    Jun 28 07:48:32.388: INFO: Pod "pod-secrets-1a021292-a186-4d95-a8cf-1d12fe724a76" satisfied condition "Succeeded or Failed"
    Jun 28 07:48:32.394: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod pod-secrets-1a021292-a186-4d95-a8cf-1d12fe724a76 container secret-volume-test: <nil>
    STEP: delete the pod 06/28/23 07:48:32.443
    Jun 28 07:48:32.456: INFO: Waiting for pod pod-secrets-1a021292-a186-4d95-a8cf-1d12fe724a76 to disappear
    Jun 28 07:48:32.460: INFO: Pod pod-secrets-1a021292-a186-4d95-a8cf-1d12fe724a76 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jun 28 07:48:32.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-216" for this suite. 06/28/23 07:48:32.467
    STEP: Destroying namespace "secret-namespace-5218" for this suite. 06/28/23 07:48:32.475
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:86
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 07:48:32.482
Jun 28 07:48:32.482: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename disruption 06/28/23 07:48:32.483
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:48:32.498
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:48:32.502
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 07:48:32.506
Jun 28 07:48:32.507: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename disruption-2 06/28/23 07:48:32.507
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:48:32.52
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:48:32.531
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:86
STEP: Waiting for the pdb to be processed 06/28/23 07:48:32.543
STEP: Waiting for the pdb to be processed 06/28/23 07:48:34.561
STEP: Waiting for the pdb to be processed 06/28/23 07:48:34.573
STEP: listing a collection of PDBs across all namespaces 06/28/23 07:48:34.577
STEP: listing a collection of PDBs in namespace disruption-5638 06/28/23 07:48:34.582
STEP: deleting a collection of PDBs 06/28/23 07:48:34.586
STEP: Waiting for the PDB collection to be deleted 06/28/23 07:48:34.598
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/framework.go:187
Jun 28 07:48:34.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2-8862" for this suite. 06/28/23 07:48:34.61
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Jun 28 07:48:34.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-5638" for this suite. 06/28/23 07:48:34.623
{"msg":"PASSED [sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]","completed":31,"skipped":557,"failed":0}
------------------------------
â€¢ [2.148 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  test/e2e/apps/disruption.go:77
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    test/e2e/apps/disruption.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 07:48:32.482
    Jun 28 07:48:32.482: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename disruption 06/28/23 07:48:32.483
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:48:32.498
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:48:32.502
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 07:48:32.506
    Jun 28 07:48:32.507: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename disruption-2 06/28/23 07:48:32.507
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:48:32.52
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:48:32.531
    [It] should list and delete a collection of PodDisruptionBudgets [Conformance]
      test/e2e/apps/disruption.go:86
    STEP: Waiting for the pdb to be processed 06/28/23 07:48:32.543
    STEP: Waiting for the pdb to be processed 06/28/23 07:48:34.561
    STEP: Waiting for the pdb to be processed 06/28/23 07:48:34.573
    STEP: listing a collection of PDBs across all namespaces 06/28/23 07:48:34.577
    STEP: listing a collection of PDBs in namespace disruption-5638 06/28/23 07:48:34.582
    STEP: deleting a collection of PDBs 06/28/23 07:48:34.586
    STEP: Waiting for the PDB collection to be deleted 06/28/23 07:48:34.598
    [AfterEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/framework.go:187
    Jun 28 07:48:34.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-2-8862" for this suite. 06/28/23 07:48:34.61
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Jun 28 07:48:34.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-5638" for this suite. 06/28/23 07:48:34.623
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 07:48:34.63
Jun 28 07:48:34.630: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename pod-network-test 06/28/23 07:48:34.631
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:48:34.644
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:48:34.648
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
STEP: Performing setup for networking test in namespace pod-network-test-7140 06/28/23 07:48:34.653
STEP: creating a selector 06/28/23 07:48:34.653
STEP: Creating the service pods in kubernetes 06/28/23 07:48:34.653
Jun 28 07:48:34.653: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jun 28 07:48:34.687: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-7140" to be "running and ready"
Jun 28 07:48:34.694: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.621487ms
Jun 28 07:48:34.694: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun 28 07:48:36.699: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.011616744s
Jun 28 07:48:36.699: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 28 07:48:38.699: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.011948346s
Jun 28 07:48:38.699: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 28 07:48:40.699: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.012391455s
Jun 28 07:48:40.700: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 28 07:48:42.699: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.011672688s
Jun 28 07:48:42.699: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 28 07:48:44.700: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.012672428s
Jun 28 07:48:44.700: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 28 07:48:46.699: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.011700368s
Jun 28 07:48:46.699: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 28 07:48:48.700: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.012678244s
Jun 28 07:48:48.700: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 28 07:48:50.701: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.013634882s
Jun 28 07:48:50.701: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 28 07:48:52.699: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.011562338s
Jun 28 07:48:52.699: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 28 07:48:54.701: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.014210035s
Jun 28 07:48:54.701: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 28 07:48:56.699: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.011657965s
Jun 28 07:48:56.699: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jun 28 07:48:56.699: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jun 28 07:48:56.705: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-7140" to be "running and ready"
Jun 28 07:48:56.710: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 5.399946ms
Jun 28 07:48:56.710: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jun 28 07:48:56.710: INFO: Pod "netserver-1" satisfied condition "running and ready"
Jun 28 07:48:56.716: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-7140" to be "running and ready"
Jun 28 07:48:56.721: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 5.663925ms
Jun 28 07:48:56.722: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Jun 28 07:48:56.722: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 06/28/23 07:48:56.726
Jun 28 07:48:56.733: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-7140" to be "running"
Jun 28 07:48:56.737: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.090277ms
Jun 28 07:48:58.743: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.010051058s
Jun 28 07:48:58.743: INFO: Pod "test-container-pod" satisfied condition "running"
Jun 28 07:48:58.747: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Jun 28 07:48:58.747: INFO: Breadth first check of 172.21.122.56 on host 192.168.11.3...
Jun 28 07:48:58.751: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.21.122.61:9080/dial?request=hostname&protocol=udp&host=172.21.122.56&port=8081&tries=1'] Namespace:pod-network-test-7140 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 28 07:48:58.751: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
Jun 28 07:48:58.751: INFO: ExecWithOptions: Clientset creation
Jun 28 07:48:58.751: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/pod-network-test-7140/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.21.122.61%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.21.122.56%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jun 28 07:48:59.162: INFO: Waiting for responses: map[]
Jun 28 07:48:59.162: INFO: reached 172.21.122.56 after 0/1 tries
Jun 28 07:48:59.162: INFO: Breadth first check of 172.21.122.85 on host 192.168.11.4...
Jun 28 07:48:59.168: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.21.122.61:9080/dial?request=hostname&protocol=udp&host=172.21.122.85&port=8081&tries=1'] Namespace:pod-network-test-7140 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 28 07:48:59.168: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
Jun 28 07:48:59.168: INFO: ExecWithOptions: Clientset creation
Jun 28 07:48:59.168: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/pod-network-test-7140/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.21.122.61%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.21.122.85%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jun 28 07:48:59.579: INFO: Waiting for responses: map[]
Jun 28 07:48:59.579: INFO: reached 172.21.122.85 after 0/1 tries
Jun 28 07:48:59.579: INFO: Breadth first check of 172.21.30.114 on host 192.168.11.5...
Jun 28 07:48:59.585: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.21.122.61:9080/dial?request=hostname&protocol=udp&host=172.21.30.114&port=8081&tries=1'] Namespace:pod-network-test-7140 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 28 07:48:59.585: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
Jun 28 07:48:59.585: INFO: ExecWithOptions: Clientset creation
Jun 28 07:48:59.585: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/pod-network-test-7140/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.21.122.61%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.21.30.114%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jun 28 07:48:59.970: INFO: Waiting for responses: map[]
Jun 28 07:48:59.970: INFO: reached 172.21.30.114 after 0/1 tries
Jun 28 07:48:59.970: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Jun 28 07:48:59.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-7140" for this suite. 06/28/23 07:48:59.978
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","completed":32,"skipped":584,"failed":0}
------------------------------
â€¢ [SLOW TEST] [25.359 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 07:48:34.63
    Jun 28 07:48:34.630: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename pod-network-test 06/28/23 07:48:34.631
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:48:34.644
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:48:34.648
    [It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:93
    STEP: Performing setup for networking test in namespace pod-network-test-7140 06/28/23 07:48:34.653
    STEP: creating a selector 06/28/23 07:48:34.653
    STEP: Creating the service pods in kubernetes 06/28/23 07:48:34.653
    Jun 28 07:48:34.653: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jun 28 07:48:34.687: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-7140" to be "running and ready"
    Jun 28 07:48:34.694: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.621487ms
    Jun 28 07:48:34.694: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jun 28 07:48:36.699: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.011616744s
    Jun 28 07:48:36.699: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 28 07:48:38.699: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.011948346s
    Jun 28 07:48:38.699: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 28 07:48:40.699: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.012391455s
    Jun 28 07:48:40.700: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 28 07:48:42.699: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.011672688s
    Jun 28 07:48:42.699: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 28 07:48:44.700: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.012672428s
    Jun 28 07:48:44.700: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 28 07:48:46.699: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.011700368s
    Jun 28 07:48:46.699: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 28 07:48:48.700: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.012678244s
    Jun 28 07:48:48.700: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 28 07:48:50.701: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.013634882s
    Jun 28 07:48:50.701: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 28 07:48:52.699: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.011562338s
    Jun 28 07:48:52.699: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 28 07:48:54.701: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.014210035s
    Jun 28 07:48:54.701: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 28 07:48:56.699: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.011657965s
    Jun 28 07:48:56.699: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jun 28 07:48:56.699: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jun 28 07:48:56.705: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-7140" to be "running and ready"
    Jun 28 07:48:56.710: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 5.399946ms
    Jun 28 07:48:56.710: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jun 28 07:48:56.710: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Jun 28 07:48:56.716: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-7140" to be "running and ready"
    Jun 28 07:48:56.721: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 5.663925ms
    Jun 28 07:48:56.722: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Jun 28 07:48:56.722: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 06/28/23 07:48:56.726
    Jun 28 07:48:56.733: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-7140" to be "running"
    Jun 28 07:48:56.737: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.090277ms
    Jun 28 07:48:58.743: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.010051058s
    Jun 28 07:48:58.743: INFO: Pod "test-container-pod" satisfied condition "running"
    Jun 28 07:48:58.747: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Jun 28 07:48:58.747: INFO: Breadth first check of 172.21.122.56 on host 192.168.11.3...
    Jun 28 07:48:58.751: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.21.122.61:9080/dial?request=hostname&protocol=udp&host=172.21.122.56&port=8081&tries=1'] Namespace:pod-network-test-7140 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 28 07:48:58.751: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    Jun 28 07:48:58.751: INFO: ExecWithOptions: Clientset creation
    Jun 28 07:48:58.751: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/pod-network-test-7140/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.21.122.61%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.21.122.56%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jun 28 07:48:59.162: INFO: Waiting for responses: map[]
    Jun 28 07:48:59.162: INFO: reached 172.21.122.56 after 0/1 tries
    Jun 28 07:48:59.162: INFO: Breadth first check of 172.21.122.85 on host 192.168.11.4...
    Jun 28 07:48:59.168: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.21.122.61:9080/dial?request=hostname&protocol=udp&host=172.21.122.85&port=8081&tries=1'] Namespace:pod-network-test-7140 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 28 07:48:59.168: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    Jun 28 07:48:59.168: INFO: ExecWithOptions: Clientset creation
    Jun 28 07:48:59.168: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/pod-network-test-7140/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.21.122.61%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.21.122.85%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jun 28 07:48:59.579: INFO: Waiting for responses: map[]
    Jun 28 07:48:59.579: INFO: reached 172.21.122.85 after 0/1 tries
    Jun 28 07:48:59.579: INFO: Breadth first check of 172.21.30.114 on host 192.168.11.5...
    Jun 28 07:48:59.585: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.21.122.61:9080/dial?request=hostname&protocol=udp&host=172.21.30.114&port=8081&tries=1'] Namespace:pod-network-test-7140 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 28 07:48:59.585: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    Jun 28 07:48:59.585: INFO: ExecWithOptions: Clientset creation
    Jun 28 07:48:59.585: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/pod-network-test-7140/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.21.122.61%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.21.30.114%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jun 28 07:48:59.970: INFO: Waiting for responses: map[]
    Jun 28 07:48:59.970: INFO: reached 172.21.30.114 after 0/1 tries
    Jun 28 07:48:59.970: INFO: Going to retry 0 out of 3 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Jun 28 07:48:59.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-7140" for this suite. 06/28/23 07:48:59.978
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 07:48:59.99
Jun 28 07:48:59.991: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename job 06/28/23 07:48:59.992
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:49:00.011
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:49:00.015
[It] should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531
STEP: Creating a suspended job 06/28/23 07:49:00.024
STEP: Patching the Job 06/28/23 07:49:00.032
STEP: Watching for Job to be patched 06/28/23 07:49:00.041
Jun 28 07:49:00.044: INFO: Event ADDED observed for Job e2e-fsq89 in namespace job-2786 with labels: map[e2e-job-label:e2e-fsq89] and annotations: map[batch.kubernetes.io/job-tracking:]
Jun 28 07:49:00.044: INFO: Event MODIFIED found for Job e2e-fsq89 in namespace job-2786 with labels: map[e2e-fsq89:patched e2e-job-label:e2e-fsq89] and annotations: map[batch.kubernetes.io/job-tracking:]
STEP: Updating the job 06/28/23 07:49:00.044
STEP: Watching for Job to be updated 06/28/23 07:49:00.054
Jun 28 07:49:00.057: INFO: Event MODIFIED found for Job e2e-fsq89 in namespace job-2786 with labels: map[e2e-fsq89:patched e2e-job-label:e2e-fsq89] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jun 28 07:49:00.057: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
STEP: Listing all Jobs with LabelSelector 06/28/23 07:49:00.057
Jun 28 07:49:00.060: INFO: Job: e2e-fsq89 as labels: map[e2e-fsq89:patched e2e-job-label:e2e-fsq89]
STEP: Waiting for job to complete 06/28/23 07:49:00.06
STEP: Delete a job collection with a labelselector 06/28/23 07:49:08.066
STEP: Watching for Job to be deleted 06/28/23 07:49:08.073
Jun 28 07:49:08.076: INFO: Event MODIFIED observed for Job e2e-fsq89 in namespace job-2786 with labels: map[e2e-fsq89:patched e2e-job-label:e2e-fsq89] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jun 28 07:49:08.076: INFO: Event MODIFIED observed for Job e2e-fsq89 in namespace job-2786 with labels: map[e2e-fsq89:patched e2e-job-label:e2e-fsq89] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jun 28 07:49:08.076: INFO: Event MODIFIED observed for Job e2e-fsq89 in namespace job-2786 with labels: map[e2e-fsq89:patched e2e-job-label:e2e-fsq89] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jun 28 07:49:08.076: INFO: Event MODIFIED observed for Job e2e-fsq89 in namespace job-2786 with labels: map[e2e-fsq89:patched e2e-job-label:e2e-fsq89] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jun 28 07:49:08.077: INFO: Event MODIFIED observed for Job e2e-fsq89 in namespace job-2786 with labels: map[e2e-fsq89:patched e2e-job-label:e2e-fsq89] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jun 28 07:49:08.077: INFO: Event DELETED found for Job e2e-fsq89 in namespace job-2786 with labels: map[e2e-fsq89:patched e2e-job-label:e2e-fsq89] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
STEP: Relist jobs to confirm deletion 06/28/23 07:49:08.077
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Jun 28 07:49:08.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-2786" for this suite. 06/28/23 07:49:08.088
{"msg":"PASSED [sig-apps] Job should manage the lifecycle of a job [Conformance]","completed":33,"skipped":601,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.104 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 07:48:59.99
    Jun 28 07:48:59.991: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename job 06/28/23 07:48:59.992
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:49:00.011
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:49:00.015
    [It] should manage the lifecycle of a job [Conformance]
      test/e2e/apps/job.go:531
    STEP: Creating a suspended job 06/28/23 07:49:00.024
    STEP: Patching the Job 06/28/23 07:49:00.032
    STEP: Watching for Job to be patched 06/28/23 07:49:00.041
    Jun 28 07:49:00.044: INFO: Event ADDED observed for Job e2e-fsq89 in namespace job-2786 with labels: map[e2e-job-label:e2e-fsq89] and annotations: map[batch.kubernetes.io/job-tracking:]
    Jun 28 07:49:00.044: INFO: Event MODIFIED found for Job e2e-fsq89 in namespace job-2786 with labels: map[e2e-fsq89:patched e2e-job-label:e2e-fsq89] and annotations: map[batch.kubernetes.io/job-tracking:]
    STEP: Updating the job 06/28/23 07:49:00.044
    STEP: Watching for Job to be updated 06/28/23 07:49:00.054
    Jun 28 07:49:00.057: INFO: Event MODIFIED found for Job e2e-fsq89 in namespace job-2786 with labels: map[e2e-fsq89:patched e2e-job-label:e2e-fsq89] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jun 28 07:49:00.057: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
    STEP: Listing all Jobs with LabelSelector 06/28/23 07:49:00.057
    Jun 28 07:49:00.060: INFO: Job: e2e-fsq89 as labels: map[e2e-fsq89:patched e2e-job-label:e2e-fsq89]
    STEP: Waiting for job to complete 06/28/23 07:49:00.06
    STEP: Delete a job collection with a labelselector 06/28/23 07:49:08.066
    STEP: Watching for Job to be deleted 06/28/23 07:49:08.073
    Jun 28 07:49:08.076: INFO: Event MODIFIED observed for Job e2e-fsq89 in namespace job-2786 with labels: map[e2e-fsq89:patched e2e-job-label:e2e-fsq89] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jun 28 07:49:08.076: INFO: Event MODIFIED observed for Job e2e-fsq89 in namespace job-2786 with labels: map[e2e-fsq89:patched e2e-job-label:e2e-fsq89] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jun 28 07:49:08.076: INFO: Event MODIFIED observed for Job e2e-fsq89 in namespace job-2786 with labels: map[e2e-fsq89:patched e2e-job-label:e2e-fsq89] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jun 28 07:49:08.076: INFO: Event MODIFIED observed for Job e2e-fsq89 in namespace job-2786 with labels: map[e2e-fsq89:patched e2e-job-label:e2e-fsq89] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jun 28 07:49:08.077: INFO: Event MODIFIED observed for Job e2e-fsq89 in namespace job-2786 with labels: map[e2e-fsq89:patched e2e-job-label:e2e-fsq89] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jun 28 07:49:08.077: INFO: Event DELETED found for Job e2e-fsq89 in namespace job-2786 with labels: map[e2e-fsq89:patched e2e-job-label:e2e-fsq89] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    STEP: Relist jobs to confirm deletion 06/28/23 07:49:08.077
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Jun 28 07:49:08.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-2786" for this suite. 06/28/23 07:49:08.088
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 07:49:08.095
Jun 28 07:49:08.095: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename dns 06/28/23 07:49:08.096
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:49:08.11
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:49:08.114
[It] should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
STEP: Creating a test headless service 06/28/23 07:49:08.119
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9240.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-9240.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9240.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9240.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-9240.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-9240.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-9240.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-9240.svc.cluster.local;sleep 1; done
 06/28/23 07:49:08.125
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9240.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-9240.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9240.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-9240.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-9240.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-9240.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-9240.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-9240.svc.cluster.local;sleep 1; done
 06/28/23 07:49:08.125
STEP: creating a pod to probe DNS 06/28/23 07:49:08.125
STEP: submitting the pod to kubernetes 06/28/23 07:49:08.125
Jun 28 07:49:08.134: INFO: Waiting up to 15m0s for pod "dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f" in namespace "dns-9240" to be "running"
Jun 28 07:49:08.139: INFO: Pod "dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.613298ms
Jun 28 07:49:10.144: INFO: Pod "dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f": Phase="Running", Reason="", readiness=true. Elapsed: 2.009678028s
Jun 28 07:49:10.144: INFO: Pod "dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f" satisfied condition "running"
STEP: retrieving the pod 06/28/23 07:49:10.144
STEP: looking for the results for each expected name from probers 06/28/23 07:49:10.149
Jun 28 07:49:10.241: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9240.svc.cluster.local from pod dns-9240/dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f: the server could not find the requested resource (get pods dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f)
Jun 28 07:49:10.285: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9240.svc.cluster.local from pod dns-9240/dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f: the server could not find the requested resource (get pods dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f)
Jun 28 07:49:10.292: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9240.svc.cluster.local from pod dns-9240/dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f: the server could not find the requested resource (get pods dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f)
Jun 28 07:49:10.299: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9240.svc.cluster.local from pod dns-9240/dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f: the server could not find the requested resource (get pods dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f)
Jun 28 07:49:10.306: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-9240.svc.cluster.local from pod dns-9240/dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f: the server could not find the requested resource (get pods dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f)
Jun 28 07:49:10.312: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9240.svc.cluster.local from pod dns-9240/dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f: the server could not find the requested resource (get pods dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f)
Jun 28 07:49:10.319: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9240.svc.cluster.local from pod dns-9240/dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f: the server could not find the requested resource (get pods dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f)
Jun 28 07:49:10.326: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9240.svc.cluster.local from pod dns-9240/dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f: the server could not find the requested resource (get pods dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f)
Jun 28 07:49:10.326: INFO: Lookups using dns-9240/dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9240.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9240.svc.cluster.local wheezy_udp@dns-test-service-2.dns-9240.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9240.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-9240.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-9240.svc.cluster.local jessie_udp@dns-test-service-2.dns-9240.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9240.svc.cluster.local]

Jun 28 07:49:15.337: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9240.svc.cluster.local from pod dns-9240/dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f: the server could not find the requested resource (get pods dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f)
Jun 28 07:49:15.381: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9240.svc.cluster.local from pod dns-9240/dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f: the server could not find the requested resource (get pods dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f)
Jun 28 07:49:15.388: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9240.svc.cluster.local from pod dns-9240/dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f: the server could not find the requested resource (get pods dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f)
Jun 28 07:49:15.395: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9240.svc.cluster.local from pod dns-9240/dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f: the server could not find the requested resource (get pods dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f)
Jun 28 07:49:15.402: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-9240.svc.cluster.local from pod dns-9240/dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f: the server could not find the requested resource (get pods dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f)
Jun 28 07:49:15.409: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9240.svc.cluster.local from pod dns-9240/dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f: the server could not find the requested resource (get pods dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f)
Jun 28 07:49:15.415: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9240.svc.cluster.local from pod dns-9240/dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f: the server could not find the requested resource (get pods dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f)
Jun 28 07:49:15.422: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9240.svc.cluster.local from pod dns-9240/dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f: the server could not find the requested resource (get pods dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f)
Jun 28 07:49:15.422: INFO: Lookups using dns-9240/dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9240.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9240.svc.cluster.local wheezy_udp@dns-test-service-2.dns-9240.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9240.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-9240.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-9240.svc.cluster.local jessie_udp@dns-test-service-2.dns-9240.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9240.svc.cluster.local]

Jun 28 07:49:20.336: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9240.svc.cluster.local from pod dns-9240/dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f: the server could not find the requested resource (get pods dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f)
Jun 28 07:49:20.381: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9240.svc.cluster.local from pod dns-9240/dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f: the server could not find the requested resource (get pods dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f)
Jun 28 07:49:20.388: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9240.svc.cluster.local from pod dns-9240/dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f: the server could not find the requested resource (get pods dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f)
Jun 28 07:49:20.395: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9240.svc.cluster.local from pod dns-9240/dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f: the server could not find the requested resource (get pods dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f)
Jun 28 07:49:20.402: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-9240.svc.cluster.local from pod dns-9240/dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f: the server could not find the requested resource (get pods dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f)
Jun 28 07:49:20.408: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9240.svc.cluster.local from pod dns-9240/dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f: the server could not find the requested resource (get pods dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f)
Jun 28 07:49:20.415: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9240.svc.cluster.local from pod dns-9240/dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f: the server could not find the requested resource (get pods dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f)
Jun 28 07:49:20.422: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9240.svc.cluster.local from pod dns-9240/dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f: the server could not find the requested resource (get pods dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f)
Jun 28 07:49:20.422: INFO: Lookups using dns-9240/dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9240.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9240.svc.cluster.local wheezy_udp@dns-test-service-2.dns-9240.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9240.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-9240.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-9240.svc.cluster.local jessie_udp@dns-test-service-2.dns-9240.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9240.svc.cluster.local]

Jun 28 07:49:25.334: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9240.svc.cluster.local from pod dns-9240/dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f: the server could not find the requested resource (get pods dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f)
Jun 28 07:49:25.378: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9240.svc.cluster.local from pod dns-9240/dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f: the server could not find the requested resource (get pods dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f)
Jun 28 07:49:25.384: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9240.svc.cluster.local from pod dns-9240/dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f: the server could not find the requested resource (get pods dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f)
Jun 28 07:49:25.391: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9240.svc.cluster.local from pod dns-9240/dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f: the server could not find the requested resource (get pods dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f)
Jun 28 07:49:25.398: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-9240.svc.cluster.local from pod dns-9240/dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f: the server could not find the requested resource (get pods dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f)
Jun 28 07:49:25.404: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9240.svc.cluster.local from pod dns-9240/dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f: the server could not find the requested resource (get pods dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f)
Jun 28 07:49:25.410: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9240.svc.cluster.local from pod dns-9240/dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f: the server could not find the requested resource (get pods dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f)
Jun 28 07:49:25.416: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9240.svc.cluster.local from pod dns-9240/dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f: the server could not find the requested resource (get pods dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f)
Jun 28 07:49:25.416: INFO: Lookups using dns-9240/dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9240.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9240.svc.cluster.local wheezy_udp@dns-test-service-2.dns-9240.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9240.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-9240.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-9240.svc.cluster.local jessie_udp@dns-test-service-2.dns-9240.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9240.svc.cluster.local]

Jun 28 07:49:30.335: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9240.svc.cluster.local from pod dns-9240/dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f: the server could not find the requested resource (get pods dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f)
Jun 28 07:49:30.380: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9240.svc.cluster.local from pod dns-9240/dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f: the server could not find the requested resource (get pods dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f)
Jun 28 07:49:30.387: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9240.svc.cluster.local from pod dns-9240/dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f: the server could not find the requested resource (get pods dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f)
Jun 28 07:49:30.395: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9240.svc.cluster.local from pod dns-9240/dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f: the server could not find the requested resource (get pods dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f)
Jun 28 07:49:30.402: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-9240.svc.cluster.local from pod dns-9240/dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f: the server could not find the requested resource (get pods dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f)
Jun 28 07:49:30.408: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9240.svc.cluster.local from pod dns-9240/dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f: the server could not find the requested resource (get pods dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f)
Jun 28 07:49:30.414: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9240.svc.cluster.local from pod dns-9240/dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f: the server could not find the requested resource (get pods dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f)
Jun 28 07:49:30.421: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9240.svc.cluster.local from pod dns-9240/dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f: the server could not find the requested resource (get pods dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f)
Jun 28 07:49:30.421: INFO: Lookups using dns-9240/dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9240.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9240.svc.cluster.local wheezy_udp@dns-test-service-2.dns-9240.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9240.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-9240.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-9240.svc.cluster.local jessie_udp@dns-test-service-2.dns-9240.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9240.svc.cluster.local]

Jun 28 07:49:35.334: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9240.svc.cluster.local from pod dns-9240/dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f: the server could not find the requested resource (get pods dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f)
Jun 28 07:49:35.378: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9240.svc.cluster.local from pod dns-9240/dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f: the server could not find the requested resource (get pods dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f)
Jun 28 07:49:35.384: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9240.svc.cluster.local from pod dns-9240/dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f: the server could not find the requested resource (get pods dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f)
Jun 28 07:49:35.390: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9240.svc.cluster.local from pod dns-9240/dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f: the server could not find the requested resource (get pods dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f)
Jun 28 07:49:35.396: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-9240.svc.cluster.local from pod dns-9240/dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f: the server could not find the requested resource (get pods dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f)
Jun 28 07:49:35.402: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9240.svc.cluster.local from pod dns-9240/dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f: the server could not find the requested resource (get pods dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f)
Jun 28 07:49:35.409: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9240.svc.cluster.local from pod dns-9240/dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f: the server could not find the requested resource (get pods dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f)
Jun 28 07:49:35.415: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9240.svc.cluster.local from pod dns-9240/dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f: the server could not find the requested resource (get pods dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f)
Jun 28 07:49:35.415: INFO: Lookups using dns-9240/dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9240.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9240.svc.cluster.local wheezy_udp@dns-test-service-2.dns-9240.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9240.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-9240.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-9240.svc.cluster.local jessie_udp@dns-test-service-2.dns-9240.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9240.svc.cluster.local]

Jun 28 07:49:40.419: INFO: DNS probes using dns-9240/dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f succeeded

STEP: deleting the pod 06/28/23 07:49:40.419
STEP: deleting the test headless service 06/28/23 07:49:40.433
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jun 28 07:49:40.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9240" for this suite. 06/28/23 07:49:40.45
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","completed":34,"skipped":623,"failed":0}
------------------------------
â€¢ [SLOW TEST] [32.361 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 07:49:08.095
    Jun 28 07:49:08.095: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename dns 06/28/23 07:49:08.096
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:49:08.11
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:49:08.114
    [It] should provide DNS for pods for Subdomain [Conformance]
      test/e2e/network/dns.go:290
    STEP: Creating a test headless service 06/28/23 07:49:08.119
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9240.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-9240.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9240.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9240.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-9240.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-9240.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-9240.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-9240.svc.cluster.local;sleep 1; done
     06/28/23 07:49:08.125
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9240.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-9240.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9240.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-9240.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-9240.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-9240.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-9240.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-9240.svc.cluster.local;sleep 1; done
     06/28/23 07:49:08.125
    STEP: creating a pod to probe DNS 06/28/23 07:49:08.125
    STEP: submitting the pod to kubernetes 06/28/23 07:49:08.125
    Jun 28 07:49:08.134: INFO: Waiting up to 15m0s for pod "dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f" in namespace "dns-9240" to be "running"
    Jun 28 07:49:08.139: INFO: Pod "dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.613298ms
    Jun 28 07:49:10.144: INFO: Pod "dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f": Phase="Running", Reason="", readiness=true. Elapsed: 2.009678028s
    Jun 28 07:49:10.144: INFO: Pod "dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f" satisfied condition "running"
    STEP: retrieving the pod 06/28/23 07:49:10.144
    STEP: looking for the results for each expected name from probers 06/28/23 07:49:10.149
    Jun 28 07:49:10.241: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9240.svc.cluster.local from pod dns-9240/dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f: the server could not find the requested resource (get pods dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f)
    Jun 28 07:49:10.285: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9240.svc.cluster.local from pod dns-9240/dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f: the server could not find the requested resource (get pods dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f)
    Jun 28 07:49:10.292: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9240.svc.cluster.local from pod dns-9240/dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f: the server could not find the requested resource (get pods dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f)
    Jun 28 07:49:10.299: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9240.svc.cluster.local from pod dns-9240/dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f: the server could not find the requested resource (get pods dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f)
    Jun 28 07:49:10.306: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-9240.svc.cluster.local from pod dns-9240/dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f: the server could not find the requested resource (get pods dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f)
    Jun 28 07:49:10.312: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9240.svc.cluster.local from pod dns-9240/dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f: the server could not find the requested resource (get pods dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f)
    Jun 28 07:49:10.319: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9240.svc.cluster.local from pod dns-9240/dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f: the server could not find the requested resource (get pods dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f)
    Jun 28 07:49:10.326: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9240.svc.cluster.local from pod dns-9240/dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f: the server could not find the requested resource (get pods dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f)
    Jun 28 07:49:10.326: INFO: Lookups using dns-9240/dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9240.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9240.svc.cluster.local wheezy_udp@dns-test-service-2.dns-9240.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9240.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-9240.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-9240.svc.cluster.local jessie_udp@dns-test-service-2.dns-9240.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9240.svc.cluster.local]

    Jun 28 07:49:15.337: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9240.svc.cluster.local from pod dns-9240/dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f: the server could not find the requested resource (get pods dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f)
    Jun 28 07:49:15.381: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9240.svc.cluster.local from pod dns-9240/dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f: the server could not find the requested resource (get pods dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f)
    Jun 28 07:49:15.388: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9240.svc.cluster.local from pod dns-9240/dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f: the server could not find the requested resource (get pods dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f)
    Jun 28 07:49:15.395: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9240.svc.cluster.local from pod dns-9240/dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f: the server could not find the requested resource (get pods dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f)
    Jun 28 07:49:15.402: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-9240.svc.cluster.local from pod dns-9240/dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f: the server could not find the requested resource (get pods dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f)
    Jun 28 07:49:15.409: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9240.svc.cluster.local from pod dns-9240/dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f: the server could not find the requested resource (get pods dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f)
    Jun 28 07:49:15.415: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9240.svc.cluster.local from pod dns-9240/dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f: the server could not find the requested resource (get pods dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f)
    Jun 28 07:49:15.422: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9240.svc.cluster.local from pod dns-9240/dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f: the server could not find the requested resource (get pods dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f)
    Jun 28 07:49:15.422: INFO: Lookups using dns-9240/dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9240.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9240.svc.cluster.local wheezy_udp@dns-test-service-2.dns-9240.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9240.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-9240.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-9240.svc.cluster.local jessie_udp@dns-test-service-2.dns-9240.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9240.svc.cluster.local]

    Jun 28 07:49:20.336: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9240.svc.cluster.local from pod dns-9240/dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f: the server could not find the requested resource (get pods dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f)
    Jun 28 07:49:20.381: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9240.svc.cluster.local from pod dns-9240/dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f: the server could not find the requested resource (get pods dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f)
    Jun 28 07:49:20.388: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9240.svc.cluster.local from pod dns-9240/dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f: the server could not find the requested resource (get pods dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f)
    Jun 28 07:49:20.395: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9240.svc.cluster.local from pod dns-9240/dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f: the server could not find the requested resource (get pods dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f)
    Jun 28 07:49:20.402: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-9240.svc.cluster.local from pod dns-9240/dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f: the server could not find the requested resource (get pods dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f)
    Jun 28 07:49:20.408: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9240.svc.cluster.local from pod dns-9240/dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f: the server could not find the requested resource (get pods dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f)
    Jun 28 07:49:20.415: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9240.svc.cluster.local from pod dns-9240/dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f: the server could not find the requested resource (get pods dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f)
    Jun 28 07:49:20.422: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9240.svc.cluster.local from pod dns-9240/dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f: the server could not find the requested resource (get pods dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f)
    Jun 28 07:49:20.422: INFO: Lookups using dns-9240/dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9240.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9240.svc.cluster.local wheezy_udp@dns-test-service-2.dns-9240.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9240.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-9240.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-9240.svc.cluster.local jessie_udp@dns-test-service-2.dns-9240.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9240.svc.cluster.local]

    Jun 28 07:49:25.334: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9240.svc.cluster.local from pod dns-9240/dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f: the server could not find the requested resource (get pods dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f)
    Jun 28 07:49:25.378: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9240.svc.cluster.local from pod dns-9240/dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f: the server could not find the requested resource (get pods dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f)
    Jun 28 07:49:25.384: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9240.svc.cluster.local from pod dns-9240/dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f: the server could not find the requested resource (get pods dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f)
    Jun 28 07:49:25.391: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9240.svc.cluster.local from pod dns-9240/dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f: the server could not find the requested resource (get pods dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f)
    Jun 28 07:49:25.398: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-9240.svc.cluster.local from pod dns-9240/dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f: the server could not find the requested resource (get pods dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f)
    Jun 28 07:49:25.404: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9240.svc.cluster.local from pod dns-9240/dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f: the server could not find the requested resource (get pods dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f)
    Jun 28 07:49:25.410: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9240.svc.cluster.local from pod dns-9240/dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f: the server could not find the requested resource (get pods dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f)
    Jun 28 07:49:25.416: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9240.svc.cluster.local from pod dns-9240/dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f: the server could not find the requested resource (get pods dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f)
    Jun 28 07:49:25.416: INFO: Lookups using dns-9240/dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9240.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9240.svc.cluster.local wheezy_udp@dns-test-service-2.dns-9240.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9240.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-9240.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-9240.svc.cluster.local jessie_udp@dns-test-service-2.dns-9240.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9240.svc.cluster.local]

    Jun 28 07:49:30.335: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9240.svc.cluster.local from pod dns-9240/dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f: the server could not find the requested resource (get pods dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f)
    Jun 28 07:49:30.380: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9240.svc.cluster.local from pod dns-9240/dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f: the server could not find the requested resource (get pods dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f)
    Jun 28 07:49:30.387: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9240.svc.cluster.local from pod dns-9240/dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f: the server could not find the requested resource (get pods dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f)
    Jun 28 07:49:30.395: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9240.svc.cluster.local from pod dns-9240/dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f: the server could not find the requested resource (get pods dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f)
    Jun 28 07:49:30.402: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-9240.svc.cluster.local from pod dns-9240/dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f: the server could not find the requested resource (get pods dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f)
    Jun 28 07:49:30.408: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9240.svc.cluster.local from pod dns-9240/dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f: the server could not find the requested resource (get pods dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f)
    Jun 28 07:49:30.414: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9240.svc.cluster.local from pod dns-9240/dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f: the server could not find the requested resource (get pods dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f)
    Jun 28 07:49:30.421: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9240.svc.cluster.local from pod dns-9240/dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f: the server could not find the requested resource (get pods dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f)
    Jun 28 07:49:30.421: INFO: Lookups using dns-9240/dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9240.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9240.svc.cluster.local wheezy_udp@dns-test-service-2.dns-9240.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9240.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-9240.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-9240.svc.cluster.local jessie_udp@dns-test-service-2.dns-9240.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9240.svc.cluster.local]

    Jun 28 07:49:35.334: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9240.svc.cluster.local from pod dns-9240/dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f: the server could not find the requested resource (get pods dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f)
    Jun 28 07:49:35.378: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9240.svc.cluster.local from pod dns-9240/dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f: the server could not find the requested resource (get pods dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f)
    Jun 28 07:49:35.384: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9240.svc.cluster.local from pod dns-9240/dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f: the server could not find the requested resource (get pods dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f)
    Jun 28 07:49:35.390: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9240.svc.cluster.local from pod dns-9240/dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f: the server could not find the requested resource (get pods dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f)
    Jun 28 07:49:35.396: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-9240.svc.cluster.local from pod dns-9240/dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f: the server could not find the requested resource (get pods dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f)
    Jun 28 07:49:35.402: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9240.svc.cluster.local from pod dns-9240/dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f: the server could not find the requested resource (get pods dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f)
    Jun 28 07:49:35.409: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9240.svc.cluster.local from pod dns-9240/dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f: the server could not find the requested resource (get pods dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f)
    Jun 28 07:49:35.415: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9240.svc.cluster.local from pod dns-9240/dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f: the server could not find the requested resource (get pods dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f)
    Jun 28 07:49:35.415: INFO: Lookups using dns-9240/dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9240.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9240.svc.cluster.local wheezy_udp@dns-test-service-2.dns-9240.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9240.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-9240.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-9240.svc.cluster.local jessie_udp@dns-test-service-2.dns-9240.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9240.svc.cluster.local]

    Jun 28 07:49:40.419: INFO: DNS probes using dns-9240/dns-test-c4a1ebf0-c269-4ce9-b75f-d87bc80b8c1f succeeded

    STEP: deleting the pod 06/28/23 07:49:40.419
    STEP: deleting the test headless service 06/28/23 07:49:40.433
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jun 28 07:49:40.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-9240" for this suite. 06/28/23 07:49:40.45
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Services
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3394
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 07:49:40.457
Jun 28 07:49:40.457: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename services 06/28/23 07:49:40.458
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:49:40.472
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:49:40.476
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3394
STEP: creating a Service 06/28/23 07:49:40.484
STEP: watching for the Service to be added 06/28/23 07:49:40.494
Jun 28 07:49:40.497: INFO: Found Service test-service-vxztc in namespace services-6118 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Jun 28 07:49:40.497: INFO: Service test-service-vxztc created
STEP: Getting /status 06/28/23 07:49:40.497
Jun 28 07:49:40.501: INFO: Service test-service-vxztc has LoadBalancer: {[]}
STEP: patching the ServiceStatus 06/28/23 07:49:40.501
STEP: watching for the Service to be patched 06/28/23 07:49:40.508
Jun 28 07:49:40.511: INFO: observed Service test-service-vxztc in namespace services-6118 with annotations: map[] & LoadBalancer: {[]}
Jun 28 07:49:40.511: INFO: Found Service test-service-vxztc in namespace services-6118 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Jun 28 07:49:40.511: INFO: Service test-service-vxztc has service status patched
STEP: updating the ServiceStatus 06/28/23 07:49:40.511
Jun 28 07:49:40.522: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated 06/28/23 07:49:40.522
Jun 28 07:49:40.525: INFO: Observed Service test-service-vxztc in namespace services-6118 with annotations: map[] & Conditions: {[]}
Jun 28 07:49:40.525: INFO: Observed event: &Service{ObjectMeta:{test-service-vxztc  services-6118  1f7cb33e-ede8-46b2-b93b-314f567824f6 50074 0 2023-06-28 07:49:40 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-06-28 07:49:40 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-06-28 07:49:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:172.20.83.105,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[172.20.83.105],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Jun 28 07:49:40.525: INFO: Found Service test-service-vxztc in namespace services-6118 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jun 28 07:49:40.525: INFO: Service test-service-vxztc has service status updated
STEP: patching the service 06/28/23 07:49:40.525
STEP: watching for the Service to be patched 06/28/23 07:49:40.539
Jun 28 07:49:40.542: INFO: observed Service test-service-vxztc in namespace services-6118 with labels: map[test-service-static:true]
Jun 28 07:49:40.542: INFO: observed Service test-service-vxztc in namespace services-6118 with labels: map[test-service-static:true]
Jun 28 07:49:40.542: INFO: observed Service test-service-vxztc in namespace services-6118 with labels: map[test-service-static:true]
Jun 28 07:49:40.543: INFO: Found Service test-service-vxztc in namespace services-6118 with labels: map[test-service:patched test-service-static:true]
Jun 28 07:49:40.543: INFO: Service test-service-vxztc patched
STEP: deleting the service 06/28/23 07:49:40.543
STEP: watching for the Service to be deleted 06/28/23 07:49:40.56
Jun 28 07:49:40.565: INFO: Observed event: ADDED
Jun 28 07:49:40.565: INFO: Observed event: MODIFIED
Jun 28 07:49:40.565: INFO: Observed event: MODIFIED
Jun 28 07:49:40.565: INFO: Observed event: MODIFIED
Jun 28 07:49:40.565: INFO: Found Service test-service-vxztc in namespace services-6118 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Jun 28 07:49:40.565: INFO: Service test-service-vxztc deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jun 28 07:49:40.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6118" for this suite. 06/28/23 07:49:40.573
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should complete a service status lifecycle [Conformance]","completed":35,"skipped":634,"failed":0}
------------------------------
â€¢ [0.126 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3394

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 07:49:40.457
    Jun 28 07:49:40.457: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename services 06/28/23 07:49:40.458
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:49:40.472
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:49:40.476
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should complete a service status lifecycle [Conformance]
      test/e2e/network/service.go:3394
    STEP: creating a Service 06/28/23 07:49:40.484
    STEP: watching for the Service to be added 06/28/23 07:49:40.494
    Jun 28 07:49:40.497: INFO: Found Service test-service-vxztc in namespace services-6118 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
    Jun 28 07:49:40.497: INFO: Service test-service-vxztc created
    STEP: Getting /status 06/28/23 07:49:40.497
    Jun 28 07:49:40.501: INFO: Service test-service-vxztc has LoadBalancer: {[]}
    STEP: patching the ServiceStatus 06/28/23 07:49:40.501
    STEP: watching for the Service to be patched 06/28/23 07:49:40.508
    Jun 28 07:49:40.511: INFO: observed Service test-service-vxztc in namespace services-6118 with annotations: map[] & LoadBalancer: {[]}
    Jun 28 07:49:40.511: INFO: Found Service test-service-vxztc in namespace services-6118 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
    Jun 28 07:49:40.511: INFO: Service test-service-vxztc has service status patched
    STEP: updating the ServiceStatus 06/28/23 07:49:40.511
    Jun 28 07:49:40.522: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Service to be updated 06/28/23 07:49:40.522
    Jun 28 07:49:40.525: INFO: Observed Service test-service-vxztc in namespace services-6118 with annotations: map[] & Conditions: {[]}
    Jun 28 07:49:40.525: INFO: Observed event: &Service{ObjectMeta:{test-service-vxztc  services-6118  1f7cb33e-ede8-46b2-b93b-314f567824f6 50074 0 2023-06-28 07:49:40 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-06-28 07:49:40 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-06-28 07:49:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:172.20.83.105,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[172.20.83.105],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
    Jun 28 07:49:40.525: INFO: Found Service test-service-vxztc in namespace services-6118 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jun 28 07:49:40.525: INFO: Service test-service-vxztc has service status updated
    STEP: patching the service 06/28/23 07:49:40.525
    STEP: watching for the Service to be patched 06/28/23 07:49:40.539
    Jun 28 07:49:40.542: INFO: observed Service test-service-vxztc in namespace services-6118 with labels: map[test-service-static:true]
    Jun 28 07:49:40.542: INFO: observed Service test-service-vxztc in namespace services-6118 with labels: map[test-service-static:true]
    Jun 28 07:49:40.542: INFO: observed Service test-service-vxztc in namespace services-6118 with labels: map[test-service-static:true]
    Jun 28 07:49:40.543: INFO: Found Service test-service-vxztc in namespace services-6118 with labels: map[test-service:patched test-service-static:true]
    Jun 28 07:49:40.543: INFO: Service test-service-vxztc patched
    STEP: deleting the service 06/28/23 07:49:40.543
    STEP: watching for the Service to be deleted 06/28/23 07:49:40.56
    Jun 28 07:49:40.565: INFO: Observed event: ADDED
    Jun 28 07:49:40.565: INFO: Observed event: MODIFIED
    Jun 28 07:49:40.565: INFO: Observed event: MODIFIED
    Jun 28 07:49:40.565: INFO: Observed event: MODIFIED
    Jun 28 07:49:40.565: INFO: Found Service test-service-vxztc in namespace services-6118 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
    Jun 28 07:49:40.565: INFO: Service test-service-vxztc deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jun 28 07:49:40.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-6118" for this suite. 06/28/23 07:49:40.573
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Pods
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 07:49:40.583
Jun 28 07:49:40.583: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename pods 06/28/23 07:49:40.584
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:49:40.608
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:49:40.613
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225
STEP: creating the pod 06/28/23 07:49:40.619
STEP: setting up watch 06/28/23 07:49:40.619
STEP: submitting the pod to kubernetes 06/28/23 07:49:40.726
STEP: verifying the pod is in kubernetes 06/28/23 07:49:40.736
STEP: verifying pod creation was observed 06/28/23 07:49:40.74
Jun 28 07:49:40.740: INFO: Waiting up to 5m0s for pod "pod-submit-remove-0f99647e-45b5-4415-9923-257e2a3b3b39" in namespace "pods-9888" to be "running"
Jun 28 07:49:40.744: INFO: Pod "pod-submit-remove-0f99647e-45b5-4415-9923-257e2a3b3b39": Phase="Pending", Reason="", readiness=false. Elapsed: 4.464083ms
Jun 28 07:49:42.749: INFO: Pod "pod-submit-remove-0f99647e-45b5-4415-9923-257e2a3b3b39": Phase="Running", Reason="", readiness=true. Elapsed: 2.009603705s
Jun 28 07:49:42.750: INFO: Pod "pod-submit-remove-0f99647e-45b5-4415-9923-257e2a3b3b39" satisfied condition "running"
STEP: deleting the pod gracefully 06/28/23 07:49:42.754
STEP: verifying pod deletion was observed 06/28/23 07:49:42.762
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jun 28 07:49:45.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9888" for this suite. 06/28/23 07:49:45.101
{"msg":"PASSED [sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]","completed":36,"skipped":634,"failed":0}
------------------------------
â€¢ [4.525 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 07:49:40.583
    Jun 28 07:49:40.583: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename pods 06/28/23 07:49:40.584
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:49:40.608
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:49:40.613
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should be submitted and removed [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:225
    STEP: creating the pod 06/28/23 07:49:40.619
    STEP: setting up watch 06/28/23 07:49:40.619
    STEP: submitting the pod to kubernetes 06/28/23 07:49:40.726
    STEP: verifying the pod is in kubernetes 06/28/23 07:49:40.736
    STEP: verifying pod creation was observed 06/28/23 07:49:40.74
    Jun 28 07:49:40.740: INFO: Waiting up to 5m0s for pod "pod-submit-remove-0f99647e-45b5-4415-9923-257e2a3b3b39" in namespace "pods-9888" to be "running"
    Jun 28 07:49:40.744: INFO: Pod "pod-submit-remove-0f99647e-45b5-4415-9923-257e2a3b3b39": Phase="Pending", Reason="", readiness=false. Elapsed: 4.464083ms
    Jun 28 07:49:42.749: INFO: Pod "pod-submit-remove-0f99647e-45b5-4415-9923-257e2a3b3b39": Phase="Running", Reason="", readiness=true. Elapsed: 2.009603705s
    Jun 28 07:49:42.750: INFO: Pod "pod-submit-remove-0f99647e-45b5-4415-9923-257e2a3b3b39" satisfied condition "running"
    STEP: deleting the pod gracefully 06/28/23 07:49:42.754
    STEP: verifying pod deletion was observed 06/28/23 07:49:42.762
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jun 28 07:49:45.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-9888" for this suite. 06/28/23 07:49:45.101
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] DNS
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 07:49:45.108
Jun 28 07:49:45.108: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename dns 06/28/23 07:49:45.109
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:49:45.12
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:49:45.125
[It] should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
STEP: Creating a test headless service 06/28/23 07:49:45.129
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3607.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-3607.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3607.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-3607.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3607.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-3607.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3607.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-3607.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3607.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-3607.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3607.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-3607.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 64.34.20.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.20.34.64_udp@PTR;check="$$(dig +tcp +noall +answer +search 64.34.20.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.20.34.64_tcp@PTR;sleep 1; done
 06/28/23 07:49:45.146
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3607.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-3607.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3607.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-3607.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3607.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-3607.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3607.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-3607.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3607.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-3607.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3607.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-3607.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 64.34.20.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.20.34.64_udp@PTR;check="$$(dig +tcp +noall +answer +search 64.34.20.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.20.34.64_tcp@PTR;sleep 1; done
 06/28/23 07:49:45.146
STEP: creating a pod to probe DNS 06/28/23 07:49:45.146
STEP: submitting the pod to kubernetes 06/28/23 07:49:45.146
Jun 28 07:49:45.159: INFO: Waiting up to 15m0s for pod "dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2" in namespace "dns-3607" to be "running"
Jun 28 07:49:45.165: INFO: Pod "dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.694212ms
Jun 28 07:49:47.171: INFO: Pod "dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2": Phase="Running", Reason="", readiness=true. Elapsed: 2.011830197s
Jun 28 07:49:47.171: INFO: Pod "dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2" satisfied condition "running"
STEP: retrieving the pod 06/28/23 07:49:47.171
STEP: looking for the results for each expected name from probers 06/28/23 07:49:47.175
Jun 28 07:49:47.343: INFO: Unable to read wheezy_udp@dns-test-service.dns-3607.svc.cluster.local from pod dns-3607/dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2: the server could not find the requested resource (get pods dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2)
Jun 28 07:49:47.386: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3607.svc.cluster.local from pod dns-3607/dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2: the server could not find the requested resource (get pods dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2)
Jun 28 07:49:47.396: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3607.svc.cluster.local from pod dns-3607/dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2: the server could not find the requested resource (get pods dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2)
Jun 28 07:49:47.403: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3607.svc.cluster.local from pod dns-3607/dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2: the server could not find the requested resource (get pods dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2)
Jun 28 07:49:47.435: INFO: Unable to read jessie_udp@dns-test-service.dns-3607.svc.cluster.local from pod dns-3607/dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2: the server could not find the requested resource (get pods dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2)
Jun 28 07:49:47.444: INFO: Unable to read jessie_tcp@dns-test-service.dns-3607.svc.cluster.local from pod dns-3607/dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2: the server could not find the requested resource (get pods dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2)
Jun 28 07:49:47.451: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3607.svc.cluster.local from pod dns-3607/dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2: the server could not find the requested resource (get pods dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2)
Jun 28 07:49:47.458: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3607.svc.cluster.local from pod dns-3607/dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2: the server could not find the requested resource (get pods dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2)
Jun 28 07:49:47.483: INFO: Lookups using dns-3607/dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2 failed for: [wheezy_udp@dns-test-service.dns-3607.svc.cluster.local wheezy_tcp@dns-test-service.dns-3607.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3607.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3607.svc.cluster.local jessie_udp@dns-test-service.dns-3607.svc.cluster.local jessie_tcp@dns-test-service.dns-3607.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3607.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3607.svc.cluster.local]

Jun 28 07:49:52.491: INFO: Unable to read wheezy_udp@dns-test-service.dns-3607.svc.cluster.local from pod dns-3607/dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2: the server could not find the requested resource (get pods dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2)
Jun 28 07:49:52.535: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3607.svc.cluster.local from pod dns-3607/dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2: the server could not find the requested resource (get pods dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2)
Jun 28 07:49:52.542: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3607.svc.cluster.local from pod dns-3607/dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2: the server could not find the requested resource (get pods dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2)
Jun 28 07:49:52.549: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3607.svc.cluster.local from pod dns-3607/dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2: the server could not find the requested resource (get pods dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2)
Jun 28 07:49:52.584: INFO: Unable to read jessie_udp@dns-test-service.dns-3607.svc.cluster.local from pod dns-3607/dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2: the server could not find the requested resource (get pods dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2)
Jun 28 07:49:52.591: INFO: Unable to read jessie_tcp@dns-test-service.dns-3607.svc.cluster.local from pod dns-3607/dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2: the server could not find the requested resource (get pods dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2)
Jun 28 07:49:52.597: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3607.svc.cluster.local from pod dns-3607/dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2: the server could not find the requested resource (get pods dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2)
Jun 28 07:49:52.604: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3607.svc.cluster.local from pod dns-3607/dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2: the server could not find the requested resource (get pods dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2)
Jun 28 07:49:52.629: INFO: Lookups using dns-3607/dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2 failed for: [wheezy_udp@dns-test-service.dns-3607.svc.cluster.local wheezy_tcp@dns-test-service.dns-3607.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3607.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3607.svc.cluster.local jessie_udp@dns-test-service.dns-3607.svc.cluster.local jessie_tcp@dns-test-service.dns-3607.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3607.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3607.svc.cluster.local]

Jun 28 07:49:57.490: INFO: Unable to read wheezy_udp@dns-test-service.dns-3607.svc.cluster.local from pod dns-3607/dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2: the server could not find the requested resource (get pods dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2)
Jun 28 07:49:57.534: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3607.svc.cluster.local from pod dns-3607/dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2: the server could not find the requested resource (get pods dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2)
Jun 28 07:49:57.541: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3607.svc.cluster.local from pod dns-3607/dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2: the server could not find the requested resource (get pods dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2)
Jun 28 07:49:57.548: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3607.svc.cluster.local from pod dns-3607/dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2: the server could not find the requested resource (get pods dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2)
Jun 28 07:49:57.581: INFO: Unable to read jessie_udp@dns-test-service.dns-3607.svc.cluster.local from pod dns-3607/dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2: the server could not find the requested resource (get pods dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2)
Jun 28 07:49:57.587: INFO: Unable to read jessie_tcp@dns-test-service.dns-3607.svc.cluster.local from pod dns-3607/dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2: the server could not find the requested resource (get pods dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2)
Jun 28 07:49:57.594: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3607.svc.cluster.local from pod dns-3607/dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2: the server could not find the requested resource (get pods dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2)
Jun 28 07:49:57.600: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3607.svc.cluster.local from pod dns-3607/dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2: the server could not find the requested resource (get pods dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2)
Jun 28 07:49:57.626: INFO: Lookups using dns-3607/dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2 failed for: [wheezy_udp@dns-test-service.dns-3607.svc.cluster.local wheezy_tcp@dns-test-service.dns-3607.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3607.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3607.svc.cluster.local jessie_udp@dns-test-service.dns-3607.svc.cluster.local jessie_tcp@dns-test-service.dns-3607.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3607.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3607.svc.cluster.local]

Jun 28 07:50:02.499: INFO: Unable to read wheezy_udp@dns-test-service.dns-3607.svc.cluster.local from pod dns-3607/dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2: the server could not find the requested resource (get pods dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2)
Jun 28 07:50:02.543: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3607.svc.cluster.local from pod dns-3607/dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2: the server could not find the requested resource (get pods dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2)
Jun 28 07:50:02.552: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3607.svc.cluster.local from pod dns-3607/dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2: the server could not find the requested resource (get pods dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2)
Jun 28 07:50:02.560: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3607.svc.cluster.local from pod dns-3607/dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2: the server could not find the requested resource (get pods dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2)
Jun 28 07:50:02.602: INFO: Unable to read jessie_udp@dns-test-service.dns-3607.svc.cluster.local from pod dns-3607/dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2: the server could not find the requested resource (get pods dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2)
Jun 28 07:50:02.610: INFO: Unable to read jessie_tcp@dns-test-service.dns-3607.svc.cluster.local from pod dns-3607/dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2: the server could not find the requested resource (get pods dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2)
Jun 28 07:50:02.617: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3607.svc.cluster.local from pod dns-3607/dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2: the server could not find the requested resource (get pods dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2)
Jun 28 07:50:02.625: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3607.svc.cluster.local from pod dns-3607/dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2: the server could not find the requested resource (get pods dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2)
Jun 28 07:50:02.656: INFO: Lookups using dns-3607/dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2 failed for: [wheezy_udp@dns-test-service.dns-3607.svc.cluster.local wheezy_tcp@dns-test-service.dns-3607.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3607.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3607.svc.cluster.local jessie_udp@dns-test-service.dns-3607.svc.cluster.local jessie_tcp@dns-test-service.dns-3607.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3607.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3607.svc.cluster.local]

Jun 28 07:50:07.495: INFO: Unable to read wheezy_udp@dns-test-service.dns-3607.svc.cluster.local from pod dns-3607/dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2: the server could not find the requested resource (get pods dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2)
Jun 28 07:50:07.540: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3607.svc.cluster.local from pod dns-3607/dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2: the server could not find the requested resource (get pods dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2)
Jun 28 07:50:07.547: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3607.svc.cluster.local from pod dns-3607/dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2: the server could not find the requested resource (get pods dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2)
Jun 28 07:50:07.555: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3607.svc.cluster.local from pod dns-3607/dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2: the server could not find the requested resource (get pods dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2)
Jun 28 07:50:07.594: INFO: Unable to read jessie_udp@dns-test-service.dns-3607.svc.cluster.local from pod dns-3607/dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2: the server could not find the requested resource (get pods dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2)
Jun 28 07:50:07.604: INFO: Unable to read jessie_tcp@dns-test-service.dns-3607.svc.cluster.local from pod dns-3607/dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2: the server could not find the requested resource (get pods dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2)
Jun 28 07:50:07.611: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3607.svc.cluster.local from pod dns-3607/dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2: the server could not find the requested resource (get pods dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2)
Jun 28 07:50:07.617: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3607.svc.cluster.local from pod dns-3607/dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2: the server could not find the requested resource (get pods dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2)
Jun 28 07:50:07.645: INFO: Lookups using dns-3607/dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2 failed for: [wheezy_udp@dns-test-service.dns-3607.svc.cluster.local wheezy_tcp@dns-test-service.dns-3607.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3607.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3607.svc.cluster.local jessie_udp@dns-test-service.dns-3607.svc.cluster.local jessie_tcp@dns-test-service.dns-3607.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3607.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3607.svc.cluster.local]

Jun 28 07:50:12.490: INFO: Unable to read wheezy_udp@dns-test-service.dns-3607.svc.cluster.local from pod dns-3607/dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2: the server could not find the requested resource (get pods dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2)
Jun 28 07:50:12.534: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3607.svc.cluster.local from pod dns-3607/dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2: the server could not find the requested resource (get pods dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2)
Jun 28 07:50:12.542: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3607.svc.cluster.local from pod dns-3607/dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2: the server could not find the requested resource (get pods dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2)
Jun 28 07:50:12.549: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3607.svc.cluster.local from pod dns-3607/dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2: the server could not find the requested resource (get pods dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2)
Jun 28 07:50:12.583: INFO: Unable to read jessie_udp@dns-test-service.dns-3607.svc.cluster.local from pod dns-3607/dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2: the server could not find the requested resource (get pods dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2)
Jun 28 07:50:12.590: INFO: Unable to read jessie_tcp@dns-test-service.dns-3607.svc.cluster.local from pod dns-3607/dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2: the server could not find the requested resource (get pods dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2)
Jun 28 07:50:12.597: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3607.svc.cluster.local from pod dns-3607/dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2: the server could not find the requested resource (get pods dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2)
Jun 28 07:50:12.604: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3607.svc.cluster.local from pod dns-3607/dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2: the server could not find the requested resource (get pods dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2)
Jun 28 07:50:12.630: INFO: Lookups using dns-3607/dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2 failed for: [wheezy_udp@dns-test-service.dns-3607.svc.cluster.local wheezy_tcp@dns-test-service.dns-3607.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3607.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3607.svc.cluster.local jessie_udp@dns-test-service.dns-3607.svc.cluster.local jessie_tcp@dns-test-service.dns-3607.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3607.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3607.svc.cluster.local]

Jun 28 07:50:17.624: INFO: DNS probes using dns-3607/dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2 succeeded

STEP: deleting the pod 06/28/23 07:50:17.624
STEP: deleting the test service 06/28/23 07:50:17.637
STEP: deleting the test headless service 06/28/23 07:50:17.652
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jun 28 07:50:17.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3607" for this suite. 06/28/23 07:50:17.669
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","completed":37,"skipped":637,"failed":0}
------------------------------
â€¢ [SLOW TEST] [32.574 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 07:49:45.108
    Jun 28 07:49:45.108: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename dns 06/28/23 07:49:45.109
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:49:45.12
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:49:45.125
    [It] should provide DNS for services  [Conformance]
      test/e2e/network/dns.go:137
    STEP: Creating a test headless service 06/28/23 07:49:45.129
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3607.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-3607.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3607.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-3607.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3607.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-3607.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3607.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-3607.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3607.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-3607.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3607.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-3607.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 64.34.20.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.20.34.64_udp@PTR;check="$$(dig +tcp +noall +answer +search 64.34.20.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.20.34.64_tcp@PTR;sleep 1; done
     06/28/23 07:49:45.146
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3607.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-3607.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3607.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-3607.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3607.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-3607.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3607.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-3607.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3607.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-3607.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3607.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-3607.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 64.34.20.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.20.34.64_udp@PTR;check="$$(dig +tcp +noall +answer +search 64.34.20.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.20.34.64_tcp@PTR;sleep 1; done
     06/28/23 07:49:45.146
    STEP: creating a pod to probe DNS 06/28/23 07:49:45.146
    STEP: submitting the pod to kubernetes 06/28/23 07:49:45.146
    Jun 28 07:49:45.159: INFO: Waiting up to 15m0s for pod "dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2" in namespace "dns-3607" to be "running"
    Jun 28 07:49:45.165: INFO: Pod "dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.694212ms
    Jun 28 07:49:47.171: INFO: Pod "dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2": Phase="Running", Reason="", readiness=true. Elapsed: 2.011830197s
    Jun 28 07:49:47.171: INFO: Pod "dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2" satisfied condition "running"
    STEP: retrieving the pod 06/28/23 07:49:47.171
    STEP: looking for the results for each expected name from probers 06/28/23 07:49:47.175
    Jun 28 07:49:47.343: INFO: Unable to read wheezy_udp@dns-test-service.dns-3607.svc.cluster.local from pod dns-3607/dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2: the server could not find the requested resource (get pods dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2)
    Jun 28 07:49:47.386: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3607.svc.cluster.local from pod dns-3607/dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2: the server could not find the requested resource (get pods dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2)
    Jun 28 07:49:47.396: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3607.svc.cluster.local from pod dns-3607/dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2: the server could not find the requested resource (get pods dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2)
    Jun 28 07:49:47.403: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3607.svc.cluster.local from pod dns-3607/dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2: the server could not find the requested resource (get pods dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2)
    Jun 28 07:49:47.435: INFO: Unable to read jessie_udp@dns-test-service.dns-3607.svc.cluster.local from pod dns-3607/dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2: the server could not find the requested resource (get pods dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2)
    Jun 28 07:49:47.444: INFO: Unable to read jessie_tcp@dns-test-service.dns-3607.svc.cluster.local from pod dns-3607/dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2: the server could not find the requested resource (get pods dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2)
    Jun 28 07:49:47.451: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3607.svc.cluster.local from pod dns-3607/dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2: the server could not find the requested resource (get pods dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2)
    Jun 28 07:49:47.458: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3607.svc.cluster.local from pod dns-3607/dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2: the server could not find the requested resource (get pods dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2)
    Jun 28 07:49:47.483: INFO: Lookups using dns-3607/dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2 failed for: [wheezy_udp@dns-test-service.dns-3607.svc.cluster.local wheezy_tcp@dns-test-service.dns-3607.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3607.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3607.svc.cluster.local jessie_udp@dns-test-service.dns-3607.svc.cluster.local jessie_tcp@dns-test-service.dns-3607.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3607.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3607.svc.cluster.local]

    Jun 28 07:49:52.491: INFO: Unable to read wheezy_udp@dns-test-service.dns-3607.svc.cluster.local from pod dns-3607/dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2: the server could not find the requested resource (get pods dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2)
    Jun 28 07:49:52.535: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3607.svc.cluster.local from pod dns-3607/dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2: the server could not find the requested resource (get pods dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2)
    Jun 28 07:49:52.542: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3607.svc.cluster.local from pod dns-3607/dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2: the server could not find the requested resource (get pods dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2)
    Jun 28 07:49:52.549: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3607.svc.cluster.local from pod dns-3607/dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2: the server could not find the requested resource (get pods dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2)
    Jun 28 07:49:52.584: INFO: Unable to read jessie_udp@dns-test-service.dns-3607.svc.cluster.local from pod dns-3607/dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2: the server could not find the requested resource (get pods dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2)
    Jun 28 07:49:52.591: INFO: Unable to read jessie_tcp@dns-test-service.dns-3607.svc.cluster.local from pod dns-3607/dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2: the server could not find the requested resource (get pods dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2)
    Jun 28 07:49:52.597: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3607.svc.cluster.local from pod dns-3607/dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2: the server could not find the requested resource (get pods dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2)
    Jun 28 07:49:52.604: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3607.svc.cluster.local from pod dns-3607/dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2: the server could not find the requested resource (get pods dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2)
    Jun 28 07:49:52.629: INFO: Lookups using dns-3607/dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2 failed for: [wheezy_udp@dns-test-service.dns-3607.svc.cluster.local wheezy_tcp@dns-test-service.dns-3607.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3607.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3607.svc.cluster.local jessie_udp@dns-test-service.dns-3607.svc.cluster.local jessie_tcp@dns-test-service.dns-3607.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3607.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3607.svc.cluster.local]

    Jun 28 07:49:57.490: INFO: Unable to read wheezy_udp@dns-test-service.dns-3607.svc.cluster.local from pod dns-3607/dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2: the server could not find the requested resource (get pods dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2)
    Jun 28 07:49:57.534: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3607.svc.cluster.local from pod dns-3607/dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2: the server could not find the requested resource (get pods dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2)
    Jun 28 07:49:57.541: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3607.svc.cluster.local from pod dns-3607/dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2: the server could not find the requested resource (get pods dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2)
    Jun 28 07:49:57.548: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3607.svc.cluster.local from pod dns-3607/dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2: the server could not find the requested resource (get pods dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2)
    Jun 28 07:49:57.581: INFO: Unable to read jessie_udp@dns-test-service.dns-3607.svc.cluster.local from pod dns-3607/dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2: the server could not find the requested resource (get pods dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2)
    Jun 28 07:49:57.587: INFO: Unable to read jessie_tcp@dns-test-service.dns-3607.svc.cluster.local from pod dns-3607/dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2: the server could not find the requested resource (get pods dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2)
    Jun 28 07:49:57.594: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3607.svc.cluster.local from pod dns-3607/dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2: the server could not find the requested resource (get pods dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2)
    Jun 28 07:49:57.600: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3607.svc.cluster.local from pod dns-3607/dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2: the server could not find the requested resource (get pods dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2)
    Jun 28 07:49:57.626: INFO: Lookups using dns-3607/dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2 failed for: [wheezy_udp@dns-test-service.dns-3607.svc.cluster.local wheezy_tcp@dns-test-service.dns-3607.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3607.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3607.svc.cluster.local jessie_udp@dns-test-service.dns-3607.svc.cluster.local jessie_tcp@dns-test-service.dns-3607.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3607.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3607.svc.cluster.local]

    Jun 28 07:50:02.499: INFO: Unable to read wheezy_udp@dns-test-service.dns-3607.svc.cluster.local from pod dns-3607/dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2: the server could not find the requested resource (get pods dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2)
    Jun 28 07:50:02.543: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3607.svc.cluster.local from pod dns-3607/dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2: the server could not find the requested resource (get pods dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2)
    Jun 28 07:50:02.552: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3607.svc.cluster.local from pod dns-3607/dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2: the server could not find the requested resource (get pods dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2)
    Jun 28 07:50:02.560: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3607.svc.cluster.local from pod dns-3607/dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2: the server could not find the requested resource (get pods dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2)
    Jun 28 07:50:02.602: INFO: Unable to read jessie_udp@dns-test-service.dns-3607.svc.cluster.local from pod dns-3607/dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2: the server could not find the requested resource (get pods dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2)
    Jun 28 07:50:02.610: INFO: Unable to read jessie_tcp@dns-test-service.dns-3607.svc.cluster.local from pod dns-3607/dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2: the server could not find the requested resource (get pods dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2)
    Jun 28 07:50:02.617: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3607.svc.cluster.local from pod dns-3607/dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2: the server could not find the requested resource (get pods dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2)
    Jun 28 07:50:02.625: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3607.svc.cluster.local from pod dns-3607/dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2: the server could not find the requested resource (get pods dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2)
    Jun 28 07:50:02.656: INFO: Lookups using dns-3607/dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2 failed for: [wheezy_udp@dns-test-service.dns-3607.svc.cluster.local wheezy_tcp@dns-test-service.dns-3607.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3607.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3607.svc.cluster.local jessie_udp@dns-test-service.dns-3607.svc.cluster.local jessie_tcp@dns-test-service.dns-3607.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3607.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3607.svc.cluster.local]

    Jun 28 07:50:07.495: INFO: Unable to read wheezy_udp@dns-test-service.dns-3607.svc.cluster.local from pod dns-3607/dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2: the server could not find the requested resource (get pods dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2)
    Jun 28 07:50:07.540: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3607.svc.cluster.local from pod dns-3607/dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2: the server could not find the requested resource (get pods dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2)
    Jun 28 07:50:07.547: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3607.svc.cluster.local from pod dns-3607/dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2: the server could not find the requested resource (get pods dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2)
    Jun 28 07:50:07.555: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3607.svc.cluster.local from pod dns-3607/dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2: the server could not find the requested resource (get pods dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2)
    Jun 28 07:50:07.594: INFO: Unable to read jessie_udp@dns-test-service.dns-3607.svc.cluster.local from pod dns-3607/dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2: the server could not find the requested resource (get pods dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2)
    Jun 28 07:50:07.604: INFO: Unable to read jessie_tcp@dns-test-service.dns-3607.svc.cluster.local from pod dns-3607/dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2: the server could not find the requested resource (get pods dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2)
    Jun 28 07:50:07.611: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3607.svc.cluster.local from pod dns-3607/dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2: the server could not find the requested resource (get pods dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2)
    Jun 28 07:50:07.617: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3607.svc.cluster.local from pod dns-3607/dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2: the server could not find the requested resource (get pods dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2)
    Jun 28 07:50:07.645: INFO: Lookups using dns-3607/dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2 failed for: [wheezy_udp@dns-test-service.dns-3607.svc.cluster.local wheezy_tcp@dns-test-service.dns-3607.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3607.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3607.svc.cluster.local jessie_udp@dns-test-service.dns-3607.svc.cluster.local jessie_tcp@dns-test-service.dns-3607.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3607.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3607.svc.cluster.local]

    Jun 28 07:50:12.490: INFO: Unable to read wheezy_udp@dns-test-service.dns-3607.svc.cluster.local from pod dns-3607/dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2: the server could not find the requested resource (get pods dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2)
    Jun 28 07:50:12.534: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3607.svc.cluster.local from pod dns-3607/dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2: the server could not find the requested resource (get pods dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2)
    Jun 28 07:50:12.542: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3607.svc.cluster.local from pod dns-3607/dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2: the server could not find the requested resource (get pods dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2)
    Jun 28 07:50:12.549: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3607.svc.cluster.local from pod dns-3607/dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2: the server could not find the requested resource (get pods dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2)
    Jun 28 07:50:12.583: INFO: Unable to read jessie_udp@dns-test-service.dns-3607.svc.cluster.local from pod dns-3607/dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2: the server could not find the requested resource (get pods dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2)
    Jun 28 07:50:12.590: INFO: Unable to read jessie_tcp@dns-test-service.dns-3607.svc.cluster.local from pod dns-3607/dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2: the server could not find the requested resource (get pods dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2)
    Jun 28 07:50:12.597: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3607.svc.cluster.local from pod dns-3607/dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2: the server could not find the requested resource (get pods dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2)
    Jun 28 07:50:12.604: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3607.svc.cluster.local from pod dns-3607/dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2: the server could not find the requested resource (get pods dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2)
    Jun 28 07:50:12.630: INFO: Lookups using dns-3607/dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2 failed for: [wheezy_udp@dns-test-service.dns-3607.svc.cluster.local wheezy_tcp@dns-test-service.dns-3607.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3607.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3607.svc.cluster.local jessie_udp@dns-test-service.dns-3607.svc.cluster.local jessie_tcp@dns-test-service.dns-3607.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3607.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3607.svc.cluster.local]

    Jun 28 07:50:17.624: INFO: DNS probes using dns-3607/dns-test-e89e1584-43de-4c25-bac7-fd4d594bf3f2 succeeded

    STEP: deleting the pod 06/28/23 07:50:17.624
    STEP: deleting the test service 06/28/23 07:50:17.637
    STEP: deleting the test headless service 06/28/23 07:50:17.652
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jun 28 07:50:17.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-3607" for this suite. 06/28/23 07:50:17.669
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 07:50:17.685
Jun 28 07:50:17.685: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename events 06/28/23 07:50:17.685
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:50:17.699
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:50:17.704
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
STEP: Create set of events 06/28/23 07:50:17.709
STEP: get a list of Events with a label in the current namespace 06/28/23 07:50:17.729
STEP: delete a list of events 06/28/23 07:50:17.734
Jun 28 07:50:17.734: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 06/28/23 07:50:17.754
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:187
Jun 28 07:50:17.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-1102" for this suite. 06/28/23 07:50:17.765
{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","completed":38,"skipped":656,"failed":0}
------------------------------
â€¢ [0.086 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 07:50:17.685
    Jun 28 07:50:17.685: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename events 06/28/23 07:50:17.685
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:50:17.699
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:50:17.704
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/events.go:207
    STEP: Create set of events 06/28/23 07:50:17.709
    STEP: get a list of Events with a label in the current namespace 06/28/23 07:50:17.729
    STEP: delete a list of events 06/28/23 07:50:17.734
    Jun 28 07:50:17.734: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 06/28/23 07:50:17.754
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:187
    Jun 28 07:50:17.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-1102" for this suite. 06/28/23 07:50:17.765
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 07:50:17.771
Jun 28 07:50:17.771: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename emptydir 06/28/23 07:50:17.772
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:50:17.784
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:50:17.79
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176
STEP: Creating a pod to test emptydir 0666 on node default medium 06/28/23 07:50:17.796
Jun 28 07:50:17.804: INFO: Waiting up to 5m0s for pod "pod-6fbfb3a8-4e78-41ad-86ab-60a64c2667a3" in namespace "emptydir-165" to be "Succeeded or Failed"
Jun 28 07:50:17.808: INFO: Pod "pod-6fbfb3a8-4e78-41ad-86ab-60a64c2667a3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.080933ms
Jun 28 07:50:19.813: INFO: Pod "pod-6fbfb3a8-4e78-41ad-86ab-60a64c2667a3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00927895s
Jun 28 07:50:21.814: INFO: Pod "pod-6fbfb3a8-4e78-41ad-86ab-60a64c2667a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009867195s
STEP: Saw pod success 06/28/23 07:50:21.814
Jun 28 07:50:21.814: INFO: Pod "pod-6fbfb3a8-4e78-41ad-86ab-60a64c2667a3" satisfied condition "Succeeded or Failed"
Jun 28 07:50:21.818: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod pod-6fbfb3a8-4e78-41ad-86ab-60a64c2667a3 container test-container: <nil>
STEP: delete the pod 06/28/23 07:50:21.868
Jun 28 07:50:21.882: INFO: Waiting for pod pod-6fbfb3a8-4e78-41ad-86ab-60a64c2667a3 to disappear
Jun 28 07:50:21.885: INFO: Pod pod-6fbfb3a8-4e78-41ad-86ab-60a64c2667a3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jun 28 07:50:21.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-165" for this suite. 06/28/23 07:50:21.892
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":39,"skipped":666,"failed":0}
------------------------------
â€¢ [4.127 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 07:50:17.771
    Jun 28 07:50:17.771: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename emptydir 06/28/23 07:50:17.772
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:50:17.784
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:50:17.79
    [It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:176
    STEP: Creating a pod to test emptydir 0666 on node default medium 06/28/23 07:50:17.796
    Jun 28 07:50:17.804: INFO: Waiting up to 5m0s for pod "pod-6fbfb3a8-4e78-41ad-86ab-60a64c2667a3" in namespace "emptydir-165" to be "Succeeded or Failed"
    Jun 28 07:50:17.808: INFO: Pod "pod-6fbfb3a8-4e78-41ad-86ab-60a64c2667a3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.080933ms
    Jun 28 07:50:19.813: INFO: Pod "pod-6fbfb3a8-4e78-41ad-86ab-60a64c2667a3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00927895s
    Jun 28 07:50:21.814: INFO: Pod "pod-6fbfb3a8-4e78-41ad-86ab-60a64c2667a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009867195s
    STEP: Saw pod success 06/28/23 07:50:21.814
    Jun 28 07:50:21.814: INFO: Pod "pod-6fbfb3a8-4e78-41ad-86ab-60a64c2667a3" satisfied condition "Succeeded or Failed"
    Jun 28 07:50:21.818: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod pod-6fbfb3a8-4e78-41ad-86ab-60a64c2667a3 container test-container: <nil>
    STEP: delete the pod 06/28/23 07:50:21.868
    Jun 28 07:50:21.882: INFO: Waiting for pod pod-6fbfb3a8-4e78-41ad-86ab-60a64c2667a3 to disappear
    Jun 28 07:50:21.885: INFO: Pod pod-6fbfb3a8-4e78-41ad-86ab-60a64c2667a3 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jun 28 07:50:21.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-165" for this suite. 06/28/23 07:50:21.892
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:695
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 07:50:21.899
Jun 28 07:50:21.899: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename statefulset 06/28/23 07:50:21.9
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:50:21.911
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:50:21.916
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-3242 06/28/23 07:50:21.92
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:695
STEP: Creating stateful set ss in namespace statefulset-3242 06/28/23 07:50:21.925
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-3242 06/28/23 07:50:21.933
Jun 28 07:50:21.936: INFO: Found 0 stateful pods, waiting for 1
Jun 28 07:50:31.942: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 06/28/23 07:50:31.942
Jun 28 07:50:31.947: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=statefulset-3242 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 28 07:50:32.382: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 28 07:50:32.382: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 28 07:50:32.382: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 28 07:50:32.386: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jun 28 07:50:42.392: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun 28 07:50:42.392: INFO: Waiting for statefulset status.replicas updated to 0
Jun 28 07:50:42.412: INFO: POD   NODE                             PHASE    GRACE  CONDITIONS
Jun 28 07:50:42.412: INFO: ss-0  ske-rhel-749f7d55c8xdd8b6-ct4cp  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-28 07:50:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-28 07:50:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-28 07:50:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-28 07:50:21 +0000 UTC  }]
Jun 28 07:50:42.412: INFO: 
Jun 28 07:50:42.412: INFO: StatefulSet ss has not reached scale 3, at 1
Jun 28 07:50:43.420: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.994746033s
Jun 28 07:50:44.426: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.987476736s
Jun 28 07:50:45.433: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.98147634s
Jun 28 07:50:46.438: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.975278613s
Jun 28 07:50:47.444: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.969603094s
Jun 28 07:50:48.451: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.963191067s
Jun 28 07:50:49.457: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.957034836s
Jun 28 07:50:50.464: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.950434587s
Jun 28 07:50:51.470: INFO: Verifying statefulset ss doesn't scale past 3 for another 944.233178ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-3242 06/28/23 07:50:52.47
Jun 28 07:50:52.477: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=statefulset-3242 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 07:50:52.918: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun 28 07:50:52.918: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 28 07:50:52.918: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 28 07:50:52.918: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=statefulset-3242 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 07:50:53.402: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jun 28 07:50:53.402: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 28 07:50:53.402: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 28 07:50:53.402: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=statefulset-3242 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 07:50:53.859: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jun 28 07:50:53.859: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 28 07:50:53.859: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 28 07:50:53.865: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 28 07:50:53.865: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 28 07:50:53.865: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod 06/28/23 07:50:53.865
Jun 28 07:50:53.870: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=statefulset-3242 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 28 07:50:54.303: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 28 07:50:54.303: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 28 07:50:54.303: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 28 07:50:54.303: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=statefulset-3242 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 28 07:50:54.735: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 28 07:50:54.736: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 28 07:50:54.736: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 28 07:50:54.736: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=statefulset-3242 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 28 07:50:55.172: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 28 07:50:55.172: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 28 07:50:55.172: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 28 07:50:55.172: INFO: Waiting for statefulset status.replicas updated to 0
Jun 28 07:50:55.179: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Jun 28 07:51:05.191: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun 28 07:51:05.191: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jun 28 07:51:05.191: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jun 28 07:51:05.210: INFO: POD   NODE                             PHASE    GRACE  CONDITIONS
Jun 28 07:51:05.210: INFO: ss-0  ske-rhel-749f7d55c8xdd8b6-ct4cp  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-28 07:50:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-28 07:50:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-28 07:50:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-28 07:50:21 +0000 UTC  }]
Jun 28 07:51:05.210: INFO: ss-1  ske-rhel-749f7d55c8xdd8b6-zxlfv  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-28 07:50:42 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-28 07:50:54 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-28 07:50:54 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-28 07:50:42 +0000 UTC  }]
Jun 28 07:51:05.210: INFO: ss-2  ske-rhel-749f7d55c8xdd8b6-srshq  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-28 07:50:42 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-28 07:50:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-28 07:50:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-28 07:50:42 +0000 UTC  }]
Jun 28 07:51:05.210: INFO: 
Jun 28 07:51:05.210: INFO: StatefulSet ss has not reached scale 0, at 3
Jun 28 07:51:06.215: INFO: POD   NODE                             PHASE    GRACE  CONDITIONS
Jun 28 07:51:06.215: INFO: ss-0  ske-rhel-749f7d55c8xdd8b6-ct4cp  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-28 07:50:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-28 07:50:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-28 07:50:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-28 07:50:21 +0000 UTC  }]
Jun 28 07:51:06.215: INFO: ss-2  ske-rhel-749f7d55c8xdd8b6-srshq  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-28 07:50:42 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-28 07:50:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-28 07:50:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-28 07:50:42 +0000 UTC  }]
Jun 28 07:51:06.215: INFO: 
Jun 28 07:51:06.215: INFO: StatefulSet ss has not reached scale 0, at 2
Jun 28 07:51:07.221: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.988380125s
Jun 28 07:51:08.226: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.982513166s
Jun 28 07:51:09.232: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.977816438s
Jun 28 07:51:10.240: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.9716154s
Jun 28 07:51:11.246: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.964324586s
Jun 28 07:51:12.252: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.957241018s
Jun 28 07:51:13.259: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.951254237s
Jun 28 07:51:14.266: INFO: Verifying statefulset ss doesn't scale past 0 for another 944.803739ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-3242 06/28/23 07:51:15.267
Jun 28 07:51:15.272: INFO: Scaling statefulset ss to 0
Jun 28 07:51:15.287: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jun 28 07:51:15.292: INFO: Deleting all statefulset in ns statefulset-3242
Jun 28 07:51:15.296: INFO: Scaling statefulset ss to 0
Jun 28 07:51:15.311: INFO: Waiting for statefulset status.replicas updated to 0
Jun 28 07:51:15.315: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jun 28 07:51:15.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3242" for this suite. 06/28/23 07:51:15.339
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","completed":40,"skipped":668,"failed":0}
------------------------------
â€¢ [SLOW TEST] [53.447 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    test/e2e/apps/statefulset.go:695

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 07:50:21.899
    Jun 28 07:50:21.899: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename statefulset 06/28/23 07:50:21.9
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:50:21.911
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:50:21.916
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-3242 06/28/23 07:50:21.92
    [It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
      test/e2e/apps/statefulset.go:695
    STEP: Creating stateful set ss in namespace statefulset-3242 06/28/23 07:50:21.925
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-3242 06/28/23 07:50:21.933
    Jun 28 07:50:21.936: INFO: Found 0 stateful pods, waiting for 1
    Jun 28 07:50:31.942: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 06/28/23 07:50:31.942
    Jun 28 07:50:31.947: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=statefulset-3242 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jun 28 07:50:32.382: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jun 28 07:50:32.382: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jun 28 07:50:32.382: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jun 28 07:50:32.386: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Jun 28 07:50:42.392: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jun 28 07:50:42.392: INFO: Waiting for statefulset status.replicas updated to 0
    Jun 28 07:50:42.412: INFO: POD   NODE                             PHASE    GRACE  CONDITIONS
    Jun 28 07:50:42.412: INFO: ss-0  ske-rhel-749f7d55c8xdd8b6-ct4cp  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-28 07:50:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-28 07:50:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-28 07:50:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-28 07:50:21 +0000 UTC  }]
    Jun 28 07:50:42.412: INFO: 
    Jun 28 07:50:42.412: INFO: StatefulSet ss has not reached scale 3, at 1
    Jun 28 07:50:43.420: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.994746033s
    Jun 28 07:50:44.426: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.987476736s
    Jun 28 07:50:45.433: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.98147634s
    Jun 28 07:50:46.438: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.975278613s
    Jun 28 07:50:47.444: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.969603094s
    Jun 28 07:50:48.451: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.963191067s
    Jun 28 07:50:49.457: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.957034836s
    Jun 28 07:50:50.464: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.950434587s
    Jun 28 07:50:51.470: INFO: Verifying statefulset ss doesn't scale past 3 for another 944.233178ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-3242 06/28/23 07:50:52.47
    Jun 28 07:50:52.477: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=statefulset-3242 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jun 28 07:50:52.918: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jun 28 07:50:52.918: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jun 28 07:50:52.918: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jun 28 07:50:52.918: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=statefulset-3242 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jun 28 07:50:53.402: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Jun 28 07:50:53.402: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jun 28 07:50:53.402: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jun 28 07:50:53.402: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=statefulset-3242 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jun 28 07:50:53.859: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Jun 28 07:50:53.859: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jun 28 07:50:53.859: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jun 28 07:50:53.865: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Jun 28 07:50:53.865: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Jun 28 07:50:53.865: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Scale down will not halt with unhealthy stateful pod 06/28/23 07:50:53.865
    Jun 28 07:50:53.870: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=statefulset-3242 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jun 28 07:50:54.303: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jun 28 07:50:54.303: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jun 28 07:50:54.303: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jun 28 07:50:54.303: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=statefulset-3242 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jun 28 07:50:54.735: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jun 28 07:50:54.736: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jun 28 07:50:54.736: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jun 28 07:50:54.736: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=statefulset-3242 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jun 28 07:50:55.172: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jun 28 07:50:55.172: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jun 28 07:50:55.172: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jun 28 07:50:55.172: INFO: Waiting for statefulset status.replicas updated to 0
    Jun 28 07:50:55.179: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
    Jun 28 07:51:05.191: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jun 28 07:51:05.191: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Jun 28 07:51:05.191: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Jun 28 07:51:05.210: INFO: POD   NODE                             PHASE    GRACE  CONDITIONS
    Jun 28 07:51:05.210: INFO: ss-0  ske-rhel-749f7d55c8xdd8b6-ct4cp  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-28 07:50:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-28 07:50:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-28 07:50:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-28 07:50:21 +0000 UTC  }]
    Jun 28 07:51:05.210: INFO: ss-1  ske-rhel-749f7d55c8xdd8b6-zxlfv  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-28 07:50:42 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-28 07:50:54 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-28 07:50:54 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-28 07:50:42 +0000 UTC  }]
    Jun 28 07:51:05.210: INFO: ss-2  ske-rhel-749f7d55c8xdd8b6-srshq  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-28 07:50:42 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-28 07:50:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-28 07:50:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-28 07:50:42 +0000 UTC  }]
    Jun 28 07:51:05.210: INFO: 
    Jun 28 07:51:05.210: INFO: StatefulSet ss has not reached scale 0, at 3
    Jun 28 07:51:06.215: INFO: POD   NODE                             PHASE    GRACE  CONDITIONS
    Jun 28 07:51:06.215: INFO: ss-0  ske-rhel-749f7d55c8xdd8b6-ct4cp  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-28 07:50:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-28 07:50:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-28 07:50:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-28 07:50:21 +0000 UTC  }]
    Jun 28 07:51:06.215: INFO: ss-2  ske-rhel-749f7d55c8xdd8b6-srshq  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-28 07:50:42 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-28 07:50:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-28 07:50:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-28 07:50:42 +0000 UTC  }]
    Jun 28 07:51:06.215: INFO: 
    Jun 28 07:51:06.215: INFO: StatefulSet ss has not reached scale 0, at 2
    Jun 28 07:51:07.221: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.988380125s
    Jun 28 07:51:08.226: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.982513166s
    Jun 28 07:51:09.232: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.977816438s
    Jun 28 07:51:10.240: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.9716154s
    Jun 28 07:51:11.246: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.964324586s
    Jun 28 07:51:12.252: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.957241018s
    Jun 28 07:51:13.259: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.951254237s
    Jun 28 07:51:14.266: INFO: Verifying statefulset ss doesn't scale past 0 for another 944.803739ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-3242 06/28/23 07:51:15.267
    Jun 28 07:51:15.272: INFO: Scaling statefulset ss to 0
    Jun 28 07:51:15.287: INFO: Waiting for statefulset status.replicas updated to 0
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jun 28 07:51:15.292: INFO: Deleting all statefulset in ns statefulset-3242
    Jun 28 07:51:15.296: INFO: Scaling statefulset ss to 0
    Jun 28 07:51:15.311: INFO: Waiting for statefulset status.replicas updated to 0
    Jun 28 07:51:15.315: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jun 28 07:51:15.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-3242" for this suite. 06/28/23 07:51:15.339
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 07:51:15.346
Jun 28 07:51:15.346: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename services 06/28/23 07:51:15.347
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:51:15.361
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:51:15.366
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-3995 06/28/23 07:51:15.37
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 06/28/23 07:51:15.381
STEP: creating service externalsvc in namespace services-3995 06/28/23 07:51:15.381
STEP: creating replication controller externalsvc in namespace services-3995 06/28/23 07:51:15.39
I0628 07:51:15.395992      18 runners.go:193] Created replication controller with name: externalsvc, namespace: services-3995, replica count: 2
I0628 07:51:18.448018      18 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName 06/28/23 07:51:18.452
Jun 28 07:51:18.468: INFO: Creating new exec pod
Jun 28 07:51:18.474: INFO: Waiting up to 5m0s for pod "execpod28gll" in namespace "services-3995" to be "running"
Jun 28 07:51:18.479: INFO: Pod "execpod28gll": Phase="Pending", Reason="", readiness=false. Elapsed: 4.839508ms
Jun 28 07:51:20.485: INFO: Pod "execpod28gll": Phase="Running", Reason="", readiness=true. Elapsed: 2.010916525s
Jun 28 07:51:20.485: INFO: Pod "execpod28gll" satisfied condition "running"
Jun 28 07:51:20.485: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-3995 exec execpod28gll -- /bin/sh -x -c nslookup clusterip-service.services-3995.svc.cluster.local'
Jun 28 07:51:21.015: INFO: stderr: "+ nslookup clusterip-service.services-3995.svc.cluster.local\n"
Jun 28 07:51:21.015: INFO: stdout: "Server:\t\t172.20.0.10\nAddress:\t172.20.0.10#53\n\nclusterip-service.services-3995.svc.cluster.local\tcanonical name = externalsvc.services-3995.svc.cluster.local.\nName:\texternalsvc.services-3995.svc.cluster.local\nAddress: 172.20.121.41\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-3995, will wait for the garbage collector to delete the pods 06/28/23 07:51:21.015
Jun 28 07:51:21.077: INFO: Deleting ReplicationController externalsvc took: 7.204067ms
Jun 28 07:51:21.178: INFO: Terminating ReplicationController externalsvc pods took: 100.44998ms
Jun 28 07:51:23.396: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jun 28 07:51:23.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3995" for this suite. 06/28/23 07:51:23.415
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","completed":41,"skipped":691,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.076 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 07:51:15.346
    Jun 28 07:51:15.346: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename services 06/28/23 07:51:15.347
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:51:15.361
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:51:15.366
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ClusterIP to ExternalName [Conformance]
      test/e2e/network/service.go:1481
    STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-3995 06/28/23 07:51:15.37
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 06/28/23 07:51:15.381
    STEP: creating service externalsvc in namespace services-3995 06/28/23 07:51:15.381
    STEP: creating replication controller externalsvc in namespace services-3995 06/28/23 07:51:15.39
    I0628 07:51:15.395992      18 runners.go:193] Created replication controller with name: externalsvc, namespace: services-3995, replica count: 2
    I0628 07:51:18.448018      18 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the ClusterIP service to type=ExternalName 06/28/23 07:51:18.452
    Jun 28 07:51:18.468: INFO: Creating new exec pod
    Jun 28 07:51:18.474: INFO: Waiting up to 5m0s for pod "execpod28gll" in namespace "services-3995" to be "running"
    Jun 28 07:51:18.479: INFO: Pod "execpod28gll": Phase="Pending", Reason="", readiness=false. Elapsed: 4.839508ms
    Jun 28 07:51:20.485: INFO: Pod "execpod28gll": Phase="Running", Reason="", readiness=true. Elapsed: 2.010916525s
    Jun 28 07:51:20.485: INFO: Pod "execpod28gll" satisfied condition "running"
    Jun 28 07:51:20.485: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-3995 exec execpod28gll -- /bin/sh -x -c nslookup clusterip-service.services-3995.svc.cluster.local'
    Jun 28 07:51:21.015: INFO: stderr: "+ nslookup clusterip-service.services-3995.svc.cluster.local\n"
    Jun 28 07:51:21.015: INFO: stdout: "Server:\t\t172.20.0.10\nAddress:\t172.20.0.10#53\n\nclusterip-service.services-3995.svc.cluster.local\tcanonical name = externalsvc.services-3995.svc.cluster.local.\nName:\texternalsvc.services-3995.svc.cluster.local\nAddress: 172.20.121.41\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-3995, will wait for the garbage collector to delete the pods 06/28/23 07:51:21.015
    Jun 28 07:51:21.077: INFO: Deleting ReplicationController externalsvc took: 7.204067ms
    Jun 28 07:51:21.178: INFO: Terminating ReplicationController externalsvc pods took: 100.44998ms
    Jun 28 07:51:23.396: INFO: Cleaning up the ClusterIP to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jun 28 07:51:23.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-3995" for this suite. 06/28/23 07:51:23.415
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-auth] ServiceAccounts
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 07:51:23.422
Jun 28 07:51:23.422: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename svcaccounts 06/28/23 07:51:23.423
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:51:23.44
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:51:23.445
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739
Jun 28 07:51:23.454: INFO: Got root ca configmap in namespace "svcaccounts-5147"
Jun 28 07:51:23.462: INFO: Deleted root ca configmap in namespace "svcaccounts-5147"
STEP: waiting for a new root ca configmap created 06/28/23 07:51:23.963
Jun 28 07:51:23.968: INFO: Recreated root ca configmap in namespace "svcaccounts-5147"
Jun 28 07:51:23.974: INFO: Updated root ca configmap in namespace "svcaccounts-5147"
STEP: waiting for the root ca configmap reconciled 06/28/23 07:51:24.475
Jun 28 07:51:24.483: INFO: Reconciled root ca configmap in namespace "svcaccounts-5147"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Jun 28 07:51:24.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-5147" for this suite. 06/28/23 07:51:24.49
{"msg":"PASSED [sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]","completed":42,"skipped":696,"failed":0}
------------------------------
â€¢ [1.075 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 07:51:23.422
    Jun 28 07:51:23.422: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename svcaccounts 06/28/23 07:51:23.423
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:51:23.44
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:51:23.445
    [It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
      test/e2e/auth/service_accounts.go:739
    Jun 28 07:51:23.454: INFO: Got root ca configmap in namespace "svcaccounts-5147"
    Jun 28 07:51:23.462: INFO: Deleted root ca configmap in namespace "svcaccounts-5147"
    STEP: waiting for a new root ca configmap created 06/28/23 07:51:23.963
    Jun 28 07:51:23.968: INFO: Recreated root ca configmap in namespace "svcaccounts-5147"
    Jun 28 07:51:23.974: INFO: Updated root ca configmap in namespace "svcaccounts-5147"
    STEP: waiting for the root ca configmap reconciled 06/28/23 07:51:24.475
    Jun 28 07:51:24.483: INFO: Reconciled root ca configmap in namespace "svcaccounts-5147"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Jun 28 07:51:24.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-5147" for this suite. 06/28/23 07:51:24.49
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 07:51:24.499
Jun 28 07:51:24.499: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename runtimeclass 06/28/23 07:51:24.5
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:51:24.512
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:51:24.517
[It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
Jun 28 07:51:24.538: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-3631 to be scheduled
Jun 28 07:51:24.542: INFO: 1 pods are not scheduled: [runtimeclass-3631/test-runtimeclass-runtimeclass-3631-preconfigured-handler-mwbc8(12fc1f81-c4f8-472d-ba3f-f0094a02d21d)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Jun 28 07:51:26.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-3631" for this suite. 06/28/23 07:51:26.564
{"msg":"PASSED [sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]","completed":43,"skipped":726,"failed":0}
------------------------------
â€¢ [2.072 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 07:51:24.499
    Jun 28 07:51:24.499: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename runtimeclass 06/28/23 07:51:24.5
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:51:24.512
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:51:24.517
    [It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:129
    Jun 28 07:51:24.538: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-3631 to be scheduled
    Jun 28 07:51:24.542: INFO: 1 pods are not scheduled: [runtimeclass-3631/test-runtimeclass-runtimeclass-3631-preconfigured-handler-mwbc8(12fc1f81-c4f8-472d-ba3f-f0094a02d21d)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Jun 28 07:51:26.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-3631" for this suite. 06/28/23 07:51:26.564
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 07:51:26.571
Jun 28 07:51:26.571: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename daemonsets 06/28/23 07:51:26.572
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:51:26.586
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:51:26.59
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822
STEP: Creating simple DaemonSet "daemon-set" 06/28/23 07:51:26.629
STEP: Check that daemon pods launch on every node of the cluster. 06/28/23 07:51:26.638
Jun 28 07:51:26.650: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 28 07:51:26.650: INFO: Node ske-rhel-749f7d55c8xdd8b6-ct4cp is running 0 daemon pod, expected 1
Jun 28 07:51:27.665: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jun 28 07:51:27.665: INFO: Node ske-rhel-749f7d55c8xdd8b6-ct4cp is running 0 daemon pod, expected 1
Jun 28 07:51:28.662: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jun 28 07:51:28.663: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: listing all DeamonSets 06/28/23 07:51:28.667
STEP: DeleteCollection of the DaemonSets 06/28/23 07:51:28.674
STEP: Verify that ReplicaSets have been deleted 06/28/23 07:51:28.685
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
Jun 28 07:51:28.710: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"51023"},"items":null}

Jun 28 07:51:28.716: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"51023"},"items":[{"metadata":{"name":"daemon-set-gg2rn","generateName":"daemon-set-","namespace":"daemonsets-9805","uid":"7233c785-a932-49ab-a194-e80882efcb13","resourceVersion":"51022","creationTimestamp":"2023-06-28T07:51:26Z","deletionTimestamp":"2023-06-28T07:51:58Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"370222b5b794f63797094edb255d41a4d5cff75ddbf94c0e33449b75eb0c62f9","cni.projectcalico.org/podIP":"172.21.122.86/32","cni.projectcalico.org/podIPs":"172.21.122.86/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"5050fab9-6915-4d9e-9113-7deac63d753f","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-06-28T07:51:26Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5050fab9-6915-4d9e-9113-7deac63d753f\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-06-28T07:51:27Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-06-28T07:51:27Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.122.86\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-rxxd7","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-rxxd7","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ske-rhel-749f7d55c8xdd8b6-srshq","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ske-rhel-749f7d55c8xdd8b6-srshq"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-28T07:51:26Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-28T07:51:27Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-28T07:51:27Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-28T07:51:26Z"}],"hostIP":"192.168.11.4","podIP":"172.21.122.86","podIPs":[{"ip":"172.21.122.86"}],"startTime":"2023-06-28T07:51:26Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-06-28T07:51:27Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://c15376bef2714332050d63431c0691a841a0b062995f2727c642ab5070b77535","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-mhxq9","generateName":"daemon-set-","namespace":"daemonsets-9805","uid":"7548e04a-ed65-4cf6-b266-bf2d3e6c1307","resourceVersion":"51020","creationTimestamp":"2023-06-28T07:51:26Z","deletionTimestamp":"2023-06-28T07:51:58Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"4a5dc882f69a03f781340e95088de71b7836df05b814c208fc817ec1d53edd5a","cni.projectcalico.org/podIP":"172.21.30.94/32","cni.projectcalico.org/podIPs":"172.21.30.94/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"5050fab9-6915-4d9e-9113-7deac63d753f","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-06-28T07:51:26Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5050fab9-6915-4d9e-9113-7deac63d753f\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-06-28T07:51:27Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-06-28T07:51:28Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.30.94\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-v5zhx","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-v5zhx","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ske-rhel-749f7d55c8xdd8b6-zxlfv","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ske-rhel-749f7d55c8xdd8b6-zxlfv"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-28T07:51:26Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-28T07:51:28Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-28T07:51:28Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-28T07:51:26Z"}],"hostIP":"192.168.11.5","podIP":"172.21.30.94","podIPs":[{"ip":"172.21.30.94"}],"startTime":"2023-06-28T07:51:26Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-06-28T07:51:27Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://f1308e2d09345ceede33ad392ab8b8159c8c9db1f886de8b1e090c2ff49141b8","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-t5k8m","generateName":"daemon-set-","namespace":"daemonsets-9805","uid":"680dda98-2e0a-4d4a-b397-46f793088567","resourceVersion":"51021","creationTimestamp":"2023-06-28T07:51:26Z","deletionTimestamp":"2023-06-28T07:51:58Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"5b50e0112071c1a5d365b78116d239b22593831cc6df0637c3506a069486790b","cni.projectcalico.org/podIP":"172.21.122.48/32","cni.projectcalico.org/podIPs":"172.21.122.48/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"5050fab9-6915-4d9e-9113-7deac63d753f","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-06-28T07:51:26Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5050fab9-6915-4d9e-9113-7deac63d753f\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-06-28T07:51:27Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-06-28T07:51:28Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.122.48\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-r8zpf","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-r8zpf","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ske-rhel-749f7d55c8xdd8b6-ct4cp","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ske-rhel-749f7d55c8xdd8b6-ct4cp"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-28T07:51:26Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-28T07:51:28Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-28T07:51:28Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-28T07:51:26Z"}],"hostIP":"192.168.11.3","podIP":"172.21.122.48","podIPs":[{"ip":"172.21.122.48"}],"startTime":"2023-06-28T07:51:26Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-06-28T07:51:27Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://2dcc2bc038a22e0403a38f9f8034e84584e21e7e0fad6e79b3450c47d22af3fd","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Jun 28 07:51:28.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9805" for this suite. 06/28/23 07:51:28.748
{"msg":"PASSED [sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]","completed":44,"skipped":745,"failed":0}
------------------------------
â€¢ [2.183 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 07:51:26.571
    Jun 28 07:51:26.571: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename daemonsets 06/28/23 07:51:26.572
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:51:26.586
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:51:26.59
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should list and delete a collection of DaemonSets [Conformance]
      test/e2e/apps/daemon_set.go:822
    STEP: Creating simple DaemonSet "daemon-set" 06/28/23 07:51:26.629
    STEP: Check that daemon pods launch on every node of the cluster. 06/28/23 07:51:26.638
    Jun 28 07:51:26.650: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun 28 07:51:26.650: INFO: Node ske-rhel-749f7d55c8xdd8b6-ct4cp is running 0 daemon pod, expected 1
    Jun 28 07:51:27.665: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jun 28 07:51:27.665: INFO: Node ske-rhel-749f7d55c8xdd8b6-ct4cp is running 0 daemon pod, expected 1
    Jun 28 07:51:28.662: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jun 28 07:51:28.663: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: listing all DeamonSets 06/28/23 07:51:28.667
    STEP: DeleteCollection of the DaemonSets 06/28/23 07:51:28.674
    STEP: Verify that ReplicaSets have been deleted 06/28/23 07:51:28.685
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    Jun 28 07:51:28.710: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"51023"},"items":null}

    Jun 28 07:51:28.716: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"51023"},"items":[{"metadata":{"name":"daemon-set-gg2rn","generateName":"daemon-set-","namespace":"daemonsets-9805","uid":"7233c785-a932-49ab-a194-e80882efcb13","resourceVersion":"51022","creationTimestamp":"2023-06-28T07:51:26Z","deletionTimestamp":"2023-06-28T07:51:58Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"370222b5b794f63797094edb255d41a4d5cff75ddbf94c0e33449b75eb0c62f9","cni.projectcalico.org/podIP":"172.21.122.86/32","cni.projectcalico.org/podIPs":"172.21.122.86/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"5050fab9-6915-4d9e-9113-7deac63d753f","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-06-28T07:51:26Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5050fab9-6915-4d9e-9113-7deac63d753f\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-06-28T07:51:27Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-06-28T07:51:27Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.122.86\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-rxxd7","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-rxxd7","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ske-rhel-749f7d55c8xdd8b6-srshq","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ske-rhel-749f7d55c8xdd8b6-srshq"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-28T07:51:26Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-28T07:51:27Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-28T07:51:27Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-28T07:51:26Z"}],"hostIP":"192.168.11.4","podIP":"172.21.122.86","podIPs":[{"ip":"172.21.122.86"}],"startTime":"2023-06-28T07:51:26Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-06-28T07:51:27Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://c15376bef2714332050d63431c0691a841a0b062995f2727c642ab5070b77535","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-mhxq9","generateName":"daemon-set-","namespace":"daemonsets-9805","uid":"7548e04a-ed65-4cf6-b266-bf2d3e6c1307","resourceVersion":"51020","creationTimestamp":"2023-06-28T07:51:26Z","deletionTimestamp":"2023-06-28T07:51:58Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"4a5dc882f69a03f781340e95088de71b7836df05b814c208fc817ec1d53edd5a","cni.projectcalico.org/podIP":"172.21.30.94/32","cni.projectcalico.org/podIPs":"172.21.30.94/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"5050fab9-6915-4d9e-9113-7deac63d753f","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-06-28T07:51:26Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5050fab9-6915-4d9e-9113-7deac63d753f\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-06-28T07:51:27Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-06-28T07:51:28Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.30.94\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-v5zhx","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-v5zhx","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ske-rhel-749f7d55c8xdd8b6-zxlfv","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ske-rhel-749f7d55c8xdd8b6-zxlfv"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-28T07:51:26Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-28T07:51:28Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-28T07:51:28Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-28T07:51:26Z"}],"hostIP":"192.168.11.5","podIP":"172.21.30.94","podIPs":[{"ip":"172.21.30.94"}],"startTime":"2023-06-28T07:51:26Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-06-28T07:51:27Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://f1308e2d09345ceede33ad392ab8b8159c8c9db1f886de8b1e090c2ff49141b8","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-t5k8m","generateName":"daemon-set-","namespace":"daemonsets-9805","uid":"680dda98-2e0a-4d4a-b397-46f793088567","resourceVersion":"51021","creationTimestamp":"2023-06-28T07:51:26Z","deletionTimestamp":"2023-06-28T07:51:58Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"5b50e0112071c1a5d365b78116d239b22593831cc6df0637c3506a069486790b","cni.projectcalico.org/podIP":"172.21.122.48/32","cni.projectcalico.org/podIPs":"172.21.122.48/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"5050fab9-6915-4d9e-9113-7deac63d753f","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-06-28T07:51:26Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5050fab9-6915-4d9e-9113-7deac63d753f\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-06-28T07:51:27Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-06-28T07:51:28Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.122.48\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-r8zpf","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-r8zpf","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ske-rhel-749f7d55c8xdd8b6-ct4cp","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ske-rhel-749f7d55c8xdd8b6-ct4cp"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-28T07:51:26Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-28T07:51:28Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-28T07:51:28Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-28T07:51:26Z"}],"hostIP":"192.168.11.3","podIP":"172.21.122.48","podIPs":[{"ip":"172.21.122.48"}],"startTime":"2023-06-28T07:51:26Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-06-28T07:51:27Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://2dcc2bc038a22e0403a38f9f8034e84584e21e7e0fad6e79b3450c47d22af3fd","started":true}],"qosClass":"BestEffort"}}]}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Jun 28 07:51:28.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-9805" for this suite. 06/28/23 07:51:28.748
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 07:51:28.755
Jun 28 07:51:28.755: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename resourcequota 06/28/23 07:51:28.757
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:51:28.768
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:51:28.773
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316
STEP: Counting existing ResourceQuota 06/28/23 07:51:45.784
STEP: Creating a ResourceQuota 06/28/23 07:51:50.789
STEP: Ensuring resource quota status is calculated 06/28/23 07:51:50.797
STEP: Creating a ConfigMap 06/28/23 07:51:52.802
STEP: Ensuring resource quota status captures configMap creation 06/28/23 07:51:52.814
STEP: Deleting a ConfigMap 06/28/23 07:51:54.82
STEP: Ensuring resource quota status released usage 06/28/23 07:51:54.828
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jun 28 07:51:56.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5615" for this suite. 06/28/23 07:51:56.844
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","completed":45,"skipped":772,"failed":0}
------------------------------
â€¢ [SLOW TEST] [28.097 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 07:51:28.755
    Jun 28 07:51:28.755: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename resourcequota 06/28/23 07:51:28.757
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:51:28.768
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:51:28.773
    [It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
      test/e2e/apimachinery/resource_quota.go:316
    STEP: Counting existing ResourceQuota 06/28/23 07:51:45.784
    STEP: Creating a ResourceQuota 06/28/23 07:51:50.789
    STEP: Ensuring resource quota status is calculated 06/28/23 07:51:50.797
    STEP: Creating a ConfigMap 06/28/23 07:51:52.802
    STEP: Ensuring resource quota status captures configMap creation 06/28/23 07:51:52.814
    STEP: Deleting a ConfigMap 06/28/23 07:51:54.82
    STEP: Ensuring resource quota status released usage 06/28/23 07:51:54.828
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jun 28 07:51:56.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-5615" for this suite. 06/28/23 07:51:56.844
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 07:51:56.853
Jun 28 07:51:56.853: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename webhook 06/28/23 07:51:56.855
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:51:56.87
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:51:56.876
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 06/28/23 07:51:56.896
STEP: Create role binding to let webhook read extension-apiserver-authentication 06/28/23 07:51:57.533
STEP: Deploying the webhook pod 06/28/23 07:51:57.543
STEP: Wait for the deployment to be ready 06/28/23 07:51:57.557
Jun 28 07:51:57.566: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 06/28/23 07:51:59.584
STEP: Verifying the service has paired with the endpoint 06/28/23 07:51:59.596
Jun 28 07:52:00.599: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196
STEP: Registering the webhook via the AdmissionRegistration API 06/28/23 07:52:00.605
STEP: create a pod that should be denied by the webhook 06/28/23 07:52:00.718
STEP: create a pod that causes the webhook to hang 06/28/23 07:52:00.857
STEP: create a configmap that should be denied by the webhook 06/28/23 07:52:10.867
STEP: create a configmap that should be admitted by the webhook 06/28/23 07:52:10.996
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 06/28/23 07:52:11.052
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 06/28/23 07:52:11.108
STEP: create a namespace that bypass the webhook 06/28/23 07:52:11.16
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 06/28/23 07:52:11.169
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jun 28 07:52:11.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6312" for this suite. 06/28/23 07:52:11.209
STEP: Destroying namespace "webhook-6312-markers" for this suite. 06/28/23 07:52:11.217
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","completed":46,"skipped":787,"failed":0}
------------------------------
â€¢ [SLOW TEST] [14.414 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 07:51:56.853
    Jun 28 07:51:56.853: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename webhook 06/28/23 07:51:56.855
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:51:56.87
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:51:56.876
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 06/28/23 07:51:56.896
    STEP: Create role binding to let webhook read extension-apiserver-authentication 06/28/23 07:51:57.533
    STEP: Deploying the webhook pod 06/28/23 07:51:57.543
    STEP: Wait for the deployment to be ready 06/28/23 07:51:57.557
    Jun 28 07:51:57.566: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 06/28/23 07:51:59.584
    STEP: Verifying the service has paired with the endpoint 06/28/23 07:51:59.596
    Jun 28 07:52:00.599: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny pod and configmap creation [Conformance]
      test/e2e/apimachinery/webhook.go:196
    STEP: Registering the webhook via the AdmissionRegistration API 06/28/23 07:52:00.605
    STEP: create a pod that should be denied by the webhook 06/28/23 07:52:00.718
    STEP: create a pod that causes the webhook to hang 06/28/23 07:52:00.857
    STEP: create a configmap that should be denied by the webhook 06/28/23 07:52:10.867
    STEP: create a configmap that should be admitted by the webhook 06/28/23 07:52:10.996
    STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 06/28/23 07:52:11.052
    STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 06/28/23 07:52:11.108
    STEP: create a namespace that bypass the webhook 06/28/23 07:52:11.16
    STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 06/28/23 07:52:11.169
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jun 28 07:52:11.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-6312" for this suite. 06/28/23 07:52:11.209
    STEP: Destroying namespace "webhook-6312-markers" for this suite. 06/28/23 07:52:11.217
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 07:52:11.268
Jun 28 07:52:11.269: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename container-probe 06/28/23 07:52:11.269
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:52:11.286
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:52:11.292
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148
STEP: Creating pod busybox-7f3975a3-a358-4c24-8e35-cb46d1589aa3 in namespace container-probe-578 06/28/23 07:52:11.297
Jun 28 07:52:11.308: INFO: Waiting up to 5m0s for pod "busybox-7f3975a3-a358-4c24-8e35-cb46d1589aa3" in namespace "container-probe-578" to be "not pending"
Jun 28 07:52:11.312: INFO: Pod "busybox-7f3975a3-a358-4c24-8e35-cb46d1589aa3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.686209ms
Jun 28 07:52:13.318: INFO: Pod "busybox-7f3975a3-a358-4c24-8e35-cb46d1589aa3": Phase="Running", Reason="", readiness=true. Elapsed: 2.010242415s
Jun 28 07:52:13.318: INFO: Pod "busybox-7f3975a3-a358-4c24-8e35-cb46d1589aa3" satisfied condition "not pending"
Jun 28 07:52:13.318: INFO: Started pod busybox-7f3975a3-a358-4c24-8e35-cb46d1589aa3 in namespace container-probe-578
STEP: checking the pod's current state and verifying that restartCount is present 06/28/23 07:52:13.318
Jun 28 07:52:13.323: INFO: Initial restart count of pod busybox-7f3975a3-a358-4c24-8e35-cb46d1589aa3 is 0
STEP: deleting the pod 06/28/23 07:56:14.062
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jun 28 07:56:14.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-578" for this suite. 06/28/23 07:56:14.089
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","completed":47,"skipped":817,"failed":0}
------------------------------
â€¢ [SLOW TEST] [242.828 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 07:52:11.268
    Jun 28 07:52:11.269: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename container-probe 06/28/23 07:52:11.269
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:52:11.286
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:52:11.292
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:148
    STEP: Creating pod busybox-7f3975a3-a358-4c24-8e35-cb46d1589aa3 in namespace container-probe-578 06/28/23 07:52:11.297
    Jun 28 07:52:11.308: INFO: Waiting up to 5m0s for pod "busybox-7f3975a3-a358-4c24-8e35-cb46d1589aa3" in namespace "container-probe-578" to be "not pending"
    Jun 28 07:52:11.312: INFO: Pod "busybox-7f3975a3-a358-4c24-8e35-cb46d1589aa3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.686209ms
    Jun 28 07:52:13.318: INFO: Pod "busybox-7f3975a3-a358-4c24-8e35-cb46d1589aa3": Phase="Running", Reason="", readiness=true. Elapsed: 2.010242415s
    Jun 28 07:52:13.318: INFO: Pod "busybox-7f3975a3-a358-4c24-8e35-cb46d1589aa3" satisfied condition "not pending"
    Jun 28 07:52:13.318: INFO: Started pod busybox-7f3975a3-a358-4c24-8e35-cb46d1589aa3 in namespace container-probe-578
    STEP: checking the pod's current state and verifying that restartCount is present 06/28/23 07:52:13.318
    Jun 28 07:52:13.323: INFO: Initial restart count of pod busybox-7f3975a3-a358-4c24-8e35-cb46d1589aa3 is 0
    STEP: deleting the pod 06/28/23 07:56:14.062
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jun 28 07:56:14.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-578" for this suite. 06/28/23 07:56:14.089
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Job
  should delete a job [Conformance]
  test/e2e/apps/job.go:309
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 07:56:14.098
Jun 28 07:56:14.098: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename job 06/28/23 07:56:14.099
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:56:14.113
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:56:14.118
[It] should delete a job [Conformance]
  test/e2e/apps/job.go:309
STEP: Creating a job 06/28/23 07:56:14.123
STEP: Ensuring active pods == parallelism 06/28/23 07:56:14.13
STEP: delete a job 06/28/23 07:56:16.137
STEP: deleting Job.batch foo in namespace job-6727, will wait for the garbage collector to delete the pods 06/28/23 07:56:16.137
Jun 28 07:56:16.201: INFO: Deleting Job.batch foo took: 8.582644ms
Jun 28 07:56:16.402: INFO: Terminating Job.batch foo pods took: 200.450051ms
STEP: Ensuring job was deleted 06/28/23 07:56:48.803
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Jun 28 07:56:48.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-6727" for this suite. 06/28/23 07:56:48.817
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","completed":48,"skipped":826,"failed":0}
------------------------------
â€¢ [SLOW TEST] [34.726 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  test/e2e/apps/job.go:309

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 07:56:14.098
    Jun 28 07:56:14.098: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename job 06/28/23 07:56:14.099
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:56:14.113
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:56:14.118
    [It] should delete a job [Conformance]
      test/e2e/apps/job.go:309
    STEP: Creating a job 06/28/23 07:56:14.123
    STEP: Ensuring active pods == parallelism 06/28/23 07:56:14.13
    STEP: delete a job 06/28/23 07:56:16.137
    STEP: deleting Job.batch foo in namespace job-6727, will wait for the garbage collector to delete the pods 06/28/23 07:56:16.137
    Jun 28 07:56:16.201: INFO: Deleting Job.batch foo took: 8.582644ms
    Jun 28 07:56:16.402: INFO: Terminating Job.batch foo pods took: 200.450051ms
    STEP: Ensuring job was deleted 06/28/23 07:56:48.803
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Jun 28 07:56:48.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-6727" for this suite. 06/28/23 07:56:48.817
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Pods
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 07:56:48.824
Jun 28 07:56:48.825: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename pods 06/28/23 07:56:48.826
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:56:48.839
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:56:48.845
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397
STEP: creating the pod 06/28/23 07:56:48.851
STEP: submitting the pod to kubernetes 06/28/23 07:56:48.852
Jun 28 07:56:48.861: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-d6d78651-e14c-4b10-8f04-99d934ed142f" in namespace "pods-4633" to be "running and ready"
Jun 28 07:56:48.867: INFO: Pod "pod-update-activedeadlineseconds-d6d78651-e14c-4b10-8f04-99d934ed142f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.359279ms
Jun 28 07:56:48.867: INFO: The phase of Pod pod-update-activedeadlineseconds-d6d78651-e14c-4b10-8f04-99d934ed142f is Pending, waiting for it to be Running (with Ready = true)
Jun 28 07:56:50.873: INFO: Pod "pod-update-activedeadlineseconds-d6d78651-e14c-4b10-8f04-99d934ed142f": Phase="Running", Reason="", readiness=true. Elapsed: 2.012181896s
Jun 28 07:56:50.873: INFO: The phase of Pod pod-update-activedeadlineseconds-d6d78651-e14c-4b10-8f04-99d934ed142f is Running (Ready = true)
Jun 28 07:56:50.873: INFO: Pod "pod-update-activedeadlineseconds-d6d78651-e14c-4b10-8f04-99d934ed142f" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 06/28/23 07:56:50.878
STEP: updating the pod 06/28/23 07:56:50.883
Jun 28 07:56:51.401: INFO: Successfully updated pod "pod-update-activedeadlineseconds-d6d78651-e14c-4b10-8f04-99d934ed142f"
Jun 28 07:56:51.401: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-d6d78651-e14c-4b10-8f04-99d934ed142f" in namespace "pods-4633" to be "terminated with reason DeadlineExceeded"
Jun 28 07:56:51.406: INFO: Pod "pod-update-activedeadlineseconds-d6d78651-e14c-4b10-8f04-99d934ed142f": Phase="Running", Reason="", readiness=true. Elapsed: 4.750626ms
Jun 28 07:56:53.413: INFO: Pod "pod-update-activedeadlineseconds-d6d78651-e14c-4b10-8f04-99d934ed142f": Phase="Running", Reason="", readiness=false. Elapsed: 2.0113922s
Jun 28 07:56:55.413: INFO: Pod "pod-update-activedeadlineseconds-d6d78651-e14c-4b10-8f04-99d934ed142f": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.012071077s
Jun 28 07:56:55.413: INFO: Pod "pod-update-activedeadlineseconds-d6d78651-e14c-4b10-8f04-99d934ed142f" satisfied condition "terminated with reason DeadlineExceeded"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jun 28 07:56:55.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4633" for this suite. 06/28/23 07:56:55.422
{"msg":"PASSED [sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","completed":49,"skipped":827,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.606 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 07:56:48.824
    Jun 28 07:56:48.825: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename pods 06/28/23 07:56:48.826
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:56:48.839
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:56:48.845
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:397
    STEP: creating the pod 06/28/23 07:56:48.851
    STEP: submitting the pod to kubernetes 06/28/23 07:56:48.852
    Jun 28 07:56:48.861: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-d6d78651-e14c-4b10-8f04-99d934ed142f" in namespace "pods-4633" to be "running and ready"
    Jun 28 07:56:48.867: INFO: Pod "pod-update-activedeadlineseconds-d6d78651-e14c-4b10-8f04-99d934ed142f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.359279ms
    Jun 28 07:56:48.867: INFO: The phase of Pod pod-update-activedeadlineseconds-d6d78651-e14c-4b10-8f04-99d934ed142f is Pending, waiting for it to be Running (with Ready = true)
    Jun 28 07:56:50.873: INFO: Pod "pod-update-activedeadlineseconds-d6d78651-e14c-4b10-8f04-99d934ed142f": Phase="Running", Reason="", readiness=true. Elapsed: 2.012181896s
    Jun 28 07:56:50.873: INFO: The phase of Pod pod-update-activedeadlineseconds-d6d78651-e14c-4b10-8f04-99d934ed142f is Running (Ready = true)
    Jun 28 07:56:50.873: INFO: Pod "pod-update-activedeadlineseconds-d6d78651-e14c-4b10-8f04-99d934ed142f" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 06/28/23 07:56:50.878
    STEP: updating the pod 06/28/23 07:56:50.883
    Jun 28 07:56:51.401: INFO: Successfully updated pod "pod-update-activedeadlineseconds-d6d78651-e14c-4b10-8f04-99d934ed142f"
    Jun 28 07:56:51.401: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-d6d78651-e14c-4b10-8f04-99d934ed142f" in namespace "pods-4633" to be "terminated with reason DeadlineExceeded"
    Jun 28 07:56:51.406: INFO: Pod "pod-update-activedeadlineseconds-d6d78651-e14c-4b10-8f04-99d934ed142f": Phase="Running", Reason="", readiness=true. Elapsed: 4.750626ms
    Jun 28 07:56:53.413: INFO: Pod "pod-update-activedeadlineseconds-d6d78651-e14c-4b10-8f04-99d934ed142f": Phase="Running", Reason="", readiness=false. Elapsed: 2.0113922s
    Jun 28 07:56:55.413: INFO: Pod "pod-update-activedeadlineseconds-d6d78651-e14c-4b10-8f04-99d934ed142f": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.012071077s
    Jun 28 07:56:55.413: INFO: Pod "pod-update-activedeadlineseconds-d6d78651-e14c-4b10-8f04-99d934ed142f" satisfied condition "terminated with reason DeadlineExceeded"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jun 28 07:56:55.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-4633" for this suite. 06/28/23 07:56:55.422
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 07:56:55.433
Jun 28 07:56:55.433: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename gc 06/28/23 07:56:55.434
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:56:55.45
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:56:55.455
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
STEP: create the rc 06/28/23 07:56:55.469
STEP: delete the rc 06/28/23 07:57:00.484
STEP: wait for the rc to be deleted 06/28/23 07:57:00.495
Jun 28 07:57:01.531: INFO: 80 pods remaining
Jun 28 07:57:01.531: INFO: 80 pods has nil DeletionTimestamp
Jun 28 07:57:01.531: INFO: 
Jun 28 07:57:02.528: INFO: 71 pods remaining
Jun 28 07:57:02.528: INFO: 71 pods has nil DeletionTimestamp
Jun 28 07:57:02.528: INFO: 
Jun 28 07:57:03.554: INFO: 60 pods remaining
Jun 28 07:57:03.554: INFO: 60 pods has nil DeletionTimestamp
Jun 28 07:57:03.554: INFO: 
Jun 28 07:57:04.525: INFO: 40 pods remaining
Jun 28 07:57:04.525: INFO: 40 pods has nil DeletionTimestamp
Jun 28 07:57:04.525: INFO: 
Jun 28 07:57:05.531: INFO: 31 pods remaining
Jun 28 07:57:05.531: INFO: 31 pods has nil DeletionTimestamp
Jun 28 07:57:05.531: INFO: 
Jun 28 07:57:06.519: INFO: 20 pods remaining
Jun 28 07:57:06.519: INFO: 20 pods has nil DeletionTimestamp
Jun 28 07:57:06.519: INFO: 
STEP: Gathering metrics 06/28/23 07:57:07.512
W0628 07:57:07.528880      18 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jun 28 07:57:07.529: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Jun 28 07:57:07.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1753" for this suite. 06/28/23 07:57:07.537
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","completed":50,"skipped":851,"failed":0}
------------------------------
â€¢ [SLOW TEST] [12.114 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 07:56:55.433
    Jun 28 07:56:55.433: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename gc 06/28/23 07:56:55.434
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:56:55.45
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:56:55.455
    [It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:650
    STEP: create the rc 06/28/23 07:56:55.469
    STEP: delete the rc 06/28/23 07:57:00.484
    STEP: wait for the rc to be deleted 06/28/23 07:57:00.495
    Jun 28 07:57:01.531: INFO: 80 pods remaining
    Jun 28 07:57:01.531: INFO: 80 pods has nil DeletionTimestamp
    Jun 28 07:57:01.531: INFO: 
    Jun 28 07:57:02.528: INFO: 71 pods remaining
    Jun 28 07:57:02.528: INFO: 71 pods has nil DeletionTimestamp
    Jun 28 07:57:02.528: INFO: 
    Jun 28 07:57:03.554: INFO: 60 pods remaining
    Jun 28 07:57:03.554: INFO: 60 pods has nil DeletionTimestamp
    Jun 28 07:57:03.554: INFO: 
    Jun 28 07:57:04.525: INFO: 40 pods remaining
    Jun 28 07:57:04.525: INFO: 40 pods has nil DeletionTimestamp
    Jun 28 07:57:04.525: INFO: 
    Jun 28 07:57:05.531: INFO: 31 pods remaining
    Jun 28 07:57:05.531: INFO: 31 pods has nil DeletionTimestamp
    Jun 28 07:57:05.531: INFO: 
    Jun 28 07:57:06.519: INFO: 20 pods remaining
    Jun 28 07:57:06.519: INFO: 20 pods has nil DeletionTimestamp
    Jun 28 07:57:06.519: INFO: 
    STEP: Gathering metrics 06/28/23 07:57:07.512
    W0628 07:57:07.528880      18 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Jun 28 07:57:07.529: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Jun 28 07:57:07.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-1753" for this suite. 06/28/23 07:57:07.537
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 07:57:07.547
Jun 28 07:57:07.547: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename resourcequota 06/28/23 07:57:07.553
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:57:07.572
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:57:07.58
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382
STEP: Counting existing ResourceQuota 06/28/23 07:57:07.588
STEP: Creating a ResourceQuota 06/28/23 07:57:12.594
STEP: Ensuring resource quota status is calculated 06/28/23 07:57:12.601
STEP: Creating a ReplicationController 06/28/23 07:57:14.607
STEP: Ensuring resource quota status captures replication controller creation 06/28/23 07:57:14.62
STEP: Deleting a ReplicationController 06/28/23 07:57:16.626
STEP: Ensuring resource quota status released usage 06/28/23 07:57:16.634
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jun 28 07:57:18.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3700" for this suite. 06/28/23 07:57:18.651
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","completed":51,"skipped":853,"failed":0}
------------------------------
â€¢ [SLOW TEST] [11.113 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 07:57:07.547
    Jun 28 07:57:07.547: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename resourcequota 06/28/23 07:57:07.553
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:57:07.572
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:57:07.58
    [It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
      test/e2e/apimachinery/resource_quota.go:382
    STEP: Counting existing ResourceQuota 06/28/23 07:57:07.588
    STEP: Creating a ResourceQuota 06/28/23 07:57:12.594
    STEP: Ensuring resource quota status is calculated 06/28/23 07:57:12.601
    STEP: Creating a ReplicationController 06/28/23 07:57:14.607
    STEP: Ensuring resource quota status captures replication controller creation 06/28/23 07:57:14.62
    STEP: Deleting a ReplicationController 06/28/23 07:57:16.626
    STEP: Ensuring resource quota status released usage 06/28/23 07:57:16.634
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jun 28 07:57:18.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-3700" for this suite. 06/28/23 07:57:18.651
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 07:57:18.661
Jun 28 07:57:18.661: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename svcaccounts 06/28/23 07:57:18.664
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:57:18.687
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:57:18.692
[It] should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158
Jun 28 07:57:18.717: INFO: created pod pod-service-account-defaultsa
Jun 28 07:57:18.717: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Jun 28 07:57:18.723: INFO: created pod pod-service-account-mountsa
Jun 28 07:57:18.723: INFO: pod pod-service-account-mountsa service account token volume mount: true
Jun 28 07:57:18.728: INFO: created pod pod-service-account-nomountsa
Jun 28 07:57:18.728: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Jun 28 07:57:18.733: INFO: created pod pod-service-account-defaultsa-mountspec
Jun 28 07:57:18.733: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Jun 28 07:57:18.739: INFO: created pod pod-service-account-mountsa-mountspec
Jun 28 07:57:18.739: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Jun 28 07:57:18.747: INFO: created pod pod-service-account-nomountsa-mountspec
Jun 28 07:57:18.747: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Jun 28 07:57:18.753: INFO: created pod pod-service-account-defaultsa-nomountspec
Jun 28 07:57:18.753: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Jun 28 07:57:18.762: INFO: created pod pod-service-account-mountsa-nomountspec
Jun 28 07:57:18.762: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Jun 28 07:57:18.769: INFO: created pod pod-service-account-nomountsa-nomountspec
Jun 28 07:57:18.769: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Jun 28 07:57:18.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-9063" for this suite. 06/28/23 07:57:18.778
{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","completed":52,"skipped":863,"failed":0}
------------------------------
â€¢ [0.125 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 07:57:18.661
    Jun 28 07:57:18.661: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename svcaccounts 06/28/23 07:57:18.664
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:57:18.687
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:57:18.692
    [It] should allow opting out of API token automount  [Conformance]
      test/e2e/auth/service_accounts.go:158
    Jun 28 07:57:18.717: INFO: created pod pod-service-account-defaultsa
    Jun 28 07:57:18.717: INFO: pod pod-service-account-defaultsa service account token volume mount: true
    Jun 28 07:57:18.723: INFO: created pod pod-service-account-mountsa
    Jun 28 07:57:18.723: INFO: pod pod-service-account-mountsa service account token volume mount: true
    Jun 28 07:57:18.728: INFO: created pod pod-service-account-nomountsa
    Jun 28 07:57:18.728: INFO: pod pod-service-account-nomountsa service account token volume mount: false
    Jun 28 07:57:18.733: INFO: created pod pod-service-account-defaultsa-mountspec
    Jun 28 07:57:18.733: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
    Jun 28 07:57:18.739: INFO: created pod pod-service-account-mountsa-mountspec
    Jun 28 07:57:18.739: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
    Jun 28 07:57:18.747: INFO: created pod pod-service-account-nomountsa-mountspec
    Jun 28 07:57:18.747: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
    Jun 28 07:57:18.753: INFO: created pod pod-service-account-defaultsa-nomountspec
    Jun 28 07:57:18.753: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
    Jun 28 07:57:18.762: INFO: created pod pod-service-account-mountsa-nomountspec
    Jun 28 07:57:18.762: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
    Jun 28 07:57:18.769: INFO: created pod pod-service-account-nomountsa-nomountspec
    Jun 28 07:57:18.769: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Jun 28 07:57:18.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-9063" for this suite. 06/28/23 07:57:18.778
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 07:57:18.792
Jun 28 07:57:18.798: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename secrets 06/28/23 07:57:18.824
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:57:18.84
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:57:18.845
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46
STEP: Creating secret with name secret-test-7d2fbc64-e0e6-4115-b09c-596c7ea4c812 06/28/23 07:57:18.851
STEP: Creating a pod to test consume secrets 06/28/23 07:57:18.856
Jun 28 07:57:18.865: INFO: Waiting up to 5m0s for pod "pod-secrets-807b6016-cfb2-4d94-8fe4-bdeeae1ef968" in namespace "secrets-9576" to be "Succeeded or Failed"
Jun 28 07:57:18.870: INFO: Pod "pod-secrets-807b6016-cfb2-4d94-8fe4-bdeeae1ef968": Phase="Pending", Reason="", readiness=false. Elapsed: 4.521857ms
Jun 28 07:57:20.876: INFO: Pod "pod-secrets-807b6016-cfb2-4d94-8fe4-bdeeae1ef968": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010989254s
Jun 28 07:57:22.876: INFO: Pod "pod-secrets-807b6016-cfb2-4d94-8fe4-bdeeae1ef968": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010505099s
Jun 28 07:57:24.878: INFO: Pod "pod-secrets-807b6016-cfb2-4d94-8fe4-bdeeae1ef968": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012123766s
STEP: Saw pod success 06/28/23 07:57:24.878
Jun 28 07:57:24.878: INFO: Pod "pod-secrets-807b6016-cfb2-4d94-8fe4-bdeeae1ef968" satisfied condition "Succeeded or Failed"
Jun 28 07:57:24.882: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-zxlfv pod pod-secrets-807b6016-cfb2-4d94-8fe4-bdeeae1ef968 container secret-volume-test: <nil>
STEP: delete the pod 06/28/23 07:57:25.144
Jun 28 07:57:25.161: INFO: Waiting for pod pod-secrets-807b6016-cfb2-4d94-8fe4-bdeeae1ef968 to disappear
Jun 28 07:57:25.167: INFO: Pod pod-secrets-807b6016-cfb2-4d94-8fe4-bdeeae1ef968 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jun 28 07:57:25.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9576" for this suite. 06/28/23 07:57:25.176
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","completed":53,"skipped":962,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.394 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 07:57:18.792
    Jun 28 07:57:18.798: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename secrets 06/28/23 07:57:18.824
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:57:18.84
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:57:18.845
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:46
    STEP: Creating secret with name secret-test-7d2fbc64-e0e6-4115-b09c-596c7ea4c812 06/28/23 07:57:18.851
    STEP: Creating a pod to test consume secrets 06/28/23 07:57:18.856
    Jun 28 07:57:18.865: INFO: Waiting up to 5m0s for pod "pod-secrets-807b6016-cfb2-4d94-8fe4-bdeeae1ef968" in namespace "secrets-9576" to be "Succeeded or Failed"
    Jun 28 07:57:18.870: INFO: Pod "pod-secrets-807b6016-cfb2-4d94-8fe4-bdeeae1ef968": Phase="Pending", Reason="", readiness=false. Elapsed: 4.521857ms
    Jun 28 07:57:20.876: INFO: Pod "pod-secrets-807b6016-cfb2-4d94-8fe4-bdeeae1ef968": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010989254s
    Jun 28 07:57:22.876: INFO: Pod "pod-secrets-807b6016-cfb2-4d94-8fe4-bdeeae1ef968": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010505099s
    Jun 28 07:57:24.878: INFO: Pod "pod-secrets-807b6016-cfb2-4d94-8fe4-bdeeae1ef968": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012123766s
    STEP: Saw pod success 06/28/23 07:57:24.878
    Jun 28 07:57:24.878: INFO: Pod "pod-secrets-807b6016-cfb2-4d94-8fe4-bdeeae1ef968" satisfied condition "Succeeded or Failed"
    Jun 28 07:57:24.882: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-zxlfv pod pod-secrets-807b6016-cfb2-4d94-8fe4-bdeeae1ef968 container secret-volume-test: <nil>
    STEP: delete the pod 06/28/23 07:57:25.144
    Jun 28 07:57:25.161: INFO: Waiting for pod pod-secrets-807b6016-cfb2-4d94-8fe4-bdeeae1ef968 to disappear
    Jun 28 07:57:25.167: INFO: Pod pod-secrets-807b6016-cfb2-4d94-8fe4-bdeeae1ef968 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jun 28 07:57:25.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-9576" for this suite. 06/28/23 07:57:25.176
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 07:57:25.19
Jun 28 07:57:25.190: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 06/28/23 07:57:25.191
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:57:25.207
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:57:25.212
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
STEP: Setting up the test 06/28/23 07:57:25.217
STEP: Creating hostNetwork=false pod 06/28/23 07:57:25.217
Jun 28 07:57:25.228: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-8117" to be "running and ready"
Jun 28 07:57:25.234: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.944025ms
Jun 28 07:57:25.234: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Jun 28 07:57:27.241: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012673022s
Jun 28 07:57:27.241: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Jun 28 07:57:29.241: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.013084612s
Jun 28 07:57:29.241: INFO: The phase of Pod test-pod is Running (Ready = true)
Jun 28 07:57:29.241: INFO: Pod "test-pod" satisfied condition "running and ready"
STEP: Creating hostNetwork=true pod 06/28/23 07:57:29.246
Jun 28 07:57:29.255: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-8117" to be "running and ready"
Jun 28 07:57:29.260: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.640438ms
Jun 28 07:57:29.260: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Jun 28 07:57:31.267: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.011857635s
Jun 28 07:57:31.267: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
Jun 28 07:57:31.267: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
STEP: Running the test 06/28/23 07:57:31.271
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 06/28/23 07:57:31.271
Jun 28 07:57:31.272: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8117 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 28 07:57:31.272: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
Jun 28 07:57:31.272: INFO: ExecWithOptions: Clientset creation
Jun 28 07:57:31.272: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8117/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jun 28 07:57:31.681: INFO: Exec stderr: ""
Jun 28 07:57:31.681: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8117 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 28 07:57:31.681: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
Jun 28 07:57:31.682: INFO: ExecWithOptions: Clientset creation
Jun 28 07:57:31.682: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8117/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jun 28 07:57:32.158: INFO: Exec stderr: ""
Jun 28 07:57:32.159: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8117 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 28 07:57:32.159: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
Jun 28 07:57:32.159: INFO: ExecWithOptions: Clientset creation
Jun 28 07:57:32.159: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8117/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jun 28 07:57:32.638: INFO: Exec stderr: ""
Jun 28 07:57:32.638: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8117 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 28 07:57:32.638: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
Jun 28 07:57:32.639: INFO: ExecWithOptions: Clientset creation
Jun 28 07:57:32.639: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8117/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jun 28 07:57:33.076: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 06/28/23 07:57:33.076
Jun 28 07:57:33.076: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8117 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 28 07:57:33.076: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
Jun 28 07:57:33.077: INFO: ExecWithOptions: Clientset creation
Jun 28 07:57:33.077: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8117/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Jun 28 07:57:33.549: INFO: Exec stderr: ""
Jun 28 07:57:33.549: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8117 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 28 07:57:33.549: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
Jun 28 07:57:33.549: INFO: ExecWithOptions: Clientset creation
Jun 28 07:57:33.549: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8117/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Jun 28 07:57:33.982: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 06/28/23 07:57:33.982
Jun 28 07:57:33.982: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8117 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 28 07:57:33.982: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
Jun 28 07:57:33.983: INFO: ExecWithOptions: Clientset creation
Jun 28 07:57:33.983: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8117/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jun 28 07:57:34.415: INFO: Exec stderr: ""
Jun 28 07:57:34.415: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8117 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 28 07:57:34.415: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
Jun 28 07:57:34.415: INFO: ExecWithOptions: Clientset creation
Jun 28 07:57:34.415: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8117/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jun 28 07:57:34.749: INFO: Exec stderr: ""
Jun 28 07:57:34.749: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8117 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 28 07:57:34.749: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
Jun 28 07:57:34.750: INFO: ExecWithOptions: Clientset creation
Jun 28 07:57:34.750: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8117/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jun 28 07:57:35.072: INFO: Exec stderr: ""
Jun 28 07:57:35.072: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8117 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 28 07:57:35.073: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
Jun 28 07:57:35.073: INFO: ExecWithOptions: Clientset creation
Jun 28 07:57:35.073: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8117/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jun 28 07:57:35.572: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/framework.go:187
Jun 28 07:57:35.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-8117" for this suite. 06/28/23 07:57:35.595
{"msg":"PASSED [sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","completed":54,"skipped":998,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.418 seconds]
[sig-node] KubeletManagedEtcHosts
test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 07:57:25.19
    Jun 28 07:57:25.190: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 06/28/23 07:57:25.191
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:57:25.207
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:57:25.212
    [It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet_etc_hosts.go:63
    STEP: Setting up the test 06/28/23 07:57:25.217
    STEP: Creating hostNetwork=false pod 06/28/23 07:57:25.217
    Jun 28 07:57:25.228: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-8117" to be "running and ready"
    Jun 28 07:57:25.234: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.944025ms
    Jun 28 07:57:25.234: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Jun 28 07:57:27.241: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012673022s
    Jun 28 07:57:27.241: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Jun 28 07:57:29.241: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.013084612s
    Jun 28 07:57:29.241: INFO: The phase of Pod test-pod is Running (Ready = true)
    Jun 28 07:57:29.241: INFO: Pod "test-pod" satisfied condition "running and ready"
    STEP: Creating hostNetwork=true pod 06/28/23 07:57:29.246
    Jun 28 07:57:29.255: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-8117" to be "running and ready"
    Jun 28 07:57:29.260: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.640438ms
    Jun 28 07:57:29.260: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
    Jun 28 07:57:31.267: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.011857635s
    Jun 28 07:57:31.267: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
    Jun 28 07:57:31.267: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
    STEP: Running the test 06/28/23 07:57:31.271
    STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 06/28/23 07:57:31.271
    Jun 28 07:57:31.272: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8117 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 28 07:57:31.272: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    Jun 28 07:57:31.272: INFO: ExecWithOptions: Clientset creation
    Jun 28 07:57:31.272: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8117/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jun 28 07:57:31.681: INFO: Exec stderr: ""
    Jun 28 07:57:31.681: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8117 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 28 07:57:31.681: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    Jun 28 07:57:31.682: INFO: ExecWithOptions: Clientset creation
    Jun 28 07:57:31.682: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8117/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jun 28 07:57:32.158: INFO: Exec stderr: ""
    Jun 28 07:57:32.159: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8117 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 28 07:57:32.159: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    Jun 28 07:57:32.159: INFO: ExecWithOptions: Clientset creation
    Jun 28 07:57:32.159: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8117/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jun 28 07:57:32.638: INFO: Exec stderr: ""
    Jun 28 07:57:32.638: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8117 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 28 07:57:32.638: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    Jun 28 07:57:32.639: INFO: ExecWithOptions: Clientset creation
    Jun 28 07:57:32.639: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8117/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jun 28 07:57:33.076: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 06/28/23 07:57:33.076
    Jun 28 07:57:33.076: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8117 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 28 07:57:33.076: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    Jun 28 07:57:33.077: INFO: ExecWithOptions: Clientset creation
    Jun 28 07:57:33.077: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8117/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Jun 28 07:57:33.549: INFO: Exec stderr: ""
    Jun 28 07:57:33.549: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8117 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 28 07:57:33.549: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    Jun 28 07:57:33.549: INFO: ExecWithOptions: Clientset creation
    Jun 28 07:57:33.549: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8117/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Jun 28 07:57:33.982: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 06/28/23 07:57:33.982
    Jun 28 07:57:33.982: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8117 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 28 07:57:33.982: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    Jun 28 07:57:33.983: INFO: ExecWithOptions: Clientset creation
    Jun 28 07:57:33.983: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8117/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jun 28 07:57:34.415: INFO: Exec stderr: ""
    Jun 28 07:57:34.415: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8117 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 28 07:57:34.415: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    Jun 28 07:57:34.415: INFO: ExecWithOptions: Clientset creation
    Jun 28 07:57:34.415: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8117/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jun 28 07:57:34.749: INFO: Exec stderr: ""
    Jun 28 07:57:34.749: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8117 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 28 07:57:34.749: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    Jun 28 07:57:34.750: INFO: ExecWithOptions: Clientset creation
    Jun 28 07:57:34.750: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8117/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jun 28 07:57:35.072: INFO: Exec stderr: ""
    Jun 28 07:57:35.072: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8117 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 28 07:57:35.073: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    Jun 28 07:57:35.073: INFO: ExecWithOptions: Clientset creation
    Jun 28 07:57:35.073: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8117/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jun 28 07:57:35.572: INFO: Exec stderr: ""
    [AfterEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/framework.go:187
    Jun 28 07:57:35.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "e2e-kubelet-etc-hosts-8117" for this suite. 06/28/23 07:57:35.595
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Ingress API
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
[BeforeEach] [sig-network] Ingress API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 07:57:35.609
Jun 28 07:57:35.609: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename ingress 06/28/23 07:57:35.61
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:57:35.649
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:57:35.674
[It] should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
STEP: getting /apis 06/28/23 07:57:35.696
STEP: getting /apis/networking.k8s.io 06/28/23 07:57:35.719
STEP: getting /apis/networking.k8s.iov1 06/28/23 07:57:35.73
STEP: creating 06/28/23 07:57:35.741
STEP: getting 06/28/23 07:57:35.809
STEP: listing 06/28/23 07:57:35.841
STEP: watching 06/28/23 07:57:35.876
Jun 28 07:57:35.876: INFO: starting watch
STEP: cluster-wide listing 06/28/23 07:57:35.913
STEP: cluster-wide watching 06/28/23 07:57:35.951
Jun 28 07:57:35.951: INFO: starting watch
STEP: patching 06/28/23 07:57:35.994
STEP: updating 06/28/23 07:57:36.031
Jun 28 07:57:36.108: INFO: waiting for watch events with expected annotations
Jun 28 07:57:36.108: INFO: saw patched and updated annotations
STEP: patching /status 06/28/23 07:57:36.108
STEP: updating /status 06/28/23 07:57:36.162
STEP: get /status 06/28/23 07:57:36.231
STEP: deleting 06/28/23 07:57:36.249
STEP: deleting a collection 06/28/23 07:57:36.309
[AfterEach] [sig-network] Ingress API
  test/e2e/framework/framework.go:187
Jun 28 07:57:36.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-6351" for this suite. 06/28/23 07:57:36.429
{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","completed":55,"skipped":1009,"failed":0}
------------------------------
â€¢ [0.845 seconds]
[sig-network] Ingress API
test/e2e/network/common/framework.go:23
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Ingress API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 07:57:35.609
    Jun 28 07:57:35.609: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename ingress 06/28/23 07:57:35.61
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:57:35.649
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:57:35.674
    [It] should support creating Ingress API operations [Conformance]
      test/e2e/network/ingress.go:552
    STEP: getting /apis 06/28/23 07:57:35.696
    STEP: getting /apis/networking.k8s.io 06/28/23 07:57:35.719
    STEP: getting /apis/networking.k8s.iov1 06/28/23 07:57:35.73
    STEP: creating 06/28/23 07:57:35.741
    STEP: getting 06/28/23 07:57:35.809
    STEP: listing 06/28/23 07:57:35.841
    STEP: watching 06/28/23 07:57:35.876
    Jun 28 07:57:35.876: INFO: starting watch
    STEP: cluster-wide listing 06/28/23 07:57:35.913
    STEP: cluster-wide watching 06/28/23 07:57:35.951
    Jun 28 07:57:35.951: INFO: starting watch
    STEP: patching 06/28/23 07:57:35.994
    STEP: updating 06/28/23 07:57:36.031
    Jun 28 07:57:36.108: INFO: waiting for watch events with expected annotations
    Jun 28 07:57:36.108: INFO: saw patched and updated annotations
    STEP: patching /status 06/28/23 07:57:36.108
    STEP: updating /status 06/28/23 07:57:36.162
    STEP: get /status 06/28/23 07:57:36.231
    STEP: deleting 06/28/23 07:57:36.249
    STEP: deleting a collection 06/28/23 07:57:36.309
    [AfterEach] [sig-network] Ingress API
      test/e2e/framework/framework.go:187
    Jun 28 07:57:36.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ingress-6351" for this suite. 06/28/23 07:57:36.429
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 07:57:36.454
Jun 28 07:57:36.455: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename configmap 06/28/23 07:57:36.455
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:57:36.519
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:57:36.564
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73
STEP: Creating configMap with name configmap-test-volume-2d959469-d92e-423d-8dc6-37a49cd304d1 06/28/23 07:57:36.607
STEP: Creating a pod to test consume configMaps 06/28/23 07:57:36.631
Jun 28 07:57:36.661: INFO: Waiting up to 5m0s for pod "pod-configmaps-63b0794a-c5e0-4ff8-b637-f5659ebe43e5" in namespace "configmap-4890" to be "Succeeded or Failed"
Jun 28 07:57:36.685: INFO: Pod "pod-configmaps-63b0794a-c5e0-4ff8-b637-f5659ebe43e5": Phase="Pending", Reason="", readiness=false. Elapsed: 23.823993ms
Jun 28 07:57:38.739: INFO: Pod "pod-configmaps-63b0794a-c5e0-4ff8-b637-f5659ebe43e5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.078350187s
Jun 28 07:57:40.773: INFO: Pod "pod-configmaps-63b0794a-c5e0-4ff8-b637-f5659ebe43e5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.11248563s
Jun 28 07:57:42.697: INFO: Pod "pod-configmaps-63b0794a-c5e0-4ff8-b637-f5659ebe43e5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.035797179s
STEP: Saw pod success 06/28/23 07:57:42.697
Jun 28 07:57:42.697: INFO: Pod "pod-configmaps-63b0794a-c5e0-4ff8-b637-f5659ebe43e5" satisfied condition "Succeeded or Failed"
Jun 28 07:57:42.710: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod pod-configmaps-63b0794a-c5e0-4ff8-b637-f5659ebe43e5 container agnhost-container: <nil>
STEP: delete the pod 06/28/23 07:57:42.749
Jun 28 07:57:42.772: INFO: Waiting for pod pod-configmaps-63b0794a-c5e0-4ff8-b637-f5659ebe43e5 to disappear
Jun 28 07:57:42.789: INFO: Pod pod-configmaps-63b0794a-c5e0-4ff8-b637-f5659ebe43e5 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jun 28 07:57:42.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4890" for this suite. 06/28/23 07:57:42.839
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","completed":56,"skipped":1012,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.401 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 07:57:36.454
    Jun 28 07:57:36.455: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename configmap 06/28/23 07:57:36.455
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:57:36.519
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:57:36.564
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:73
    STEP: Creating configMap with name configmap-test-volume-2d959469-d92e-423d-8dc6-37a49cd304d1 06/28/23 07:57:36.607
    STEP: Creating a pod to test consume configMaps 06/28/23 07:57:36.631
    Jun 28 07:57:36.661: INFO: Waiting up to 5m0s for pod "pod-configmaps-63b0794a-c5e0-4ff8-b637-f5659ebe43e5" in namespace "configmap-4890" to be "Succeeded or Failed"
    Jun 28 07:57:36.685: INFO: Pod "pod-configmaps-63b0794a-c5e0-4ff8-b637-f5659ebe43e5": Phase="Pending", Reason="", readiness=false. Elapsed: 23.823993ms
    Jun 28 07:57:38.739: INFO: Pod "pod-configmaps-63b0794a-c5e0-4ff8-b637-f5659ebe43e5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.078350187s
    Jun 28 07:57:40.773: INFO: Pod "pod-configmaps-63b0794a-c5e0-4ff8-b637-f5659ebe43e5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.11248563s
    Jun 28 07:57:42.697: INFO: Pod "pod-configmaps-63b0794a-c5e0-4ff8-b637-f5659ebe43e5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.035797179s
    STEP: Saw pod success 06/28/23 07:57:42.697
    Jun 28 07:57:42.697: INFO: Pod "pod-configmaps-63b0794a-c5e0-4ff8-b637-f5659ebe43e5" satisfied condition "Succeeded or Failed"
    Jun 28 07:57:42.710: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod pod-configmaps-63b0794a-c5e0-4ff8-b637-f5659ebe43e5 container agnhost-container: <nil>
    STEP: delete the pod 06/28/23 07:57:42.749
    Jun 28 07:57:42.772: INFO: Waiting for pod pod-configmaps-63b0794a-c5e0-4ff8-b637-f5659ebe43e5 to disappear
    Jun 28 07:57:42.789: INFO: Pod pod-configmaps-63b0794a-c5e0-4ff8-b637-f5659ebe43e5 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jun 28 07:57:42.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-4890" for this suite. 06/28/23 07:57:42.839
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 07:57:42.856
Jun 28 07:57:42.856: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename daemonsets 06/28/23 07:57:42.857
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:57:42.898
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:57:42.92
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165
STEP: Creating simple DaemonSet "daemon-set" 06/28/23 07:57:42.987
STEP: Check that daemon pods launch on every node of the cluster. 06/28/23 07:57:42.996
Jun 28 07:57:43.012: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 28 07:57:43.012: INFO: Node ske-rhel-749f7d55c8xdd8b6-ct4cp is running 0 daemon pod, expected 1
Jun 28 07:57:44.031: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jun 28 07:57:44.031: INFO: Node ske-rhel-749f7d55c8xdd8b6-ct4cp is running 0 daemon pod, expected 1
Jun 28 07:57:45.034: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jun 28 07:57:45.034: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived. 06/28/23 07:57:45.039
Jun 28 07:57:45.070: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jun 28 07:57:45.070: INFO: Node ske-rhel-749f7d55c8xdd8b6-zxlfv is running 0 daemon pod, expected 1
Jun 28 07:57:46.087: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jun 28 07:57:46.087: INFO: Node ske-rhel-749f7d55c8xdd8b6-zxlfv is running 0 daemon pod, expected 1
Jun 28 07:57:47.088: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jun 28 07:57:47.088: INFO: Node ske-rhel-749f7d55c8xdd8b6-zxlfv is running 0 daemon pod, expected 1
Jun 28 07:57:48.084: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jun 28 07:57:48.084: INFO: Node ske-rhel-749f7d55c8xdd8b6-zxlfv is running 0 daemon pod, expected 1
Jun 28 07:57:49.084: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jun 28 07:57:49.084: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 06/28/23 07:57:49.09
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8934, will wait for the garbage collector to delete the pods 06/28/23 07:57:49.09
Jun 28 07:57:49.156: INFO: Deleting DaemonSet.extensions daemon-set took: 10.819227ms
Jun 28 07:57:49.256: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.244031ms
Jun 28 07:57:51.862: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 28 07:57:51.862: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jun 28 07:57:51.867: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"54694"},"items":null}

Jun 28 07:57:51.871: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"54694"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Jun 28 07:57:51.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8934" for this suite. 06/28/23 07:57:51.903
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","completed":57,"skipped":1014,"failed":0}
------------------------------
â€¢ [SLOW TEST] [9.054 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 07:57:42.856
    Jun 28 07:57:42.856: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename daemonsets 06/28/23 07:57:42.857
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:57:42.898
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:57:42.92
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should run and stop simple daemon [Conformance]
      test/e2e/apps/daemon_set.go:165
    STEP: Creating simple DaemonSet "daemon-set" 06/28/23 07:57:42.987
    STEP: Check that daemon pods launch on every node of the cluster. 06/28/23 07:57:42.996
    Jun 28 07:57:43.012: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun 28 07:57:43.012: INFO: Node ske-rhel-749f7d55c8xdd8b6-ct4cp is running 0 daemon pod, expected 1
    Jun 28 07:57:44.031: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jun 28 07:57:44.031: INFO: Node ske-rhel-749f7d55c8xdd8b6-ct4cp is running 0 daemon pod, expected 1
    Jun 28 07:57:45.034: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jun 28 07:57:45.034: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Stop a daemon pod, check that the daemon pod is revived. 06/28/23 07:57:45.039
    Jun 28 07:57:45.070: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jun 28 07:57:45.070: INFO: Node ske-rhel-749f7d55c8xdd8b6-zxlfv is running 0 daemon pod, expected 1
    Jun 28 07:57:46.087: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jun 28 07:57:46.087: INFO: Node ske-rhel-749f7d55c8xdd8b6-zxlfv is running 0 daemon pod, expected 1
    Jun 28 07:57:47.088: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jun 28 07:57:47.088: INFO: Node ske-rhel-749f7d55c8xdd8b6-zxlfv is running 0 daemon pod, expected 1
    Jun 28 07:57:48.084: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jun 28 07:57:48.084: INFO: Node ske-rhel-749f7d55c8xdd8b6-zxlfv is running 0 daemon pod, expected 1
    Jun 28 07:57:49.084: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jun 28 07:57:49.084: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 06/28/23 07:57:49.09
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8934, will wait for the garbage collector to delete the pods 06/28/23 07:57:49.09
    Jun 28 07:57:49.156: INFO: Deleting DaemonSet.extensions daemon-set took: 10.819227ms
    Jun 28 07:57:49.256: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.244031ms
    Jun 28 07:57:51.862: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun 28 07:57:51.862: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jun 28 07:57:51.867: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"54694"},"items":null}

    Jun 28 07:57:51.871: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"54694"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Jun 28 07:57:51.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-8934" for this suite. 06/28/23 07:57:51.903
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 07:57:51.912
Jun 28 07:57:51.912: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename crd-webhook 06/28/23 07:57:51.912
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:57:51.926
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:57:51.931
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 06/28/23 07:57:51.935
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 06/28/23 07:57:52.249
STEP: Deploying the custom resource conversion webhook pod 06/28/23 07:57:52.259
STEP: Wait for the deployment to be ready 06/28/23 07:57:52.272
Jun 28 07:57:52.281: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 06/28/23 07:57:54.3
STEP: Verifying the service has paired with the endpoint 06/28/23 07:57:54.312
Jun 28 07:57:55.312: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
Jun 28 07:57:55.317: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Creating a v1 custom resource 06/28/23 07:57:58.075
STEP: Create a v2 custom resource 06/28/23 07:57:58.094
STEP: List CRs in v1 06/28/23 07:57:58.161
STEP: List CRs in v2 06/28/23 07:57:58.171
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jun 28 07:57:58.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-536" for this suite. 06/28/23 07:57:58.708
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","completed":58,"skipped":1044,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.842 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 07:57:51.912
    Jun 28 07:57:51.912: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename crd-webhook 06/28/23 07:57:51.912
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:57:51.926
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:57:51.931
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 06/28/23 07:57:51.935
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 06/28/23 07:57:52.249
    STEP: Deploying the custom resource conversion webhook pod 06/28/23 07:57:52.259
    STEP: Wait for the deployment to be ready 06/28/23 07:57:52.272
    Jun 28 07:57:52.281: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 06/28/23 07:57:54.3
    STEP: Verifying the service has paired with the endpoint 06/28/23 07:57:54.312
    Jun 28 07:57:55.312: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert a non homogeneous list of CRs [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:184
    Jun 28 07:57:55.317: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Creating a v1 custom resource 06/28/23 07:57:58.075
    STEP: Create a v2 custom resource 06/28/23 07:57:58.094
    STEP: List CRs in v1 06/28/23 07:57:58.161
    STEP: List CRs in v2 06/28/23 07:57:58.171
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jun 28 07:57:58.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-webhook-536" for this suite. 06/28/23 07:57:58.708
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Kubectl label
  should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1507
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 07:57:58.754
Jun 28 07:57:58.754: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename kubectl 06/28/23 07:57:58.755
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:57:58.769
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:57:58.773
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1492
STEP: creating the pod 06/28/23 07:57:58.777
Jun 28 07:57:58.778: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-5576 create -f -'
Jun 28 07:57:59.623: INFO: stderr: ""
Jun 28 07:57:59.623: INFO: stdout: "pod/pause created\n"
Jun 28 07:57:59.623: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Jun 28 07:57:59.623: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-5576" to be "running and ready"
Jun 28 07:57:59.627: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021642ms
Jun 28 07:57:59.627: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'ske-rhel-749f7d55c8xdd8b6-ct4cp' to be 'Running' but was 'Pending'
Jun 28 07:58:01.634: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.010800713s
Jun 28 07:58:01.634: INFO: Pod "pause" satisfied condition "running and ready"
Jun 28 07:58:01.634: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1507
STEP: adding the label testing-label with value testing-label-value to a pod 06/28/23 07:58:01.634
Jun 28 07:58:01.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-5576 label pods pause testing-label=testing-label-value'
Jun 28 07:58:01.725: INFO: stderr: ""
Jun 28 07:58:01.725: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value 06/28/23 07:58:01.725
Jun 28 07:58:01.725: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-5576 get pod pause -L testing-label'
Jun 28 07:58:01.806: INFO: stderr: ""
Jun 28 07:58:01.806: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod 06/28/23 07:58:01.806
Jun 28 07:58:01.806: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-5576 label pods pause testing-label-'
Jun 28 07:58:01.889: INFO: stderr: ""
Jun 28 07:58:01.889: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label 06/28/23 07:58:01.889
Jun 28 07:58:01.889: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-5576 get pod pause -L testing-label'
Jun 28 07:58:01.972: INFO: stderr: ""
Jun 28 07:58:01.972: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1498
STEP: using delete to clean up resources 06/28/23 07:58:01.972
Jun 28 07:58:01.972: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-5576 delete --grace-period=0 --force -f -'
Jun 28 07:58:02.056: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 28 07:58:02.056: INFO: stdout: "pod \"pause\" force deleted\n"
Jun 28 07:58:02.056: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-5576 get rc,svc -l name=pause --no-headers'
Jun 28 07:58:02.135: INFO: stderr: "No resources found in kubectl-5576 namespace.\n"
Jun 28 07:58:02.135: INFO: stdout: ""
Jun 28 07:58:02.135: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-5576 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun 28 07:58:02.208: INFO: stderr: ""
Jun 28 07:58:02.208: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jun 28 07:58:02.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5576" for this suite. 06/28/23 07:58:02.219
{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","completed":59,"skipped":1047,"failed":0}
------------------------------
â€¢ [3.473 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl label
  test/e2e/kubectl/kubectl.go:1490
    should update the label on a resource  [Conformance]
    test/e2e/kubectl/kubectl.go:1507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 07:57:58.754
    Jun 28 07:57:58.754: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename kubectl 06/28/23 07:57:58.755
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:57:58.769
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:57:58.773
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1492
    STEP: creating the pod 06/28/23 07:57:58.777
    Jun 28 07:57:58.778: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-5576 create -f -'
    Jun 28 07:57:59.623: INFO: stderr: ""
    Jun 28 07:57:59.623: INFO: stdout: "pod/pause created\n"
    Jun 28 07:57:59.623: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
    Jun 28 07:57:59.623: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-5576" to be "running and ready"
    Jun 28 07:57:59.627: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021642ms
    Jun 28 07:57:59.627: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'ske-rhel-749f7d55c8xdd8b6-ct4cp' to be 'Running' but was 'Pending'
    Jun 28 07:58:01.634: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.010800713s
    Jun 28 07:58:01.634: INFO: Pod "pause" satisfied condition "running and ready"
    Jun 28 07:58:01.634: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
    [It] should update the label on a resource  [Conformance]
      test/e2e/kubectl/kubectl.go:1507
    STEP: adding the label testing-label with value testing-label-value to a pod 06/28/23 07:58:01.634
    Jun 28 07:58:01.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-5576 label pods pause testing-label=testing-label-value'
    Jun 28 07:58:01.725: INFO: stderr: ""
    Jun 28 07:58:01.725: INFO: stdout: "pod/pause labeled\n"
    STEP: verifying the pod has the label testing-label with the value testing-label-value 06/28/23 07:58:01.725
    Jun 28 07:58:01.725: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-5576 get pod pause -L testing-label'
    Jun 28 07:58:01.806: INFO: stderr: ""
    Jun 28 07:58:01.806: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
    STEP: removing the label testing-label of a pod 06/28/23 07:58:01.806
    Jun 28 07:58:01.806: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-5576 label pods pause testing-label-'
    Jun 28 07:58:01.889: INFO: stderr: ""
    Jun 28 07:58:01.889: INFO: stdout: "pod/pause unlabeled\n"
    STEP: verifying the pod doesn't have the label testing-label 06/28/23 07:58:01.889
    Jun 28 07:58:01.889: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-5576 get pod pause -L testing-label'
    Jun 28 07:58:01.972: INFO: stderr: ""
    Jun 28 07:58:01.972: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
    [AfterEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1498
    STEP: using delete to clean up resources 06/28/23 07:58:01.972
    Jun 28 07:58:01.972: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-5576 delete --grace-period=0 --force -f -'
    Jun 28 07:58:02.056: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jun 28 07:58:02.056: INFO: stdout: "pod \"pause\" force deleted\n"
    Jun 28 07:58:02.056: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-5576 get rc,svc -l name=pause --no-headers'
    Jun 28 07:58:02.135: INFO: stderr: "No resources found in kubectl-5576 namespace.\n"
    Jun 28 07:58:02.135: INFO: stdout: ""
    Jun 28 07:58:02.135: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-5576 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Jun 28 07:58:02.208: INFO: stderr: ""
    Jun 28 07:58:02.208: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jun 28 07:58:02.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-5576" for this suite. 06/28/23 07:58:02.219
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:152
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 07:58:02.228
Jun 28 07:58:02.228: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename container-lifecycle-hook 06/28/23 07:58:02.229
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:58:02.245
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:58:02.25
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 06/28/23 07:58:02.265
Jun 28 07:58:02.274: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-6944" to be "running and ready"
Jun 28 07:58:02.280: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 5.308023ms
Jun 28 07:58:02.280: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jun 28 07:58:04.287: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.012141881s
Jun 28 07:58:04.287: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jun 28 07:58:04.287: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:152
STEP: create the pod with lifecycle hook 06/28/23 07:58:04.292
Jun 28 07:58:04.300: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-6944" to be "running and ready"
Jun 28 07:58:04.304: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 4.384909ms
Jun 28 07:58:04.304: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jun 28 07:58:06.311: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.011081558s
Jun 28 07:58:06.311: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
Jun 28 07:58:06.311: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 06/28/23 07:58:06.316
Jun 28 07:58:06.325: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun 28 07:58:06.330: INFO: Pod pod-with-prestop-http-hook still exists
Jun 28 07:58:08.331: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun 28 07:58:08.336: INFO: Pod pod-with-prestop-http-hook still exists
Jun 28 07:58:10.331: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun 28 07:58:10.337: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook 06/28/23 07:58:10.337
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Jun 28 07:58:10.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6944" for this suite. 06/28/23 07:58:10.359
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","completed":60,"skipped":1065,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.140 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 07:58:02.228
    Jun 28 07:58:02.228: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename container-lifecycle-hook 06/28/23 07:58:02.229
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:58:02.245
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:58:02.25
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 06/28/23 07:58:02.265
    Jun 28 07:58:02.274: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-6944" to be "running and ready"
    Jun 28 07:58:02.280: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 5.308023ms
    Jun 28 07:58:02.280: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jun 28 07:58:04.287: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.012141881s
    Jun 28 07:58:04.287: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jun 28 07:58:04.287: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:152
    STEP: create the pod with lifecycle hook 06/28/23 07:58:04.292
    Jun 28 07:58:04.300: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-6944" to be "running and ready"
    Jun 28 07:58:04.304: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 4.384909ms
    Jun 28 07:58:04.304: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Jun 28 07:58:06.311: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.011081558s
    Jun 28 07:58:06.311: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
    Jun 28 07:58:06.311: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 06/28/23 07:58:06.316
    Jun 28 07:58:06.325: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Jun 28 07:58:06.330: INFO: Pod pod-with-prestop-http-hook still exists
    Jun 28 07:58:08.331: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Jun 28 07:58:08.336: INFO: Pod pod-with-prestop-http-hook still exists
    Jun 28 07:58:10.331: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Jun 28 07:58:10.337: INFO: Pod pod-with-prestop-http-hook no longer exists
    STEP: check prestop hook 06/28/23 07:58:10.337
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Jun 28 07:58:10.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-6944" for this suite. 06/28/23 07:58:10.359
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 07:58:10.369
Jun 28 07:58:10.370: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename projected 06/28/23 07:58:10.371
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:58:10.385
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:58:10.392
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46
STEP: Creating configMap with name projected-configmap-test-volume-63c37f7d-bf86-4b37-b846-7c9b706f4912 06/28/23 07:58:10.398
STEP: Creating a pod to test consume configMaps 06/28/23 07:58:10.406
Jun 28 07:58:10.416: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-81dd3d4b-0280-4c9b-b67d-cd93ba05bdca" in namespace "projected-6385" to be "Succeeded or Failed"
Jun 28 07:58:10.422: INFO: Pod "pod-projected-configmaps-81dd3d4b-0280-4c9b-b67d-cd93ba05bdca": Phase="Pending", Reason="", readiness=false. Elapsed: 5.721392ms
Jun 28 07:58:12.429: INFO: Pod "pod-projected-configmaps-81dd3d4b-0280-4c9b-b67d-cd93ba05bdca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012521945s
Jun 28 07:58:14.428: INFO: Pod "pod-projected-configmaps-81dd3d4b-0280-4c9b-b67d-cd93ba05bdca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011872898s
STEP: Saw pod success 06/28/23 07:58:14.428
Jun 28 07:58:14.428: INFO: Pod "pod-projected-configmaps-81dd3d4b-0280-4c9b-b67d-cd93ba05bdca" satisfied condition "Succeeded or Failed"
Jun 28 07:58:14.434: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod pod-projected-configmaps-81dd3d4b-0280-4c9b-b67d-cd93ba05bdca container agnhost-container: <nil>
STEP: delete the pod 06/28/23 07:58:14.503
Jun 28 07:58:14.515: INFO: Waiting for pod pod-projected-configmaps-81dd3d4b-0280-4c9b-b67d-cd93ba05bdca to disappear
Jun 28 07:58:14.519: INFO: Pod pod-projected-configmaps-81dd3d4b-0280-4c9b-b67d-cd93ba05bdca no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jun 28 07:58:14.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6385" for this suite. 06/28/23 07:58:14.527
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","completed":61,"skipped":1085,"failed":0}
------------------------------
â€¢ [4.164 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 07:58:10.369
    Jun 28 07:58:10.370: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename projected 06/28/23 07:58:10.371
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:58:10.385
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:58:10.392
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:46
    STEP: Creating configMap with name projected-configmap-test-volume-63c37f7d-bf86-4b37-b846-7c9b706f4912 06/28/23 07:58:10.398
    STEP: Creating a pod to test consume configMaps 06/28/23 07:58:10.406
    Jun 28 07:58:10.416: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-81dd3d4b-0280-4c9b-b67d-cd93ba05bdca" in namespace "projected-6385" to be "Succeeded or Failed"
    Jun 28 07:58:10.422: INFO: Pod "pod-projected-configmaps-81dd3d4b-0280-4c9b-b67d-cd93ba05bdca": Phase="Pending", Reason="", readiness=false. Elapsed: 5.721392ms
    Jun 28 07:58:12.429: INFO: Pod "pod-projected-configmaps-81dd3d4b-0280-4c9b-b67d-cd93ba05bdca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012521945s
    Jun 28 07:58:14.428: INFO: Pod "pod-projected-configmaps-81dd3d4b-0280-4c9b-b67d-cd93ba05bdca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011872898s
    STEP: Saw pod success 06/28/23 07:58:14.428
    Jun 28 07:58:14.428: INFO: Pod "pod-projected-configmaps-81dd3d4b-0280-4c9b-b67d-cd93ba05bdca" satisfied condition "Succeeded or Failed"
    Jun 28 07:58:14.434: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod pod-projected-configmaps-81dd3d4b-0280-4c9b-b67d-cd93ba05bdca container agnhost-container: <nil>
    STEP: delete the pod 06/28/23 07:58:14.503
    Jun 28 07:58:14.515: INFO: Waiting for pod pod-projected-configmaps-81dd3d4b-0280-4c9b-b67d-cd93ba05bdca to disappear
    Jun 28 07:58:14.519: INFO: Pod pod-projected-configmaps-81dd3d4b-0280-4c9b-b67d-cd93ba05bdca no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jun 28 07:58:14.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6385" for this suite. 06/28/23 07:58:14.527
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] DNS
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 07:58:14.534
Jun 28 07:58:14.534: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename dns 06/28/23 07:58:14.535
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:58:14.549
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:58:14.554
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
STEP: Creating a test headless service 06/28/23 07:58:14.559
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6780 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6780;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6780 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6780;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6780.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6780.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6780.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6780.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6780.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-6780.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6780.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-6780.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6780.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-6780.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6780.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-6780.svc;check="$$(dig +notcp +noall +answer +search 222.87.20.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.20.87.222_udp@PTR;check="$$(dig +tcp +noall +answer +search 222.87.20.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.20.87.222_tcp@PTR;sleep 1; done
 06/28/23 07:58:14.577
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6780 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6780;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6780 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6780;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6780.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6780.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6780.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6780.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6780.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-6780.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6780.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-6780.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6780.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-6780.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6780.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-6780.svc;check="$$(dig +notcp +noall +answer +search 222.87.20.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.20.87.222_udp@PTR;check="$$(dig +tcp +noall +answer +search 222.87.20.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.20.87.222_tcp@PTR;sleep 1; done
 06/28/23 07:58:14.577
STEP: creating a pod to probe DNS 06/28/23 07:58:14.577
STEP: submitting the pod to kubernetes 06/28/23 07:58:14.577
Jun 28 07:58:14.588: INFO: Waiting up to 15m0s for pod "dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf" in namespace "dns-6780" to be "running"
Jun 28 07:58:14.600: INFO: Pod "dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf": Phase="Pending", Reason="", readiness=false. Elapsed: 12.297181ms
Jun 28 07:58:16.606: INFO: Pod "dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018098103s
Jun 28 07:58:18.607: INFO: Pod "dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf": Phase="Running", Reason="", readiness=true. Elapsed: 4.018780381s
Jun 28 07:58:18.607: INFO: Pod "dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf" satisfied condition "running"
STEP: retrieving the pod 06/28/23 07:58:18.607
STEP: looking for the results for each expected name from probers 06/28/23 07:58:18.612
Jun 28 07:58:18.741: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
Jun 28 07:58:18.806: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
Jun 28 07:58:18.825: INFO: Unable to read wheezy_udp@dns-test-service.dns-6780 from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
Jun 28 07:58:18.842: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6780 from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
Jun 28 07:58:18.865: INFO: Unable to read wheezy_udp@dns-test-service.dns-6780.svc from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
Jun 28 07:58:18.882: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6780.svc from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
Jun 28 07:58:18.899: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6780.svc from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
Jun 28 07:58:18.916: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6780.svc from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
Jun 28 07:58:19.002: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
Jun 28 07:58:19.019: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
Jun 28 07:58:19.036: INFO: Unable to read jessie_udp@dns-test-service.dns-6780 from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
Jun 28 07:58:19.065: INFO: Unable to read jessie_tcp@dns-test-service.dns-6780 from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
Jun 28 07:58:19.141: INFO: Unable to read jessie_udp@dns-test-service.dns-6780.svc from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
Jun 28 07:58:19.162: INFO: Unable to read jessie_tcp@dns-test-service.dns-6780.svc from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
Jun 28 07:58:19.190: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6780.svc from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
Jun 28 07:58:19.215: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6780.svc from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
Jun 28 07:58:19.310: INFO: Lookups using dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6780 wheezy_tcp@dns-test-service.dns-6780 wheezy_udp@dns-test-service.dns-6780.svc wheezy_tcp@dns-test-service.dns-6780.svc wheezy_udp@_http._tcp.dns-test-service.dns-6780.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6780.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6780 jessie_tcp@dns-test-service.dns-6780 jessie_udp@dns-test-service.dns-6780.svc jessie_tcp@dns-test-service.dns-6780.svc jessie_udp@_http._tcp.dns-test-service.dns-6780.svc jessie_tcp@_http._tcp.dns-test-service.dns-6780.svc]

Jun 28 07:58:24.319: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
Jun 28 07:58:24.363: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
Jun 28 07:58:24.369: INFO: Unable to read wheezy_udp@dns-test-service.dns-6780 from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
Jun 28 07:58:24.375: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6780 from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
Jun 28 07:58:24.382: INFO: Unable to read wheezy_udp@dns-test-service.dns-6780.svc from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
Jun 28 07:58:24.390: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6780.svc from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
Jun 28 07:58:24.396: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6780.svc from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
Jun 28 07:58:24.403: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6780.svc from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
Jun 28 07:58:24.447: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
Jun 28 07:58:24.454: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
Jun 28 07:58:24.462: INFO: Unable to read jessie_udp@dns-test-service.dns-6780 from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
Jun 28 07:58:24.470: INFO: Unable to read jessie_tcp@dns-test-service.dns-6780 from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
Jun 28 07:58:24.478: INFO: Unable to read jessie_udp@dns-test-service.dns-6780.svc from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
Jun 28 07:58:24.487: INFO: Unable to read jessie_tcp@dns-test-service.dns-6780.svc from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
Jun 28 07:58:24.495: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6780.svc from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
Jun 28 07:58:24.501: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6780.svc from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
Jun 28 07:58:24.535: INFO: Lookups using dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6780 wheezy_tcp@dns-test-service.dns-6780 wheezy_udp@dns-test-service.dns-6780.svc wheezy_tcp@dns-test-service.dns-6780.svc wheezy_udp@_http._tcp.dns-test-service.dns-6780.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6780.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6780 jessie_tcp@dns-test-service.dns-6780 jessie_udp@dns-test-service.dns-6780.svc jessie_tcp@dns-test-service.dns-6780.svc jessie_udp@_http._tcp.dns-test-service.dns-6780.svc jessie_tcp@_http._tcp.dns-test-service.dns-6780.svc]

Jun 28 07:58:29.318: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
Jun 28 07:58:29.362: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
Jun 28 07:58:29.370: INFO: Unable to read wheezy_udp@dns-test-service.dns-6780 from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
Jun 28 07:58:29.377: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6780 from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
Jun 28 07:58:29.384: INFO: Unable to read wheezy_udp@dns-test-service.dns-6780.svc from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
Jun 28 07:58:29.391: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6780.svc from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
Jun 28 07:58:29.398: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6780.svc from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
Jun 28 07:58:29.405: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6780.svc from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
Jun 28 07:58:29.439: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
Jun 28 07:58:29.446: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
Jun 28 07:58:29.452: INFO: Unable to read jessie_udp@dns-test-service.dns-6780 from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
Jun 28 07:58:29.459: INFO: Unable to read jessie_tcp@dns-test-service.dns-6780 from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
Jun 28 07:58:29.465: INFO: Unable to read jessie_udp@dns-test-service.dns-6780.svc from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
Jun 28 07:58:29.472: INFO: Unable to read jessie_tcp@dns-test-service.dns-6780.svc from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
Jun 28 07:58:29.479: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6780.svc from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
Jun 28 07:58:29.485: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6780.svc from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
Jun 28 07:58:29.512: INFO: Lookups using dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6780 wheezy_tcp@dns-test-service.dns-6780 wheezy_udp@dns-test-service.dns-6780.svc wheezy_tcp@dns-test-service.dns-6780.svc wheezy_udp@_http._tcp.dns-test-service.dns-6780.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6780.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6780 jessie_tcp@dns-test-service.dns-6780 jessie_udp@dns-test-service.dns-6780.svc jessie_tcp@dns-test-service.dns-6780.svc jessie_udp@_http._tcp.dns-test-service.dns-6780.svc jessie_tcp@_http._tcp.dns-test-service.dns-6780.svc]

Jun 28 07:58:34.319: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
Jun 28 07:58:34.339: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
Jun 28 07:58:34.389: INFO: Unable to read wheezy_udp@dns-test-service.dns-6780 from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
Jun 28 07:58:34.396: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6780 from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
Jun 28 07:58:34.403: INFO: Unable to read wheezy_udp@dns-test-service.dns-6780.svc from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
Jun 28 07:58:34.410: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6780.svc from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
Jun 28 07:58:34.416: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6780.svc from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
Jun 28 07:58:34.423: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6780.svc from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
Jun 28 07:58:34.457: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
Jun 28 07:58:34.464: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
Jun 28 07:58:34.470: INFO: Unable to read jessie_udp@dns-test-service.dns-6780 from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
Jun 28 07:58:34.477: INFO: Unable to read jessie_tcp@dns-test-service.dns-6780 from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
Jun 28 07:58:34.491: INFO: Unable to read jessie_udp@dns-test-service.dns-6780.svc from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
Jun 28 07:58:34.538: INFO: Unable to read jessie_tcp@dns-test-service.dns-6780.svc from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
Jun 28 07:58:34.545: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6780.svc from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
Jun 28 07:58:34.552: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6780.svc from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
Jun 28 07:58:34.579: INFO: Lookups using dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6780 wheezy_tcp@dns-test-service.dns-6780 wheezy_udp@dns-test-service.dns-6780.svc wheezy_tcp@dns-test-service.dns-6780.svc wheezy_udp@_http._tcp.dns-test-service.dns-6780.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6780.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6780 jessie_tcp@dns-test-service.dns-6780 jessie_udp@dns-test-service.dns-6780.svc jessie_tcp@dns-test-service.dns-6780.svc jessie_udp@_http._tcp.dns-test-service.dns-6780.svc jessie_tcp@_http._tcp.dns-test-service.dns-6780.svc]

Jun 28 07:58:39.359: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
Jun 28 07:58:39.443: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
Jun 28 07:58:39.450: INFO: Unable to read wheezy_udp@dns-test-service.dns-6780 from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
Jun 28 07:58:39.457: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6780 from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
Jun 28 07:58:39.464: INFO: Unable to read wheezy_udp@dns-test-service.dns-6780.svc from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
Jun 28 07:58:39.471: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6780.svc from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
Jun 28 07:58:39.478: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6780.svc from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
Jun 28 07:58:39.486: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6780.svc from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
Jun 28 07:58:39.522: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
Jun 28 07:58:39.529: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
Jun 28 07:58:39.535: INFO: Unable to read jessie_udp@dns-test-service.dns-6780 from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
Jun 28 07:58:39.542: INFO: Unable to read jessie_tcp@dns-test-service.dns-6780 from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
Jun 28 07:58:39.549: INFO: Unable to read jessie_udp@dns-test-service.dns-6780.svc from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
Jun 28 07:58:39.557: INFO: Unable to read jessie_tcp@dns-test-service.dns-6780.svc from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
Jun 28 07:58:39.564: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6780.svc from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
Jun 28 07:58:39.571: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6780.svc from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
Jun 28 07:58:39.602: INFO: Lookups using dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6780 wheezy_tcp@dns-test-service.dns-6780 wheezy_udp@dns-test-service.dns-6780.svc wheezy_tcp@dns-test-service.dns-6780.svc wheezy_udp@_http._tcp.dns-test-service.dns-6780.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6780.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6780 jessie_tcp@dns-test-service.dns-6780 jessie_udp@dns-test-service.dns-6780.svc jessie_tcp@dns-test-service.dns-6780.svc jessie_udp@_http._tcp.dns-test-service.dns-6780.svc jessie_tcp@_http._tcp.dns-test-service.dns-6780.svc]

Jun 28 07:58:44.319: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
Jun 28 07:58:44.362: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
Jun 28 07:58:44.370: INFO: Unable to read wheezy_udp@dns-test-service.dns-6780 from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
Jun 28 07:58:44.376: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6780 from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
Jun 28 07:58:44.383: INFO: Unable to read wheezy_udp@dns-test-service.dns-6780.svc from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
Jun 28 07:58:44.390: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6780.svc from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
Jun 28 07:58:44.396: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6780.svc from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
Jun 28 07:58:44.404: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6780.svc from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
Jun 28 07:58:44.436: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
Jun 28 07:58:44.442: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
Jun 28 07:58:44.450: INFO: Unable to read jessie_udp@dns-test-service.dns-6780 from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
Jun 28 07:58:44.456: INFO: Unable to read jessie_tcp@dns-test-service.dns-6780 from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
Jun 28 07:58:44.463: INFO: Unable to read jessie_udp@dns-test-service.dns-6780.svc from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
Jun 28 07:58:44.470: INFO: Unable to read jessie_tcp@dns-test-service.dns-6780.svc from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
Jun 28 07:58:44.477: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6780.svc from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
Jun 28 07:58:44.484: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6780.svc from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
Jun 28 07:58:44.510: INFO: Lookups using dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6780 wheezy_tcp@dns-test-service.dns-6780 wheezy_udp@dns-test-service.dns-6780.svc wheezy_tcp@dns-test-service.dns-6780.svc wheezy_udp@_http._tcp.dns-test-service.dns-6780.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6780.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6780 jessie_tcp@dns-test-service.dns-6780 jessie_udp@dns-test-service.dns-6780.svc jessie_tcp@dns-test-service.dns-6780.svc jessie_udp@_http._tcp.dns-test-service.dns-6780.svc jessie_tcp@_http._tcp.dns-test-service.dns-6780.svc]

Jun 28 07:58:49.514: INFO: DNS probes using dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf succeeded

STEP: deleting the pod 06/28/23 07:58:49.514
STEP: deleting the test service 06/28/23 07:58:49.533
STEP: deleting the test headless service 06/28/23 07:58:49.549
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jun 28 07:58:49.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6780" for this suite. 06/28/23 07:58:49.567
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","completed":62,"skipped":1085,"failed":0}
------------------------------
â€¢ [SLOW TEST] [35.039 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 07:58:14.534
    Jun 28 07:58:14.534: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename dns 06/28/23 07:58:14.535
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:58:14.549
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:58:14.554
    [It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
      test/e2e/network/dns.go:193
    STEP: Creating a test headless service 06/28/23 07:58:14.559
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6780 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6780;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6780 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6780;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6780.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6780.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6780.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6780.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6780.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-6780.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6780.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-6780.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6780.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-6780.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6780.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-6780.svc;check="$$(dig +notcp +noall +answer +search 222.87.20.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.20.87.222_udp@PTR;check="$$(dig +tcp +noall +answer +search 222.87.20.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.20.87.222_tcp@PTR;sleep 1; done
     06/28/23 07:58:14.577
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6780 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6780;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6780 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6780;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6780.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6780.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6780.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6780.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6780.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-6780.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6780.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-6780.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6780.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-6780.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6780.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-6780.svc;check="$$(dig +notcp +noall +answer +search 222.87.20.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.20.87.222_udp@PTR;check="$$(dig +tcp +noall +answer +search 222.87.20.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.20.87.222_tcp@PTR;sleep 1; done
     06/28/23 07:58:14.577
    STEP: creating a pod to probe DNS 06/28/23 07:58:14.577
    STEP: submitting the pod to kubernetes 06/28/23 07:58:14.577
    Jun 28 07:58:14.588: INFO: Waiting up to 15m0s for pod "dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf" in namespace "dns-6780" to be "running"
    Jun 28 07:58:14.600: INFO: Pod "dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf": Phase="Pending", Reason="", readiness=false. Elapsed: 12.297181ms
    Jun 28 07:58:16.606: INFO: Pod "dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018098103s
    Jun 28 07:58:18.607: INFO: Pod "dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf": Phase="Running", Reason="", readiness=true. Elapsed: 4.018780381s
    Jun 28 07:58:18.607: INFO: Pod "dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf" satisfied condition "running"
    STEP: retrieving the pod 06/28/23 07:58:18.607
    STEP: looking for the results for each expected name from probers 06/28/23 07:58:18.612
    Jun 28 07:58:18.741: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
    Jun 28 07:58:18.806: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
    Jun 28 07:58:18.825: INFO: Unable to read wheezy_udp@dns-test-service.dns-6780 from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
    Jun 28 07:58:18.842: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6780 from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
    Jun 28 07:58:18.865: INFO: Unable to read wheezy_udp@dns-test-service.dns-6780.svc from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
    Jun 28 07:58:18.882: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6780.svc from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
    Jun 28 07:58:18.899: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6780.svc from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
    Jun 28 07:58:18.916: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6780.svc from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
    Jun 28 07:58:19.002: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
    Jun 28 07:58:19.019: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
    Jun 28 07:58:19.036: INFO: Unable to read jessie_udp@dns-test-service.dns-6780 from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
    Jun 28 07:58:19.065: INFO: Unable to read jessie_tcp@dns-test-service.dns-6780 from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
    Jun 28 07:58:19.141: INFO: Unable to read jessie_udp@dns-test-service.dns-6780.svc from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
    Jun 28 07:58:19.162: INFO: Unable to read jessie_tcp@dns-test-service.dns-6780.svc from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
    Jun 28 07:58:19.190: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6780.svc from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
    Jun 28 07:58:19.215: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6780.svc from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
    Jun 28 07:58:19.310: INFO: Lookups using dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6780 wheezy_tcp@dns-test-service.dns-6780 wheezy_udp@dns-test-service.dns-6780.svc wheezy_tcp@dns-test-service.dns-6780.svc wheezy_udp@_http._tcp.dns-test-service.dns-6780.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6780.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6780 jessie_tcp@dns-test-service.dns-6780 jessie_udp@dns-test-service.dns-6780.svc jessie_tcp@dns-test-service.dns-6780.svc jessie_udp@_http._tcp.dns-test-service.dns-6780.svc jessie_tcp@_http._tcp.dns-test-service.dns-6780.svc]

    Jun 28 07:58:24.319: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
    Jun 28 07:58:24.363: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
    Jun 28 07:58:24.369: INFO: Unable to read wheezy_udp@dns-test-service.dns-6780 from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
    Jun 28 07:58:24.375: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6780 from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
    Jun 28 07:58:24.382: INFO: Unable to read wheezy_udp@dns-test-service.dns-6780.svc from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
    Jun 28 07:58:24.390: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6780.svc from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
    Jun 28 07:58:24.396: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6780.svc from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
    Jun 28 07:58:24.403: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6780.svc from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
    Jun 28 07:58:24.447: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
    Jun 28 07:58:24.454: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
    Jun 28 07:58:24.462: INFO: Unable to read jessie_udp@dns-test-service.dns-6780 from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
    Jun 28 07:58:24.470: INFO: Unable to read jessie_tcp@dns-test-service.dns-6780 from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
    Jun 28 07:58:24.478: INFO: Unable to read jessie_udp@dns-test-service.dns-6780.svc from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
    Jun 28 07:58:24.487: INFO: Unable to read jessie_tcp@dns-test-service.dns-6780.svc from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
    Jun 28 07:58:24.495: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6780.svc from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
    Jun 28 07:58:24.501: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6780.svc from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
    Jun 28 07:58:24.535: INFO: Lookups using dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6780 wheezy_tcp@dns-test-service.dns-6780 wheezy_udp@dns-test-service.dns-6780.svc wheezy_tcp@dns-test-service.dns-6780.svc wheezy_udp@_http._tcp.dns-test-service.dns-6780.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6780.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6780 jessie_tcp@dns-test-service.dns-6780 jessie_udp@dns-test-service.dns-6780.svc jessie_tcp@dns-test-service.dns-6780.svc jessie_udp@_http._tcp.dns-test-service.dns-6780.svc jessie_tcp@_http._tcp.dns-test-service.dns-6780.svc]

    Jun 28 07:58:29.318: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
    Jun 28 07:58:29.362: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
    Jun 28 07:58:29.370: INFO: Unable to read wheezy_udp@dns-test-service.dns-6780 from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
    Jun 28 07:58:29.377: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6780 from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
    Jun 28 07:58:29.384: INFO: Unable to read wheezy_udp@dns-test-service.dns-6780.svc from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
    Jun 28 07:58:29.391: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6780.svc from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
    Jun 28 07:58:29.398: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6780.svc from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
    Jun 28 07:58:29.405: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6780.svc from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
    Jun 28 07:58:29.439: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
    Jun 28 07:58:29.446: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
    Jun 28 07:58:29.452: INFO: Unable to read jessie_udp@dns-test-service.dns-6780 from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
    Jun 28 07:58:29.459: INFO: Unable to read jessie_tcp@dns-test-service.dns-6780 from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
    Jun 28 07:58:29.465: INFO: Unable to read jessie_udp@dns-test-service.dns-6780.svc from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
    Jun 28 07:58:29.472: INFO: Unable to read jessie_tcp@dns-test-service.dns-6780.svc from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
    Jun 28 07:58:29.479: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6780.svc from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
    Jun 28 07:58:29.485: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6780.svc from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
    Jun 28 07:58:29.512: INFO: Lookups using dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6780 wheezy_tcp@dns-test-service.dns-6780 wheezy_udp@dns-test-service.dns-6780.svc wheezy_tcp@dns-test-service.dns-6780.svc wheezy_udp@_http._tcp.dns-test-service.dns-6780.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6780.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6780 jessie_tcp@dns-test-service.dns-6780 jessie_udp@dns-test-service.dns-6780.svc jessie_tcp@dns-test-service.dns-6780.svc jessie_udp@_http._tcp.dns-test-service.dns-6780.svc jessie_tcp@_http._tcp.dns-test-service.dns-6780.svc]

    Jun 28 07:58:34.319: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
    Jun 28 07:58:34.339: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
    Jun 28 07:58:34.389: INFO: Unable to read wheezy_udp@dns-test-service.dns-6780 from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
    Jun 28 07:58:34.396: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6780 from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
    Jun 28 07:58:34.403: INFO: Unable to read wheezy_udp@dns-test-service.dns-6780.svc from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
    Jun 28 07:58:34.410: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6780.svc from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
    Jun 28 07:58:34.416: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6780.svc from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
    Jun 28 07:58:34.423: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6780.svc from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
    Jun 28 07:58:34.457: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
    Jun 28 07:58:34.464: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
    Jun 28 07:58:34.470: INFO: Unable to read jessie_udp@dns-test-service.dns-6780 from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
    Jun 28 07:58:34.477: INFO: Unable to read jessie_tcp@dns-test-service.dns-6780 from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
    Jun 28 07:58:34.491: INFO: Unable to read jessie_udp@dns-test-service.dns-6780.svc from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
    Jun 28 07:58:34.538: INFO: Unable to read jessie_tcp@dns-test-service.dns-6780.svc from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
    Jun 28 07:58:34.545: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6780.svc from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
    Jun 28 07:58:34.552: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6780.svc from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
    Jun 28 07:58:34.579: INFO: Lookups using dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6780 wheezy_tcp@dns-test-service.dns-6780 wheezy_udp@dns-test-service.dns-6780.svc wheezy_tcp@dns-test-service.dns-6780.svc wheezy_udp@_http._tcp.dns-test-service.dns-6780.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6780.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6780 jessie_tcp@dns-test-service.dns-6780 jessie_udp@dns-test-service.dns-6780.svc jessie_tcp@dns-test-service.dns-6780.svc jessie_udp@_http._tcp.dns-test-service.dns-6780.svc jessie_tcp@_http._tcp.dns-test-service.dns-6780.svc]

    Jun 28 07:58:39.359: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
    Jun 28 07:58:39.443: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
    Jun 28 07:58:39.450: INFO: Unable to read wheezy_udp@dns-test-service.dns-6780 from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
    Jun 28 07:58:39.457: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6780 from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
    Jun 28 07:58:39.464: INFO: Unable to read wheezy_udp@dns-test-service.dns-6780.svc from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
    Jun 28 07:58:39.471: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6780.svc from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
    Jun 28 07:58:39.478: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6780.svc from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
    Jun 28 07:58:39.486: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6780.svc from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
    Jun 28 07:58:39.522: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
    Jun 28 07:58:39.529: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
    Jun 28 07:58:39.535: INFO: Unable to read jessie_udp@dns-test-service.dns-6780 from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
    Jun 28 07:58:39.542: INFO: Unable to read jessie_tcp@dns-test-service.dns-6780 from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
    Jun 28 07:58:39.549: INFO: Unable to read jessie_udp@dns-test-service.dns-6780.svc from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
    Jun 28 07:58:39.557: INFO: Unable to read jessie_tcp@dns-test-service.dns-6780.svc from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
    Jun 28 07:58:39.564: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6780.svc from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
    Jun 28 07:58:39.571: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6780.svc from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
    Jun 28 07:58:39.602: INFO: Lookups using dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6780 wheezy_tcp@dns-test-service.dns-6780 wheezy_udp@dns-test-service.dns-6780.svc wheezy_tcp@dns-test-service.dns-6780.svc wheezy_udp@_http._tcp.dns-test-service.dns-6780.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6780.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6780 jessie_tcp@dns-test-service.dns-6780 jessie_udp@dns-test-service.dns-6780.svc jessie_tcp@dns-test-service.dns-6780.svc jessie_udp@_http._tcp.dns-test-service.dns-6780.svc jessie_tcp@_http._tcp.dns-test-service.dns-6780.svc]

    Jun 28 07:58:44.319: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
    Jun 28 07:58:44.362: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
    Jun 28 07:58:44.370: INFO: Unable to read wheezy_udp@dns-test-service.dns-6780 from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
    Jun 28 07:58:44.376: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6780 from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
    Jun 28 07:58:44.383: INFO: Unable to read wheezy_udp@dns-test-service.dns-6780.svc from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
    Jun 28 07:58:44.390: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6780.svc from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
    Jun 28 07:58:44.396: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6780.svc from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
    Jun 28 07:58:44.404: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6780.svc from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
    Jun 28 07:58:44.436: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
    Jun 28 07:58:44.442: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
    Jun 28 07:58:44.450: INFO: Unable to read jessie_udp@dns-test-service.dns-6780 from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
    Jun 28 07:58:44.456: INFO: Unable to read jessie_tcp@dns-test-service.dns-6780 from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
    Jun 28 07:58:44.463: INFO: Unable to read jessie_udp@dns-test-service.dns-6780.svc from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
    Jun 28 07:58:44.470: INFO: Unable to read jessie_tcp@dns-test-service.dns-6780.svc from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
    Jun 28 07:58:44.477: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6780.svc from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
    Jun 28 07:58:44.484: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6780.svc from pod dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf: the server could not find the requested resource (get pods dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf)
    Jun 28 07:58:44.510: INFO: Lookups using dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6780 wheezy_tcp@dns-test-service.dns-6780 wheezy_udp@dns-test-service.dns-6780.svc wheezy_tcp@dns-test-service.dns-6780.svc wheezy_udp@_http._tcp.dns-test-service.dns-6780.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6780.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6780 jessie_tcp@dns-test-service.dns-6780 jessie_udp@dns-test-service.dns-6780.svc jessie_tcp@dns-test-service.dns-6780.svc jessie_udp@_http._tcp.dns-test-service.dns-6780.svc jessie_tcp@_http._tcp.dns-test-service.dns-6780.svc]

    Jun 28 07:58:49.514: INFO: DNS probes using dns-6780/dns-test-892f6bc4-65a4-468b-85b7-2f6ff626b6cf succeeded

    STEP: deleting the pod 06/28/23 07:58:49.514
    STEP: deleting the test service 06/28/23 07:58:49.533
    STEP: deleting the test headless service 06/28/23 07:58:49.549
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jun 28 07:58:49.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-6780" for this suite. 06/28/23 07:58:49.567
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial]
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 07:58:49.573
Jun 28 07:58:49.574: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename taint-single-pod 06/28/23 07:58:49.576
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:58:49.59
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:58:49.594
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/node/taints.go:166
Jun 28 07:58:49.599: INFO: Waiting up to 1m0s for all nodes to be ready
Jun 28 07:59:49.642: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289
Jun 28 07:59:49.647: INFO: Starting informer...
STEP: Starting pod... 06/28/23 07:59:49.647
Jun 28 07:59:49.866: INFO: Pod is running on ske-rhel-749f7d55c8xdd8b6-ct4cp. Tainting Node
STEP: Trying to apply a taint on the Node 06/28/23 07:59:49.866
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 06/28/23 07:59:49.886
STEP: Waiting short time to make sure Pod is queued for deletion 06/28/23 07:59:49.893
Jun 28 07:59:49.893: INFO: Pod wasn't evicted. Proceeding
Jun 28 07:59:49.893: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 06/28/23 07:59:49.914
STEP: Waiting some time to make sure that toleration time passed. 06/28/23 07:59:49.92
Jun 28 08:01:04.920: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/framework.go:187
Jun 28 08:01:04.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-7014" for this suite. 06/28/23 08:01:04.929
{"msg":"PASSED [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","completed":63,"skipped":1092,"failed":0}
------------------------------
â€¢ [SLOW TEST] [135.365 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 07:58:49.573
    Jun 28 07:58:49.574: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename taint-single-pod 06/28/23 07:58:49.576
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 07:58:49.59
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 07:58:49.594
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/node/taints.go:166
    Jun 28 07:58:49.599: INFO: Waiting up to 1m0s for all nodes to be ready
    Jun 28 07:59:49.642: INFO: Waiting for terminating namespaces to be deleted...
    [It] removing taint cancels eviction [Disruptive] [Conformance]
      test/e2e/node/taints.go:289
    Jun 28 07:59:49.647: INFO: Starting informer...
    STEP: Starting pod... 06/28/23 07:59:49.647
    Jun 28 07:59:49.866: INFO: Pod is running on ske-rhel-749f7d55c8xdd8b6-ct4cp. Tainting Node
    STEP: Trying to apply a taint on the Node 06/28/23 07:59:49.866
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 06/28/23 07:59:49.886
    STEP: Waiting short time to make sure Pod is queued for deletion 06/28/23 07:59:49.893
    Jun 28 07:59:49.893: INFO: Pod wasn't evicted. Proceeding
    Jun 28 07:59:49.893: INFO: Removing taint from Node
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 06/28/23 07:59:49.914
    STEP: Waiting some time to make sure that toleration time passed. 06/28/23 07:59:49.92
    Jun 28 08:01:04.920: INFO: Pod wasn't evicted. Test successful
    [AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/framework.go:187
    Jun 28 08:01:04.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "taint-single-pod-7014" for this suite. 06/28/23 08:01:04.929
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:01:04.94
Jun 28 08:01:04.940: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename downward-api 06/28/23 08:01:04.941
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:01:04.955
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:01:04.959
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216
STEP: Creating a pod to test downward api env vars 06/28/23 08:01:04.963
Jun 28 08:01:04.970: INFO: Waiting up to 5m0s for pod "downward-api-053d61bb-0d19-4848-89aa-b2f0732d6274" in namespace "downward-api-3178" to be "Succeeded or Failed"
Jun 28 08:01:04.977: INFO: Pod "downward-api-053d61bb-0d19-4848-89aa-b2f0732d6274": Phase="Pending", Reason="", readiness=false. Elapsed: 6.304298ms
Jun 28 08:01:06.983: INFO: Pod "downward-api-053d61bb-0d19-4848-89aa-b2f0732d6274": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01236869s
Jun 28 08:01:08.982: INFO: Pod "downward-api-053d61bb-0d19-4848-89aa-b2f0732d6274": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011615662s
STEP: Saw pod success 06/28/23 08:01:08.982
Jun 28 08:01:08.982: INFO: Pod "downward-api-053d61bb-0d19-4848-89aa-b2f0732d6274" satisfied condition "Succeeded or Failed"
Jun 28 08:01:08.987: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod downward-api-053d61bb-0d19-4848-89aa-b2f0732d6274 container dapi-container: <nil>
STEP: delete the pod 06/28/23 08:01:09
Jun 28 08:01:09.013: INFO: Waiting for pod downward-api-053d61bb-0d19-4848-89aa-b2f0732d6274 to disappear
Jun 28 08:01:09.017: INFO: Pod downward-api-053d61bb-0d19-4848-89aa-b2f0732d6274 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Jun 28 08:01:09.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3178" for this suite. 06/28/23 08:01:09.024
{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","completed":64,"skipped":1122,"failed":0}
------------------------------
â€¢ [4.091 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:01:04.94
    Jun 28 08:01:04.940: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename downward-api 06/28/23 08:01:04.941
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:01:04.955
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:01:04.959
    [It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:216
    STEP: Creating a pod to test downward api env vars 06/28/23 08:01:04.963
    Jun 28 08:01:04.970: INFO: Waiting up to 5m0s for pod "downward-api-053d61bb-0d19-4848-89aa-b2f0732d6274" in namespace "downward-api-3178" to be "Succeeded or Failed"
    Jun 28 08:01:04.977: INFO: Pod "downward-api-053d61bb-0d19-4848-89aa-b2f0732d6274": Phase="Pending", Reason="", readiness=false. Elapsed: 6.304298ms
    Jun 28 08:01:06.983: INFO: Pod "downward-api-053d61bb-0d19-4848-89aa-b2f0732d6274": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01236869s
    Jun 28 08:01:08.982: INFO: Pod "downward-api-053d61bb-0d19-4848-89aa-b2f0732d6274": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011615662s
    STEP: Saw pod success 06/28/23 08:01:08.982
    Jun 28 08:01:08.982: INFO: Pod "downward-api-053d61bb-0d19-4848-89aa-b2f0732d6274" satisfied condition "Succeeded or Failed"
    Jun 28 08:01:08.987: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod downward-api-053d61bb-0d19-4848-89aa-b2f0732d6274 container dapi-container: <nil>
    STEP: delete the pod 06/28/23 08:01:09
    Jun 28 08:01:09.013: INFO: Waiting for pod downward-api-053d61bb-0d19-4848-89aa-b2f0732d6274 to disappear
    Jun 28 08:01:09.017: INFO: Pod downward-api-053d61bb-0d19-4848-89aa-b2f0732d6274 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Jun 28 08:01:09.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-3178" for this suite. 06/28/23 08:01:09.024
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2194
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:01:09.032
Jun 28 08:01:09.032: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename services 06/28/23 08:01:09.033
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:01:09.048
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:01:09.052
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2194
STEP: creating service in namespace services-3328 06/28/23 08:01:09.057
STEP: creating service affinity-nodeport in namespace services-3328 06/28/23 08:01:09.057
STEP: creating replication controller affinity-nodeport in namespace services-3328 06/28/23 08:01:09.073
I0628 08:01:09.080320      18 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-3328, replica count: 3
I0628 08:01:12.130751      18 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 28 08:01:12.146: INFO: Creating new exec pod
Jun 28 08:01:12.152: INFO: Waiting up to 5m0s for pod "execpod-affinityjbqvd" in namespace "services-3328" to be "running"
Jun 28 08:01:12.156: INFO: Pod "execpod-affinityjbqvd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.814726ms
Jun 28 08:01:14.161: INFO: Pod "execpod-affinityjbqvd": Phase="Running", Reason="", readiness=true. Elapsed: 2.008916011s
Jun 28 08:01:14.161: INFO: Pod "execpod-affinityjbqvd" satisfied condition "running"
Jun 28 08:01:15.171: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-3328 exec execpod-affinityjbqvd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
Jun 28 08:01:15.604: INFO: stderr: "+ + ncecho -v hostName -t\n -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Jun 28 08:01:15.604: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun 28 08:01:15.604: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-3328 exec execpod-affinityjbqvd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.20.96.141 80'
Jun 28 08:01:16.017: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.20.96.141 80\nConnection to 172.20.96.141 80 port [tcp/http] succeeded!\n"
Jun 28 08:01:16.017: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun 28 08:01:16.017: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-3328 exec execpod-affinityjbqvd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.11.5 32004'
Jun 28 08:01:16.402: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.11.5 32004\nConnection to 192.168.11.5 32004 port [tcp/*] succeeded!\n"
Jun 28 08:01:16.402: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun 28 08:01:16.402: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-3328 exec execpod-affinityjbqvd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.11.4 32004'
Jun 28 08:01:16.880: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.11.4 32004\nConnection to 192.168.11.4 32004 port [tcp/*] succeeded!\n"
Jun 28 08:01:16.880: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun 28 08:01:16.880: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-3328 exec execpod-affinityjbqvd -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.11.3:32004/ ; done'
Jun 28 08:01:17.285: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:32004/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:32004/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:32004/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:32004/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:32004/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:32004/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:32004/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:32004/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:32004/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:32004/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:32004/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:32004/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:32004/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:32004/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:32004/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:32004/\n"
Jun 28 08:01:17.286: INFO: stdout: "\naffinity-nodeport-nh74q\naffinity-nodeport-nh74q\naffinity-nodeport-nh74q\naffinity-nodeport-nh74q\naffinity-nodeport-nh74q\naffinity-nodeport-nh74q\naffinity-nodeport-nh74q\naffinity-nodeport-nh74q\naffinity-nodeport-nh74q\naffinity-nodeport-nh74q\naffinity-nodeport-nh74q\naffinity-nodeport-nh74q\naffinity-nodeport-nh74q\naffinity-nodeport-nh74q\naffinity-nodeport-nh74q\naffinity-nodeport-nh74q"
Jun 28 08:01:17.286: INFO: Received response from host: affinity-nodeport-nh74q
Jun 28 08:01:17.286: INFO: Received response from host: affinity-nodeport-nh74q
Jun 28 08:01:17.286: INFO: Received response from host: affinity-nodeport-nh74q
Jun 28 08:01:17.286: INFO: Received response from host: affinity-nodeport-nh74q
Jun 28 08:01:17.286: INFO: Received response from host: affinity-nodeport-nh74q
Jun 28 08:01:17.286: INFO: Received response from host: affinity-nodeport-nh74q
Jun 28 08:01:17.286: INFO: Received response from host: affinity-nodeport-nh74q
Jun 28 08:01:17.286: INFO: Received response from host: affinity-nodeport-nh74q
Jun 28 08:01:17.286: INFO: Received response from host: affinity-nodeport-nh74q
Jun 28 08:01:17.286: INFO: Received response from host: affinity-nodeport-nh74q
Jun 28 08:01:17.286: INFO: Received response from host: affinity-nodeport-nh74q
Jun 28 08:01:17.286: INFO: Received response from host: affinity-nodeport-nh74q
Jun 28 08:01:17.286: INFO: Received response from host: affinity-nodeport-nh74q
Jun 28 08:01:17.286: INFO: Received response from host: affinity-nodeport-nh74q
Jun 28 08:01:17.286: INFO: Received response from host: affinity-nodeport-nh74q
Jun 28 08:01:17.286: INFO: Received response from host: affinity-nodeport-nh74q
Jun 28 08:01:17.286: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-3328, will wait for the garbage collector to delete the pods 06/28/23 08:01:17.297
Jun 28 08:01:17.360: INFO: Deleting ReplicationController affinity-nodeport took: 7.391328ms
Jun 28 08:01:17.561: INFO: Terminating ReplicationController affinity-nodeport pods took: 200.97385ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jun 28 08:01:19.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3328" for this suite. 06/28/23 08:01:19.486
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","completed":65,"skipped":1136,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.461 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:01:09.032
    Jun 28 08:01:09.032: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename services 06/28/23 08:01:09.033
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:01:09.048
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:01:09.052
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2194
    STEP: creating service in namespace services-3328 06/28/23 08:01:09.057
    STEP: creating service affinity-nodeport in namespace services-3328 06/28/23 08:01:09.057
    STEP: creating replication controller affinity-nodeport in namespace services-3328 06/28/23 08:01:09.073
    I0628 08:01:09.080320      18 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-3328, replica count: 3
    I0628 08:01:12.130751      18 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jun 28 08:01:12.146: INFO: Creating new exec pod
    Jun 28 08:01:12.152: INFO: Waiting up to 5m0s for pod "execpod-affinityjbqvd" in namespace "services-3328" to be "running"
    Jun 28 08:01:12.156: INFO: Pod "execpod-affinityjbqvd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.814726ms
    Jun 28 08:01:14.161: INFO: Pod "execpod-affinityjbqvd": Phase="Running", Reason="", readiness=true. Elapsed: 2.008916011s
    Jun 28 08:01:14.161: INFO: Pod "execpod-affinityjbqvd" satisfied condition "running"
    Jun 28 08:01:15.171: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-3328 exec execpod-affinityjbqvd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
    Jun 28 08:01:15.604: INFO: stderr: "+ + ncecho -v hostName -t\n -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
    Jun 28 08:01:15.604: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jun 28 08:01:15.604: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-3328 exec execpod-affinityjbqvd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.20.96.141 80'
    Jun 28 08:01:16.017: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.20.96.141 80\nConnection to 172.20.96.141 80 port [tcp/http] succeeded!\n"
    Jun 28 08:01:16.017: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jun 28 08:01:16.017: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-3328 exec execpod-affinityjbqvd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.11.5 32004'
    Jun 28 08:01:16.402: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.11.5 32004\nConnection to 192.168.11.5 32004 port [tcp/*] succeeded!\n"
    Jun 28 08:01:16.402: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jun 28 08:01:16.402: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-3328 exec execpod-affinityjbqvd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.11.4 32004'
    Jun 28 08:01:16.880: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.11.4 32004\nConnection to 192.168.11.4 32004 port [tcp/*] succeeded!\n"
    Jun 28 08:01:16.880: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jun 28 08:01:16.880: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-3328 exec execpod-affinityjbqvd -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.11.3:32004/ ; done'
    Jun 28 08:01:17.285: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:32004/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:32004/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:32004/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:32004/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:32004/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:32004/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:32004/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:32004/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:32004/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:32004/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:32004/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:32004/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:32004/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:32004/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:32004/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:32004/\n"
    Jun 28 08:01:17.286: INFO: stdout: "\naffinity-nodeport-nh74q\naffinity-nodeport-nh74q\naffinity-nodeport-nh74q\naffinity-nodeport-nh74q\naffinity-nodeport-nh74q\naffinity-nodeport-nh74q\naffinity-nodeport-nh74q\naffinity-nodeport-nh74q\naffinity-nodeport-nh74q\naffinity-nodeport-nh74q\naffinity-nodeport-nh74q\naffinity-nodeport-nh74q\naffinity-nodeport-nh74q\naffinity-nodeport-nh74q\naffinity-nodeport-nh74q\naffinity-nodeport-nh74q"
    Jun 28 08:01:17.286: INFO: Received response from host: affinity-nodeport-nh74q
    Jun 28 08:01:17.286: INFO: Received response from host: affinity-nodeport-nh74q
    Jun 28 08:01:17.286: INFO: Received response from host: affinity-nodeport-nh74q
    Jun 28 08:01:17.286: INFO: Received response from host: affinity-nodeport-nh74q
    Jun 28 08:01:17.286: INFO: Received response from host: affinity-nodeport-nh74q
    Jun 28 08:01:17.286: INFO: Received response from host: affinity-nodeport-nh74q
    Jun 28 08:01:17.286: INFO: Received response from host: affinity-nodeport-nh74q
    Jun 28 08:01:17.286: INFO: Received response from host: affinity-nodeport-nh74q
    Jun 28 08:01:17.286: INFO: Received response from host: affinity-nodeport-nh74q
    Jun 28 08:01:17.286: INFO: Received response from host: affinity-nodeport-nh74q
    Jun 28 08:01:17.286: INFO: Received response from host: affinity-nodeport-nh74q
    Jun 28 08:01:17.286: INFO: Received response from host: affinity-nodeport-nh74q
    Jun 28 08:01:17.286: INFO: Received response from host: affinity-nodeport-nh74q
    Jun 28 08:01:17.286: INFO: Received response from host: affinity-nodeport-nh74q
    Jun 28 08:01:17.286: INFO: Received response from host: affinity-nodeport-nh74q
    Jun 28 08:01:17.286: INFO: Received response from host: affinity-nodeport-nh74q
    Jun 28 08:01:17.286: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport in namespace services-3328, will wait for the garbage collector to delete the pods 06/28/23 08:01:17.297
    Jun 28 08:01:17.360: INFO: Deleting ReplicationController affinity-nodeport took: 7.391328ms
    Jun 28 08:01:17.561: INFO: Terminating ReplicationController affinity-nodeport pods took: 200.97385ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jun 28 08:01:19.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-3328" for this suite. 06/28/23 08:01:19.486
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:01:19.493
Jun 28 08:01:19.493: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename webhook 06/28/23 08:01:19.494
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:01:19.507
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:01:19.511
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 06/28/23 08:01:19.528
STEP: Create role binding to let webhook read extension-apiserver-authentication 06/28/23 08:01:19.78
STEP: Deploying the webhook pod 06/28/23 08:01:19.789
STEP: Wait for the deployment to be ready 06/28/23 08:01:19.803
Jun 28 08:01:19.813: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 06/28/23 08:01:21.829
STEP: Verifying the service has paired with the endpoint 06/28/23 08:01:21.84
Jun 28 08:01:22.840: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412
STEP: Creating a validating webhook configuration 06/28/23 08:01:22.846
STEP: Creating a configMap that does not comply to the validation webhook rules 06/28/23 08:01:22.951
STEP: Updating a validating webhook configuration's rules to not include the create operation 06/28/23 08:01:23.049
STEP: Creating a configMap that does not comply to the validation webhook rules 06/28/23 08:01:23.061
STEP: Patching a validating webhook configuration's rules to include the create operation 06/28/23 08:01:23.075
STEP: Creating a configMap that does not comply to the validation webhook rules 06/28/23 08:01:23.082
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jun 28 08:01:23.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7998" for this suite. 06/28/23 08:01:23.152
STEP: Destroying namespace "webhook-7998-markers" for this suite. 06/28/23 08:01:23.16
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","completed":66,"skipped":1144,"failed":0}
------------------------------
â€¢ [3.715 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:01:19.493
    Jun 28 08:01:19.493: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename webhook 06/28/23 08:01:19.494
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:01:19.507
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:01:19.511
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 06/28/23 08:01:19.528
    STEP: Create role binding to let webhook read extension-apiserver-authentication 06/28/23 08:01:19.78
    STEP: Deploying the webhook pod 06/28/23 08:01:19.789
    STEP: Wait for the deployment to be ready 06/28/23 08:01:19.803
    Jun 28 08:01:19.813: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 06/28/23 08:01:21.829
    STEP: Verifying the service has paired with the endpoint 06/28/23 08:01:21.84
    Jun 28 08:01:22.840: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a validating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:412
    STEP: Creating a validating webhook configuration 06/28/23 08:01:22.846
    STEP: Creating a configMap that does not comply to the validation webhook rules 06/28/23 08:01:22.951
    STEP: Updating a validating webhook configuration's rules to not include the create operation 06/28/23 08:01:23.049
    STEP: Creating a configMap that does not comply to the validation webhook rules 06/28/23 08:01:23.061
    STEP: Patching a validating webhook configuration's rules to include the create operation 06/28/23 08:01:23.075
    STEP: Creating a configMap that does not comply to the validation webhook rules 06/28/23 08:01:23.082
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jun 28 08:01:23.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-7998" for this suite. 06/28/23 08:01:23.152
    STEP: Destroying namespace "webhook-7998-markers" for this suite. 06/28/23 08:01:23.16
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] Services
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:01:23.208
Jun 28 08:01:23.209: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename services 06/28/23 08:01:23.209
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:01:23.224
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:01:23.228
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852
STEP: creating service multi-endpoint-test in namespace services-7835 06/28/23 08:01:23.232
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7835 to expose endpoints map[] 06/28/23 08:01:23.241
Jun 28 08:01:23.253: INFO: successfully validated that service multi-endpoint-test in namespace services-7835 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-7835 06/28/23 08:01:23.253
Jun 28 08:01:23.262: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-7835" to be "running and ready"
Jun 28 08:01:23.266: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.712327ms
Jun 28 08:01:23.266: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jun 28 08:01:25.271: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.009670214s
Jun 28 08:01:25.272: INFO: The phase of Pod pod1 is Running (Ready = true)
Jun 28 08:01:25.272: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7835 to expose endpoints map[pod1:[100]] 06/28/23 08:01:25.276
Jun 28 08:01:25.292: INFO: successfully validated that service multi-endpoint-test in namespace services-7835 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-7835 06/28/23 08:01:25.292
Jun 28 08:01:25.300: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-7835" to be "running and ready"
Jun 28 08:01:25.304: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.707693ms
Jun 28 08:01:25.304: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jun 28 08:01:27.309: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.008823163s
Jun 28 08:01:27.309: INFO: The phase of Pod pod2 is Running (Ready = true)
Jun 28 08:01:27.309: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7835 to expose endpoints map[pod1:[100] pod2:[101]] 06/28/23 08:01:27.313
Jun 28 08:01:27.334: INFO: successfully validated that service multi-endpoint-test in namespace services-7835 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods 06/28/23 08:01:27.335
Jun 28 08:01:27.335: INFO: Creating new exec pod
Jun 28 08:01:27.340: INFO: Waiting up to 5m0s for pod "execpod5sqdh" in namespace "services-7835" to be "running"
Jun 28 08:01:27.344: INFO: Pod "execpod5sqdh": Phase="Pending", Reason="", readiness=false. Elapsed: 4.195372ms
Jun 28 08:01:29.349: INFO: Pod "execpod5sqdh": Phase="Running", Reason="", readiness=true. Elapsed: 2.009530385s
Jun 28 08:01:29.349: INFO: Pod "execpod5sqdh" satisfied condition "running"
Jun 28 08:01:30.350: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-7835 exec execpod5sqdh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
Jun 28 08:01:30.788: INFO: stderr: "+ nc -v -t -w 2 multi-endpoint-test 80\n+ echo hostName\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Jun 28 08:01:30.788: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun 28 08:01:30.788: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-7835 exec execpod5sqdh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.20.48.186 80'
Jun 28 08:01:31.270: INFO: stderr: "+ nc -v -t -w 2 172.20.48.186 80\nConnection to 172.20.48.186 80 port [tcp/http] succeeded!\n+ echo hostName\n"
Jun 28 08:01:31.270: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun 28 08:01:31.270: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-7835 exec execpod5sqdh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
Jun 28 08:01:31.620: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Jun 28 08:01:31.620: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun 28 08:01:31.620: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-7835 exec execpod5sqdh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.20.48.186 81'
Jun 28 08:01:32.060: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.20.48.186 81\nConnection to 172.20.48.186 81 port [tcp/*] succeeded!\n"
Jun 28 08:01:32.060: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-7835 06/28/23 08:01:32.06
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7835 to expose endpoints map[pod2:[101]] 06/28/23 08:01:32.072
Jun 28 08:01:32.092: INFO: successfully validated that service multi-endpoint-test in namespace services-7835 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-7835 06/28/23 08:01:32.092
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7835 to expose endpoints map[] 06/28/23 08:01:32.114
Jun 28 08:01:32.149: INFO: successfully validated that service multi-endpoint-test in namespace services-7835 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jun 28 08:01:32.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7835" for this suite. 06/28/23 08:01:32.18
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","completed":67,"skipped":1148,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.979 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:01:23.208
    Jun 28 08:01:23.209: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename services 06/28/23 08:01:23.209
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:01:23.224
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:01:23.228
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should serve multiport endpoints from pods  [Conformance]
      test/e2e/network/service.go:852
    STEP: creating service multi-endpoint-test in namespace services-7835 06/28/23 08:01:23.232
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7835 to expose endpoints map[] 06/28/23 08:01:23.241
    Jun 28 08:01:23.253: INFO: successfully validated that service multi-endpoint-test in namespace services-7835 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-7835 06/28/23 08:01:23.253
    Jun 28 08:01:23.262: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-7835" to be "running and ready"
    Jun 28 08:01:23.266: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.712327ms
    Jun 28 08:01:23.266: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jun 28 08:01:25.271: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.009670214s
    Jun 28 08:01:25.272: INFO: The phase of Pod pod1 is Running (Ready = true)
    Jun 28 08:01:25.272: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7835 to expose endpoints map[pod1:[100]] 06/28/23 08:01:25.276
    Jun 28 08:01:25.292: INFO: successfully validated that service multi-endpoint-test in namespace services-7835 exposes endpoints map[pod1:[100]]
    STEP: Creating pod pod2 in namespace services-7835 06/28/23 08:01:25.292
    Jun 28 08:01:25.300: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-7835" to be "running and ready"
    Jun 28 08:01:25.304: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.707693ms
    Jun 28 08:01:25.304: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jun 28 08:01:27.309: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.008823163s
    Jun 28 08:01:27.309: INFO: The phase of Pod pod2 is Running (Ready = true)
    Jun 28 08:01:27.309: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7835 to expose endpoints map[pod1:[100] pod2:[101]] 06/28/23 08:01:27.313
    Jun 28 08:01:27.334: INFO: successfully validated that service multi-endpoint-test in namespace services-7835 exposes endpoints map[pod1:[100] pod2:[101]]
    STEP: Checking if the Service forwards traffic to pods 06/28/23 08:01:27.335
    Jun 28 08:01:27.335: INFO: Creating new exec pod
    Jun 28 08:01:27.340: INFO: Waiting up to 5m0s for pod "execpod5sqdh" in namespace "services-7835" to be "running"
    Jun 28 08:01:27.344: INFO: Pod "execpod5sqdh": Phase="Pending", Reason="", readiness=false. Elapsed: 4.195372ms
    Jun 28 08:01:29.349: INFO: Pod "execpod5sqdh": Phase="Running", Reason="", readiness=true. Elapsed: 2.009530385s
    Jun 28 08:01:29.349: INFO: Pod "execpod5sqdh" satisfied condition "running"
    Jun 28 08:01:30.350: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-7835 exec execpod5sqdh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
    Jun 28 08:01:30.788: INFO: stderr: "+ nc -v -t -w 2 multi-endpoint-test 80\n+ echo hostName\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
    Jun 28 08:01:30.788: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jun 28 08:01:30.788: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-7835 exec execpod5sqdh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.20.48.186 80'
    Jun 28 08:01:31.270: INFO: stderr: "+ nc -v -t -w 2 172.20.48.186 80\nConnection to 172.20.48.186 80 port [tcp/http] succeeded!\n+ echo hostName\n"
    Jun 28 08:01:31.270: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jun 28 08:01:31.270: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-7835 exec execpod5sqdh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
    Jun 28 08:01:31.620: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
    Jun 28 08:01:31.620: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jun 28 08:01:31.620: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-7835 exec execpod5sqdh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.20.48.186 81'
    Jun 28 08:01:32.060: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.20.48.186 81\nConnection to 172.20.48.186 81 port [tcp/*] succeeded!\n"
    Jun 28 08:01:32.060: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod1 in namespace services-7835 06/28/23 08:01:32.06
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7835 to expose endpoints map[pod2:[101]] 06/28/23 08:01:32.072
    Jun 28 08:01:32.092: INFO: successfully validated that service multi-endpoint-test in namespace services-7835 exposes endpoints map[pod2:[101]]
    STEP: Deleting pod pod2 in namespace services-7835 06/28/23 08:01:32.092
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7835 to expose endpoints map[] 06/28/23 08:01:32.114
    Jun 28 08:01:32.149: INFO: successfully validated that service multi-endpoint-test in namespace services-7835 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jun 28 08:01:32.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-7835" for this suite. 06/28/23 08:01:32.18
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:01:32.188
Jun 28 08:01:32.188: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename kubelet-test 06/28/23 08:01:32.189
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:01:32.204
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:01:32.209
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
STEP: Waiting for pod completion 06/28/23 08:01:32.228
Jun 28 08:01:32.228: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases7c4af0ac-056f-4978-8e8e-eec76b081876" in namespace "kubelet-test-4151" to be "completed"
Jun 28 08:01:32.237: INFO: Pod "agnhost-host-aliases7c4af0ac-056f-4978-8e8e-eec76b081876": Phase="Pending", Reason="", readiness=false. Elapsed: 8.916244ms
Jun 28 08:01:34.243: INFO: Pod "agnhost-host-aliases7c4af0ac-056f-4978-8e8e-eec76b081876": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014572623s
Jun 28 08:01:36.243: INFO: Pod "agnhost-host-aliases7c4af0ac-056f-4978-8e8e-eec76b081876": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014697428s
Jun 28 08:01:36.243: INFO: Pod "agnhost-host-aliases7c4af0ac-056f-4978-8e8e-eec76b081876" satisfied condition "completed"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Jun 28 08:01:36.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4151" for this suite. 06/28/23 08:01:36.305
{"msg":"PASSED [sig-node] Kubelet when scheduling an agnhost Pod with hostAliases should write entries to /etc/hosts [NodeConformance] [Conformance]","completed":68,"skipped":1148,"failed":0}
------------------------------
â€¢ [4.124 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling an agnhost Pod with hostAliases
  test/e2e/common/node/kubelet.go:140
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:01:32.188
    Jun 28 08:01:32.188: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename kubelet-test 06/28/23 08:01:32.189
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:01:32.204
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:01:32.209
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should write entries to /etc/hosts [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:148
    STEP: Waiting for pod completion 06/28/23 08:01:32.228
    Jun 28 08:01:32.228: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases7c4af0ac-056f-4978-8e8e-eec76b081876" in namespace "kubelet-test-4151" to be "completed"
    Jun 28 08:01:32.237: INFO: Pod "agnhost-host-aliases7c4af0ac-056f-4978-8e8e-eec76b081876": Phase="Pending", Reason="", readiness=false. Elapsed: 8.916244ms
    Jun 28 08:01:34.243: INFO: Pod "agnhost-host-aliases7c4af0ac-056f-4978-8e8e-eec76b081876": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014572623s
    Jun 28 08:01:36.243: INFO: Pod "agnhost-host-aliases7c4af0ac-056f-4978-8e8e-eec76b081876": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014697428s
    Jun 28 08:01:36.243: INFO: Pod "agnhost-host-aliases7c4af0ac-056f-4978-8e8e-eec76b081876" satisfied condition "completed"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Jun 28 08:01:36.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-4151" for this suite. 06/28/23 08:01:36.305
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:01:36.313
Jun 28 08:01:36.313: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename sysctl 06/28/23 08:01:36.314
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:01:36.327
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:01:36.331
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 06/28/23 08:01:36.336
STEP: Watching for error events or started pod 06/28/23 08:01:36.345
STEP: Waiting for pod completion 06/28/23 08:01:38.353
Jun 28 08:01:38.353: INFO: Waiting up to 3m0s for pod "sysctl-299740c7-8e36-422c-b485-51ae6eb6e3e2" in namespace "sysctl-101" to be "completed"
Jun 28 08:01:38.358: INFO: Pod "sysctl-299740c7-8e36-422c-b485-51ae6eb6e3e2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.125072ms
Jun 28 08:01:40.364: INFO: Pod "sysctl-299740c7-8e36-422c-b485-51ae6eb6e3e2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010393063s
Jun 28 08:01:40.364: INFO: Pod "sysctl-299740c7-8e36-422c-b485-51ae6eb6e3e2" satisfied condition "completed"
STEP: Checking that the pod succeeded 06/28/23 08:01:40.368
STEP: Getting logs from the pod 06/28/23 08:01:40.368
STEP: Checking that the sysctl is actually updated 06/28/23 08:01:40.378
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:187
Jun 28 08:01:40.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-101" for this suite. 06/28/23 08:01:40.385
{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]","completed":69,"skipped":1188,"failed":0}
------------------------------
â€¢ [4.079 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:01:36.313
    Jun 28 08:01:36.313: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename sysctl 06/28/23 08:01:36.314
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:01:36.327
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:01:36.331
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:77
    STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 06/28/23 08:01:36.336
    STEP: Watching for error events or started pod 06/28/23 08:01:36.345
    STEP: Waiting for pod completion 06/28/23 08:01:38.353
    Jun 28 08:01:38.353: INFO: Waiting up to 3m0s for pod "sysctl-299740c7-8e36-422c-b485-51ae6eb6e3e2" in namespace "sysctl-101" to be "completed"
    Jun 28 08:01:38.358: INFO: Pod "sysctl-299740c7-8e36-422c-b485-51ae6eb6e3e2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.125072ms
    Jun 28 08:01:40.364: INFO: Pod "sysctl-299740c7-8e36-422c-b485-51ae6eb6e3e2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010393063s
    Jun 28 08:01:40.364: INFO: Pod "sysctl-299740c7-8e36-422c-b485-51ae6eb6e3e2" satisfied condition "completed"
    STEP: Checking that the pod succeeded 06/28/23 08:01:40.368
    STEP: Getting logs from the pod 06/28/23 08:01:40.368
    STEP: Checking that the sysctl is actually updated 06/28/23 08:01:40.378
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:187
    Jun 28 08:01:40.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sysctl-101" for this suite. 06/28/23 08:01:40.385
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:01:40.393
Jun 28 08:01:40.393: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename custom-resource-definition 06/28/23 08:01:40.393
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:01:40.408
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:01:40.412
[It] should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
STEP: fetching the /apis discovery document 06/28/23 08:01:40.416
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 06/28/23 08:01:40.418
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 06/28/23 08:01:40.418
STEP: fetching the /apis/apiextensions.k8s.io discovery document 06/28/23 08:01:40.418
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 06/28/23 08:01:40.419
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 06/28/23 08:01:40.42
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 06/28/23 08:01:40.421
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jun 28 08:01:40.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-157" for this suite. 06/28/23 08:01:40.429
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","completed":70,"skipped":1189,"failed":0}
------------------------------
â€¢ [0.043 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:01:40.393
    Jun 28 08:01:40.393: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename custom-resource-definition 06/28/23 08:01:40.393
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:01:40.408
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:01:40.412
    [It] should include custom resource definition resources in discovery documents [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:198
    STEP: fetching the /apis discovery document 06/28/23 08:01:40.416
    STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 06/28/23 08:01:40.418
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 06/28/23 08:01:40.418
    STEP: fetching the /apis/apiextensions.k8s.io discovery document 06/28/23 08:01:40.418
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 06/28/23 08:01:40.419
    STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 06/28/23 08:01:40.42
    STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 06/28/23 08:01:40.421
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jun 28 08:01:40.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-157" for this suite. 06/28/23 08:01:40.429
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1248
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:01:40.437
Jun 28 08:01:40.437: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename kubectl 06/28/23 08:01:40.438
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:01:40.452
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:01:40.456
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1248
STEP: validating cluster-info 06/28/23 08:01:40.462
Jun 28 08:01:40.462: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-9193 cluster-info'
Jun 28 08:01:40.527: INFO: stderr: ""
Jun 28 08:01:40.527: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://172.20.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jun 28 08:01:40.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9193" for this suite. 06/28/23 08:01:40.535
{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","completed":71,"skipped":1213,"failed":0}
------------------------------
â€¢ [0.105 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  test/e2e/kubectl/kubectl.go:1242
    should check if Kubernetes control plane services is included in cluster-info  [Conformance]
    test/e2e/kubectl/kubectl.go:1248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:01:40.437
    Jun 28 08:01:40.437: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename kubectl 06/28/23 08:01:40.438
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:01:40.452
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:01:40.456
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
      test/e2e/kubectl/kubectl.go:1248
    STEP: validating cluster-info 06/28/23 08:01:40.462
    Jun 28 08:01:40.462: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-9193 cluster-info'
    Jun 28 08:01:40.527: INFO: stderr: ""
    Jun 28 08:01:40.527: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://172.20.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jun 28 08:01:40.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-9193" for this suite. 06/28/23 08:01:40.535
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:01:40.544
Jun 28 08:01:40.544: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename downward-api 06/28/23 08:01:40.545
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:01:40.559
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:01:40.563
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260
STEP: Creating a pod to test downward API volume plugin 06/28/23 08:01:40.568
Jun 28 08:01:40.577: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3af9deb5-7eaf-4a5d-8c1e-784a2e1b8b6a" in namespace "downward-api-8537" to be "Succeeded or Failed"
Jun 28 08:01:40.581: INFO: Pod "downwardapi-volume-3af9deb5-7eaf-4a5d-8c1e-784a2e1b8b6a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.409245ms
Jun 28 08:01:42.587: INFO: Pod "downwardapi-volume-3af9deb5-7eaf-4a5d-8c1e-784a2e1b8b6a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010129482s
Jun 28 08:01:44.588: INFO: Pod "downwardapi-volume-3af9deb5-7eaf-4a5d-8c1e-784a2e1b8b6a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011628972s
STEP: Saw pod success 06/28/23 08:01:44.588
Jun 28 08:01:44.588: INFO: Pod "downwardapi-volume-3af9deb5-7eaf-4a5d-8c1e-784a2e1b8b6a" satisfied condition "Succeeded or Failed"
Jun 28 08:01:44.593: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod downwardapi-volume-3af9deb5-7eaf-4a5d-8c1e-784a2e1b8b6a container client-container: <nil>
STEP: delete the pod 06/28/23 08:01:44.603
Jun 28 08:01:44.615: INFO: Waiting for pod downwardapi-volume-3af9deb5-7eaf-4a5d-8c1e-784a2e1b8b6a to disappear
Jun 28 08:01:44.619: INFO: Pod downwardapi-volume-3af9deb5-7eaf-4a5d-8c1e-784a2e1b8b6a no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jun 28 08:01:44.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8537" for this suite. 06/28/23 08:01:44.627
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","completed":72,"skipped":1227,"failed":0}
------------------------------
â€¢ [4.091 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:01:40.544
    Jun 28 08:01:40.544: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename downward-api 06/28/23 08:01:40.545
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:01:40.559
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:01:40.563
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:260
    STEP: Creating a pod to test downward API volume plugin 06/28/23 08:01:40.568
    Jun 28 08:01:40.577: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3af9deb5-7eaf-4a5d-8c1e-784a2e1b8b6a" in namespace "downward-api-8537" to be "Succeeded or Failed"
    Jun 28 08:01:40.581: INFO: Pod "downwardapi-volume-3af9deb5-7eaf-4a5d-8c1e-784a2e1b8b6a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.409245ms
    Jun 28 08:01:42.587: INFO: Pod "downwardapi-volume-3af9deb5-7eaf-4a5d-8c1e-784a2e1b8b6a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010129482s
    Jun 28 08:01:44.588: INFO: Pod "downwardapi-volume-3af9deb5-7eaf-4a5d-8c1e-784a2e1b8b6a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011628972s
    STEP: Saw pod success 06/28/23 08:01:44.588
    Jun 28 08:01:44.588: INFO: Pod "downwardapi-volume-3af9deb5-7eaf-4a5d-8c1e-784a2e1b8b6a" satisfied condition "Succeeded or Failed"
    Jun 28 08:01:44.593: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod downwardapi-volume-3af9deb5-7eaf-4a5d-8c1e-784a2e1b8b6a container client-container: <nil>
    STEP: delete the pod 06/28/23 08:01:44.603
    Jun 28 08:01:44.615: INFO: Waiting for pod downwardapi-volume-3af9deb5-7eaf-4a5d-8c1e-784a2e1b8b6a to disappear
    Jun 28 08:01:44.619: INFO: Pod downwardapi-volume-3af9deb5-7eaf-4a5d-8c1e-784a2e1b8b6a no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jun 28 08:01:44.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-8537" for this suite. 06/28/23 08:01:44.627
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:01:44.636
Jun 28 08:01:44.636: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename pod-network-test 06/28/23 08:01:44.637
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:01:44.65
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:01:44.655
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
STEP: Performing setup for networking test in namespace pod-network-test-4129 06/28/23 08:01:44.659
STEP: creating a selector 06/28/23 08:01:44.659
STEP: Creating the service pods in kubernetes 06/28/23 08:01:44.659
Jun 28 08:01:44.659: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jun 28 08:01:44.696: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-4129" to be "running and ready"
Jun 28 08:01:44.705: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 9.198766ms
Jun 28 08:01:44.705: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun 28 08:01:46.711: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.015431378s
Jun 28 08:01:46.711: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 28 08:01:48.709: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.013536222s
Jun 28 08:01:48.709: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 28 08:01:50.711: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.015127862s
Jun 28 08:01:50.711: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 28 08:01:52.711: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.01554773s
Jun 28 08:01:52.712: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 28 08:01:54.712: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.015965783s
Jun 28 08:01:54.712: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 28 08:01:56.711: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.015169174s
Jun 28 08:01:56.711: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jun 28 08:01:56.711: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jun 28 08:01:56.716: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-4129" to be "running and ready"
Jun 28 08:01:56.722: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 5.623257ms
Jun 28 08:01:56.722: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jun 28 08:01:56.722: INFO: Pod "netserver-1" satisfied condition "running and ready"
Jun 28 08:01:56.727: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-4129" to be "running and ready"
Jun 28 08:01:56.732: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 4.598802ms
Jun 28 08:01:56.732: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Jun 28 08:01:56.732: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 06/28/23 08:01:56.736
Jun 28 08:01:56.751: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-4129" to be "running"
Jun 28 08:01:56.756: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.991978ms
Jun 28 08:01:58.763: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.011755979s
Jun 28 08:01:58.763: INFO: Pod "test-container-pod" satisfied condition "running"
Jun 28 08:01:58.768: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-4129" to be "running"
Jun 28 08:01:58.772: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.218012ms
Jun 28 08:01:58.772: INFO: Pod "host-test-container-pod" satisfied condition "running"
Jun 28 08:01:58.776: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Jun 28 08:01:58.776: INFO: Going to poll 172.21.122.61 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Jun 28 08:01:58.780: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.21.122.61 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4129 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 28 08:01:58.780: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
Jun 28 08:01:58.781: INFO: ExecWithOptions: Clientset creation
Jun 28 08:01:58.781: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/pod-network-test-4129/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.21.122.61+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jun 28 08:02:00.113: INFO: Found all 1 expected endpoints: [netserver-0]
Jun 28 08:02:00.113: INFO: Going to poll 172.21.122.106 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Jun 28 08:02:00.118: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.21.122.106 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4129 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 28 08:02:00.118: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
Jun 28 08:02:00.119: INFO: ExecWithOptions: Clientset creation
Jun 28 08:02:00.119: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/pod-network-test-4129/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.21.122.106+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jun 28 08:02:01.501: INFO: Found all 1 expected endpoints: [netserver-1]
Jun 28 08:02:01.501: INFO: Going to poll 172.21.30.76 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Jun 28 08:02:01.508: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.21.30.76 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4129 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 28 08:02:01.508: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
Jun 28 08:02:01.508: INFO: ExecWithOptions: Clientset creation
Jun 28 08:02:01.508: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/pod-network-test-4129/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.21.30.76+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jun 28 08:02:02.943: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Jun 28 08:02:02.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4129" for this suite. 06/28/23 08:02:02.953
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","completed":73,"skipped":1266,"failed":0}
------------------------------
â€¢ [SLOW TEST] [18.325 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:01:44.636
    Jun 28 08:01:44.636: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename pod-network-test 06/28/23 08:01:44.637
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:01:44.65
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:01:44.655
    [It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:122
    STEP: Performing setup for networking test in namespace pod-network-test-4129 06/28/23 08:01:44.659
    STEP: creating a selector 06/28/23 08:01:44.659
    STEP: Creating the service pods in kubernetes 06/28/23 08:01:44.659
    Jun 28 08:01:44.659: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jun 28 08:01:44.696: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-4129" to be "running and ready"
    Jun 28 08:01:44.705: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 9.198766ms
    Jun 28 08:01:44.705: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jun 28 08:01:46.711: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.015431378s
    Jun 28 08:01:46.711: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 28 08:01:48.709: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.013536222s
    Jun 28 08:01:48.709: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 28 08:01:50.711: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.015127862s
    Jun 28 08:01:50.711: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 28 08:01:52.711: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.01554773s
    Jun 28 08:01:52.712: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 28 08:01:54.712: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.015965783s
    Jun 28 08:01:54.712: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 28 08:01:56.711: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.015169174s
    Jun 28 08:01:56.711: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jun 28 08:01:56.711: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jun 28 08:01:56.716: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-4129" to be "running and ready"
    Jun 28 08:01:56.722: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 5.623257ms
    Jun 28 08:01:56.722: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jun 28 08:01:56.722: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Jun 28 08:01:56.727: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-4129" to be "running and ready"
    Jun 28 08:01:56.732: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 4.598802ms
    Jun 28 08:01:56.732: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Jun 28 08:01:56.732: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 06/28/23 08:01:56.736
    Jun 28 08:01:56.751: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-4129" to be "running"
    Jun 28 08:01:56.756: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.991978ms
    Jun 28 08:01:58.763: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.011755979s
    Jun 28 08:01:58.763: INFO: Pod "test-container-pod" satisfied condition "running"
    Jun 28 08:01:58.768: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-4129" to be "running"
    Jun 28 08:01:58.772: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.218012ms
    Jun 28 08:01:58.772: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Jun 28 08:01:58.776: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Jun 28 08:01:58.776: INFO: Going to poll 172.21.122.61 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Jun 28 08:01:58.780: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.21.122.61 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4129 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 28 08:01:58.780: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    Jun 28 08:01:58.781: INFO: ExecWithOptions: Clientset creation
    Jun 28 08:01:58.781: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/pod-network-test-4129/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.21.122.61+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jun 28 08:02:00.113: INFO: Found all 1 expected endpoints: [netserver-0]
    Jun 28 08:02:00.113: INFO: Going to poll 172.21.122.106 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Jun 28 08:02:00.118: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.21.122.106 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4129 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 28 08:02:00.118: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    Jun 28 08:02:00.119: INFO: ExecWithOptions: Clientset creation
    Jun 28 08:02:00.119: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/pod-network-test-4129/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.21.122.106+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jun 28 08:02:01.501: INFO: Found all 1 expected endpoints: [netserver-1]
    Jun 28 08:02:01.501: INFO: Going to poll 172.21.30.76 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Jun 28 08:02:01.508: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.21.30.76 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4129 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 28 08:02:01.508: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    Jun 28 08:02:01.508: INFO: ExecWithOptions: Clientset creation
    Jun 28 08:02:01.508: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/pod-network-test-4129/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.21.30.76+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jun 28 08:02:02.943: INFO: Found all 1 expected endpoints: [netserver-2]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Jun 28 08:02:02.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-4129" for this suite. 06/28/23 08:02:02.953
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff
  should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:929
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:02:02.961
Jun 28 08:02:02.961: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename kubectl 06/28/23 08:02:02.962
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:02:02.979
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:02:02.985
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:929
STEP: create deployment with httpd image 06/28/23 08:02:02.991
Jun 28 08:02:02.991: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-1388 create -f -'
Jun 28 08:02:03.809: INFO: stderr: ""
Jun 28 08:02:03.809: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image 06/28/23 08:02:03.809
Jun 28 08:02:03.809: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-1388 diff -f -'
Jun 28 08:02:04.045: INFO: rc: 1
Jun 28 08:02:04.045: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-1388 delete -f -'
Jun 28 08:02:04.122: INFO: stderr: ""
Jun 28 08:02:04.122: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jun 28 08:02:04.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1388" for this suite. 06/28/23 08:02:04.129
{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","completed":74,"skipped":1277,"failed":0}
------------------------------
â€¢ [1.175 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl diff
  test/e2e/kubectl/kubectl.go:923
    should check if kubectl diff finds a difference for Deployments [Conformance]
    test/e2e/kubectl/kubectl.go:929

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:02:02.961
    Jun 28 08:02:02.961: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename kubectl 06/28/23 08:02:02.962
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:02:02.979
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:02:02.985
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl diff finds a difference for Deployments [Conformance]
      test/e2e/kubectl/kubectl.go:929
    STEP: create deployment with httpd image 06/28/23 08:02:02.991
    Jun 28 08:02:02.991: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-1388 create -f -'
    Jun 28 08:02:03.809: INFO: stderr: ""
    Jun 28 08:02:03.809: INFO: stdout: "deployment.apps/httpd-deployment created\n"
    STEP: verify diff finds difference between live and declared image 06/28/23 08:02:03.809
    Jun 28 08:02:03.809: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-1388 diff -f -'
    Jun 28 08:02:04.045: INFO: rc: 1
    Jun 28 08:02:04.045: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-1388 delete -f -'
    Jun 28 08:02:04.122: INFO: stderr: ""
    Jun 28 08:02:04.122: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jun 28 08:02:04.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-1388" for this suite. 06/28/23 08:02:04.129
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:975
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:02:04.136
Jun 28 08:02:04.136: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename statefulset 06/28/23 08:02:04.138
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:02:04.153
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:02:04.158
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-5132 06/28/23 08:02:04.163
[It] should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:975
STEP: Creating statefulset ss in namespace statefulset-5132 06/28/23 08:02:04.178
Jun 28 08:02:04.189: INFO: Found 0 stateful pods, waiting for 1
Jun 28 08:02:14.194: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label 06/28/23 08:02:14.204
STEP: Getting /status 06/28/23 08:02:14.211
Jun 28 08:02:14.217: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status 06/28/23 08:02:14.217
Jun 28 08:02:14.228: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated 06/28/23 08:02:14.229
Jun 28 08:02:14.232: INFO: Observed &StatefulSet event: ADDED
Jun 28 08:02:14.232: INFO: Found Statefulset ss in namespace statefulset-5132 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jun 28 08:02:14.232: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status 06/28/23 08:02:14.232
Jun 28 08:02:14.232: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jun 28 08:02:14.240: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched 06/28/23 08:02:14.24
Jun 28 08:02:14.243: INFO: Observed &StatefulSet event: ADDED
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jun 28 08:02:14.243: INFO: Deleting all statefulset in ns statefulset-5132
Jun 28 08:02:14.248: INFO: Scaling statefulset ss to 0
Jun 28 08:02:24.271: INFO: Waiting for statefulset status.replicas updated to 0
Jun 28 08:02:24.275: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jun 28 08:02:24.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5132" for this suite. 06/28/23 08:02:24.302
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]","completed":75,"skipped":1286,"failed":0}
------------------------------
â€¢ [SLOW TEST] [20.172 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should validate Statefulset Status endpoints [Conformance]
    test/e2e/apps/statefulset.go:975

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:02:04.136
    Jun 28 08:02:04.136: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename statefulset 06/28/23 08:02:04.138
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:02:04.153
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:02:04.158
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-5132 06/28/23 08:02:04.163
    [It] should validate Statefulset Status endpoints [Conformance]
      test/e2e/apps/statefulset.go:975
    STEP: Creating statefulset ss in namespace statefulset-5132 06/28/23 08:02:04.178
    Jun 28 08:02:04.189: INFO: Found 0 stateful pods, waiting for 1
    Jun 28 08:02:14.194: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Patch Statefulset to include a label 06/28/23 08:02:14.204
    STEP: Getting /status 06/28/23 08:02:14.211
    Jun 28 08:02:14.217: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
    STEP: updating the StatefulSet Status 06/28/23 08:02:14.217
    Jun 28 08:02:14.228: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the statefulset status to be updated 06/28/23 08:02:14.229
    Jun 28 08:02:14.232: INFO: Observed &StatefulSet event: ADDED
    Jun 28 08:02:14.232: INFO: Found Statefulset ss in namespace statefulset-5132 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jun 28 08:02:14.232: INFO: Statefulset ss has an updated status
    STEP: patching the Statefulset Status 06/28/23 08:02:14.232
    Jun 28 08:02:14.232: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Jun 28 08:02:14.240: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Statefulset status to be patched 06/28/23 08:02:14.24
    Jun 28 08:02:14.243: INFO: Observed &StatefulSet event: ADDED
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jun 28 08:02:14.243: INFO: Deleting all statefulset in ns statefulset-5132
    Jun 28 08:02:14.248: INFO: Scaling statefulset ss to 0
    Jun 28 08:02:24.271: INFO: Waiting for statefulset status.replicas updated to 0
    Jun 28 08:02:24.275: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jun 28 08:02:24.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-5132" for this suite. 06/28/23 08:02:24.302
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:02:24.312
Jun 28 08:02:24.312: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename kubelet-test 06/28/23 08:02:24.312
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:02:24.325
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:02:24.329
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Jun 28 08:02:24.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7143" for this suite. 06/28/23 08:02:24.359
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","completed":76,"skipped":1289,"failed":0}
------------------------------
â€¢ [0.054 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should be possible to delete [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:02:24.312
    Jun 28 08:02:24.312: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename kubelet-test 06/28/23 08:02:24.312
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:02:24.325
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:02:24.329
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should be possible to delete [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:135
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Jun 28 08:02:24.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-7143" for this suite. 06/28/23 08:02:24.359
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:02:24.367
Jun 28 08:02:24.367: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename emptydir-wrapper 06/28/23 08:02:24.367
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:02:24.38
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:02:24.385
[It] should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
Jun 28 08:02:24.408: INFO: Waiting up to 5m0s for pod "pod-secrets-6f82898c-0880-4b7f-ae9f-2335d98e1d8a" in namespace "emptydir-wrapper-1203" to be "running and ready"
Jun 28 08:02:24.413: INFO: Pod "pod-secrets-6f82898c-0880-4b7f-ae9f-2335d98e1d8a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.780528ms
Jun 28 08:02:24.414: INFO: The phase of Pod pod-secrets-6f82898c-0880-4b7f-ae9f-2335d98e1d8a is Pending, waiting for it to be Running (with Ready = true)
Jun 28 08:02:26.419: INFO: Pod "pod-secrets-6f82898c-0880-4b7f-ae9f-2335d98e1d8a": Phase="Running", Reason="", readiness=true. Elapsed: 2.011353647s
Jun 28 08:02:26.419: INFO: The phase of Pod pod-secrets-6f82898c-0880-4b7f-ae9f-2335d98e1d8a is Running (Ready = true)
Jun 28 08:02:26.419: INFO: Pod "pod-secrets-6f82898c-0880-4b7f-ae9f-2335d98e1d8a" satisfied condition "running and ready"
STEP: Cleaning up the secret 06/28/23 08:02:26.423
STEP: Cleaning up the configmap 06/28/23 08:02:26.429
STEP: Cleaning up the pod 06/28/23 08:02:26.437
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:187
Jun 28 08:02:26.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-1203" for this suite. 06/28/23 08:02:26.456
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","completed":77,"skipped":1335,"failed":0}
------------------------------
â€¢ [2.096 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:02:24.367
    Jun 28 08:02:24.367: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename emptydir-wrapper 06/28/23 08:02:24.367
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:02:24.38
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:02:24.385
    [It] should not conflict [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:67
    Jun 28 08:02:24.408: INFO: Waiting up to 5m0s for pod "pod-secrets-6f82898c-0880-4b7f-ae9f-2335d98e1d8a" in namespace "emptydir-wrapper-1203" to be "running and ready"
    Jun 28 08:02:24.413: INFO: Pod "pod-secrets-6f82898c-0880-4b7f-ae9f-2335d98e1d8a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.780528ms
    Jun 28 08:02:24.414: INFO: The phase of Pod pod-secrets-6f82898c-0880-4b7f-ae9f-2335d98e1d8a is Pending, waiting for it to be Running (with Ready = true)
    Jun 28 08:02:26.419: INFO: Pod "pod-secrets-6f82898c-0880-4b7f-ae9f-2335d98e1d8a": Phase="Running", Reason="", readiness=true. Elapsed: 2.011353647s
    Jun 28 08:02:26.419: INFO: The phase of Pod pod-secrets-6f82898c-0880-4b7f-ae9f-2335d98e1d8a is Running (Ready = true)
    Jun 28 08:02:26.419: INFO: Pod "pod-secrets-6f82898c-0880-4b7f-ae9f-2335d98e1d8a" satisfied condition "running and ready"
    STEP: Cleaning up the secret 06/28/23 08:02:26.423
    STEP: Cleaning up the configmap 06/28/23 08:02:26.429
    STEP: Cleaning up the pod 06/28/23 08:02:26.437
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:187
    Jun 28 08:02:26.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-wrapper-1203" for this suite. 06/28/23 08:02:26.456
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:02:26.463
Jun 28 08:02:26.464: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename secrets 06/28/23 08:02:26.464
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:02:26.477
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:02:26.484
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56
STEP: Creating secret with name secret-test-6a2f43fb-69db-4c20-bbec-919bfa97029a 06/28/23 08:02:26.489
STEP: Creating a pod to test consume secrets 06/28/23 08:02:26.495
Jun 28 08:02:26.506: INFO: Waiting up to 5m0s for pod "pod-secrets-cc33473b-4e89-4e12-82b8-1a71f1eff822" in namespace "secrets-4760" to be "Succeeded or Failed"
Jun 28 08:02:26.510: INFO: Pod "pod-secrets-cc33473b-4e89-4e12-82b8-1a71f1eff822": Phase="Pending", Reason="", readiness=false. Elapsed: 4.336082ms
Jun 28 08:02:28.515: INFO: Pod "pod-secrets-cc33473b-4e89-4e12-82b8-1a71f1eff822": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009422606s
Jun 28 08:02:30.516: INFO: Pod "pod-secrets-cc33473b-4e89-4e12-82b8-1a71f1eff822": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010168971s
STEP: Saw pod success 06/28/23 08:02:30.516
Jun 28 08:02:30.516: INFO: Pod "pod-secrets-cc33473b-4e89-4e12-82b8-1a71f1eff822" satisfied condition "Succeeded or Failed"
Jun 28 08:02:30.521: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod pod-secrets-cc33473b-4e89-4e12-82b8-1a71f1eff822 container secret-volume-test: <nil>
STEP: delete the pod 06/28/23 08:02:30.571
Jun 28 08:02:30.584: INFO: Waiting for pod pod-secrets-cc33473b-4e89-4e12-82b8-1a71f1eff822 to disappear
Jun 28 08:02:30.592: INFO: Pod pod-secrets-cc33473b-4e89-4e12-82b8-1a71f1eff822 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jun 28 08:02:30.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4760" for this suite. 06/28/23 08:02:30.601
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":78,"skipped":1362,"failed":0}
------------------------------
â€¢ [4.145 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:02:26.463
    Jun 28 08:02:26.464: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename secrets 06/28/23 08:02:26.464
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:02:26.477
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:02:26.484
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:56
    STEP: Creating secret with name secret-test-6a2f43fb-69db-4c20-bbec-919bfa97029a 06/28/23 08:02:26.489
    STEP: Creating a pod to test consume secrets 06/28/23 08:02:26.495
    Jun 28 08:02:26.506: INFO: Waiting up to 5m0s for pod "pod-secrets-cc33473b-4e89-4e12-82b8-1a71f1eff822" in namespace "secrets-4760" to be "Succeeded or Failed"
    Jun 28 08:02:26.510: INFO: Pod "pod-secrets-cc33473b-4e89-4e12-82b8-1a71f1eff822": Phase="Pending", Reason="", readiness=false. Elapsed: 4.336082ms
    Jun 28 08:02:28.515: INFO: Pod "pod-secrets-cc33473b-4e89-4e12-82b8-1a71f1eff822": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009422606s
    Jun 28 08:02:30.516: INFO: Pod "pod-secrets-cc33473b-4e89-4e12-82b8-1a71f1eff822": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010168971s
    STEP: Saw pod success 06/28/23 08:02:30.516
    Jun 28 08:02:30.516: INFO: Pod "pod-secrets-cc33473b-4e89-4e12-82b8-1a71f1eff822" satisfied condition "Succeeded or Failed"
    Jun 28 08:02:30.521: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod pod-secrets-cc33473b-4e89-4e12-82b8-1a71f1eff822 container secret-volume-test: <nil>
    STEP: delete the pod 06/28/23 08:02:30.571
    Jun 28 08:02:30.584: INFO: Waiting for pod pod-secrets-cc33473b-4e89-4e12-82b8-1a71f1eff822 to disappear
    Jun 28 08:02:30.592: INFO: Pod pod-secrets-cc33473b-4e89-4e12-82b8-1a71f1eff822 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jun 28 08:02:30.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-4760" for this suite. 06/28/23 08:02:30.601
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:02:30.61
Jun 28 08:02:30.610: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename emptydir 06/28/23 08:02:30.612
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:02:30.633
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:02:30.639
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96
STEP: Creating a pod to test emptydir 0644 on tmpfs 06/28/23 08:02:30.644
Jun 28 08:02:30.658: INFO: Waiting up to 5m0s for pod "pod-cf9c3463-86bb-41b6-a309-62607a6218ed" in namespace "emptydir-1592" to be "Succeeded or Failed"
Jun 28 08:02:30.663: INFO: Pod "pod-cf9c3463-86bb-41b6-a309-62607a6218ed": Phase="Pending", Reason="", readiness=false. Elapsed: 4.645298ms
Jun 28 08:02:32.668: INFO: Pod "pod-cf9c3463-86bb-41b6-a309-62607a6218ed": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009903012s
Jun 28 08:02:34.669: INFO: Pod "pod-cf9c3463-86bb-41b6-a309-62607a6218ed": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010715935s
STEP: Saw pod success 06/28/23 08:02:34.669
Jun 28 08:02:34.669: INFO: Pod "pod-cf9c3463-86bb-41b6-a309-62607a6218ed" satisfied condition "Succeeded or Failed"
Jun 28 08:02:34.673: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod pod-cf9c3463-86bb-41b6-a309-62607a6218ed container test-container: <nil>
STEP: delete the pod 06/28/23 08:02:34.684
Jun 28 08:02:34.695: INFO: Waiting for pod pod-cf9c3463-86bb-41b6-a309-62607a6218ed to disappear
Jun 28 08:02:34.699: INFO: Pod pod-cf9c3463-86bb-41b6-a309-62607a6218ed no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jun 28 08:02:34.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1592" for this suite. 06/28/23 08:02:34.707
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":79,"skipped":1412,"failed":0}
------------------------------
â€¢ [4.104 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:02:30.61
    Jun 28 08:02:30.610: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename emptydir 06/28/23 08:02:30.612
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:02:30.633
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:02:30.639
    [It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:96
    STEP: Creating a pod to test emptydir 0644 on tmpfs 06/28/23 08:02:30.644
    Jun 28 08:02:30.658: INFO: Waiting up to 5m0s for pod "pod-cf9c3463-86bb-41b6-a309-62607a6218ed" in namespace "emptydir-1592" to be "Succeeded or Failed"
    Jun 28 08:02:30.663: INFO: Pod "pod-cf9c3463-86bb-41b6-a309-62607a6218ed": Phase="Pending", Reason="", readiness=false. Elapsed: 4.645298ms
    Jun 28 08:02:32.668: INFO: Pod "pod-cf9c3463-86bb-41b6-a309-62607a6218ed": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009903012s
    Jun 28 08:02:34.669: INFO: Pod "pod-cf9c3463-86bb-41b6-a309-62607a6218ed": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010715935s
    STEP: Saw pod success 06/28/23 08:02:34.669
    Jun 28 08:02:34.669: INFO: Pod "pod-cf9c3463-86bb-41b6-a309-62607a6218ed" satisfied condition "Succeeded or Failed"
    Jun 28 08:02:34.673: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod pod-cf9c3463-86bb-41b6-a309-62607a6218ed container test-container: <nil>
    STEP: delete the pod 06/28/23 08:02:34.684
    Jun 28 08:02:34.695: INFO: Waiting for pod pod-cf9c3463-86bb-41b6-a309-62607a6218ed to disappear
    Jun 28 08:02:34.699: INFO: Pod pod-cf9c3463-86bb-41b6-a309-62607a6218ed no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jun 28 08:02:34.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-1592" for this suite. 06/28/23 08:02:34.707
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:02:34.715
Jun 28 08:02:34.715: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename watch 06/28/23 08:02:34.715
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:02:34.728
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:02:34.733
[It] should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
STEP: creating a new configmap 06/28/23 08:02:34.738
STEP: modifying the configmap once 06/28/23 08:02:34.744
STEP: modifying the configmap a second time 06/28/23 08:02:34.754
STEP: deleting the configmap 06/28/23 08:02:34.765
STEP: creating a watch on configmaps from the resource version returned by the first update 06/28/23 08:02:34.773
STEP: Expecting to observe notifications for all changes to the configmap after the first update 06/28/23 08:02:34.776
Jun 28 08:02:34.776: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-7802  a9bf3179-f9dc-49b2-9aeb-5eb6fcb5108b 56771 0 2023-06-28 08:02:34 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-06-28 08:02:34 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 28 08:02:34.776: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-7802  a9bf3179-f9dc-49b2-9aeb-5eb6fcb5108b 56772 0 2023-06-28 08:02:34 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-06-28 08:02:34 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Jun 28 08:02:34.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7802" for this suite. 06/28/23 08:02:34.785
{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","completed":80,"skipped":1426,"failed":0}
------------------------------
â€¢ [0.081 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:02:34.715
    Jun 28 08:02:34.715: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename watch 06/28/23 08:02:34.715
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:02:34.728
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:02:34.733
    [It] should be able to start watching from a specific resource version [Conformance]
      test/e2e/apimachinery/watch.go:142
    STEP: creating a new configmap 06/28/23 08:02:34.738
    STEP: modifying the configmap once 06/28/23 08:02:34.744
    STEP: modifying the configmap a second time 06/28/23 08:02:34.754
    STEP: deleting the configmap 06/28/23 08:02:34.765
    STEP: creating a watch on configmaps from the resource version returned by the first update 06/28/23 08:02:34.773
    STEP: Expecting to observe notifications for all changes to the configmap after the first update 06/28/23 08:02:34.776
    Jun 28 08:02:34.776: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-7802  a9bf3179-f9dc-49b2-9aeb-5eb6fcb5108b 56771 0 2023-06-28 08:02:34 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-06-28 08:02:34 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jun 28 08:02:34.776: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-7802  a9bf3179-f9dc-49b2-9aeb-5eb6fcb5108b 56772 0 2023-06-28 08:02:34 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-06-28 08:02:34 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Jun 28 08:02:34.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-7802" for this suite. 06/28/23 08:02:34.785
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] DNS
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:02:34.796
Jun 28 08:02:34.796: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename dns 06/28/23 08:02:34.797
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:02:34.812
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:02:34.817
[It] should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-7697.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-7697.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
 06/28/23 08:02:34.823
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-7697.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-7697.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
 06/28/23 08:02:34.823
STEP: creating a pod to probe /etc/hosts 06/28/23 08:02:34.823
STEP: submitting the pod to kubernetes 06/28/23 08:02:34.823
Jun 28 08:02:34.833: INFO: Waiting up to 15m0s for pod "dns-test-60455e74-7bce-4348-8dc4-2491292bff28" in namespace "dns-7697" to be "running"
Jun 28 08:02:34.839: INFO: Pod "dns-test-60455e74-7bce-4348-8dc4-2491292bff28": Phase="Pending", Reason="", readiness=false. Elapsed: 6.56698ms
Jun 28 08:02:36.845: INFO: Pod "dns-test-60455e74-7bce-4348-8dc4-2491292bff28": Phase="Running", Reason="", readiness=true. Elapsed: 2.011924603s
Jun 28 08:02:36.845: INFO: Pod "dns-test-60455e74-7bce-4348-8dc4-2491292bff28" satisfied condition "running"
STEP: retrieving the pod 06/28/23 08:02:36.845
STEP: looking for the results for each expected name from probers 06/28/23 08:02:36.849
Jun 28 08:02:36.999: INFO: DNS probes using dns-7697/dns-test-60455e74-7bce-4348-8dc4-2491292bff28 succeeded

STEP: deleting the pod 06/28/23 08:02:36.999
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jun 28 08:02:37.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7697" for this suite. 06/28/23 08:02:37.022
{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [Conformance]","completed":81,"skipped":1429,"failed":0}
------------------------------
â€¢ [2.234 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:02:34.796
    Jun 28 08:02:34.796: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename dns 06/28/23 08:02:34.797
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:02:34.812
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:02:34.817
    [It] should provide /etc/hosts entries for the cluster [Conformance]
      test/e2e/network/dns.go:117
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-7697.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-7697.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
     06/28/23 08:02:34.823
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-7697.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-7697.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
     06/28/23 08:02:34.823
    STEP: creating a pod to probe /etc/hosts 06/28/23 08:02:34.823
    STEP: submitting the pod to kubernetes 06/28/23 08:02:34.823
    Jun 28 08:02:34.833: INFO: Waiting up to 15m0s for pod "dns-test-60455e74-7bce-4348-8dc4-2491292bff28" in namespace "dns-7697" to be "running"
    Jun 28 08:02:34.839: INFO: Pod "dns-test-60455e74-7bce-4348-8dc4-2491292bff28": Phase="Pending", Reason="", readiness=false. Elapsed: 6.56698ms
    Jun 28 08:02:36.845: INFO: Pod "dns-test-60455e74-7bce-4348-8dc4-2491292bff28": Phase="Running", Reason="", readiness=true. Elapsed: 2.011924603s
    Jun 28 08:02:36.845: INFO: Pod "dns-test-60455e74-7bce-4348-8dc4-2491292bff28" satisfied condition "running"
    STEP: retrieving the pod 06/28/23 08:02:36.845
    STEP: looking for the results for each expected name from probers 06/28/23 08:02:36.849
    Jun 28 08:02:36.999: INFO: DNS probes using dns-7697/dns-test-60455e74-7bce-4348-8dc4-2491292bff28 succeeded

    STEP: deleting the pod 06/28/23 08:02:36.999
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jun 28 08:02:37.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-7697" for this suite. 06/28/23 08:02:37.022
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:02:37.031
Jun 28 08:02:37.031: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename services 06/28/23 08:02:37.032
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:02:37.045
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:02:37.051
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404
STEP: creating a service externalname-service with the type=ExternalName in namespace services-4214 06/28/23 08:02:37.056
STEP: changing the ExternalName service to type=ClusterIP 06/28/23 08:02:37.063
STEP: creating replication controller externalname-service in namespace services-4214 06/28/23 08:02:37.077
I0628 08:02:37.082903      18 runners.go:193] Created replication controller with name: externalname-service, namespace: services-4214, replica count: 2
I0628 08:02:40.133761      18 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 28 08:02:40.133: INFO: Creating new exec pod
Jun 28 08:02:40.141: INFO: Waiting up to 5m0s for pod "execpodhgrlv" in namespace "services-4214" to be "running"
Jun 28 08:02:40.145: INFO: Pod "execpodhgrlv": Phase="Pending", Reason="", readiness=false. Elapsed: 4.657124ms
Jun 28 08:02:42.150: INFO: Pod "execpodhgrlv": Phase="Running", Reason="", readiness=true. Elapsed: 2.009447484s
Jun 28 08:02:42.150: INFO: Pod "execpodhgrlv" satisfied condition "running"
Jun 28 08:02:43.151: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-4214 exec execpodhgrlv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jun 28 08:02:43.545: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jun 28 08:02:43.545: INFO: stdout: ""
Jun 28 08:02:44.546: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-4214 exec execpodhgrlv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jun 28 08:02:44.987: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jun 28 08:02:44.987: INFO: stdout: ""
Jun 28 08:02:45.546: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-4214 exec execpodhgrlv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jun 28 08:02:45.989: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jun 28 08:02:45.989: INFO: stdout: ""
Jun 28 08:02:46.546: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-4214 exec execpodhgrlv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jun 28 08:02:46.979: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jun 28 08:02:46.979: INFO: stdout: "externalname-service-w5phk"
Jun 28 08:02:46.979: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-4214 exec execpodhgrlv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.20.13.115 80'
Jun 28 08:02:47.316: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.20.13.115 80\nConnection to 172.20.13.115 80 port [tcp/http] succeeded!\n"
Jun 28 08:02:47.316: INFO: stdout: "externalname-service-w5phk"
Jun 28 08:02:47.316: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jun 28 08:02:47.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4214" for this suite. 06/28/23 08:02:47.342
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","completed":82,"skipped":1431,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.320 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:02:37.031
    Jun 28 08:02:37.031: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename services 06/28/23 08:02:37.032
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:02:37.045
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:02:37.051
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ExternalName to ClusterIP [Conformance]
      test/e2e/network/service.go:1404
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-4214 06/28/23 08:02:37.056
    STEP: changing the ExternalName service to type=ClusterIP 06/28/23 08:02:37.063
    STEP: creating replication controller externalname-service in namespace services-4214 06/28/23 08:02:37.077
    I0628 08:02:37.082903      18 runners.go:193] Created replication controller with name: externalname-service, namespace: services-4214, replica count: 2
    I0628 08:02:40.133761      18 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jun 28 08:02:40.133: INFO: Creating new exec pod
    Jun 28 08:02:40.141: INFO: Waiting up to 5m0s for pod "execpodhgrlv" in namespace "services-4214" to be "running"
    Jun 28 08:02:40.145: INFO: Pod "execpodhgrlv": Phase="Pending", Reason="", readiness=false. Elapsed: 4.657124ms
    Jun 28 08:02:42.150: INFO: Pod "execpodhgrlv": Phase="Running", Reason="", readiness=true. Elapsed: 2.009447484s
    Jun 28 08:02:42.150: INFO: Pod "execpodhgrlv" satisfied condition "running"
    Jun 28 08:02:43.151: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-4214 exec execpodhgrlv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Jun 28 08:02:43.545: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Jun 28 08:02:43.545: INFO: stdout: ""
    Jun 28 08:02:44.546: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-4214 exec execpodhgrlv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Jun 28 08:02:44.987: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Jun 28 08:02:44.987: INFO: stdout: ""
    Jun 28 08:02:45.546: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-4214 exec execpodhgrlv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Jun 28 08:02:45.989: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Jun 28 08:02:45.989: INFO: stdout: ""
    Jun 28 08:02:46.546: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-4214 exec execpodhgrlv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Jun 28 08:02:46.979: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Jun 28 08:02:46.979: INFO: stdout: "externalname-service-w5phk"
    Jun 28 08:02:46.979: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-4214 exec execpodhgrlv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.20.13.115 80'
    Jun 28 08:02:47.316: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.20.13.115 80\nConnection to 172.20.13.115 80 port [tcp/http] succeeded!\n"
    Jun 28 08:02:47.316: INFO: stdout: "externalname-service-w5phk"
    Jun 28 08:02:47.316: INFO: Cleaning up the ExternalName to ClusterIP test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jun 28 08:02:47.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-4214" for this suite. 06/28/23 08:02:47.342
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
[BeforeEach] [sig-network] Service endpoints latency
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:02:47.352
Jun 28 08:02:47.352: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename svc-latency 06/28/23 08:02:47.353
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:02:47.367
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:02:47.372
[It] should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
Jun 28 08:02:47.377: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: creating replication controller svc-latency-rc in namespace svc-latency-5671 06/28/23 08:02:47.377
I0628 08:02:47.383810      18 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-5671, replica count: 1
I0628 08:02:48.436817      18 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 28 08:02:48.549: INFO: Created: latency-svc-4s8xn
Jun 28 08:02:48.552: INFO: Got endpoints: latency-svc-4s8xn [15.297472ms]
Jun 28 08:02:48.562: INFO: Created: latency-svc-ppjfd
Jun 28 08:02:48.566: INFO: Created: latency-svc-64bfp
Jun 28 08:02:48.569: INFO: Got endpoints: latency-svc-ppjfd [15.943991ms]
Jun 28 08:02:48.569: INFO: Created: latency-svc-skmzq
Jun 28 08:02:48.572: INFO: Got endpoints: latency-svc-64bfp [19.666503ms]
Jun 28 08:02:48.575: INFO: Got endpoints: latency-svc-skmzq [22.32193ms]
Jun 28 08:02:48.576: INFO: Created: latency-svc-vn4xn
Jun 28 08:02:48.578: INFO: Created: latency-svc-tbj46
Jun 28 08:02:48.580: INFO: Got endpoints: latency-svc-vn4xn [26.877023ms]
Jun 28 08:02:48.583: INFO: Got endpoints: latency-svc-tbj46 [29.646621ms]
Jun 28 08:02:48.583: INFO: Created: latency-svc-2w6ht
Jun 28 08:02:48.587: INFO: Got endpoints: latency-svc-2w6ht [33.734975ms]
Jun 28 08:02:48.588: INFO: Created: latency-svc-wsq7r
Jun 28 08:02:48.593: INFO: Created: latency-svc-r5swf
Jun 28 08:02:48.594: INFO: Got endpoints: latency-svc-wsq7r [40.365723ms]
Jun 28 08:02:48.598: INFO: Got endpoints: latency-svc-r5swf [44.927804ms]
Jun 28 08:02:48.599: INFO: Created: latency-svc-p8tng
Jun 28 08:02:48.600: INFO: Created: latency-svc-5h868
Jun 28 08:02:48.605: INFO: Got endpoints: latency-svc-p8tng [51.213ms]
Jun 28 08:02:48.606: INFO: Got endpoints: latency-svc-5h868 [51.389837ms]
Jun 28 08:02:48.606: INFO: Created: latency-svc-kwb4t
Jun 28 08:02:48.609: INFO: Got endpoints: latency-svc-kwb4t [54.468826ms]
Jun 28 08:02:48.662: INFO: Created: latency-svc-pmvvk
Jun 28 08:02:48.662: INFO: Created: latency-svc-qlt7z
Jun 28 08:02:48.663: INFO: Created: latency-svc-2c8xt
Jun 28 08:02:48.669: INFO: Created: latency-svc-f5x4l
Jun 28 08:02:48.669: INFO: Created: latency-svc-xp2lg
Jun 28 08:02:48.669: INFO: Created: latency-svc-cnckq
Jun 28 08:02:48.669: INFO: Created: latency-svc-7756c
Jun 28 08:02:48.669: INFO: Created: latency-svc-6dpjz
Jun 28 08:02:48.669: INFO: Created: latency-svc-7kmdc
Jun 28 08:02:48.670: INFO: Created: latency-svc-pfbt7
Jun 28 08:02:48.670: INFO: Created: latency-svc-btx59
Jun 28 08:02:48.670: INFO: Created: latency-svc-xf7b4
Jun 28 08:02:48.670: INFO: Got endpoints: latency-svc-2c8xt [101.068628ms]
Jun 28 08:02:48.672: INFO: Got endpoints: latency-svc-pmvvk [88.695464ms]
Jun 28 08:02:48.672: INFO: Got endpoints: latency-svc-7756c [78.280268ms]
Jun 28 08:02:48.672: INFO: Got endpoints: latency-svc-xp2lg [63.204242ms]
Jun 28 08:02:48.672: INFO: Got endpoints: latency-svc-qlt7z [117.487982ms]
Jun 28 08:02:48.673: INFO: Created: latency-svc-wt4tm
Jun 28 08:02:48.673: INFO: Created: latency-svc-sf9n7
Jun 28 08:02:48.673: INFO: Created: latency-svc-n9c8n
Jun 28 08:02:48.675: INFO: Got endpoints: latency-svc-f5x4l [119.900089ms]
Jun 28 08:02:48.679: INFO: Got endpoints: latency-svc-n9c8n [106.820143ms]
Jun 28 08:02:48.680: INFO: Got endpoints: latency-svc-7kmdc [100.171644ms]
Jun 28 08:02:48.680: INFO: Got endpoints: latency-svc-sf9n7 [125.771195ms]
Jun 28 08:02:48.681: INFO: Got endpoints: latency-svc-btx59 [83.203274ms]
Jun 28 08:02:48.682: INFO: Created: latency-svc-x974d
Jun 28 08:02:48.682: INFO: Got endpoints: latency-svc-pfbt7 [106.705864ms]
Jun 28 08:02:48.682: INFO: Got endpoints: latency-svc-xf7b4 [127.271961ms]
Jun 28 08:02:48.682: INFO: Got endpoints: latency-svc-wt4tm [94.999612ms]
Jun 28 08:02:48.684: INFO: Got endpoints: latency-svc-cnckq [78.504448ms]
Jun 28 08:02:48.688: INFO: Created: latency-svc-bw5cv
Jun 28 08:02:48.688: INFO: Got endpoints: latency-svc-x974d [17.758238ms]
Jun 28 08:02:48.691: INFO: Got endpoints: latency-svc-6dpjz [84.909645ms]
Jun 28 08:02:48.697: INFO: Created: latency-svc-qd8cp
Jun 28 08:02:48.697: INFO: Created: latency-svc-ltb4v
Jun 28 08:02:48.697: INFO: Got endpoints: latency-svc-qd8cp [25.263179ms]
Jun 28 08:02:48.699: INFO: Created: latency-svc-z6m9q
Jun 28 08:02:48.699: INFO: Got endpoints: latency-svc-bw5cv [27.063654ms]
Jun 28 08:02:48.699: INFO: Got endpoints: latency-svc-ltb4v [26.993344ms]
Jun 28 08:02:48.702: INFO: Created: latency-svc-p9j5g
Jun 28 08:02:48.703: INFO: Got endpoints: latency-svc-z6m9q [30.937838ms]
Jun 28 08:02:48.705: INFO: Got endpoints: latency-svc-p9j5g [30.138879ms]
Jun 28 08:02:48.709: INFO: Created: latency-svc-8p2jl
Jun 28 08:02:48.711: INFO: Created: latency-svc-dwsrn
Jun 28 08:02:48.717: INFO: Created: latency-svc-qx82p
Jun 28 08:02:48.725: INFO: Created: latency-svc-rl9wz
Jun 28 08:02:48.727: INFO: Created: latency-svc-sf5f2
Jun 28 08:02:48.730: INFO: Created: latency-svc-fvg7f
Jun 28 08:02:48.735: INFO: Created: latency-svc-nj8cd
Jun 28 08:02:48.740: INFO: Created: latency-svc-jrsfs
Jun 28 08:02:48.742: INFO: Created: latency-svc-np8kh
Jun 28 08:02:48.747: INFO: Created: latency-svc-bjkr4
Jun 28 08:02:48.752: INFO: Created: latency-svc-fl7qx
Jun 28 08:02:48.753: INFO: Got endpoints: latency-svc-8p2jl [73.917853ms]
Jun 28 08:02:48.754: INFO: Created: latency-svc-khcsg
Jun 28 08:02:48.758: INFO: Created: latency-svc-tf65c
Jun 28 08:02:48.762: INFO: Created: latency-svc-gkd2b
Jun 28 08:02:48.766: INFO: Created: latency-svc-9bf8z
Jun 28 08:02:48.769: INFO: Created: latency-svc-22pl8
Jun 28 08:02:48.804: INFO: Got endpoints: latency-svc-dwsrn [123.491507ms]
Jun 28 08:02:48.814: INFO: Created: latency-svc-nq8c9
Jun 28 08:02:48.853: INFO: Got endpoints: latency-svc-qx82p [171.660627ms]
Jun 28 08:02:48.864: INFO: Created: latency-svc-lbq24
Jun 28 08:02:48.903: INFO: Got endpoints: latency-svc-rl9wz [220.954954ms]
Jun 28 08:02:48.913: INFO: Created: latency-svc-v9s28
Jun 28 08:02:48.956: INFO: Got endpoints: latency-svc-sf5f2 [273.818474ms]
Jun 28 08:02:48.970: INFO: Created: latency-svc-gvzns
Jun 28 08:02:49.004: INFO: Got endpoints: latency-svc-fvg7f [323.696174ms]
Jun 28 08:02:49.015: INFO: Created: latency-svc-gczpz
Jun 28 08:02:49.059: INFO: Got endpoints: latency-svc-nj8cd [376.816484ms]
Jun 28 08:02:49.071: INFO: Created: latency-svc-hlhnn
Jun 28 08:02:49.103: INFO: Got endpoints: latency-svc-jrsfs [419.303081ms]
Jun 28 08:02:49.118: INFO: Created: latency-svc-vqdx7
Jun 28 08:02:49.153: INFO: Got endpoints: latency-svc-np8kh [464.703762ms]
Jun 28 08:02:49.165: INFO: Created: latency-svc-l2kwm
Jun 28 08:02:49.203: INFO: Got endpoints: latency-svc-bjkr4 [511.892117ms]
Jun 28 08:02:49.214: INFO: Created: latency-svc-hxjd7
Jun 28 08:02:49.253: INFO: Got endpoints: latency-svc-fl7qx [553.921775ms]
Jun 28 08:02:49.265: INFO: Created: latency-svc-mn976
Jun 28 08:02:49.303: INFO: Got endpoints: latency-svc-khcsg [604.010824ms]
Jun 28 08:02:49.313: INFO: Created: latency-svc-jw7zt
Jun 28 08:02:49.354: INFO: Got endpoints: latency-svc-tf65c [656.568656ms]
Jun 28 08:02:49.364: INFO: Created: latency-svc-nhwwp
Jun 28 08:02:49.404: INFO: Got endpoints: latency-svc-gkd2b [700.235686ms]
Jun 28 08:02:49.416: INFO: Created: latency-svc-7xblc
Jun 28 08:02:49.453: INFO: Got endpoints: latency-svc-9bf8z [748.600796ms]
Jun 28 08:02:49.464: INFO: Created: latency-svc-d7nql
Jun 28 08:02:49.503: INFO: Got endpoints: latency-svc-22pl8 [750.390319ms]
Jun 28 08:02:49.514: INFO: Created: latency-svc-d7gd4
Jun 28 08:02:49.553: INFO: Got endpoints: latency-svc-nq8c9 [748.928472ms]
Jun 28 08:02:49.563: INFO: Created: latency-svc-k695t
Jun 28 08:02:49.603: INFO: Got endpoints: latency-svc-lbq24 [749.919348ms]
Jun 28 08:02:49.616: INFO: Created: latency-svc-5c9dr
Jun 28 08:02:49.653: INFO: Got endpoints: latency-svc-v9s28 [750.088199ms]
Jun 28 08:02:49.663: INFO: Created: latency-svc-wzmtq
Jun 28 08:02:49.702: INFO: Got endpoints: latency-svc-gvzns [746.299153ms]
Jun 28 08:02:49.714: INFO: Created: latency-svc-8rhvz
Jun 28 08:02:49.754: INFO: Got endpoints: latency-svc-gczpz [749.522185ms]
Jun 28 08:02:49.764: INFO: Created: latency-svc-nnf79
Jun 28 08:02:49.803: INFO: Got endpoints: latency-svc-hlhnn [744.186863ms]
Jun 28 08:02:49.814: INFO: Created: latency-svc-z6xcz
Jun 28 08:02:49.854: INFO: Got endpoints: latency-svc-vqdx7 [750.163501ms]
Jun 28 08:02:49.864: INFO: Created: latency-svc-kl5mn
Jun 28 08:02:49.903: INFO: Got endpoints: latency-svc-l2kwm [750.279546ms]
Jun 28 08:02:49.913: INFO: Created: latency-svc-mcltm
Jun 28 08:02:49.954: INFO: Got endpoints: latency-svc-hxjd7 [751.114485ms]
Jun 28 08:02:49.965: INFO: Created: latency-svc-g68rl
Jun 28 08:02:50.003: INFO: Got endpoints: latency-svc-mn976 [750.590906ms]
Jun 28 08:02:50.014: INFO: Created: latency-svc-mgf6p
Jun 28 08:02:50.053: INFO: Got endpoints: latency-svc-jw7zt [750.176892ms]
Jun 28 08:02:50.063: INFO: Created: latency-svc-lz9mx
Jun 28 08:02:50.103: INFO: Got endpoints: latency-svc-nhwwp [748.745878ms]
Jun 28 08:02:50.113: INFO: Created: latency-svc-vnlll
Jun 28 08:02:50.153: INFO: Got endpoints: latency-svc-7xblc [749.301163ms]
Jun 28 08:02:50.163: INFO: Created: latency-svc-2p94x
Jun 28 08:02:50.203: INFO: Got endpoints: latency-svc-d7nql [749.732838ms]
Jun 28 08:02:50.214: INFO: Created: latency-svc-hrt8g
Jun 28 08:02:50.253: INFO: Got endpoints: latency-svc-d7gd4 [749.454336ms]
Jun 28 08:02:50.263: INFO: Created: latency-svc-g4scz
Jun 28 08:02:50.303: INFO: Got endpoints: latency-svc-k695t [750.703272ms]
Jun 28 08:02:50.314: INFO: Created: latency-svc-bxnhc
Jun 28 08:02:50.354: INFO: Got endpoints: latency-svc-5c9dr [750.457941ms]
Jun 28 08:02:50.364: INFO: Created: latency-svc-zqcvm
Jun 28 08:02:50.403: INFO: Got endpoints: latency-svc-wzmtq [749.725271ms]
Jun 28 08:02:50.413: INFO: Created: latency-svc-2wr8p
Jun 28 08:02:50.453: INFO: Got endpoints: latency-svc-8rhvz [750.970443ms]
Jun 28 08:02:50.463: INFO: Created: latency-svc-5xr48
Jun 28 08:02:50.503: INFO: Got endpoints: latency-svc-nnf79 [748.904915ms]
Jun 28 08:02:50.514: INFO: Created: latency-svc-bbjqq
Jun 28 08:02:50.553: INFO: Got endpoints: latency-svc-z6xcz [749.931632ms]
Jun 28 08:02:50.564: INFO: Created: latency-svc-z775t
Jun 28 08:02:50.605: INFO: Got endpoints: latency-svc-kl5mn [751.825915ms]
Jun 28 08:02:50.620: INFO: Created: latency-svc-rbxpg
Jun 28 08:02:50.654: INFO: Got endpoints: latency-svc-mcltm [751.041928ms]
Jun 28 08:02:50.664: INFO: Created: latency-svc-jw28q
Jun 28 08:02:50.704: INFO: Got endpoints: latency-svc-g68rl [749.963765ms]
Jun 28 08:02:50.715: INFO: Created: latency-svc-48mks
Jun 28 08:02:50.755: INFO: Got endpoints: latency-svc-mgf6p [751.575094ms]
Jun 28 08:02:50.765: INFO: Created: latency-svc-9r6l7
Jun 28 08:02:50.802: INFO: Got endpoints: latency-svc-lz9mx [748.423596ms]
Jun 28 08:02:50.812: INFO: Created: latency-svc-xvtc4
Jun 28 08:02:50.854: INFO: Got endpoints: latency-svc-vnlll [751.507539ms]
Jun 28 08:02:50.864: INFO: Created: latency-svc-6c9sb
Jun 28 08:02:50.904: INFO: Got endpoints: latency-svc-2p94x [750.588698ms]
Jun 28 08:02:50.914: INFO: Created: latency-svc-r59f8
Jun 28 08:02:50.953: INFO: Got endpoints: latency-svc-hrt8g [749.560383ms]
Jun 28 08:02:50.964: INFO: Created: latency-svc-rjlpz
Jun 28 08:02:51.003: INFO: Got endpoints: latency-svc-g4scz [749.675522ms]
Jun 28 08:02:51.014: INFO: Created: latency-svc-9hwjd
Jun 28 08:02:51.054: INFO: Got endpoints: latency-svc-bxnhc [750.330807ms]
Jun 28 08:02:51.064: INFO: Created: latency-svc-v8r9n
Jun 28 08:02:51.103: INFO: Got endpoints: latency-svc-zqcvm [749.365935ms]
Jun 28 08:02:51.113: INFO: Created: latency-svc-knwbl
Jun 28 08:02:51.153: INFO: Got endpoints: latency-svc-2wr8p [750.682878ms]
Jun 28 08:02:51.164: INFO: Created: latency-svc-jjpcl
Jun 28 08:02:51.204: INFO: Got endpoints: latency-svc-5xr48 [750.298879ms]
Jun 28 08:02:51.214: INFO: Created: latency-svc-gzsnj
Jun 28 08:02:51.253: INFO: Got endpoints: latency-svc-bbjqq [750.28856ms]
Jun 28 08:02:51.265: INFO: Created: latency-svc-hjj5h
Jun 28 08:02:51.303: INFO: Got endpoints: latency-svc-z775t [749.852503ms]
Jun 28 08:02:51.314: INFO: Created: latency-svc-2hj9d
Jun 28 08:02:51.356: INFO: Got endpoints: latency-svc-rbxpg [750.42463ms]
Jun 28 08:02:51.366: INFO: Created: latency-svc-q5sjr
Jun 28 08:02:51.405: INFO: Got endpoints: latency-svc-jw28q [750.784943ms]
Jun 28 08:02:51.417: INFO: Created: latency-svc-9stnm
Jun 28 08:02:51.452: INFO: Got endpoints: latency-svc-48mks [748.420116ms]
Jun 28 08:02:51.462: INFO: Created: latency-svc-fdn7h
Jun 28 08:02:51.503: INFO: Got endpoints: latency-svc-9r6l7 [748.378858ms]
Jun 28 08:02:51.514: INFO: Created: latency-svc-45gqg
Jun 28 08:02:51.553: INFO: Got endpoints: latency-svc-xvtc4 [751.310519ms]
Jun 28 08:02:51.564: INFO: Created: latency-svc-gfdsl
Jun 28 08:02:51.604: INFO: Got endpoints: latency-svc-6c9sb [749.396606ms]
Jun 28 08:02:51.614: INFO: Created: latency-svc-5msbz
Jun 28 08:02:51.654: INFO: Got endpoints: latency-svc-r59f8 [750.319012ms]
Jun 28 08:02:51.666: INFO: Created: latency-svc-krfk9
Jun 28 08:02:51.704: INFO: Got endpoints: latency-svc-rjlpz [751.480646ms]
Jun 28 08:02:51.716: INFO: Created: latency-svc-m626z
Jun 28 08:02:51.753: INFO: Got endpoints: latency-svc-9hwjd [750.567042ms]
Jun 28 08:02:51.763: INFO: Created: latency-svc-4c2m9
Jun 28 08:02:51.805: INFO: Got endpoints: latency-svc-v8r9n [751.038795ms]
Jun 28 08:02:51.816: INFO: Created: latency-svc-r9xdx
Jun 28 08:02:51.853: INFO: Got endpoints: latency-svc-knwbl [749.618649ms]
Jun 28 08:02:51.863: INFO: Created: latency-svc-pzgl4
Jun 28 08:02:51.904: INFO: Got endpoints: latency-svc-jjpcl [750.669252ms]
Jun 28 08:02:51.915: INFO: Created: latency-svc-z4qnr
Jun 28 08:02:51.954: INFO: Got endpoints: latency-svc-gzsnj [750.499458ms]
Jun 28 08:02:51.967: INFO: Created: latency-svc-7nfns
Jun 28 08:02:52.005: INFO: Got endpoints: latency-svc-hjj5h [751.81575ms]
Jun 28 08:02:52.018: INFO: Created: latency-svc-69dq5
Jun 28 08:02:52.055: INFO: Got endpoints: latency-svc-2hj9d [752.373308ms]
Jun 28 08:02:52.067: INFO: Created: latency-svc-ncshn
Jun 28 08:02:52.104: INFO: Got endpoints: latency-svc-q5sjr [747.619853ms]
Jun 28 08:02:52.117: INFO: Created: latency-svc-lvs5z
Jun 28 08:02:52.154: INFO: Got endpoints: latency-svc-9stnm [749.376488ms]
Jun 28 08:02:52.166: INFO: Created: latency-svc-shc62
Jun 28 08:02:52.205: INFO: Got endpoints: latency-svc-fdn7h [752.96ms]
Jun 28 08:02:52.217: INFO: Created: latency-svc-6942b
Jun 28 08:02:52.255: INFO: Got endpoints: latency-svc-45gqg [751.181077ms]
Jun 28 08:02:52.266: INFO: Created: latency-svc-d4fh7
Jun 28 08:02:52.303: INFO: Got endpoints: latency-svc-gfdsl [749.862787ms]
Jun 28 08:02:52.313: INFO: Created: latency-svc-j4bnk
Jun 28 08:02:52.353: INFO: Got endpoints: latency-svc-5msbz [749.429187ms]
Jun 28 08:02:52.366: INFO: Created: latency-svc-9q8hf
Jun 28 08:02:52.405: INFO: Got endpoints: latency-svc-krfk9 [750.881149ms]
Jun 28 08:02:52.416: INFO: Created: latency-svc-kwbxp
Jun 28 08:02:52.453: INFO: Got endpoints: latency-svc-m626z [748.948463ms]
Jun 28 08:02:52.464: INFO: Created: latency-svc-n45f6
Jun 28 08:02:52.504: INFO: Got endpoints: latency-svc-4c2m9 [750.810937ms]
Jun 28 08:02:52.516: INFO: Created: latency-svc-zpk6b
Jun 28 08:02:52.553: INFO: Got endpoints: latency-svc-r9xdx [748.531895ms]
Jun 28 08:02:52.564: INFO: Created: latency-svc-6dh2l
Jun 28 08:02:52.603: INFO: Got endpoints: latency-svc-pzgl4 [750.654027ms]
Jun 28 08:02:52.614: INFO: Created: latency-svc-nwg5t
Jun 28 08:02:52.653: INFO: Got endpoints: latency-svc-z4qnr [748.694063ms]
Jun 28 08:02:52.662: INFO: Created: latency-svc-mt8mx
Jun 28 08:02:52.703: INFO: Got endpoints: latency-svc-7nfns [748.638036ms]
Jun 28 08:02:52.713: INFO: Created: latency-svc-vlnkd
Jun 28 08:02:52.755: INFO: Got endpoints: latency-svc-69dq5 [750.1611ms]
Jun 28 08:02:52.766: INFO: Created: latency-svc-znvf4
Jun 28 08:02:52.803: INFO: Got endpoints: latency-svc-ncshn [747.739293ms]
Jun 28 08:02:52.814: INFO: Created: latency-svc-4jjn4
Jun 28 08:02:52.853: INFO: Got endpoints: latency-svc-lvs5z [749.504821ms]
Jun 28 08:02:52.865: INFO: Created: latency-svc-z5xnw
Jun 28 08:02:52.905: INFO: Got endpoints: latency-svc-shc62 [750.27713ms]
Jun 28 08:02:52.918: INFO: Created: latency-svc-mm7c2
Jun 28 08:02:52.953: INFO: Got endpoints: latency-svc-6942b [747.871635ms]
Jun 28 08:02:52.966: INFO: Created: latency-svc-tjktb
Jun 28 08:02:53.005: INFO: Got endpoints: latency-svc-d4fh7 [750.310564ms]
Jun 28 08:02:53.020: INFO: Created: latency-svc-pw5sm
Jun 28 08:02:53.057: INFO: Got endpoints: latency-svc-j4bnk [753.637063ms]
Jun 28 08:02:53.069: INFO: Created: latency-svc-ftx42
Jun 28 08:02:53.104: INFO: Got endpoints: latency-svc-9q8hf [751.222322ms]
Jun 28 08:02:53.117: INFO: Created: latency-svc-mx86s
Jun 28 08:02:53.156: INFO: Got endpoints: latency-svc-kwbxp [751.0231ms]
Jun 28 08:02:53.171: INFO: Created: latency-svc-cswk7
Jun 28 08:02:53.204: INFO: Got endpoints: latency-svc-n45f6 [750.840368ms]
Jun 28 08:02:53.222: INFO: Created: latency-svc-4k47g
Jun 28 08:02:53.256: INFO: Got endpoints: latency-svc-zpk6b [751.368509ms]
Jun 28 08:02:53.268: INFO: Created: latency-svc-q852q
Jun 28 08:02:53.304: INFO: Got endpoints: latency-svc-6dh2l [750.274528ms]
Jun 28 08:02:53.317: INFO: Created: latency-svc-8sbp2
Jun 28 08:02:53.354: INFO: Got endpoints: latency-svc-nwg5t [750.471765ms]
Jun 28 08:02:53.365: INFO: Created: latency-svc-lh6wr
Jun 28 08:02:53.403: INFO: Got endpoints: latency-svc-mt8mx [749.936337ms]
Jun 28 08:02:53.417: INFO: Created: latency-svc-vfltd
Jun 28 08:02:53.455: INFO: Got endpoints: latency-svc-vlnkd [752.322093ms]
Jun 28 08:02:53.467: INFO: Created: latency-svc-p65vz
Jun 28 08:02:53.502: INFO: Got endpoints: latency-svc-znvf4 [747.176067ms]
Jun 28 08:02:53.512: INFO: Created: latency-svc-5pxnb
Jun 28 08:02:53.554: INFO: Got endpoints: latency-svc-4jjn4 [750.408199ms]
Jun 28 08:02:53.568: INFO: Created: latency-svc-mrw8d
Jun 28 08:02:53.620: INFO: Got endpoints: latency-svc-z5xnw [766.607539ms]
Jun 28 08:02:53.639: INFO: Created: latency-svc-qg548
Jun 28 08:02:53.656: INFO: Got endpoints: latency-svc-mm7c2 [750.974528ms]
Jun 28 08:02:53.668: INFO: Created: latency-svc-zcx6g
Jun 28 08:02:53.705: INFO: Got endpoints: latency-svc-tjktb [751.935005ms]
Jun 28 08:02:53.717: INFO: Created: latency-svc-glvwn
Jun 28 08:02:53.756: INFO: Got endpoints: latency-svc-pw5sm [751.043491ms]
Jun 28 08:02:53.768: INFO: Created: latency-svc-kptvr
Jun 28 08:02:53.803: INFO: Got endpoints: latency-svc-ftx42 [746.206986ms]
Jun 28 08:02:53.818: INFO: Created: latency-svc-4x24t
Jun 28 08:02:53.853: INFO: Got endpoints: latency-svc-mx86s [748.623008ms]
Jun 28 08:02:53.863: INFO: Created: latency-svc-wxvkz
Jun 28 08:02:53.903: INFO: Got endpoints: latency-svc-cswk7 [746.786708ms]
Jun 28 08:02:53.916: INFO: Created: latency-svc-m9sfx
Jun 28 08:02:53.955: INFO: Got endpoints: latency-svc-4k47g [750.947151ms]
Jun 28 08:02:53.970: INFO: Created: latency-svc-rdzpc
Jun 28 08:02:54.004: INFO: Got endpoints: latency-svc-q852q [747.930483ms]
Jun 28 08:02:54.015: INFO: Created: latency-svc-glhlc
Jun 28 08:02:54.058: INFO: Got endpoints: latency-svc-8sbp2 [754.233795ms]
Jun 28 08:02:54.080: INFO: Created: latency-svc-4zgh5
Jun 28 08:02:54.106: INFO: Got endpoints: latency-svc-lh6wr [752.133082ms]
Jun 28 08:02:54.120: INFO: Created: latency-svc-lcmwr
Jun 28 08:02:54.155: INFO: Got endpoints: latency-svc-vfltd [752.335885ms]
Jun 28 08:02:54.168: INFO: Created: latency-svc-zwd8m
Jun 28 08:02:54.205: INFO: Got endpoints: latency-svc-p65vz [749.485415ms]
Jun 28 08:02:54.215: INFO: Created: latency-svc-tkf9z
Jun 28 08:02:54.255: INFO: Got endpoints: latency-svc-5pxnb [752.924211ms]
Jun 28 08:02:54.267: INFO: Created: latency-svc-qprk9
Jun 28 08:02:54.304: INFO: Got endpoints: latency-svc-mrw8d [750.343515ms]
Jun 28 08:02:54.316: INFO: Created: latency-svc-2wch4
Jun 28 08:02:54.352: INFO: Got endpoints: latency-svc-qg548 [732.530327ms]
Jun 28 08:02:54.361: INFO: Created: latency-svc-qk2hr
Jun 28 08:02:54.402: INFO: Got endpoints: latency-svc-zcx6g [746.578394ms]
Jun 28 08:02:54.412: INFO: Created: latency-svc-4pgnt
Jun 28 08:02:54.453: INFO: Got endpoints: latency-svc-glvwn [747.851656ms]
Jun 28 08:02:54.464: INFO: Created: latency-svc-svwc7
Jun 28 08:02:54.504: INFO: Got endpoints: latency-svc-kptvr [747.986909ms]
Jun 28 08:02:54.516: INFO: Created: latency-svc-mdlmd
Jun 28 08:02:54.553: INFO: Got endpoints: latency-svc-4x24t [749.946296ms]
Jun 28 08:02:54.565: INFO: Created: latency-svc-s4w66
Jun 28 08:02:54.608: INFO: Got endpoints: latency-svc-wxvkz [754.500193ms]
Jun 28 08:02:54.619: INFO: Created: latency-svc-xkztv
Jun 28 08:02:54.653: INFO: Got endpoints: latency-svc-m9sfx [749.874667ms]
Jun 28 08:02:54.666: INFO: Created: latency-svc-4gvvs
Jun 28 08:02:54.703: INFO: Got endpoints: latency-svc-rdzpc [748.042324ms]
Jun 28 08:02:54.715: INFO: Created: latency-svc-nnwbx
Jun 28 08:02:54.754: INFO: Got endpoints: latency-svc-glhlc [750.127228ms]
Jun 28 08:02:54.768: INFO: Created: latency-svc-frlbn
Jun 28 08:02:54.803: INFO: Got endpoints: latency-svc-4zgh5 [744.808559ms]
Jun 28 08:02:54.815: INFO: Created: latency-svc-f7mxm
Jun 28 08:02:54.853: INFO: Got endpoints: latency-svc-lcmwr [746.985991ms]
Jun 28 08:02:54.863: INFO: Created: latency-svc-flsxn
Jun 28 08:02:54.903: INFO: Got endpoints: latency-svc-zwd8m [748.081682ms]
Jun 28 08:02:54.914: INFO: Created: latency-svc-bhxst
Jun 28 08:02:54.955: INFO: Got endpoints: latency-svc-tkf9z [750.262409ms]
Jun 28 08:02:54.966: INFO: Created: latency-svc-jrfmm
Jun 28 08:02:55.004: INFO: Got endpoints: latency-svc-qprk9 [748.305002ms]
Jun 28 08:02:55.016: INFO: Created: latency-svc-k2jhk
Jun 28 08:02:55.053: INFO: Got endpoints: latency-svc-2wch4 [749.155194ms]
Jun 28 08:02:55.064: INFO: Created: latency-svc-9njzl
Jun 28 08:02:55.103: INFO: Got endpoints: latency-svc-qk2hr [750.617635ms]
Jun 28 08:02:55.116: INFO: Created: latency-svc-dbp6d
Jun 28 08:02:55.153: INFO: Got endpoints: latency-svc-4pgnt [750.506264ms]
Jun 28 08:02:55.164: INFO: Created: latency-svc-6qvqr
Jun 28 08:02:55.204: INFO: Got endpoints: latency-svc-svwc7 [750.747068ms]
Jun 28 08:02:55.215: INFO: Created: latency-svc-6cdl7
Jun 28 08:02:55.253: INFO: Got endpoints: latency-svc-mdlmd [749.089606ms]
Jun 28 08:02:55.264: INFO: Created: latency-svc-4v2fm
Jun 28 08:02:55.303: INFO: Got endpoints: latency-svc-s4w66 [749.482483ms]
Jun 28 08:02:55.313: INFO: Created: latency-svc-tzv2l
Jun 28 08:02:55.353: INFO: Got endpoints: latency-svc-xkztv [745.577141ms]
Jun 28 08:02:55.364: INFO: Created: latency-svc-l4t97
Jun 28 08:02:55.404: INFO: Got endpoints: latency-svc-4gvvs [751.091279ms]
Jun 28 08:02:55.415: INFO: Created: latency-svc-7t8kt
Jun 28 08:02:55.454: INFO: Got endpoints: latency-svc-nnwbx [751.034408ms]
Jun 28 08:02:55.467: INFO: Created: latency-svc-wpclm
Jun 28 08:02:55.503: INFO: Got endpoints: latency-svc-frlbn [748.808393ms]
Jun 28 08:02:55.512: INFO: Created: latency-svc-7hlss
Jun 28 08:02:55.554: INFO: Got endpoints: latency-svc-f7mxm [750.58674ms]
Jun 28 08:02:55.568: INFO: Created: latency-svc-ms68z
Jun 28 08:02:55.603: INFO: Got endpoints: latency-svc-flsxn [749.919196ms]
Jun 28 08:02:55.614: INFO: Created: latency-svc-f9bh4
Jun 28 08:02:55.654: INFO: Got endpoints: latency-svc-bhxst [750.36986ms]
Jun 28 08:02:55.665: INFO: Created: latency-svc-pggn5
Jun 28 08:02:55.703: INFO: Got endpoints: latency-svc-jrfmm [747.856488ms]
Jun 28 08:02:55.715: INFO: Created: latency-svc-4zfg9
Jun 28 08:02:55.755: INFO: Got endpoints: latency-svc-k2jhk [751.183703ms]
Jun 28 08:02:55.767: INFO: Created: latency-svc-fzrhz
Jun 28 08:02:55.802: INFO: Got endpoints: latency-svc-9njzl [748.753877ms]
Jun 28 08:02:55.813: INFO: Created: latency-svc-vtwl6
Jun 28 08:02:55.854: INFO: Got endpoints: latency-svc-dbp6d [751.2557ms]
Jun 28 08:02:55.866: INFO: Created: latency-svc-4v8d8
Jun 28 08:02:55.904: INFO: Got endpoints: latency-svc-6qvqr [751.342689ms]
Jun 28 08:02:55.915: INFO: Created: latency-svc-25gnt
Jun 28 08:02:55.954: INFO: Got endpoints: latency-svc-6cdl7 [749.909837ms]
Jun 28 08:02:55.965: INFO: Created: latency-svc-nbhbb
Jun 28 08:02:56.004: INFO: Got endpoints: latency-svc-4v2fm [751.14955ms]
Jun 28 08:02:56.016: INFO: Created: latency-svc-xdw58
Jun 28 08:02:56.053: INFO: Got endpoints: latency-svc-tzv2l [750.363546ms]
Jun 28 08:02:56.063: INFO: Created: latency-svc-66gsz
Jun 28 08:02:56.103: INFO: Got endpoints: latency-svc-l4t97 [749.746369ms]
Jun 28 08:02:56.115: INFO: Created: latency-svc-2q5qd
Jun 28 08:02:56.153: INFO: Got endpoints: latency-svc-7t8kt [749.029621ms]
Jun 28 08:02:56.163: INFO: Created: latency-svc-tl8d6
Jun 28 08:02:56.203: INFO: Got endpoints: latency-svc-wpclm [748.864664ms]
Jun 28 08:02:56.216: INFO: Created: latency-svc-6bptk
Jun 28 08:02:56.253: INFO: Got endpoints: latency-svc-7hlss [750.796815ms]
Jun 28 08:02:56.264: INFO: Created: latency-svc-4ndmn
Jun 28 08:02:56.303: INFO: Got endpoints: latency-svc-ms68z [749.577551ms]
Jun 28 08:02:56.314: INFO: Created: latency-svc-vknbv
Jun 28 08:02:56.355: INFO: Got endpoints: latency-svc-f9bh4 [751.637445ms]
Jun 28 08:02:56.365: INFO: Created: latency-svc-spg6n
Jun 28 08:02:56.403: INFO: Got endpoints: latency-svc-pggn5 [749.046196ms]
Jun 28 08:02:56.453: INFO: Got endpoints: latency-svc-4zfg9 [750.273297ms]
Jun 28 08:02:56.504: INFO: Got endpoints: latency-svc-fzrhz [749.425061ms]
Jun 28 08:02:56.554: INFO: Got endpoints: latency-svc-vtwl6 [751.530515ms]
Jun 28 08:02:56.605: INFO: Got endpoints: latency-svc-4v8d8 [750.206221ms]
Jun 28 08:02:56.655: INFO: Got endpoints: latency-svc-25gnt [750.700418ms]
Jun 28 08:02:56.705: INFO: Got endpoints: latency-svc-nbhbb [751.258421ms]
Jun 28 08:02:56.754: INFO: Got endpoints: latency-svc-xdw58 [749.622278ms]
Jun 28 08:02:56.803: INFO: Got endpoints: latency-svc-66gsz [749.611735ms]
Jun 28 08:02:56.854: INFO: Got endpoints: latency-svc-2q5qd [751.152452ms]
Jun 28 08:02:56.906: INFO: Got endpoints: latency-svc-tl8d6 [752.748809ms]
Jun 28 08:02:56.953: INFO: Got endpoints: latency-svc-6bptk [750.002947ms]
Jun 28 08:02:57.002: INFO: Got endpoints: latency-svc-4ndmn [748.514416ms]
Jun 28 08:02:57.053: INFO: Got endpoints: latency-svc-vknbv [750.033117ms]
Jun 28 08:02:57.103: INFO: Got endpoints: latency-svc-spg6n [747.83102ms]
Jun 28 08:02:57.103: INFO: Latencies: [15.943991ms 17.758238ms 19.666503ms 22.32193ms 25.263179ms 26.877023ms 26.993344ms 27.063654ms 29.646621ms 30.138879ms 30.937838ms 33.734975ms 40.365723ms 44.927804ms 51.213ms 51.389837ms 54.468826ms 63.204242ms 73.917853ms 78.280268ms 78.504448ms 83.203274ms 84.909645ms 88.695464ms 94.999612ms 100.171644ms 101.068628ms 106.705864ms 106.820143ms 117.487982ms 119.900089ms 123.491507ms 125.771195ms 127.271961ms 171.660627ms 220.954954ms 273.818474ms 323.696174ms 376.816484ms 419.303081ms 464.703762ms 511.892117ms 553.921775ms 604.010824ms 656.568656ms 700.235686ms 732.530327ms 744.186863ms 744.808559ms 745.577141ms 746.206986ms 746.299153ms 746.578394ms 746.786708ms 746.985991ms 747.176067ms 747.619853ms 747.739293ms 747.83102ms 747.851656ms 747.856488ms 747.871635ms 747.930483ms 747.986909ms 748.042324ms 748.081682ms 748.305002ms 748.378858ms 748.420116ms 748.423596ms 748.514416ms 748.531895ms 748.600796ms 748.623008ms 748.638036ms 748.694063ms 748.745878ms 748.753877ms 748.808393ms 748.864664ms 748.904915ms 748.928472ms 748.948463ms 749.029621ms 749.046196ms 749.089606ms 749.155194ms 749.301163ms 749.365935ms 749.376488ms 749.396606ms 749.425061ms 749.429187ms 749.454336ms 749.482483ms 749.485415ms 749.504821ms 749.522185ms 749.560383ms 749.577551ms 749.611735ms 749.618649ms 749.622278ms 749.675522ms 749.725271ms 749.732838ms 749.746369ms 749.852503ms 749.862787ms 749.874667ms 749.909837ms 749.919196ms 749.919348ms 749.931632ms 749.936337ms 749.946296ms 749.963765ms 750.002947ms 750.033117ms 750.088199ms 750.127228ms 750.1611ms 750.163501ms 750.176892ms 750.206221ms 750.262409ms 750.273297ms 750.274528ms 750.27713ms 750.279546ms 750.28856ms 750.298879ms 750.310564ms 750.319012ms 750.330807ms 750.343515ms 750.363546ms 750.36986ms 750.390319ms 750.408199ms 750.42463ms 750.457941ms 750.471765ms 750.499458ms 750.506264ms 750.567042ms 750.58674ms 750.588698ms 750.590906ms 750.617635ms 750.654027ms 750.669252ms 750.682878ms 750.700418ms 750.703272ms 750.747068ms 750.784943ms 750.796815ms 750.810937ms 750.840368ms 750.881149ms 750.947151ms 750.970443ms 750.974528ms 751.0231ms 751.034408ms 751.038795ms 751.041928ms 751.043491ms 751.091279ms 751.114485ms 751.14955ms 751.152452ms 751.181077ms 751.183703ms 751.222322ms 751.2557ms 751.258421ms 751.310519ms 751.342689ms 751.368509ms 751.480646ms 751.507539ms 751.530515ms 751.575094ms 751.637445ms 751.81575ms 751.825915ms 751.935005ms 752.133082ms 752.322093ms 752.335885ms 752.373308ms 752.748809ms 752.924211ms 752.96ms 753.637063ms 754.233795ms 754.500193ms 766.607539ms]
Jun 28 08:02:57.103: INFO: 50 %ile: 749.611735ms
Jun 28 08:02:57.103: INFO: 90 %ile: 751.368509ms
Jun 28 08:02:57.103: INFO: 99 %ile: 754.500193ms
Jun 28 08:02:57.103: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  test/e2e/framework/framework.go:187
Jun 28 08:02:57.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-5671" for this suite. 06/28/23 08:02:57.113
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","completed":83,"skipped":1465,"failed":0}
------------------------------
â€¢ [SLOW TEST] [9.768 seconds]
[sig-network] Service endpoints latency
test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Service endpoints latency
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:02:47.352
    Jun 28 08:02:47.352: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename svc-latency 06/28/23 08:02:47.353
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:02:47.367
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:02:47.372
    [It] should not be very high  [Conformance]
      test/e2e/network/service_latency.go:59
    Jun 28 08:02:47.377: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: creating replication controller svc-latency-rc in namespace svc-latency-5671 06/28/23 08:02:47.377
    I0628 08:02:47.383810      18 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-5671, replica count: 1
    I0628 08:02:48.436817      18 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jun 28 08:02:48.549: INFO: Created: latency-svc-4s8xn
    Jun 28 08:02:48.552: INFO: Got endpoints: latency-svc-4s8xn [15.297472ms]
    Jun 28 08:02:48.562: INFO: Created: latency-svc-ppjfd
    Jun 28 08:02:48.566: INFO: Created: latency-svc-64bfp
    Jun 28 08:02:48.569: INFO: Got endpoints: latency-svc-ppjfd [15.943991ms]
    Jun 28 08:02:48.569: INFO: Created: latency-svc-skmzq
    Jun 28 08:02:48.572: INFO: Got endpoints: latency-svc-64bfp [19.666503ms]
    Jun 28 08:02:48.575: INFO: Got endpoints: latency-svc-skmzq [22.32193ms]
    Jun 28 08:02:48.576: INFO: Created: latency-svc-vn4xn
    Jun 28 08:02:48.578: INFO: Created: latency-svc-tbj46
    Jun 28 08:02:48.580: INFO: Got endpoints: latency-svc-vn4xn [26.877023ms]
    Jun 28 08:02:48.583: INFO: Got endpoints: latency-svc-tbj46 [29.646621ms]
    Jun 28 08:02:48.583: INFO: Created: latency-svc-2w6ht
    Jun 28 08:02:48.587: INFO: Got endpoints: latency-svc-2w6ht [33.734975ms]
    Jun 28 08:02:48.588: INFO: Created: latency-svc-wsq7r
    Jun 28 08:02:48.593: INFO: Created: latency-svc-r5swf
    Jun 28 08:02:48.594: INFO: Got endpoints: latency-svc-wsq7r [40.365723ms]
    Jun 28 08:02:48.598: INFO: Got endpoints: latency-svc-r5swf [44.927804ms]
    Jun 28 08:02:48.599: INFO: Created: latency-svc-p8tng
    Jun 28 08:02:48.600: INFO: Created: latency-svc-5h868
    Jun 28 08:02:48.605: INFO: Got endpoints: latency-svc-p8tng [51.213ms]
    Jun 28 08:02:48.606: INFO: Got endpoints: latency-svc-5h868 [51.389837ms]
    Jun 28 08:02:48.606: INFO: Created: latency-svc-kwb4t
    Jun 28 08:02:48.609: INFO: Got endpoints: latency-svc-kwb4t [54.468826ms]
    Jun 28 08:02:48.662: INFO: Created: latency-svc-pmvvk
    Jun 28 08:02:48.662: INFO: Created: latency-svc-qlt7z
    Jun 28 08:02:48.663: INFO: Created: latency-svc-2c8xt
    Jun 28 08:02:48.669: INFO: Created: latency-svc-f5x4l
    Jun 28 08:02:48.669: INFO: Created: latency-svc-xp2lg
    Jun 28 08:02:48.669: INFO: Created: latency-svc-cnckq
    Jun 28 08:02:48.669: INFO: Created: latency-svc-7756c
    Jun 28 08:02:48.669: INFO: Created: latency-svc-6dpjz
    Jun 28 08:02:48.669: INFO: Created: latency-svc-7kmdc
    Jun 28 08:02:48.670: INFO: Created: latency-svc-pfbt7
    Jun 28 08:02:48.670: INFO: Created: latency-svc-btx59
    Jun 28 08:02:48.670: INFO: Created: latency-svc-xf7b4
    Jun 28 08:02:48.670: INFO: Got endpoints: latency-svc-2c8xt [101.068628ms]
    Jun 28 08:02:48.672: INFO: Got endpoints: latency-svc-pmvvk [88.695464ms]
    Jun 28 08:02:48.672: INFO: Got endpoints: latency-svc-7756c [78.280268ms]
    Jun 28 08:02:48.672: INFO: Got endpoints: latency-svc-xp2lg [63.204242ms]
    Jun 28 08:02:48.672: INFO: Got endpoints: latency-svc-qlt7z [117.487982ms]
    Jun 28 08:02:48.673: INFO: Created: latency-svc-wt4tm
    Jun 28 08:02:48.673: INFO: Created: latency-svc-sf9n7
    Jun 28 08:02:48.673: INFO: Created: latency-svc-n9c8n
    Jun 28 08:02:48.675: INFO: Got endpoints: latency-svc-f5x4l [119.900089ms]
    Jun 28 08:02:48.679: INFO: Got endpoints: latency-svc-n9c8n [106.820143ms]
    Jun 28 08:02:48.680: INFO: Got endpoints: latency-svc-7kmdc [100.171644ms]
    Jun 28 08:02:48.680: INFO: Got endpoints: latency-svc-sf9n7 [125.771195ms]
    Jun 28 08:02:48.681: INFO: Got endpoints: latency-svc-btx59 [83.203274ms]
    Jun 28 08:02:48.682: INFO: Created: latency-svc-x974d
    Jun 28 08:02:48.682: INFO: Got endpoints: latency-svc-pfbt7 [106.705864ms]
    Jun 28 08:02:48.682: INFO: Got endpoints: latency-svc-xf7b4 [127.271961ms]
    Jun 28 08:02:48.682: INFO: Got endpoints: latency-svc-wt4tm [94.999612ms]
    Jun 28 08:02:48.684: INFO: Got endpoints: latency-svc-cnckq [78.504448ms]
    Jun 28 08:02:48.688: INFO: Created: latency-svc-bw5cv
    Jun 28 08:02:48.688: INFO: Got endpoints: latency-svc-x974d [17.758238ms]
    Jun 28 08:02:48.691: INFO: Got endpoints: latency-svc-6dpjz [84.909645ms]
    Jun 28 08:02:48.697: INFO: Created: latency-svc-qd8cp
    Jun 28 08:02:48.697: INFO: Created: latency-svc-ltb4v
    Jun 28 08:02:48.697: INFO: Got endpoints: latency-svc-qd8cp [25.263179ms]
    Jun 28 08:02:48.699: INFO: Created: latency-svc-z6m9q
    Jun 28 08:02:48.699: INFO: Got endpoints: latency-svc-bw5cv [27.063654ms]
    Jun 28 08:02:48.699: INFO: Got endpoints: latency-svc-ltb4v [26.993344ms]
    Jun 28 08:02:48.702: INFO: Created: latency-svc-p9j5g
    Jun 28 08:02:48.703: INFO: Got endpoints: latency-svc-z6m9q [30.937838ms]
    Jun 28 08:02:48.705: INFO: Got endpoints: latency-svc-p9j5g [30.138879ms]
    Jun 28 08:02:48.709: INFO: Created: latency-svc-8p2jl
    Jun 28 08:02:48.711: INFO: Created: latency-svc-dwsrn
    Jun 28 08:02:48.717: INFO: Created: latency-svc-qx82p
    Jun 28 08:02:48.725: INFO: Created: latency-svc-rl9wz
    Jun 28 08:02:48.727: INFO: Created: latency-svc-sf5f2
    Jun 28 08:02:48.730: INFO: Created: latency-svc-fvg7f
    Jun 28 08:02:48.735: INFO: Created: latency-svc-nj8cd
    Jun 28 08:02:48.740: INFO: Created: latency-svc-jrsfs
    Jun 28 08:02:48.742: INFO: Created: latency-svc-np8kh
    Jun 28 08:02:48.747: INFO: Created: latency-svc-bjkr4
    Jun 28 08:02:48.752: INFO: Created: latency-svc-fl7qx
    Jun 28 08:02:48.753: INFO: Got endpoints: latency-svc-8p2jl [73.917853ms]
    Jun 28 08:02:48.754: INFO: Created: latency-svc-khcsg
    Jun 28 08:02:48.758: INFO: Created: latency-svc-tf65c
    Jun 28 08:02:48.762: INFO: Created: latency-svc-gkd2b
    Jun 28 08:02:48.766: INFO: Created: latency-svc-9bf8z
    Jun 28 08:02:48.769: INFO: Created: latency-svc-22pl8
    Jun 28 08:02:48.804: INFO: Got endpoints: latency-svc-dwsrn [123.491507ms]
    Jun 28 08:02:48.814: INFO: Created: latency-svc-nq8c9
    Jun 28 08:02:48.853: INFO: Got endpoints: latency-svc-qx82p [171.660627ms]
    Jun 28 08:02:48.864: INFO: Created: latency-svc-lbq24
    Jun 28 08:02:48.903: INFO: Got endpoints: latency-svc-rl9wz [220.954954ms]
    Jun 28 08:02:48.913: INFO: Created: latency-svc-v9s28
    Jun 28 08:02:48.956: INFO: Got endpoints: latency-svc-sf5f2 [273.818474ms]
    Jun 28 08:02:48.970: INFO: Created: latency-svc-gvzns
    Jun 28 08:02:49.004: INFO: Got endpoints: latency-svc-fvg7f [323.696174ms]
    Jun 28 08:02:49.015: INFO: Created: latency-svc-gczpz
    Jun 28 08:02:49.059: INFO: Got endpoints: latency-svc-nj8cd [376.816484ms]
    Jun 28 08:02:49.071: INFO: Created: latency-svc-hlhnn
    Jun 28 08:02:49.103: INFO: Got endpoints: latency-svc-jrsfs [419.303081ms]
    Jun 28 08:02:49.118: INFO: Created: latency-svc-vqdx7
    Jun 28 08:02:49.153: INFO: Got endpoints: latency-svc-np8kh [464.703762ms]
    Jun 28 08:02:49.165: INFO: Created: latency-svc-l2kwm
    Jun 28 08:02:49.203: INFO: Got endpoints: latency-svc-bjkr4 [511.892117ms]
    Jun 28 08:02:49.214: INFO: Created: latency-svc-hxjd7
    Jun 28 08:02:49.253: INFO: Got endpoints: latency-svc-fl7qx [553.921775ms]
    Jun 28 08:02:49.265: INFO: Created: latency-svc-mn976
    Jun 28 08:02:49.303: INFO: Got endpoints: latency-svc-khcsg [604.010824ms]
    Jun 28 08:02:49.313: INFO: Created: latency-svc-jw7zt
    Jun 28 08:02:49.354: INFO: Got endpoints: latency-svc-tf65c [656.568656ms]
    Jun 28 08:02:49.364: INFO: Created: latency-svc-nhwwp
    Jun 28 08:02:49.404: INFO: Got endpoints: latency-svc-gkd2b [700.235686ms]
    Jun 28 08:02:49.416: INFO: Created: latency-svc-7xblc
    Jun 28 08:02:49.453: INFO: Got endpoints: latency-svc-9bf8z [748.600796ms]
    Jun 28 08:02:49.464: INFO: Created: latency-svc-d7nql
    Jun 28 08:02:49.503: INFO: Got endpoints: latency-svc-22pl8 [750.390319ms]
    Jun 28 08:02:49.514: INFO: Created: latency-svc-d7gd4
    Jun 28 08:02:49.553: INFO: Got endpoints: latency-svc-nq8c9 [748.928472ms]
    Jun 28 08:02:49.563: INFO: Created: latency-svc-k695t
    Jun 28 08:02:49.603: INFO: Got endpoints: latency-svc-lbq24 [749.919348ms]
    Jun 28 08:02:49.616: INFO: Created: latency-svc-5c9dr
    Jun 28 08:02:49.653: INFO: Got endpoints: latency-svc-v9s28 [750.088199ms]
    Jun 28 08:02:49.663: INFO: Created: latency-svc-wzmtq
    Jun 28 08:02:49.702: INFO: Got endpoints: latency-svc-gvzns [746.299153ms]
    Jun 28 08:02:49.714: INFO: Created: latency-svc-8rhvz
    Jun 28 08:02:49.754: INFO: Got endpoints: latency-svc-gczpz [749.522185ms]
    Jun 28 08:02:49.764: INFO: Created: latency-svc-nnf79
    Jun 28 08:02:49.803: INFO: Got endpoints: latency-svc-hlhnn [744.186863ms]
    Jun 28 08:02:49.814: INFO: Created: latency-svc-z6xcz
    Jun 28 08:02:49.854: INFO: Got endpoints: latency-svc-vqdx7 [750.163501ms]
    Jun 28 08:02:49.864: INFO: Created: latency-svc-kl5mn
    Jun 28 08:02:49.903: INFO: Got endpoints: latency-svc-l2kwm [750.279546ms]
    Jun 28 08:02:49.913: INFO: Created: latency-svc-mcltm
    Jun 28 08:02:49.954: INFO: Got endpoints: latency-svc-hxjd7 [751.114485ms]
    Jun 28 08:02:49.965: INFO: Created: latency-svc-g68rl
    Jun 28 08:02:50.003: INFO: Got endpoints: latency-svc-mn976 [750.590906ms]
    Jun 28 08:02:50.014: INFO: Created: latency-svc-mgf6p
    Jun 28 08:02:50.053: INFO: Got endpoints: latency-svc-jw7zt [750.176892ms]
    Jun 28 08:02:50.063: INFO: Created: latency-svc-lz9mx
    Jun 28 08:02:50.103: INFO: Got endpoints: latency-svc-nhwwp [748.745878ms]
    Jun 28 08:02:50.113: INFO: Created: latency-svc-vnlll
    Jun 28 08:02:50.153: INFO: Got endpoints: latency-svc-7xblc [749.301163ms]
    Jun 28 08:02:50.163: INFO: Created: latency-svc-2p94x
    Jun 28 08:02:50.203: INFO: Got endpoints: latency-svc-d7nql [749.732838ms]
    Jun 28 08:02:50.214: INFO: Created: latency-svc-hrt8g
    Jun 28 08:02:50.253: INFO: Got endpoints: latency-svc-d7gd4 [749.454336ms]
    Jun 28 08:02:50.263: INFO: Created: latency-svc-g4scz
    Jun 28 08:02:50.303: INFO: Got endpoints: latency-svc-k695t [750.703272ms]
    Jun 28 08:02:50.314: INFO: Created: latency-svc-bxnhc
    Jun 28 08:02:50.354: INFO: Got endpoints: latency-svc-5c9dr [750.457941ms]
    Jun 28 08:02:50.364: INFO: Created: latency-svc-zqcvm
    Jun 28 08:02:50.403: INFO: Got endpoints: latency-svc-wzmtq [749.725271ms]
    Jun 28 08:02:50.413: INFO: Created: latency-svc-2wr8p
    Jun 28 08:02:50.453: INFO: Got endpoints: latency-svc-8rhvz [750.970443ms]
    Jun 28 08:02:50.463: INFO: Created: latency-svc-5xr48
    Jun 28 08:02:50.503: INFO: Got endpoints: latency-svc-nnf79 [748.904915ms]
    Jun 28 08:02:50.514: INFO: Created: latency-svc-bbjqq
    Jun 28 08:02:50.553: INFO: Got endpoints: latency-svc-z6xcz [749.931632ms]
    Jun 28 08:02:50.564: INFO: Created: latency-svc-z775t
    Jun 28 08:02:50.605: INFO: Got endpoints: latency-svc-kl5mn [751.825915ms]
    Jun 28 08:02:50.620: INFO: Created: latency-svc-rbxpg
    Jun 28 08:02:50.654: INFO: Got endpoints: latency-svc-mcltm [751.041928ms]
    Jun 28 08:02:50.664: INFO: Created: latency-svc-jw28q
    Jun 28 08:02:50.704: INFO: Got endpoints: latency-svc-g68rl [749.963765ms]
    Jun 28 08:02:50.715: INFO: Created: latency-svc-48mks
    Jun 28 08:02:50.755: INFO: Got endpoints: latency-svc-mgf6p [751.575094ms]
    Jun 28 08:02:50.765: INFO: Created: latency-svc-9r6l7
    Jun 28 08:02:50.802: INFO: Got endpoints: latency-svc-lz9mx [748.423596ms]
    Jun 28 08:02:50.812: INFO: Created: latency-svc-xvtc4
    Jun 28 08:02:50.854: INFO: Got endpoints: latency-svc-vnlll [751.507539ms]
    Jun 28 08:02:50.864: INFO: Created: latency-svc-6c9sb
    Jun 28 08:02:50.904: INFO: Got endpoints: latency-svc-2p94x [750.588698ms]
    Jun 28 08:02:50.914: INFO: Created: latency-svc-r59f8
    Jun 28 08:02:50.953: INFO: Got endpoints: latency-svc-hrt8g [749.560383ms]
    Jun 28 08:02:50.964: INFO: Created: latency-svc-rjlpz
    Jun 28 08:02:51.003: INFO: Got endpoints: latency-svc-g4scz [749.675522ms]
    Jun 28 08:02:51.014: INFO: Created: latency-svc-9hwjd
    Jun 28 08:02:51.054: INFO: Got endpoints: latency-svc-bxnhc [750.330807ms]
    Jun 28 08:02:51.064: INFO: Created: latency-svc-v8r9n
    Jun 28 08:02:51.103: INFO: Got endpoints: latency-svc-zqcvm [749.365935ms]
    Jun 28 08:02:51.113: INFO: Created: latency-svc-knwbl
    Jun 28 08:02:51.153: INFO: Got endpoints: latency-svc-2wr8p [750.682878ms]
    Jun 28 08:02:51.164: INFO: Created: latency-svc-jjpcl
    Jun 28 08:02:51.204: INFO: Got endpoints: latency-svc-5xr48 [750.298879ms]
    Jun 28 08:02:51.214: INFO: Created: latency-svc-gzsnj
    Jun 28 08:02:51.253: INFO: Got endpoints: latency-svc-bbjqq [750.28856ms]
    Jun 28 08:02:51.265: INFO: Created: latency-svc-hjj5h
    Jun 28 08:02:51.303: INFO: Got endpoints: latency-svc-z775t [749.852503ms]
    Jun 28 08:02:51.314: INFO: Created: latency-svc-2hj9d
    Jun 28 08:02:51.356: INFO: Got endpoints: latency-svc-rbxpg [750.42463ms]
    Jun 28 08:02:51.366: INFO: Created: latency-svc-q5sjr
    Jun 28 08:02:51.405: INFO: Got endpoints: latency-svc-jw28q [750.784943ms]
    Jun 28 08:02:51.417: INFO: Created: latency-svc-9stnm
    Jun 28 08:02:51.452: INFO: Got endpoints: latency-svc-48mks [748.420116ms]
    Jun 28 08:02:51.462: INFO: Created: latency-svc-fdn7h
    Jun 28 08:02:51.503: INFO: Got endpoints: latency-svc-9r6l7 [748.378858ms]
    Jun 28 08:02:51.514: INFO: Created: latency-svc-45gqg
    Jun 28 08:02:51.553: INFO: Got endpoints: latency-svc-xvtc4 [751.310519ms]
    Jun 28 08:02:51.564: INFO: Created: latency-svc-gfdsl
    Jun 28 08:02:51.604: INFO: Got endpoints: latency-svc-6c9sb [749.396606ms]
    Jun 28 08:02:51.614: INFO: Created: latency-svc-5msbz
    Jun 28 08:02:51.654: INFO: Got endpoints: latency-svc-r59f8 [750.319012ms]
    Jun 28 08:02:51.666: INFO: Created: latency-svc-krfk9
    Jun 28 08:02:51.704: INFO: Got endpoints: latency-svc-rjlpz [751.480646ms]
    Jun 28 08:02:51.716: INFO: Created: latency-svc-m626z
    Jun 28 08:02:51.753: INFO: Got endpoints: latency-svc-9hwjd [750.567042ms]
    Jun 28 08:02:51.763: INFO: Created: latency-svc-4c2m9
    Jun 28 08:02:51.805: INFO: Got endpoints: latency-svc-v8r9n [751.038795ms]
    Jun 28 08:02:51.816: INFO: Created: latency-svc-r9xdx
    Jun 28 08:02:51.853: INFO: Got endpoints: latency-svc-knwbl [749.618649ms]
    Jun 28 08:02:51.863: INFO: Created: latency-svc-pzgl4
    Jun 28 08:02:51.904: INFO: Got endpoints: latency-svc-jjpcl [750.669252ms]
    Jun 28 08:02:51.915: INFO: Created: latency-svc-z4qnr
    Jun 28 08:02:51.954: INFO: Got endpoints: latency-svc-gzsnj [750.499458ms]
    Jun 28 08:02:51.967: INFO: Created: latency-svc-7nfns
    Jun 28 08:02:52.005: INFO: Got endpoints: latency-svc-hjj5h [751.81575ms]
    Jun 28 08:02:52.018: INFO: Created: latency-svc-69dq5
    Jun 28 08:02:52.055: INFO: Got endpoints: latency-svc-2hj9d [752.373308ms]
    Jun 28 08:02:52.067: INFO: Created: latency-svc-ncshn
    Jun 28 08:02:52.104: INFO: Got endpoints: latency-svc-q5sjr [747.619853ms]
    Jun 28 08:02:52.117: INFO: Created: latency-svc-lvs5z
    Jun 28 08:02:52.154: INFO: Got endpoints: latency-svc-9stnm [749.376488ms]
    Jun 28 08:02:52.166: INFO: Created: latency-svc-shc62
    Jun 28 08:02:52.205: INFO: Got endpoints: latency-svc-fdn7h [752.96ms]
    Jun 28 08:02:52.217: INFO: Created: latency-svc-6942b
    Jun 28 08:02:52.255: INFO: Got endpoints: latency-svc-45gqg [751.181077ms]
    Jun 28 08:02:52.266: INFO: Created: latency-svc-d4fh7
    Jun 28 08:02:52.303: INFO: Got endpoints: latency-svc-gfdsl [749.862787ms]
    Jun 28 08:02:52.313: INFO: Created: latency-svc-j4bnk
    Jun 28 08:02:52.353: INFO: Got endpoints: latency-svc-5msbz [749.429187ms]
    Jun 28 08:02:52.366: INFO: Created: latency-svc-9q8hf
    Jun 28 08:02:52.405: INFO: Got endpoints: latency-svc-krfk9 [750.881149ms]
    Jun 28 08:02:52.416: INFO: Created: latency-svc-kwbxp
    Jun 28 08:02:52.453: INFO: Got endpoints: latency-svc-m626z [748.948463ms]
    Jun 28 08:02:52.464: INFO: Created: latency-svc-n45f6
    Jun 28 08:02:52.504: INFO: Got endpoints: latency-svc-4c2m9 [750.810937ms]
    Jun 28 08:02:52.516: INFO: Created: latency-svc-zpk6b
    Jun 28 08:02:52.553: INFO: Got endpoints: latency-svc-r9xdx [748.531895ms]
    Jun 28 08:02:52.564: INFO: Created: latency-svc-6dh2l
    Jun 28 08:02:52.603: INFO: Got endpoints: latency-svc-pzgl4 [750.654027ms]
    Jun 28 08:02:52.614: INFO: Created: latency-svc-nwg5t
    Jun 28 08:02:52.653: INFO: Got endpoints: latency-svc-z4qnr [748.694063ms]
    Jun 28 08:02:52.662: INFO: Created: latency-svc-mt8mx
    Jun 28 08:02:52.703: INFO: Got endpoints: latency-svc-7nfns [748.638036ms]
    Jun 28 08:02:52.713: INFO: Created: latency-svc-vlnkd
    Jun 28 08:02:52.755: INFO: Got endpoints: latency-svc-69dq5 [750.1611ms]
    Jun 28 08:02:52.766: INFO: Created: latency-svc-znvf4
    Jun 28 08:02:52.803: INFO: Got endpoints: latency-svc-ncshn [747.739293ms]
    Jun 28 08:02:52.814: INFO: Created: latency-svc-4jjn4
    Jun 28 08:02:52.853: INFO: Got endpoints: latency-svc-lvs5z [749.504821ms]
    Jun 28 08:02:52.865: INFO: Created: latency-svc-z5xnw
    Jun 28 08:02:52.905: INFO: Got endpoints: latency-svc-shc62 [750.27713ms]
    Jun 28 08:02:52.918: INFO: Created: latency-svc-mm7c2
    Jun 28 08:02:52.953: INFO: Got endpoints: latency-svc-6942b [747.871635ms]
    Jun 28 08:02:52.966: INFO: Created: latency-svc-tjktb
    Jun 28 08:02:53.005: INFO: Got endpoints: latency-svc-d4fh7 [750.310564ms]
    Jun 28 08:02:53.020: INFO: Created: latency-svc-pw5sm
    Jun 28 08:02:53.057: INFO: Got endpoints: latency-svc-j4bnk [753.637063ms]
    Jun 28 08:02:53.069: INFO: Created: latency-svc-ftx42
    Jun 28 08:02:53.104: INFO: Got endpoints: latency-svc-9q8hf [751.222322ms]
    Jun 28 08:02:53.117: INFO: Created: latency-svc-mx86s
    Jun 28 08:02:53.156: INFO: Got endpoints: latency-svc-kwbxp [751.0231ms]
    Jun 28 08:02:53.171: INFO: Created: latency-svc-cswk7
    Jun 28 08:02:53.204: INFO: Got endpoints: latency-svc-n45f6 [750.840368ms]
    Jun 28 08:02:53.222: INFO: Created: latency-svc-4k47g
    Jun 28 08:02:53.256: INFO: Got endpoints: latency-svc-zpk6b [751.368509ms]
    Jun 28 08:02:53.268: INFO: Created: latency-svc-q852q
    Jun 28 08:02:53.304: INFO: Got endpoints: latency-svc-6dh2l [750.274528ms]
    Jun 28 08:02:53.317: INFO: Created: latency-svc-8sbp2
    Jun 28 08:02:53.354: INFO: Got endpoints: latency-svc-nwg5t [750.471765ms]
    Jun 28 08:02:53.365: INFO: Created: latency-svc-lh6wr
    Jun 28 08:02:53.403: INFO: Got endpoints: latency-svc-mt8mx [749.936337ms]
    Jun 28 08:02:53.417: INFO: Created: latency-svc-vfltd
    Jun 28 08:02:53.455: INFO: Got endpoints: latency-svc-vlnkd [752.322093ms]
    Jun 28 08:02:53.467: INFO: Created: latency-svc-p65vz
    Jun 28 08:02:53.502: INFO: Got endpoints: latency-svc-znvf4 [747.176067ms]
    Jun 28 08:02:53.512: INFO: Created: latency-svc-5pxnb
    Jun 28 08:02:53.554: INFO: Got endpoints: latency-svc-4jjn4 [750.408199ms]
    Jun 28 08:02:53.568: INFO: Created: latency-svc-mrw8d
    Jun 28 08:02:53.620: INFO: Got endpoints: latency-svc-z5xnw [766.607539ms]
    Jun 28 08:02:53.639: INFO: Created: latency-svc-qg548
    Jun 28 08:02:53.656: INFO: Got endpoints: latency-svc-mm7c2 [750.974528ms]
    Jun 28 08:02:53.668: INFO: Created: latency-svc-zcx6g
    Jun 28 08:02:53.705: INFO: Got endpoints: latency-svc-tjktb [751.935005ms]
    Jun 28 08:02:53.717: INFO: Created: latency-svc-glvwn
    Jun 28 08:02:53.756: INFO: Got endpoints: latency-svc-pw5sm [751.043491ms]
    Jun 28 08:02:53.768: INFO: Created: latency-svc-kptvr
    Jun 28 08:02:53.803: INFO: Got endpoints: latency-svc-ftx42 [746.206986ms]
    Jun 28 08:02:53.818: INFO: Created: latency-svc-4x24t
    Jun 28 08:02:53.853: INFO: Got endpoints: latency-svc-mx86s [748.623008ms]
    Jun 28 08:02:53.863: INFO: Created: latency-svc-wxvkz
    Jun 28 08:02:53.903: INFO: Got endpoints: latency-svc-cswk7 [746.786708ms]
    Jun 28 08:02:53.916: INFO: Created: latency-svc-m9sfx
    Jun 28 08:02:53.955: INFO: Got endpoints: latency-svc-4k47g [750.947151ms]
    Jun 28 08:02:53.970: INFO: Created: latency-svc-rdzpc
    Jun 28 08:02:54.004: INFO: Got endpoints: latency-svc-q852q [747.930483ms]
    Jun 28 08:02:54.015: INFO: Created: latency-svc-glhlc
    Jun 28 08:02:54.058: INFO: Got endpoints: latency-svc-8sbp2 [754.233795ms]
    Jun 28 08:02:54.080: INFO: Created: latency-svc-4zgh5
    Jun 28 08:02:54.106: INFO: Got endpoints: latency-svc-lh6wr [752.133082ms]
    Jun 28 08:02:54.120: INFO: Created: latency-svc-lcmwr
    Jun 28 08:02:54.155: INFO: Got endpoints: latency-svc-vfltd [752.335885ms]
    Jun 28 08:02:54.168: INFO: Created: latency-svc-zwd8m
    Jun 28 08:02:54.205: INFO: Got endpoints: latency-svc-p65vz [749.485415ms]
    Jun 28 08:02:54.215: INFO: Created: latency-svc-tkf9z
    Jun 28 08:02:54.255: INFO: Got endpoints: latency-svc-5pxnb [752.924211ms]
    Jun 28 08:02:54.267: INFO: Created: latency-svc-qprk9
    Jun 28 08:02:54.304: INFO: Got endpoints: latency-svc-mrw8d [750.343515ms]
    Jun 28 08:02:54.316: INFO: Created: latency-svc-2wch4
    Jun 28 08:02:54.352: INFO: Got endpoints: latency-svc-qg548 [732.530327ms]
    Jun 28 08:02:54.361: INFO: Created: latency-svc-qk2hr
    Jun 28 08:02:54.402: INFO: Got endpoints: latency-svc-zcx6g [746.578394ms]
    Jun 28 08:02:54.412: INFO: Created: latency-svc-4pgnt
    Jun 28 08:02:54.453: INFO: Got endpoints: latency-svc-glvwn [747.851656ms]
    Jun 28 08:02:54.464: INFO: Created: latency-svc-svwc7
    Jun 28 08:02:54.504: INFO: Got endpoints: latency-svc-kptvr [747.986909ms]
    Jun 28 08:02:54.516: INFO: Created: latency-svc-mdlmd
    Jun 28 08:02:54.553: INFO: Got endpoints: latency-svc-4x24t [749.946296ms]
    Jun 28 08:02:54.565: INFO: Created: latency-svc-s4w66
    Jun 28 08:02:54.608: INFO: Got endpoints: latency-svc-wxvkz [754.500193ms]
    Jun 28 08:02:54.619: INFO: Created: latency-svc-xkztv
    Jun 28 08:02:54.653: INFO: Got endpoints: latency-svc-m9sfx [749.874667ms]
    Jun 28 08:02:54.666: INFO: Created: latency-svc-4gvvs
    Jun 28 08:02:54.703: INFO: Got endpoints: latency-svc-rdzpc [748.042324ms]
    Jun 28 08:02:54.715: INFO: Created: latency-svc-nnwbx
    Jun 28 08:02:54.754: INFO: Got endpoints: latency-svc-glhlc [750.127228ms]
    Jun 28 08:02:54.768: INFO: Created: latency-svc-frlbn
    Jun 28 08:02:54.803: INFO: Got endpoints: latency-svc-4zgh5 [744.808559ms]
    Jun 28 08:02:54.815: INFO: Created: latency-svc-f7mxm
    Jun 28 08:02:54.853: INFO: Got endpoints: latency-svc-lcmwr [746.985991ms]
    Jun 28 08:02:54.863: INFO: Created: latency-svc-flsxn
    Jun 28 08:02:54.903: INFO: Got endpoints: latency-svc-zwd8m [748.081682ms]
    Jun 28 08:02:54.914: INFO: Created: latency-svc-bhxst
    Jun 28 08:02:54.955: INFO: Got endpoints: latency-svc-tkf9z [750.262409ms]
    Jun 28 08:02:54.966: INFO: Created: latency-svc-jrfmm
    Jun 28 08:02:55.004: INFO: Got endpoints: latency-svc-qprk9 [748.305002ms]
    Jun 28 08:02:55.016: INFO: Created: latency-svc-k2jhk
    Jun 28 08:02:55.053: INFO: Got endpoints: latency-svc-2wch4 [749.155194ms]
    Jun 28 08:02:55.064: INFO: Created: latency-svc-9njzl
    Jun 28 08:02:55.103: INFO: Got endpoints: latency-svc-qk2hr [750.617635ms]
    Jun 28 08:02:55.116: INFO: Created: latency-svc-dbp6d
    Jun 28 08:02:55.153: INFO: Got endpoints: latency-svc-4pgnt [750.506264ms]
    Jun 28 08:02:55.164: INFO: Created: latency-svc-6qvqr
    Jun 28 08:02:55.204: INFO: Got endpoints: latency-svc-svwc7 [750.747068ms]
    Jun 28 08:02:55.215: INFO: Created: latency-svc-6cdl7
    Jun 28 08:02:55.253: INFO: Got endpoints: latency-svc-mdlmd [749.089606ms]
    Jun 28 08:02:55.264: INFO: Created: latency-svc-4v2fm
    Jun 28 08:02:55.303: INFO: Got endpoints: latency-svc-s4w66 [749.482483ms]
    Jun 28 08:02:55.313: INFO: Created: latency-svc-tzv2l
    Jun 28 08:02:55.353: INFO: Got endpoints: latency-svc-xkztv [745.577141ms]
    Jun 28 08:02:55.364: INFO: Created: latency-svc-l4t97
    Jun 28 08:02:55.404: INFO: Got endpoints: latency-svc-4gvvs [751.091279ms]
    Jun 28 08:02:55.415: INFO: Created: latency-svc-7t8kt
    Jun 28 08:02:55.454: INFO: Got endpoints: latency-svc-nnwbx [751.034408ms]
    Jun 28 08:02:55.467: INFO: Created: latency-svc-wpclm
    Jun 28 08:02:55.503: INFO: Got endpoints: latency-svc-frlbn [748.808393ms]
    Jun 28 08:02:55.512: INFO: Created: latency-svc-7hlss
    Jun 28 08:02:55.554: INFO: Got endpoints: latency-svc-f7mxm [750.58674ms]
    Jun 28 08:02:55.568: INFO: Created: latency-svc-ms68z
    Jun 28 08:02:55.603: INFO: Got endpoints: latency-svc-flsxn [749.919196ms]
    Jun 28 08:02:55.614: INFO: Created: latency-svc-f9bh4
    Jun 28 08:02:55.654: INFO: Got endpoints: latency-svc-bhxst [750.36986ms]
    Jun 28 08:02:55.665: INFO: Created: latency-svc-pggn5
    Jun 28 08:02:55.703: INFO: Got endpoints: latency-svc-jrfmm [747.856488ms]
    Jun 28 08:02:55.715: INFO: Created: latency-svc-4zfg9
    Jun 28 08:02:55.755: INFO: Got endpoints: latency-svc-k2jhk [751.183703ms]
    Jun 28 08:02:55.767: INFO: Created: latency-svc-fzrhz
    Jun 28 08:02:55.802: INFO: Got endpoints: latency-svc-9njzl [748.753877ms]
    Jun 28 08:02:55.813: INFO: Created: latency-svc-vtwl6
    Jun 28 08:02:55.854: INFO: Got endpoints: latency-svc-dbp6d [751.2557ms]
    Jun 28 08:02:55.866: INFO: Created: latency-svc-4v8d8
    Jun 28 08:02:55.904: INFO: Got endpoints: latency-svc-6qvqr [751.342689ms]
    Jun 28 08:02:55.915: INFO: Created: latency-svc-25gnt
    Jun 28 08:02:55.954: INFO: Got endpoints: latency-svc-6cdl7 [749.909837ms]
    Jun 28 08:02:55.965: INFO: Created: latency-svc-nbhbb
    Jun 28 08:02:56.004: INFO: Got endpoints: latency-svc-4v2fm [751.14955ms]
    Jun 28 08:02:56.016: INFO: Created: latency-svc-xdw58
    Jun 28 08:02:56.053: INFO: Got endpoints: latency-svc-tzv2l [750.363546ms]
    Jun 28 08:02:56.063: INFO: Created: latency-svc-66gsz
    Jun 28 08:02:56.103: INFO: Got endpoints: latency-svc-l4t97 [749.746369ms]
    Jun 28 08:02:56.115: INFO: Created: latency-svc-2q5qd
    Jun 28 08:02:56.153: INFO: Got endpoints: latency-svc-7t8kt [749.029621ms]
    Jun 28 08:02:56.163: INFO: Created: latency-svc-tl8d6
    Jun 28 08:02:56.203: INFO: Got endpoints: latency-svc-wpclm [748.864664ms]
    Jun 28 08:02:56.216: INFO: Created: latency-svc-6bptk
    Jun 28 08:02:56.253: INFO: Got endpoints: latency-svc-7hlss [750.796815ms]
    Jun 28 08:02:56.264: INFO: Created: latency-svc-4ndmn
    Jun 28 08:02:56.303: INFO: Got endpoints: latency-svc-ms68z [749.577551ms]
    Jun 28 08:02:56.314: INFO: Created: latency-svc-vknbv
    Jun 28 08:02:56.355: INFO: Got endpoints: latency-svc-f9bh4 [751.637445ms]
    Jun 28 08:02:56.365: INFO: Created: latency-svc-spg6n
    Jun 28 08:02:56.403: INFO: Got endpoints: latency-svc-pggn5 [749.046196ms]
    Jun 28 08:02:56.453: INFO: Got endpoints: latency-svc-4zfg9 [750.273297ms]
    Jun 28 08:02:56.504: INFO: Got endpoints: latency-svc-fzrhz [749.425061ms]
    Jun 28 08:02:56.554: INFO: Got endpoints: latency-svc-vtwl6 [751.530515ms]
    Jun 28 08:02:56.605: INFO: Got endpoints: latency-svc-4v8d8 [750.206221ms]
    Jun 28 08:02:56.655: INFO: Got endpoints: latency-svc-25gnt [750.700418ms]
    Jun 28 08:02:56.705: INFO: Got endpoints: latency-svc-nbhbb [751.258421ms]
    Jun 28 08:02:56.754: INFO: Got endpoints: latency-svc-xdw58 [749.622278ms]
    Jun 28 08:02:56.803: INFO: Got endpoints: latency-svc-66gsz [749.611735ms]
    Jun 28 08:02:56.854: INFO: Got endpoints: latency-svc-2q5qd [751.152452ms]
    Jun 28 08:02:56.906: INFO: Got endpoints: latency-svc-tl8d6 [752.748809ms]
    Jun 28 08:02:56.953: INFO: Got endpoints: latency-svc-6bptk [750.002947ms]
    Jun 28 08:02:57.002: INFO: Got endpoints: latency-svc-4ndmn [748.514416ms]
    Jun 28 08:02:57.053: INFO: Got endpoints: latency-svc-vknbv [750.033117ms]
    Jun 28 08:02:57.103: INFO: Got endpoints: latency-svc-spg6n [747.83102ms]
    Jun 28 08:02:57.103: INFO: Latencies: [15.943991ms 17.758238ms 19.666503ms 22.32193ms 25.263179ms 26.877023ms 26.993344ms 27.063654ms 29.646621ms 30.138879ms 30.937838ms 33.734975ms 40.365723ms 44.927804ms 51.213ms 51.389837ms 54.468826ms 63.204242ms 73.917853ms 78.280268ms 78.504448ms 83.203274ms 84.909645ms 88.695464ms 94.999612ms 100.171644ms 101.068628ms 106.705864ms 106.820143ms 117.487982ms 119.900089ms 123.491507ms 125.771195ms 127.271961ms 171.660627ms 220.954954ms 273.818474ms 323.696174ms 376.816484ms 419.303081ms 464.703762ms 511.892117ms 553.921775ms 604.010824ms 656.568656ms 700.235686ms 732.530327ms 744.186863ms 744.808559ms 745.577141ms 746.206986ms 746.299153ms 746.578394ms 746.786708ms 746.985991ms 747.176067ms 747.619853ms 747.739293ms 747.83102ms 747.851656ms 747.856488ms 747.871635ms 747.930483ms 747.986909ms 748.042324ms 748.081682ms 748.305002ms 748.378858ms 748.420116ms 748.423596ms 748.514416ms 748.531895ms 748.600796ms 748.623008ms 748.638036ms 748.694063ms 748.745878ms 748.753877ms 748.808393ms 748.864664ms 748.904915ms 748.928472ms 748.948463ms 749.029621ms 749.046196ms 749.089606ms 749.155194ms 749.301163ms 749.365935ms 749.376488ms 749.396606ms 749.425061ms 749.429187ms 749.454336ms 749.482483ms 749.485415ms 749.504821ms 749.522185ms 749.560383ms 749.577551ms 749.611735ms 749.618649ms 749.622278ms 749.675522ms 749.725271ms 749.732838ms 749.746369ms 749.852503ms 749.862787ms 749.874667ms 749.909837ms 749.919196ms 749.919348ms 749.931632ms 749.936337ms 749.946296ms 749.963765ms 750.002947ms 750.033117ms 750.088199ms 750.127228ms 750.1611ms 750.163501ms 750.176892ms 750.206221ms 750.262409ms 750.273297ms 750.274528ms 750.27713ms 750.279546ms 750.28856ms 750.298879ms 750.310564ms 750.319012ms 750.330807ms 750.343515ms 750.363546ms 750.36986ms 750.390319ms 750.408199ms 750.42463ms 750.457941ms 750.471765ms 750.499458ms 750.506264ms 750.567042ms 750.58674ms 750.588698ms 750.590906ms 750.617635ms 750.654027ms 750.669252ms 750.682878ms 750.700418ms 750.703272ms 750.747068ms 750.784943ms 750.796815ms 750.810937ms 750.840368ms 750.881149ms 750.947151ms 750.970443ms 750.974528ms 751.0231ms 751.034408ms 751.038795ms 751.041928ms 751.043491ms 751.091279ms 751.114485ms 751.14955ms 751.152452ms 751.181077ms 751.183703ms 751.222322ms 751.2557ms 751.258421ms 751.310519ms 751.342689ms 751.368509ms 751.480646ms 751.507539ms 751.530515ms 751.575094ms 751.637445ms 751.81575ms 751.825915ms 751.935005ms 752.133082ms 752.322093ms 752.335885ms 752.373308ms 752.748809ms 752.924211ms 752.96ms 753.637063ms 754.233795ms 754.500193ms 766.607539ms]
    Jun 28 08:02:57.103: INFO: 50 %ile: 749.611735ms
    Jun 28 08:02:57.103: INFO: 90 %ile: 751.368509ms
    Jun 28 08:02:57.103: INFO: 99 %ile: 754.500193ms
    Jun 28 08:02:57.103: INFO: Total sample count: 200
    [AfterEach] [sig-network] Service endpoints latency
      test/e2e/framework/framework.go:187
    Jun 28 08:02:57.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svc-latency-5671" for this suite. 06/28/23 08:02:57.113
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:02:57.12
Jun 28 08:02:57.121: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename deployment 06/28/23 08:02:57.122
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:02:57.134
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:02:57.139
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
Jun 28 08:02:57.143: INFO: Creating deployment "webserver-deployment"
Jun 28 08:02:57.150: INFO: Waiting for observed generation 1
Jun 28 08:02:59.162: INFO: Waiting for all required pods to come up
Jun 28 08:02:59.171: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running 06/28/23 08:02:59.171
Jun 28 08:02:59.171: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-rtjj2" in namespace "deployment-2432" to be "running"
Jun 28 08:02:59.172: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-57jbw" in namespace "deployment-2432" to be "running"
Jun 28 08:02:59.176: INFO: Pod "webserver-deployment-845c8977d9-57jbw": Phase="Pending", Reason="", readiness=false. Elapsed: 4.695235ms
Jun 28 08:02:59.176: INFO: Pod "webserver-deployment-845c8977d9-rtjj2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.881065ms
Jun 28 08:03:01.183: INFO: Pod "webserver-deployment-845c8977d9-rtjj2": Phase="Running", Reason="", readiness=true. Elapsed: 2.011619261s
Jun 28 08:03:01.183: INFO: Pod "webserver-deployment-845c8977d9-rtjj2" satisfied condition "running"
Jun 28 08:03:01.183: INFO: Pod "webserver-deployment-845c8977d9-57jbw": Phase="Running", Reason="", readiness=true. Elapsed: 2.011470678s
Jun 28 08:03:01.183: INFO: Pod "webserver-deployment-845c8977d9-57jbw" satisfied condition "running"
Jun 28 08:03:01.183: INFO: Waiting for deployment "webserver-deployment" to complete
Jun 28 08:03:01.193: INFO: Updating deployment "webserver-deployment" with a non-existent image
Jun 28 08:03:01.208: INFO: Updating deployment webserver-deployment
Jun 28 08:03:01.208: INFO: Waiting for observed generation 2
Jun 28 08:03:03.220: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Jun 28 08:03:03.224: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Jun 28 08:03:03.229: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jun 28 08:03:03.244: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Jun 28 08:03:03.244: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Jun 28 08:03:03.249: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jun 28 08:03:03.259: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Jun 28 08:03:03.259: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Jun 28 08:03:03.272: INFO: Updating deployment webserver-deployment
Jun 28 08:03:03.272: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Jun 28 08:03:03.283: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Jun 28 08:03:03.290: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jun 28 08:03:03.307: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-2432  d4abfe49-fb74-4682-bced-eb1a1135bb1c 58351 3 2023-06-28 08:02:57 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-06-28 08:03:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-28 08:03:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0030c4718 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-69b7448995" is progressing.,LastUpdateTime:2023-06-28 08:03:01 +0000 UTC,LastTransitionTime:2023-06-28 08:02:57 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-06-28 08:03:03 +0000 UTC,LastTransitionTime:2023-06-28 08:03:03 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Jun 28 08:03:03.321: INFO: New ReplicaSet "webserver-deployment-69b7448995" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-69b7448995  deployment-2432  856ab508-beb4-4308-9f76-62868b6545b9 58331 3 2023-06-28 08:03:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment d4abfe49-fb74-4682-bced-eb1a1135bb1c 0xc0031203c7 0xc0031203c8}] [] [{kube-controller-manager Update apps/v1 2023-06-28 08:03:01 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-06-28 08:03:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d4abfe49-fb74-4682-bced-eb1a1135bb1c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 69b7448995,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003120468 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun 28 08:03:03.321: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Jun 28 08:03:03.321: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-845c8977d9  deployment-2432  79eaf046-3c6c-452e-be65-a514934249dc 58327 3 2023-06-28 08:02:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment d4abfe49-fb74-4682-bced-eb1a1135bb1c 0xc0031204c7 0xc0031204c8}] [] [{kube-controller-manager Update apps/v1 2023-06-28 08:03:01 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-06-28 08:03:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d4abfe49-fb74-4682-bced-eb1a1135bb1c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003120558 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Jun 28 08:03:03.341: INFO: Pod "webserver-deployment-69b7448995-5j8vl" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-5j8vl webserver-deployment-69b7448995- deployment-2432  8e33c108-701c-4ebc-b78a-6312f1c0dba1 58079 0 2023-06-28 08:03:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:27847c7279b840f1b5105d82670171214933f6c352597e9d61f9b69b7bdf6cc9 cni.projectcalico.org/podIP:172.21.122.5/32 cni.projectcalico.org/podIPs:172.21.122.5/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 856ab508-beb4-4308-9f76-62868b6545b9 0xc0030c4b37 0xc0030c4b38}] [] [{kube-controller-manager Update v1 2023-06-28 08:03:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"856ab508-beb4-4308-9f76-62868b6545b9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-28 08:03:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-06-28 08:03:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qj9wd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qj9wd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-rhel-749f7d55c8xdd8b6-ct4cp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:03:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:03:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:03:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:03:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.11.3,PodIP:,StartTime:2023-06-28 08:03:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 28 08:03:03.341: INFO: Pod "webserver-deployment-69b7448995-bldk8" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-bldk8 webserver-deployment-69b7448995- deployment-2432  d8c00f4f-0666-43f6-bf8d-ec507b19ccd0 58068 0 2023-06-28 08:03:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:2e935492f80fdc07e065357120469a66bfa8f0466946230afd6e4c43567c6cc7 cni.projectcalico.org/podIP:172.21.30.112/32 cni.projectcalico.org/podIPs:172.21.30.112/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 856ab508-beb4-4308-9f76-62868b6545b9 0xc0030c4d57 0xc0030c4d58}] [] [{calico Update v1 2023-06-28 08:03:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-06-28 08:03:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"856ab508-beb4-4308-9f76-62868b6545b9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-28 08:03:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-j5grc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-j5grc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-rhel-749f7d55c8xdd8b6-zxlfv,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:03:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:03:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:03:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:03:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.11.5,PodIP:,StartTime:2023-06-28 08:03:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 28 08:03:03.341: INFO: Pod "webserver-deployment-69b7448995-cgj8q" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-cgj8q webserver-deployment-69b7448995- deployment-2432  c78ae1b5-6e9d-4b0e-91c4-8701d0794b44 58378 0 2023-06-28 08:03:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 856ab508-beb4-4308-9f76-62868b6545b9 0xc0030c4f47 0xc0030c4f48}] [] [{kube-controller-manager Update v1 2023-06-28 08:03:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"856ab508-beb4-4308-9f76-62868b6545b9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-j8kdz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-j8kdz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-rhel-749f7d55c8xdd8b6-srshq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:03:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 28 08:03:03.341: INFO: Pod "webserver-deployment-69b7448995-crggf" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-crggf webserver-deployment-69b7448995- deployment-2432  3e871ea4-b55e-4562-ba8b-a9d35fa23870 58392 0 2023-06-28 08:03:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 856ab508-beb4-4308-9f76-62868b6545b9 0xc0030c50b0 0xc0030c50b1}] [] [{kube-controller-manager Update v1 2023-06-28 08:03:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"856ab508-beb4-4308-9f76-62868b6545b9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-q9dsd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-q9dsd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-rhel-749f7d55c8xdd8b6-zxlfv,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:03:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 28 08:03:03.341: INFO: Pod "webserver-deployment-69b7448995-f6gsf" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-f6gsf webserver-deployment-69b7448995- deployment-2432  5d2334a5-76fc-49e8-a84f-0317cd33c4fe 58376 0 2023-06-28 08:03:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 856ab508-beb4-4308-9f76-62868b6545b9 0xc0030c5210 0xc0030c5211}] [] [{kube-controller-manager Update v1 2023-06-28 08:03:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"856ab508-beb4-4308-9f76-62868b6545b9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-x6dvz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-x6dvz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-rhel-749f7d55c8xdd8b6-ct4cp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:03:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 28 08:03:03.341: INFO: Pod "webserver-deployment-69b7448995-hw779" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-hw779 webserver-deployment-69b7448995- deployment-2432  5d85ed25-247e-4424-94a4-03e65839571a 58063 0 2023-06-28 08:03:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:d11edf7dfb550a14e0f1b186427762774a19384ced532c2b4573adc57c18364c cni.projectcalico.org/podIP:172.21.122.107/32 cni.projectcalico.org/podIPs:172.21.122.107/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 856ab508-beb4-4308-9f76-62868b6545b9 0xc0030c5370 0xc0030c5371}] [] [{calico Update v1 2023-06-28 08:03:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-06-28 08:03:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"856ab508-beb4-4308-9f76-62868b6545b9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-28 08:03:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8rptm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8rptm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-rhel-749f7d55c8xdd8b6-srshq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:03:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:03:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:03:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:03:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.11.4,PodIP:,StartTime:2023-06-28 08:03:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 28 08:03:03.342: INFO: Pod "webserver-deployment-69b7448995-jcf98" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-jcf98 webserver-deployment-69b7448995- deployment-2432  c0c1d95f-569a-418c-888e-d466cb2c32ec 58382 0 2023-06-28 08:03:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 856ab508-beb4-4308-9f76-62868b6545b9 0xc0030c5567 0xc0030c5568}] [] [{kube-controller-manager Update v1 2023-06-28 08:03:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"856ab508-beb4-4308-9f76-62868b6545b9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-68sd7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-68sd7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-rhel-749f7d55c8xdd8b6-srshq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:03:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 28 08:03:03.342: INFO: Pod "webserver-deployment-69b7448995-kksn2" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-kksn2 webserver-deployment-69b7448995- deployment-2432  589e9cd9-d827-4bdf-b0ff-7347b6e9587a 58359 0 2023-06-28 08:03:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 856ab508-beb4-4308-9f76-62868b6545b9 0xc0030c56d0 0xc0030c56d1}] [] [{kube-controller-manager Update v1 2023-06-28 08:03:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"856ab508-beb4-4308-9f76-62868b6545b9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tgr5r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tgr5r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-rhel-749f7d55c8xdd8b6-zxlfv,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:03:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 28 08:03:03.342: INFO: Pod "webserver-deployment-69b7448995-ksrdq" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-ksrdq webserver-deployment-69b7448995- deployment-2432  b837eca4-9edb-4c14-80f5-96561664e70d 58377 0 2023-06-28 08:03:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 856ab508-beb4-4308-9f76-62868b6545b9 0xc0030c5830 0xc0030c5831}] [] [{kube-controller-manager Update v1 2023-06-28 08:03:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"856ab508-beb4-4308-9f76-62868b6545b9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7c99p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7c99p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-rhel-749f7d55c8xdd8b6-zxlfv,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:03:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 28 08:03:03.342: INFO: Pod "webserver-deployment-69b7448995-lccz2" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-lccz2 webserver-deployment-69b7448995- deployment-2432  4402a461-f01b-4057-a214-66f70053fdfa 58073 0 2023-06-28 08:03:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:7233ea770f633254973d39a5aff8bc1553c5680e4429130844a884191faac513 cni.projectcalico.org/podIP:172.21.30.78/32 cni.projectcalico.org/podIPs:172.21.30.78/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 856ab508-beb4-4308-9f76-62868b6545b9 0xc0030c59b0 0xc0030c59b1}] [] [{calico Update v1 2023-06-28 08:03:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-06-28 08:03:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"856ab508-beb4-4308-9f76-62868b6545b9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-28 08:03:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6zjbp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6zjbp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-rhel-749f7d55c8xdd8b6-zxlfv,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:03:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:03:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:03:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:03:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.11.5,PodIP:,StartTime:2023-06-28 08:03:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 28 08:03:03.342: INFO: Pod "webserver-deployment-69b7448995-q8sj5" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-q8sj5 webserver-deployment-69b7448995- deployment-2432  7e083577-36c3-4bfb-b118-8959926b6355 58356 0 2023-06-28 08:03:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 856ab508-beb4-4308-9f76-62868b6545b9 0xc0030c5bc7 0xc0030c5bc8}] [] [{kube-controller-manager Update v1 2023-06-28 08:03:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"856ab508-beb4-4308-9f76-62868b6545b9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5k67g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5k67g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-rhel-749f7d55c8xdd8b6-ct4cp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:03:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 28 08:03:03.342: INFO: Pod "webserver-deployment-69b7448995-qmxtc" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-qmxtc webserver-deployment-69b7448995- deployment-2432  7eecf179-d474-4049-a4e0-42fc06f1241c 58075 0 2023-06-28 08:03:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:f92d4f41d9e2485104dc14982806287afe55f43821a9ebb058431a702d54716d cni.projectcalico.org/podIP:172.21.122.4/32 cni.projectcalico.org/podIPs:172.21.122.4/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 856ab508-beb4-4308-9f76-62868b6545b9 0xc0030c5d60 0xc0030c5d61}] [] [{calico Update v1 2023-06-28 08:03:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-06-28 08:03:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"856ab508-beb4-4308-9f76-62868b6545b9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-28 08:03:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fhf5l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fhf5l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-rhel-749f7d55c8xdd8b6-ct4cp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:03:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:03:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:03:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:03:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.11.3,PodIP:,StartTime:2023-06-28 08:03:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 28 08:03:03.342: INFO: Pod "webserver-deployment-69b7448995-sqcvw" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-sqcvw webserver-deployment-69b7448995- deployment-2432  20e8e535-e6db-4caa-9ab6-39a8ab508630 58387 0 2023-06-28 08:03:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 856ab508-beb4-4308-9f76-62868b6545b9 0xc0030c5f87 0xc0030c5f88}] [] [{kube-controller-manager Update v1 2023-06-28 08:03:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"856ab508-beb4-4308-9f76-62868b6545b9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-28 08:03:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pd7d8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pd7d8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-rhel-749f7d55c8xdd8b6-srshq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:03:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:03:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:03:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:03:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.11.4,PodIP:,StartTime:2023-06-28 08:03:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 28 08:03:03.343: INFO: Pod "webserver-deployment-845c8977d9-2hsq5" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-2hsq5 webserver-deployment-845c8977d9- deployment-2432  94a39242-24dc-48ec-a028-b4594a7e3949 57985 0 2023-06-28 08:02:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:3a137a9496061c8f304fc70256748341a1f33332cf20c64ce20bf3a70f9ffba8 cni.projectcalico.org/podIP:172.21.122.104/32 cni.projectcalico.org/podIPs:172.21.122.104/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 79eaf046-3c6c-452e-be65-a514934249dc 0xc002e18167 0xc002e18168}] [] [{calico Update v1 2023-06-28 08:02:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-06-28 08:02:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"79eaf046-3c6c-452e-be65-a514934249dc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-28 08:02:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.122.104\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-246b5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-246b5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-rhel-749f7d55c8xdd8b6-srshq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:02:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:02:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:02:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:02:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.11.4,PodIP:172.21.122.104,StartTime:2023-06-28 08:02:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-28 08:02:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://83b239a726d72d4d67211d9a2c2b8a7646ddbe397a2b2ad9d10eaf8a9c99aeb2,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.21.122.104,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 28 08:03:03.343: INFO: Pod "webserver-deployment-845c8977d9-49n92" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-49n92 webserver-deployment-845c8977d9- deployment-2432  f0eea0c1-b25b-4aba-b075-466325174d1d 58354 0 2023-06-28 08:03:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 79eaf046-3c6c-452e-be65-a514934249dc 0xc002e18357 0xc002e18358}] [] [{kube-controller-manager Update v1 2023-06-28 08:03:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"79eaf046-3c6c-452e-be65-a514934249dc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vrcxj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vrcxj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-rhel-749f7d55c8xdd8b6-srshq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:03:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 28 08:03:03.343: INFO: Pod "webserver-deployment-845c8977d9-6k6w7" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-6k6w7 webserver-deployment-845c8977d9- deployment-2432  3941229d-649a-49a4-9c1e-18b6a7b6177e 57982 0 2023-06-28 08:02:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:cccecc22243f15963b11f37f62933cfce924e8a772ff78318be8974cec0a5538 cni.projectcalico.org/podIP:172.21.122.108/32 cni.projectcalico.org/podIPs:172.21.122.108/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 79eaf046-3c6c-452e-be65-a514934249dc 0xc002e184b0 0xc002e184b1}] [] [{calico Update v1 2023-06-28 08:02:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-06-28 08:02:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"79eaf046-3c6c-452e-be65-a514934249dc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-28 08:02:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.122.108\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fgdmk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fgdmk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-rhel-749f7d55c8xdd8b6-srshq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:02:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:02:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:02:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:02:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.11.4,PodIP:172.21.122.108,StartTime:2023-06-28 08:02:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-28 08:02:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://649b6c47e198e19f4052380c1c87cfffe5cb33e5f173e701425d361d270e79bf,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.21.122.108,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 28 08:03:03.343: INFO: Pod "webserver-deployment-845c8977d9-8ppnz" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-8ppnz webserver-deployment-845c8977d9- deployment-2432  3aa0ceae-2801-4106-aea7-9c412f16efc6 57975 0 2023-06-28 08:02:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:bd991fc0f9e61a5859cd54e39dc1c8c3b9e3f6757bb753962772f6a478ba42e9 cni.projectcalico.org/podIP:172.21.30.100/32 cni.projectcalico.org/podIPs:172.21.30.100/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 79eaf046-3c6c-452e-be65-a514934249dc 0xc002e186c7 0xc002e186c8}] [] [{calico Update v1 2023-06-28 08:02:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-06-28 08:02:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"79eaf046-3c6c-452e-be65-a514934249dc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-28 08:02:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.30.100\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qljfv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qljfv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-rhel-749f7d55c8xdd8b6-zxlfv,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:02:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:02:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:02:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:02:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.11.5,PodIP:172.21.30.100,StartTime:2023-06-28 08:02:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-28 08:02:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://0917e9f74d5a509f4a4e65df5c1c28df1d7546d3dbc3572703ed73c556bb4064,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.21.30.100,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 28 08:03:03.343: INFO: Pod "webserver-deployment-845c8977d9-9tqk5" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-9tqk5 webserver-deployment-845c8977d9- deployment-2432  ab856911-deb2-49f4-b2bd-b4f8c2d99fcc 58375 0 2023-06-28 08:03:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 79eaf046-3c6c-452e-be65-a514934249dc 0xc002e188b7 0xc002e188b8}] [] [{kube-controller-manager Update v1 2023-06-28 08:03:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"79eaf046-3c6c-452e-be65-a514934249dc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cvqxt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cvqxt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-rhel-749f7d55c8xdd8b6-srshq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:03:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 28 08:03:03.343: INFO: Pod "webserver-deployment-845c8977d9-bkwkx" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-bkwkx webserver-deployment-845c8977d9- deployment-2432  4f511aae-ec2b-4091-8bc7-9784d1e6b2e9 58355 0 2023-06-28 08:03:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 79eaf046-3c6c-452e-be65-a514934249dc 0xc002e18a10 0xc002e18a11}] [] [{kube-controller-manager Update v1 2023-06-28 08:03:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"79eaf046-3c6c-452e-be65-a514934249dc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vfcx2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vfcx2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-rhel-749f7d55c8xdd8b6-ct4cp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:03:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 28 08:03:03.343: INFO: Pod "webserver-deployment-845c8977d9-d4crr" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-d4crr webserver-deployment-845c8977d9- deployment-2432  a432f6b7-69d5-478f-a5bf-0fff7fd1eb16 58391 0 2023-06-28 08:03:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 79eaf046-3c6c-452e-be65-a514934249dc 0xc002e18b70 0xc002e18b71}] [] [{kube-controller-manager Update v1 2023-06-28 08:03:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"79eaf046-3c6c-452e-be65-a514934249dc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-28 08:03:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cjq55,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cjq55,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-rhel-749f7d55c8xdd8b6-ct4cp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:03:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:03:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:03:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:03:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.11.3,PodIP:,StartTime:2023-06-28 08:03:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 28 08:03:03.344: INFO: Pod "webserver-deployment-845c8977d9-f2xwf" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-f2xwf webserver-deployment-845c8977d9- deployment-2432  d27f3afb-f133-4eec-a276-ce0a8d6a4a0c 57963 0 2023-06-28 08:02:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:fb0d6402cff8a4960a1e4f685b87331acceb258324a5588d36fc3d3b99d54c6b cni.projectcalico.org/podIP:172.21.122.48/32 cni.projectcalico.org/podIPs:172.21.122.48/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 79eaf046-3c6c-452e-be65-a514934249dc 0xc002e18d47 0xc002e18d48}] [] [{calico Update v1 2023-06-28 08:02:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-06-28 08:02:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"79eaf046-3c6c-452e-be65-a514934249dc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-28 08:02:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.122.48\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cwnkw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cwnkw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-rhel-749f7d55c8xdd8b6-ct4cp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:02:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:02:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:02:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:02:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.11.3,PodIP:172.21.122.48,StartTime:2023-06-28 08:02:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-28 08:02:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://882f6f7cbe15f9b7fa4629a9246653ad0028246d210ea8e455930dd81e9d1094,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.21.122.48,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 28 08:03:03.344: INFO: Pod "webserver-deployment-845c8977d9-g7cv2" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-g7cv2 webserver-deployment-845c8977d9- deployment-2432  11569d1f-8275-45bf-97c2-fc8eaa8d68cb 57968 0 2023-06-28 08:02:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:6e56560c91f0f7c13c5ad54884e970a2a7aa1fed60fcd4effdade65a3af15bb5 cni.projectcalico.org/podIP:172.21.30.86/32 cni.projectcalico.org/podIPs:172.21.30.86/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 79eaf046-3c6c-452e-be65-a514934249dc 0xc002e18f57 0xc002e18f58}] [] [{calico Update v1 2023-06-28 08:02:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-06-28 08:02:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"79eaf046-3c6c-452e-be65-a514934249dc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-28 08:02:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.30.86\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2bklg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2bklg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-rhel-749f7d55c8xdd8b6-zxlfv,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:02:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:02:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:02:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:02:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.11.5,PodIP:172.21.30.86,StartTime:2023-06-28 08:02:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-28 08:02:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://fe70b73ab54b265e80473b944ef5b86a08de4eb7c0b5e76edeca5008b4e6eed7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.21.30.86,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 28 08:03:03.346: INFO: Pod "webserver-deployment-845c8977d9-gqbkr" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-gqbkr webserver-deployment-845c8977d9- deployment-2432  2acad839-1522-4caa-8cb2-e44f8cc3dc7a 57972 0 2023-06-28 08:02:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:48b047782b411422b313ce2469ad6fd330a1ae3d09c97067c8b149a4b1138544 cni.projectcalico.org/podIP:172.21.30.82/32 cni.projectcalico.org/podIPs:172.21.30.82/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 79eaf046-3c6c-452e-be65-a514934249dc 0xc002e19167 0xc002e19168}] [] [{calico Update v1 2023-06-28 08:02:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-06-28 08:02:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"79eaf046-3c6c-452e-be65-a514934249dc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-28 08:02:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.30.82\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fq68b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fq68b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-rhel-749f7d55c8xdd8b6-zxlfv,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:02:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:02:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:02:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:02:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.11.5,PodIP:172.21.30.82,StartTime:2023-06-28 08:02:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-28 08:02:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://c333089f29c4acbaab7667c6a853288518671e484cd6d3d0ab1c9dd0ca06e133,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.21.30.82,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 28 08:03:03.346: INFO: Pod "webserver-deployment-845c8977d9-jxpgc" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-jxpgc webserver-deployment-845c8977d9- deployment-2432  35e3bfdf-9891-4509-9cfe-3b85d32f40b0 58381 0 2023-06-28 08:03:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 79eaf046-3c6c-452e-be65-a514934249dc 0xc002e19357 0xc002e19358}] [] [{kube-controller-manager Update v1 2023-06-28 08:03:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"79eaf046-3c6c-452e-be65-a514934249dc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mkllm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mkllm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-rhel-749f7d55c8xdd8b6-ct4cp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:03:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 28 08:03:03.346: INFO: Pod "webserver-deployment-845c8977d9-knxq6" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-knxq6 webserver-deployment-845c8977d9- deployment-2432  835fdc7e-e40d-451a-bc7d-f9610b0f6bfb 57979 0 2023-06-28 08:02:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:2282cc0bccd51c38a4531410dfe31a3a6f6dd5b6b57113358632dbfcdb98e519 cni.projectcalico.org/podIP:172.21.122.2/32 cni.projectcalico.org/podIPs:172.21.122.2/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 79eaf046-3c6c-452e-be65-a514934249dc 0xc002e194e0 0xc002e194e1}] [] [{calico Update v1 2023-06-28 08:02:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-06-28 08:02:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"79eaf046-3c6c-452e-be65-a514934249dc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-28 08:02:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.122.2\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-th4km,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-th4km,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-rhel-749f7d55c8xdd8b6-ct4cp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:02:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:02:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:02:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:02:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.11.3,PodIP:172.21.122.2,StartTime:2023-06-28 08:02:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-28 08:02:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://befe4e06e4ec267f471eb667108b5d232b7bbdb7117bf5a7980fa5aa0f3194c3,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.21.122.2,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 28 08:03:03.346: INFO: Pod "webserver-deployment-845c8977d9-kzgwx" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-kzgwx webserver-deployment-845c8977d9- deployment-2432  ef5c0d82-59ba-4922-b665-29b761717ce9 58388 0 2023-06-28 08:03:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 79eaf046-3c6c-452e-be65-a514934249dc 0xc002e196d7 0xc002e196d8}] [] [{kube-controller-manager Update v1 2023-06-28 08:03:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"79eaf046-3c6c-452e-be65-a514934249dc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-28 08:03:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g2gv4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g2gv4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-rhel-749f7d55c8xdd8b6-zxlfv,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:03:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:03:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:03:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:03:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.11.5,PodIP:,StartTime:2023-06-28 08:03:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 28 08:03:03.346: INFO: Pod "webserver-deployment-845c8977d9-l7g6n" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-l7g6n webserver-deployment-845c8977d9- deployment-2432  95532018-8757-41ce-82f3-3a227a63f46a 58360 0 2023-06-28 08:03:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 79eaf046-3c6c-452e-be65-a514934249dc 0xc002e19897 0xc002e19898}] [] [{kube-controller-manager Update v1 2023-06-28 08:03:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"79eaf046-3c6c-452e-be65-a514934249dc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-66w5l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-66w5l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-rhel-749f7d55c8xdd8b6-zxlfv,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:03:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 28 08:03:03.347: INFO: Pod "webserver-deployment-845c8977d9-mdh5f" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-mdh5f webserver-deployment-845c8977d9- deployment-2432  a45be03f-2bc6-4005-a93d-385ce1699d5a 57988 0 2023-06-28 08:02:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:3af53969724c52b41d9c79712a26b37c97b25629776613a682265d0c5f84fc8b cni.projectcalico.org/podIP:172.21.122.109/32 cni.projectcalico.org/podIPs:172.21.122.109/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 79eaf046-3c6c-452e-be65-a514934249dc 0xc002e199f0 0xc002e199f1}] [] [{calico Update v1 2023-06-28 08:02:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-06-28 08:02:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"79eaf046-3c6c-452e-be65-a514934249dc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-28 08:02:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.122.109\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-chlhn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-chlhn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-rhel-749f7d55c8xdd8b6-srshq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:02:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:02:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:02:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:02:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.11.4,PodIP:172.21.122.109,StartTime:2023-06-28 08:02:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-28 08:02:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://af92fafad4e7e1842687a263e64c348be7b6ca4acfe0e4e7ce3b07c4e2f5bfa2,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.21.122.109,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 28 08:03:03.347: INFO: Pod "webserver-deployment-845c8977d9-qmlrg" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-qmlrg webserver-deployment-845c8977d9- deployment-2432  7740a89d-07a7-47e0-9b3a-b063ea57ac98 58357 0 2023-06-28 08:03:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 79eaf046-3c6c-452e-be65-a514934249dc 0xc002e19be7 0xc002e19be8}] [] [{kube-controller-manager Update v1 2023-06-28 08:03:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"79eaf046-3c6c-452e-be65-a514934249dc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-28 08:03:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-s6p62,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-s6p62,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-rhel-749f7d55c8xdd8b6-ct4cp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:03:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:03:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:03:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:03:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.11.3,PodIP:,StartTime:2023-06-28 08:03:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 28 08:03:03.347: INFO: Pod "webserver-deployment-845c8977d9-rscwp" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-rscwp webserver-deployment-845c8977d9- deployment-2432  6a79cd59-443e-41e8-9dbc-45c3f1d8d5e3 58380 0 2023-06-28 08:03:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 79eaf046-3c6c-452e-be65-a514934249dc 0xc002e19da7 0xc002e19da8}] [] [{kube-controller-manager Update v1 2023-06-28 08:03:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"79eaf046-3c6c-452e-be65-a514934249dc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7d4j7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7d4j7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-rhel-749f7d55c8xdd8b6-srshq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:03:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 28 08:03:03.347: INFO: Pod "webserver-deployment-845c8977d9-tnd56" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-tnd56 webserver-deployment-845c8977d9- deployment-2432  1f6b97f2-a03d-4d77-ba56-9427867c216b 58370 0 2023-06-28 08:03:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 79eaf046-3c6c-452e-be65-a514934249dc 0xc002e19f00 0xc002e19f01}] [] [{kube-controller-manager Update v1 2023-06-28 08:03:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"79eaf046-3c6c-452e-be65-a514934249dc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wqkwp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wqkwp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-rhel-749f7d55c8xdd8b6-ct4cp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:03:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 28 08:03:03.347: INFO: Pod "webserver-deployment-845c8977d9-w2v6c" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-w2v6c webserver-deployment-845c8977d9- deployment-2432  01c05350-7c3e-4491-99a5-cd1f7d3fa8a7 58385 0 2023-06-28 08:03:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 79eaf046-3c6c-452e-be65-a514934249dc 0xc003ae6050 0xc003ae6051}] [] [{kube-controller-manager Update v1 2023-06-28 08:03:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"79eaf046-3c6c-452e-be65-a514934249dc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-knsm4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-knsm4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-rhel-749f7d55c8xdd8b6-zxlfv,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:03:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 28 08:03:03.347: INFO: Pod "webserver-deployment-845c8977d9-xvld8" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-xvld8 webserver-deployment-845c8977d9- deployment-2432  83aaae8d-15f9-4eed-9f07-f7e62feb60c0 58379 0 2023-06-28 08:03:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 79eaf046-3c6c-452e-be65-a514934249dc 0xc003ae61a0 0xc003ae61a1}] [] [{kube-controller-manager Update v1 2023-06-28 08:03:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"79eaf046-3c6c-452e-be65-a514934249dc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cg99q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cg99q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-rhel-749f7d55c8xdd8b6-zxlfv,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:03:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jun 28 08:03:03.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2432" for this suite. 06/28/23 08:03:03.358
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","completed":84,"skipped":1472,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.245 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:02:57.12
    Jun 28 08:02:57.121: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename deployment 06/28/23 08:02:57.122
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:02:57.134
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:02:57.139
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support proportional scaling [Conformance]
      test/e2e/apps/deployment.go:160
    Jun 28 08:02:57.143: INFO: Creating deployment "webserver-deployment"
    Jun 28 08:02:57.150: INFO: Waiting for observed generation 1
    Jun 28 08:02:59.162: INFO: Waiting for all required pods to come up
    Jun 28 08:02:59.171: INFO: Pod name httpd: Found 10 pods out of 10
    STEP: ensuring each pod is running 06/28/23 08:02:59.171
    Jun 28 08:02:59.171: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-rtjj2" in namespace "deployment-2432" to be "running"
    Jun 28 08:02:59.172: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-57jbw" in namespace "deployment-2432" to be "running"
    Jun 28 08:02:59.176: INFO: Pod "webserver-deployment-845c8977d9-57jbw": Phase="Pending", Reason="", readiness=false. Elapsed: 4.695235ms
    Jun 28 08:02:59.176: INFO: Pod "webserver-deployment-845c8977d9-rtjj2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.881065ms
    Jun 28 08:03:01.183: INFO: Pod "webserver-deployment-845c8977d9-rtjj2": Phase="Running", Reason="", readiness=true. Elapsed: 2.011619261s
    Jun 28 08:03:01.183: INFO: Pod "webserver-deployment-845c8977d9-rtjj2" satisfied condition "running"
    Jun 28 08:03:01.183: INFO: Pod "webserver-deployment-845c8977d9-57jbw": Phase="Running", Reason="", readiness=true. Elapsed: 2.011470678s
    Jun 28 08:03:01.183: INFO: Pod "webserver-deployment-845c8977d9-57jbw" satisfied condition "running"
    Jun 28 08:03:01.183: INFO: Waiting for deployment "webserver-deployment" to complete
    Jun 28 08:03:01.193: INFO: Updating deployment "webserver-deployment" with a non-existent image
    Jun 28 08:03:01.208: INFO: Updating deployment webserver-deployment
    Jun 28 08:03:01.208: INFO: Waiting for observed generation 2
    Jun 28 08:03:03.220: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
    Jun 28 08:03:03.224: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
    Jun 28 08:03:03.229: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Jun 28 08:03:03.244: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
    Jun 28 08:03:03.244: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
    Jun 28 08:03:03.249: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Jun 28 08:03:03.259: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
    Jun 28 08:03:03.259: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
    Jun 28 08:03:03.272: INFO: Updating deployment webserver-deployment
    Jun 28 08:03:03.272: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
    Jun 28 08:03:03.283: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
    Jun 28 08:03:03.290: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jun 28 08:03:03.307: INFO: Deployment "webserver-deployment":
    &Deployment{ObjectMeta:{webserver-deployment  deployment-2432  d4abfe49-fb74-4682-bced-eb1a1135bb1c 58351 3 2023-06-28 08:02:57 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-06-28 08:03:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-28 08:03:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0030c4718 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-69b7448995" is progressing.,LastUpdateTime:2023-06-28 08:03:01 +0000 UTC,LastTransitionTime:2023-06-28 08:02:57 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-06-28 08:03:03 +0000 UTC,LastTransitionTime:2023-06-28 08:03:03 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

    Jun 28 08:03:03.321: INFO: New ReplicaSet "webserver-deployment-69b7448995" of Deployment "webserver-deployment":
    &ReplicaSet{ObjectMeta:{webserver-deployment-69b7448995  deployment-2432  856ab508-beb4-4308-9f76-62868b6545b9 58331 3 2023-06-28 08:03:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment d4abfe49-fb74-4682-bced-eb1a1135bb1c 0xc0031203c7 0xc0031203c8}] [] [{kube-controller-manager Update apps/v1 2023-06-28 08:03:01 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-06-28 08:03:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d4abfe49-fb74-4682-bced-eb1a1135bb1c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 69b7448995,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003120468 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jun 28 08:03:03.321: INFO: All old ReplicaSets of Deployment "webserver-deployment":
    Jun 28 08:03:03.321: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-845c8977d9  deployment-2432  79eaf046-3c6c-452e-be65-a514934249dc 58327 3 2023-06-28 08:02:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment d4abfe49-fb74-4682-bced-eb1a1135bb1c 0xc0031204c7 0xc0031204c8}] [] [{kube-controller-manager Update apps/v1 2023-06-28 08:03:01 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-06-28 08:03:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d4abfe49-fb74-4682-bced-eb1a1135bb1c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003120558 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
    Jun 28 08:03:03.341: INFO: Pod "webserver-deployment-69b7448995-5j8vl" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-5j8vl webserver-deployment-69b7448995- deployment-2432  8e33c108-701c-4ebc-b78a-6312f1c0dba1 58079 0 2023-06-28 08:03:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:27847c7279b840f1b5105d82670171214933f6c352597e9d61f9b69b7bdf6cc9 cni.projectcalico.org/podIP:172.21.122.5/32 cni.projectcalico.org/podIPs:172.21.122.5/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 856ab508-beb4-4308-9f76-62868b6545b9 0xc0030c4b37 0xc0030c4b38}] [] [{kube-controller-manager Update v1 2023-06-28 08:03:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"856ab508-beb4-4308-9f76-62868b6545b9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-28 08:03:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-06-28 08:03:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qj9wd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qj9wd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-rhel-749f7d55c8xdd8b6-ct4cp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:03:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:03:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:03:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:03:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.11.3,PodIP:,StartTime:2023-06-28 08:03:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 28 08:03:03.341: INFO: Pod "webserver-deployment-69b7448995-bldk8" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-bldk8 webserver-deployment-69b7448995- deployment-2432  d8c00f4f-0666-43f6-bf8d-ec507b19ccd0 58068 0 2023-06-28 08:03:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:2e935492f80fdc07e065357120469a66bfa8f0466946230afd6e4c43567c6cc7 cni.projectcalico.org/podIP:172.21.30.112/32 cni.projectcalico.org/podIPs:172.21.30.112/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 856ab508-beb4-4308-9f76-62868b6545b9 0xc0030c4d57 0xc0030c4d58}] [] [{calico Update v1 2023-06-28 08:03:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-06-28 08:03:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"856ab508-beb4-4308-9f76-62868b6545b9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-28 08:03:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-j5grc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-j5grc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-rhel-749f7d55c8xdd8b6-zxlfv,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:03:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:03:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:03:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:03:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.11.5,PodIP:,StartTime:2023-06-28 08:03:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 28 08:03:03.341: INFO: Pod "webserver-deployment-69b7448995-cgj8q" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-cgj8q webserver-deployment-69b7448995- deployment-2432  c78ae1b5-6e9d-4b0e-91c4-8701d0794b44 58378 0 2023-06-28 08:03:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 856ab508-beb4-4308-9f76-62868b6545b9 0xc0030c4f47 0xc0030c4f48}] [] [{kube-controller-manager Update v1 2023-06-28 08:03:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"856ab508-beb4-4308-9f76-62868b6545b9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-j8kdz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-j8kdz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-rhel-749f7d55c8xdd8b6-srshq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:03:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 28 08:03:03.341: INFO: Pod "webserver-deployment-69b7448995-crggf" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-crggf webserver-deployment-69b7448995- deployment-2432  3e871ea4-b55e-4562-ba8b-a9d35fa23870 58392 0 2023-06-28 08:03:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 856ab508-beb4-4308-9f76-62868b6545b9 0xc0030c50b0 0xc0030c50b1}] [] [{kube-controller-manager Update v1 2023-06-28 08:03:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"856ab508-beb4-4308-9f76-62868b6545b9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-q9dsd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-q9dsd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-rhel-749f7d55c8xdd8b6-zxlfv,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:03:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 28 08:03:03.341: INFO: Pod "webserver-deployment-69b7448995-f6gsf" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-f6gsf webserver-deployment-69b7448995- deployment-2432  5d2334a5-76fc-49e8-a84f-0317cd33c4fe 58376 0 2023-06-28 08:03:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 856ab508-beb4-4308-9f76-62868b6545b9 0xc0030c5210 0xc0030c5211}] [] [{kube-controller-manager Update v1 2023-06-28 08:03:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"856ab508-beb4-4308-9f76-62868b6545b9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-x6dvz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-x6dvz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-rhel-749f7d55c8xdd8b6-ct4cp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:03:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 28 08:03:03.341: INFO: Pod "webserver-deployment-69b7448995-hw779" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-hw779 webserver-deployment-69b7448995- deployment-2432  5d85ed25-247e-4424-94a4-03e65839571a 58063 0 2023-06-28 08:03:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:d11edf7dfb550a14e0f1b186427762774a19384ced532c2b4573adc57c18364c cni.projectcalico.org/podIP:172.21.122.107/32 cni.projectcalico.org/podIPs:172.21.122.107/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 856ab508-beb4-4308-9f76-62868b6545b9 0xc0030c5370 0xc0030c5371}] [] [{calico Update v1 2023-06-28 08:03:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-06-28 08:03:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"856ab508-beb4-4308-9f76-62868b6545b9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-28 08:03:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8rptm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8rptm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-rhel-749f7d55c8xdd8b6-srshq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:03:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:03:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:03:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:03:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.11.4,PodIP:,StartTime:2023-06-28 08:03:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 28 08:03:03.342: INFO: Pod "webserver-deployment-69b7448995-jcf98" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-jcf98 webserver-deployment-69b7448995- deployment-2432  c0c1d95f-569a-418c-888e-d466cb2c32ec 58382 0 2023-06-28 08:03:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 856ab508-beb4-4308-9f76-62868b6545b9 0xc0030c5567 0xc0030c5568}] [] [{kube-controller-manager Update v1 2023-06-28 08:03:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"856ab508-beb4-4308-9f76-62868b6545b9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-68sd7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-68sd7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-rhel-749f7d55c8xdd8b6-srshq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:03:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 28 08:03:03.342: INFO: Pod "webserver-deployment-69b7448995-kksn2" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-kksn2 webserver-deployment-69b7448995- deployment-2432  589e9cd9-d827-4bdf-b0ff-7347b6e9587a 58359 0 2023-06-28 08:03:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 856ab508-beb4-4308-9f76-62868b6545b9 0xc0030c56d0 0xc0030c56d1}] [] [{kube-controller-manager Update v1 2023-06-28 08:03:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"856ab508-beb4-4308-9f76-62868b6545b9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tgr5r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tgr5r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-rhel-749f7d55c8xdd8b6-zxlfv,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:03:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 28 08:03:03.342: INFO: Pod "webserver-deployment-69b7448995-ksrdq" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-ksrdq webserver-deployment-69b7448995- deployment-2432  b837eca4-9edb-4c14-80f5-96561664e70d 58377 0 2023-06-28 08:03:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 856ab508-beb4-4308-9f76-62868b6545b9 0xc0030c5830 0xc0030c5831}] [] [{kube-controller-manager Update v1 2023-06-28 08:03:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"856ab508-beb4-4308-9f76-62868b6545b9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7c99p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7c99p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-rhel-749f7d55c8xdd8b6-zxlfv,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:03:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 28 08:03:03.342: INFO: Pod "webserver-deployment-69b7448995-lccz2" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-lccz2 webserver-deployment-69b7448995- deployment-2432  4402a461-f01b-4057-a214-66f70053fdfa 58073 0 2023-06-28 08:03:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:7233ea770f633254973d39a5aff8bc1553c5680e4429130844a884191faac513 cni.projectcalico.org/podIP:172.21.30.78/32 cni.projectcalico.org/podIPs:172.21.30.78/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 856ab508-beb4-4308-9f76-62868b6545b9 0xc0030c59b0 0xc0030c59b1}] [] [{calico Update v1 2023-06-28 08:03:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-06-28 08:03:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"856ab508-beb4-4308-9f76-62868b6545b9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-28 08:03:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6zjbp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6zjbp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-rhel-749f7d55c8xdd8b6-zxlfv,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:03:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:03:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:03:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:03:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.11.5,PodIP:,StartTime:2023-06-28 08:03:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 28 08:03:03.342: INFO: Pod "webserver-deployment-69b7448995-q8sj5" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-q8sj5 webserver-deployment-69b7448995- deployment-2432  7e083577-36c3-4bfb-b118-8959926b6355 58356 0 2023-06-28 08:03:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 856ab508-beb4-4308-9f76-62868b6545b9 0xc0030c5bc7 0xc0030c5bc8}] [] [{kube-controller-manager Update v1 2023-06-28 08:03:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"856ab508-beb4-4308-9f76-62868b6545b9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5k67g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5k67g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-rhel-749f7d55c8xdd8b6-ct4cp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:03:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 28 08:03:03.342: INFO: Pod "webserver-deployment-69b7448995-qmxtc" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-qmxtc webserver-deployment-69b7448995- deployment-2432  7eecf179-d474-4049-a4e0-42fc06f1241c 58075 0 2023-06-28 08:03:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:f92d4f41d9e2485104dc14982806287afe55f43821a9ebb058431a702d54716d cni.projectcalico.org/podIP:172.21.122.4/32 cni.projectcalico.org/podIPs:172.21.122.4/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 856ab508-beb4-4308-9f76-62868b6545b9 0xc0030c5d60 0xc0030c5d61}] [] [{calico Update v1 2023-06-28 08:03:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-06-28 08:03:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"856ab508-beb4-4308-9f76-62868b6545b9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-28 08:03:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fhf5l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fhf5l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-rhel-749f7d55c8xdd8b6-ct4cp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:03:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:03:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:03:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:03:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.11.3,PodIP:,StartTime:2023-06-28 08:03:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 28 08:03:03.342: INFO: Pod "webserver-deployment-69b7448995-sqcvw" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-sqcvw webserver-deployment-69b7448995- deployment-2432  20e8e535-e6db-4caa-9ab6-39a8ab508630 58387 0 2023-06-28 08:03:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 856ab508-beb4-4308-9f76-62868b6545b9 0xc0030c5f87 0xc0030c5f88}] [] [{kube-controller-manager Update v1 2023-06-28 08:03:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"856ab508-beb4-4308-9f76-62868b6545b9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-28 08:03:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pd7d8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pd7d8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-rhel-749f7d55c8xdd8b6-srshq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:03:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:03:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:03:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:03:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.11.4,PodIP:,StartTime:2023-06-28 08:03:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 28 08:03:03.343: INFO: Pod "webserver-deployment-845c8977d9-2hsq5" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-2hsq5 webserver-deployment-845c8977d9- deployment-2432  94a39242-24dc-48ec-a028-b4594a7e3949 57985 0 2023-06-28 08:02:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:3a137a9496061c8f304fc70256748341a1f33332cf20c64ce20bf3a70f9ffba8 cni.projectcalico.org/podIP:172.21.122.104/32 cni.projectcalico.org/podIPs:172.21.122.104/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 79eaf046-3c6c-452e-be65-a514934249dc 0xc002e18167 0xc002e18168}] [] [{calico Update v1 2023-06-28 08:02:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-06-28 08:02:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"79eaf046-3c6c-452e-be65-a514934249dc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-28 08:02:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.122.104\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-246b5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-246b5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-rhel-749f7d55c8xdd8b6-srshq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:02:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:02:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:02:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:02:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.11.4,PodIP:172.21.122.104,StartTime:2023-06-28 08:02:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-28 08:02:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://83b239a726d72d4d67211d9a2c2b8a7646ddbe397a2b2ad9d10eaf8a9c99aeb2,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.21.122.104,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 28 08:03:03.343: INFO: Pod "webserver-deployment-845c8977d9-49n92" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-49n92 webserver-deployment-845c8977d9- deployment-2432  f0eea0c1-b25b-4aba-b075-466325174d1d 58354 0 2023-06-28 08:03:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 79eaf046-3c6c-452e-be65-a514934249dc 0xc002e18357 0xc002e18358}] [] [{kube-controller-manager Update v1 2023-06-28 08:03:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"79eaf046-3c6c-452e-be65-a514934249dc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vrcxj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vrcxj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-rhel-749f7d55c8xdd8b6-srshq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:03:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 28 08:03:03.343: INFO: Pod "webserver-deployment-845c8977d9-6k6w7" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-6k6w7 webserver-deployment-845c8977d9- deployment-2432  3941229d-649a-49a4-9c1e-18b6a7b6177e 57982 0 2023-06-28 08:02:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:cccecc22243f15963b11f37f62933cfce924e8a772ff78318be8974cec0a5538 cni.projectcalico.org/podIP:172.21.122.108/32 cni.projectcalico.org/podIPs:172.21.122.108/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 79eaf046-3c6c-452e-be65-a514934249dc 0xc002e184b0 0xc002e184b1}] [] [{calico Update v1 2023-06-28 08:02:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-06-28 08:02:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"79eaf046-3c6c-452e-be65-a514934249dc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-28 08:02:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.122.108\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fgdmk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fgdmk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-rhel-749f7d55c8xdd8b6-srshq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:02:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:02:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:02:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:02:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.11.4,PodIP:172.21.122.108,StartTime:2023-06-28 08:02:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-28 08:02:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://649b6c47e198e19f4052380c1c87cfffe5cb33e5f173e701425d361d270e79bf,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.21.122.108,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 28 08:03:03.343: INFO: Pod "webserver-deployment-845c8977d9-8ppnz" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-8ppnz webserver-deployment-845c8977d9- deployment-2432  3aa0ceae-2801-4106-aea7-9c412f16efc6 57975 0 2023-06-28 08:02:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:bd991fc0f9e61a5859cd54e39dc1c8c3b9e3f6757bb753962772f6a478ba42e9 cni.projectcalico.org/podIP:172.21.30.100/32 cni.projectcalico.org/podIPs:172.21.30.100/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 79eaf046-3c6c-452e-be65-a514934249dc 0xc002e186c7 0xc002e186c8}] [] [{calico Update v1 2023-06-28 08:02:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-06-28 08:02:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"79eaf046-3c6c-452e-be65-a514934249dc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-28 08:02:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.30.100\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qljfv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qljfv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-rhel-749f7d55c8xdd8b6-zxlfv,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:02:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:02:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:02:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:02:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.11.5,PodIP:172.21.30.100,StartTime:2023-06-28 08:02:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-28 08:02:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://0917e9f74d5a509f4a4e65df5c1c28df1d7546d3dbc3572703ed73c556bb4064,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.21.30.100,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 28 08:03:03.343: INFO: Pod "webserver-deployment-845c8977d9-9tqk5" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-9tqk5 webserver-deployment-845c8977d9- deployment-2432  ab856911-deb2-49f4-b2bd-b4f8c2d99fcc 58375 0 2023-06-28 08:03:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 79eaf046-3c6c-452e-be65-a514934249dc 0xc002e188b7 0xc002e188b8}] [] [{kube-controller-manager Update v1 2023-06-28 08:03:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"79eaf046-3c6c-452e-be65-a514934249dc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cvqxt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cvqxt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-rhel-749f7d55c8xdd8b6-srshq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:03:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 28 08:03:03.343: INFO: Pod "webserver-deployment-845c8977d9-bkwkx" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-bkwkx webserver-deployment-845c8977d9- deployment-2432  4f511aae-ec2b-4091-8bc7-9784d1e6b2e9 58355 0 2023-06-28 08:03:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 79eaf046-3c6c-452e-be65-a514934249dc 0xc002e18a10 0xc002e18a11}] [] [{kube-controller-manager Update v1 2023-06-28 08:03:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"79eaf046-3c6c-452e-be65-a514934249dc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vfcx2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vfcx2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-rhel-749f7d55c8xdd8b6-ct4cp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:03:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 28 08:03:03.343: INFO: Pod "webserver-deployment-845c8977d9-d4crr" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-d4crr webserver-deployment-845c8977d9- deployment-2432  a432f6b7-69d5-478f-a5bf-0fff7fd1eb16 58391 0 2023-06-28 08:03:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 79eaf046-3c6c-452e-be65-a514934249dc 0xc002e18b70 0xc002e18b71}] [] [{kube-controller-manager Update v1 2023-06-28 08:03:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"79eaf046-3c6c-452e-be65-a514934249dc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-28 08:03:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cjq55,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cjq55,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-rhel-749f7d55c8xdd8b6-ct4cp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:03:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:03:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:03:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:03:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.11.3,PodIP:,StartTime:2023-06-28 08:03:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 28 08:03:03.344: INFO: Pod "webserver-deployment-845c8977d9-f2xwf" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-f2xwf webserver-deployment-845c8977d9- deployment-2432  d27f3afb-f133-4eec-a276-ce0a8d6a4a0c 57963 0 2023-06-28 08:02:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:fb0d6402cff8a4960a1e4f685b87331acceb258324a5588d36fc3d3b99d54c6b cni.projectcalico.org/podIP:172.21.122.48/32 cni.projectcalico.org/podIPs:172.21.122.48/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 79eaf046-3c6c-452e-be65-a514934249dc 0xc002e18d47 0xc002e18d48}] [] [{calico Update v1 2023-06-28 08:02:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-06-28 08:02:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"79eaf046-3c6c-452e-be65-a514934249dc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-28 08:02:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.122.48\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cwnkw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cwnkw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-rhel-749f7d55c8xdd8b6-ct4cp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:02:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:02:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:02:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:02:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.11.3,PodIP:172.21.122.48,StartTime:2023-06-28 08:02:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-28 08:02:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://882f6f7cbe15f9b7fa4629a9246653ad0028246d210ea8e455930dd81e9d1094,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.21.122.48,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 28 08:03:03.344: INFO: Pod "webserver-deployment-845c8977d9-g7cv2" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-g7cv2 webserver-deployment-845c8977d9- deployment-2432  11569d1f-8275-45bf-97c2-fc8eaa8d68cb 57968 0 2023-06-28 08:02:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:6e56560c91f0f7c13c5ad54884e970a2a7aa1fed60fcd4effdade65a3af15bb5 cni.projectcalico.org/podIP:172.21.30.86/32 cni.projectcalico.org/podIPs:172.21.30.86/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 79eaf046-3c6c-452e-be65-a514934249dc 0xc002e18f57 0xc002e18f58}] [] [{calico Update v1 2023-06-28 08:02:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-06-28 08:02:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"79eaf046-3c6c-452e-be65-a514934249dc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-28 08:02:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.30.86\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2bklg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2bklg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-rhel-749f7d55c8xdd8b6-zxlfv,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:02:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:02:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:02:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:02:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.11.5,PodIP:172.21.30.86,StartTime:2023-06-28 08:02:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-28 08:02:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://fe70b73ab54b265e80473b944ef5b86a08de4eb7c0b5e76edeca5008b4e6eed7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.21.30.86,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 28 08:03:03.346: INFO: Pod "webserver-deployment-845c8977d9-gqbkr" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-gqbkr webserver-deployment-845c8977d9- deployment-2432  2acad839-1522-4caa-8cb2-e44f8cc3dc7a 57972 0 2023-06-28 08:02:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:48b047782b411422b313ce2469ad6fd330a1ae3d09c97067c8b149a4b1138544 cni.projectcalico.org/podIP:172.21.30.82/32 cni.projectcalico.org/podIPs:172.21.30.82/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 79eaf046-3c6c-452e-be65-a514934249dc 0xc002e19167 0xc002e19168}] [] [{calico Update v1 2023-06-28 08:02:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-06-28 08:02:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"79eaf046-3c6c-452e-be65-a514934249dc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-28 08:02:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.30.82\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fq68b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fq68b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-rhel-749f7d55c8xdd8b6-zxlfv,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:02:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:02:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:02:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:02:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.11.5,PodIP:172.21.30.82,StartTime:2023-06-28 08:02:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-28 08:02:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://c333089f29c4acbaab7667c6a853288518671e484cd6d3d0ab1c9dd0ca06e133,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.21.30.82,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 28 08:03:03.346: INFO: Pod "webserver-deployment-845c8977d9-jxpgc" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-jxpgc webserver-deployment-845c8977d9- deployment-2432  35e3bfdf-9891-4509-9cfe-3b85d32f40b0 58381 0 2023-06-28 08:03:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 79eaf046-3c6c-452e-be65-a514934249dc 0xc002e19357 0xc002e19358}] [] [{kube-controller-manager Update v1 2023-06-28 08:03:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"79eaf046-3c6c-452e-be65-a514934249dc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mkllm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mkllm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-rhel-749f7d55c8xdd8b6-ct4cp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:03:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 28 08:03:03.346: INFO: Pod "webserver-deployment-845c8977d9-knxq6" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-knxq6 webserver-deployment-845c8977d9- deployment-2432  835fdc7e-e40d-451a-bc7d-f9610b0f6bfb 57979 0 2023-06-28 08:02:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:2282cc0bccd51c38a4531410dfe31a3a6f6dd5b6b57113358632dbfcdb98e519 cni.projectcalico.org/podIP:172.21.122.2/32 cni.projectcalico.org/podIPs:172.21.122.2/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 79eaf046-3c6c-452e-be65-a514934249dc 0xc002e194e0 0xc002e194e1}] [] [{calico Update v1 2023-06-28 08:02:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-06-28 08:02:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"79eaf046-3c6c-452e-be65-a514934249dc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-28 08:02:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.122.2\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-th4km,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-th4km,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-rhel-749f7d55c8xdd8b6-ct4cp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:02:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:02:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:02:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:02:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.11.3,PodIP:172.21.122.2,StartTime:2023-06-28 08:02:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-28 08:02:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://befe4e06e4ec267f471eb667108b5d232b7bbdb7117bf5a7980fa5aa0f3194c3,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.21.122.2,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 28 08:03:03.346: INFO: Pod "webserver-deployment-845c8977d9-kzgwx" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-kzgwx webserver-deployment-845c8977d9- deployment-2432  ef5c0d82-59ba-4922-b665-29b761717ce9 58388 0 2023-06-28 08:03:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 79eaf046-3c6c-452e-be65-a514934249dc 0xc002e196d7 0xc002e196d8}] [] [{kube-controller-manager Update v1 2023-06-28 08:03:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"79eaf046-3c6c-452e-be65-a514934249dc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-28 08:03:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g2gv4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g2gv4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-rhel-749f7d55c8xdd8b6-zxlfv,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:03:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:03:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:03:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:03:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.11.5,PodIP:,StartTime:2023-06-28 08:03:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 28 08:03:03.346: INFO: Pod "webserver-deployment-845c8977d9-l7g6n" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-l7g6n webserver-deployment-845c8977d9- deployment-2432  95532018-8757-41ce-82f3-3a227a63f46a 58360 0 2023-06-28 08:03:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 79eaf046-3c6c-452e-be65-a514934249dc 0xc002e19897 0xc002e19898}] [] [{kube-controller-manager Update v1 2023-06-28 08:03:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"79eaf046-3c6c-452e-be65-a514934249dc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-66w5l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-66w5l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-rhel-749f7d55c8xdd8b6-zxlfv,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:03:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 28 08:03:03.347: INFO: Pod "webserver-deployment-845c8977d9-mdh5f" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-mdh5f webserver-deployment-845c8977d9- deployment-2432  a45be03f-2bc6-4005-a93d-385ce1699d5a 57988 0 2023-06-28 08:02:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:3af53969724c52b41d9c79712a26b37c97b25629776613a682265d0c5f84fc8b cni.projectcalico.org/podIP:172.21.122.109/32 cni.projectcalico.org/podIPs:172.21.122.109/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 79eaf046-3c6c-452e-be65-a514934249dc 0xc002e199f0 0xc002e199f1}] [] [{calico Update v1 2023-06-28 08:02:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-06-28 08:02:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"79eaf046-3c6c-452e-be65-a514934249dc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-28 08:02:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.122.109\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-chlhn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-chlhn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-rhel-749f7d55c8xdd8b6-srshq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:02:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:02:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:02:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:02:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.11.4,PodIP:172.21.122.109,StartTime:2023-06-28 08:02:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-28 08:02:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://af92fafad4e7e1842687a263e64c348be7b6ca4acfe0e4e7ce3b07c4e2f5bfa2,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.21.122.109,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 28 08:03:03.347: INFO: Pod "webserver-deployment-845c8977d9-qmlrg" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-qmlrg webserver-deployment-845c8977d9- deployment-2432  7740a89d-07a7-47e0-9b3a-b063ea57ac98 58357 0 2023-06-28 08:03:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 79eaf046-3c6c-452e-be65-a514934249dc 0xc002e19be7 0xc002e19be8}] [] [{kube-controller-manager Update v1 2023-06-28 08:03:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"79eaf046-3c6c-452e-be65-a514934249dc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-28 08:03:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-s6p62,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-s6p62,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-rhel-749f7d55c8xdd8b6-ct4cp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:03:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:03:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:03:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:03:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.11.3,PodIP:,StartTime:2023-06-28 08:03:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 28 08:03:03.347: INFO: Pod "webserver-deployment-845c8977d9-rscwp" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-rscwp webserver-deployment-845c8977d9- deployment-2432  6a79cd59-443e-41e8-9dbc-45c3f1d8d5e3 58380 0 2023-06-28 08:03:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 79eaf046-3c6c-452e-be65-a514934249dc 0xc002e19da7 0xc002e19da8}] [] [{kube-controller-manager Update v1 2023-06-28 08:03:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"79eaf046-3c6c-452e-be65-a514934249dc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7d4j7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7d4j7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-rhel-749f7d55c8xdd8b6-srshq,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:03:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 28 08:03:03.347: INFO: Pod "webserver-deployment-845c8977d9-tnd56" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-tnd56 webserver-deployment-845c8977d9- deployment-2432  1f6b97f2-a03d-4d77-ba56-9427867c216b 58370 0 2023-06-28 08:03:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 79eaf046-3c6c-452e-be65-a514934249dc 0xc002e19f00 0xc002e19f01}] [] [{kube-controller-manager Update v1 2023-06-28 08:03:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"79eaf046-3c6c-452e-be65-a514934249dc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wqkwp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wqkwp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-rhel-749f7d55c8xdd8b6-ct4cp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:03:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 28 08:03:03.347: INFO: Pod "webserver-deployment-845c8977d9-w2v6c" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-w2v6c webserver-deployment-845c8977d9- deployment-2432  01c05350-7c3e-4491-99a5-cd1f7d3fa8a7 58385 0 2023-06-28 08:03:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 79eaf046-3c6c-452e-be65-a514934249dc 0xc003ae6050 0xc003ae6051}] [] [{kube-controller-manager Update v1 2023-06-28 08:03:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"79eaf046-3c6c-452e-be65-a514934249dc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-knsm4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-knsm4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-rhel-749f7d55c8xdd8b6-zxlfv,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:03:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 28 08:03:03.347: INFO: Pod "webserver-deployment-845c8977d9-xvld8" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-xvld8 webserver-deployment-845c8977d9- deployment-2432  83aaae8d-15f9-4eed-9f07-f7e62feb60c0 58379 0 2023-06-28 08:03:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 79eaf046-3c6c-452e-be65-a514934249dc 0xc003ae61a0 0xc003ae61a1}] [] [{kube-controller-manager Update v1 2023-06-28 08:03:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"79eaf046-3c6c-452e-be65-a514934249dc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cg99q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cg99q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-rhel-749f7d55c8xdd8b6-zxlfv,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:03:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jun 28 08:03:03.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-2432" for this suite. 06/28/23 08:03:03.358
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2216
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:03:03.374
Jun 28 08:03:03.374: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename services 06/28/23 08:03:03.375
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:03:03.391
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:03:03.397
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2216
STEP: creating service in namespace services-2723 06/28/23 08:03:03.403
STEP: creating service affinity-nodeport-transition in namespace services-2723 06/28/23 08:03:03.403
STEP: creating replication controller affinity-nodeport-transition in namespace services-2723 06/28/23 08:03:03.418
I0628 08:03:03.425876      18 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-2723, replica count: 3
I0628 08:03:06.480844      18 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0628 08:03:09.481244      18 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0628 08:03:12.483388      18 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 28 08:03:12.502: INFO: Creating new exec pod
Jun 28 08:03:12.512: INFO: Waiting up to 5m0s for pod "execpod-affinityzr2ns" in namespace "services-2723" to be "running"
Jun 28 08:03:12.517: INFO: Pod "execpod-affinityzr2ns": Phase="Pending", Reason="", readiness=false. Elapsed: 4.745201ms
Jun 28 08:03:14.522: INFO: Pod "execpod-affinityzr2ns": Phase="Running", Reason="", readiness=true. Elapsed: 2.009970282s
Jun 28 08:03:14.522: INFO: Pod "execpod-affinityzr2ns" satisfied condition "running"
Jun 28 08:03:15.531: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-2723 exec execpod-affinityzr2ns -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
Jun 28 08:03:15.999: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Jun 28 08:03:15.999: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun 28 08:03:15.999: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-2723 exec execpod-affinityzr2ns -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.20.93.168 80'
Jun 28 08:03:16.431: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.20.93.168 80\nConnection to 172.20.93.168 80 port [tcp/http] succeeded!\n"
Jun 28 08:03:16.431: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun 28 08:03:16.431: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-2723 exec execpod-affinityzr2ns -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.11.3 32069'
Jun 28 08:03:16.748: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.11.3 32069\nConnection to 192.168.11.3 32069 port [tcp/*] succeeded!\n"
Jun 28 08:03:16.748: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun 28 08:03:16.748: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-2723 exec execpod-affinityzr2ns -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.11.4 32069'
Jun 28 08:03:17.193: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.11.4 32069\nConnection to 192.168.11.4 32069 port [tcp/*] succeeded!\n"
Jun 28 08:03:17.193: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun 28 08:03:17.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-2723 exec execpod-affinityzr2ns -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.11.3:32069/ ; done'
Jun 28 08:03:17.817: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:32069/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:32069/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:32069/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:32069/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:32069/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:32069/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:32069/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:32069/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:32069/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:32069/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:32069/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:32069/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:32069/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:32069/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:32069/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:32069/\n"
Jun 28 08:03:17.817: INFO: stdout: "\naffinity-nodeport-transition-z2p76\naffinity-nodeport-transition-nqskc\naffinity-nodeport-transition-rm7pv\naffinity-nodeport-transition-z2p76\naffinity-nodeport-transition-nqskc\naffinity-nodeport-transition-nqskc\naffinity-nodeport-transition-z2p76\naffinity-nodeport-transition-z2p76\naffinity-nodeport-transition-rm7pv\naffinity-nodeport-transition-z2p76\naffinity-nodeport-transition-z2p76\naffinity-nodeport-transition-z2p76\naffinity-nodeport-transition-nqskc\naffinity-nodeport-transition-rm7pv\naffinity-nodeport-transition-rm7pv\naffinity-nodeport-transition-rm7pv"
Jun 28 08:03:17.817: INFO: Received response from host: affinity-nodeport-transition-z2p76
Jun 28 08:03:17.817: INFO: Received response from host: affinity-nodeport-transition-nqskc
Jun 28 08:03:17.817: INFO: Received response from host: affinity-nodeport-transition-rm7pv
Jun 28 08:03:17.817: INFO: Received response from host: affinity-nodeport-transition-z2p76
Jun 28 08:03:17.817: INFO: Received response from host: affinity-nodeport-transition-nqskc
Jun 28 08:03:17.817: INFO: Received response from host: affinity-nodeport-transition-nqskc
Jun 28 08:03:17.817: INFO: Received response from host: affinity-nodeport-transition-z2p76
Jun 28 08:03:17.817: INFO: Received response from host: affinity-nodeport-transition-z2p76
Jun 28 08:03:17.817: INFO: Received response from host: affinity-nodeport-transition-rm7pv
Jun 28 08:03:17.817: INFO: Received response from host: affinity-nodeport-transition-z2p76
Jun 28 08:03:17.817: INFO: Received response from host: affinity-nodeport-transition-z2p76
Jun 28 08:03:17.817: INFO: Received response from host: affinity-nodeport-transition-z2p76
Jun 28 08:03:17.817: INFO: Received response from host: affinity-nodeport-transition-nqskc
Jun 28 08:03:17.817: INFO: Received response from host: affinity-nodeport-transition-rm7pv
Jun 28 08:03:17.817: INFO: Received response from host: affinity-nodeport-transition-rm7pv
Jun 28 08:03:17.817: INFO: Received response from host: affinity-nodeport-transition-rm7pv
Jun 28 08:03:17.831: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-2723 exec execpod-affinityzr2ns -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.11.3:32069/ ; done'
Jun 28 08:03:18.347: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:32069/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:32069/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:32069/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:32069/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:32069/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:32069/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:32069/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:32069/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:32069/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:32069/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:32069/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:32069/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:32069/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:32069/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:32069/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:32069/\n"
Jun 28 08:03:18.347: INFO: stdout: "\naffinity-nodeport-transition-z2p76\naffinity-nodeport-transition-z2p76\naffinity-nodeport-transition-z2p76\naffinity-nodeport-transition-z2p76\naffinity-nodeport-transition-z2p76\naffinity-nodeport-transition-z2p76\naffinity-nodeport-transition-z2p76\naffinity-nodeport-transition-z2p76\naffinity-nodeport-transition-z2p76\naffinity-nodeport-transition-z2p76\naffinity-nodeport-transition-z2p76\naffinity-nodeport-transition-z2p76\naffinity-nodeport-transition-z2p76\naffinity-nodeport-transition-z2p76\naffinity-nodeport-transition-z2p76\naffinity-nodeport-transition-z2p76"
Jun 28 08:03:18.347: INFO: Received response from host: affinity-nodeport-transition-z2p76
Jun 28 08:03:18.347: INFO: Received response from host: affinity-nodeport-transition-z2p76
Jun 28 08:03:18.347: INFO: Received response from host: affinity-nodeport-transition-z2p76
Jun 28 08:03:18.347: INFO: Received response from host: affinity-nodeport-transition-z2p76
Jun 28 08:03:18.347: INFO: Received response from host: affinity-nodeport-transition-z2p76
Jun 28 08:03:18.347: INFO: Received response from host: affinity-nodeport-transition-z2p76
Jun 28 08:03:18.347: INFO: Received response from host: affinity-nodeport-transition-z2p76
Jun 28 08:03:18.347: INFO: Received response from host: affinity-nodeport-transition-z2p76
Jun 28 08:03:18.347: INFO: Received response from host: affinity-nodeport-transition-z2p76
Jun 28 08:03:18.347: INFO: Received response from host: affinity-nodeport-transition-z2p76
Jun 28 08:03:18.347: INFO: Received response from host: affinity-nodeport-transition-z2p76
Jun 28 08:03:18.347: INFO: Received response from host: affinity-nodeport-transition-z2p76
Jun 28 08:03:18.347: INFO: Received response from host: affinity-nodeport-transition-z2p76
Jun 28 08:03:18.347: INFO: Received response from host: affinity-nodeport-transition-z2p76
Jun 28 08:03:18.347: INFO: Received response from host: affinity-nodeport-transition-z2p76
Jun 28 08:03:18.347: INFO: Received response from host: affinity-nodeport-transition-z2p76
Jun 28 08:03:18.347: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-2723, will wait for the garbage collector to delete the pods 06/28/23 08:03:18.36
Jun 28 08:03:18.422: INFO: Deleting ReplicationController affinity-nodeport-transition took: 8.179757ms
Jun 28 08:03:18.523: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.655561ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jun 28 08:03:20.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2723" for this suite. 06/28/23 08:03:20.858
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","completed":85,"skipped":1518,"failed":0}
------------------------------
â€¢ [SLOW TEST] [17.492 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:03:03.374
    Jun 28 08:03:03.374: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename services 06/28/23 08:03:03.375
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:03:03.391
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:03:03.397
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2216
    STEP: creating service in namespace services-2723 06/28/23 08:03:03.403
    STEP: creating service affinity-nodeport-transition in namespace services-2723 06/28/23 08:03:03.403
    STEP: creating replication controller affinity-nodeport-transition in namespace services-2723 06/28/23 08:03:03.418
    I0628 08:03:03.425876      18 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-2723, replica count: 3
    I0628 08:03:06.480844      18 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0628 08:03:09.481244      18 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0628 08:03:12.483388      18 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jun 28 08:03:12.502: INFO: Creating new exec pod
    Jun 28 08:03:12.512: INFO: Waiting up to 5m0s for pod "execpod-affinityzr2ns" in namespace "services-2723" to be "running"
    Jun 28 08:03:12.517: INFO: Pod "execpod-affinityzr2ns": Phase="Pending", Reason="", readiness=false. Elapsed: 4.745201ms
    Jun 28 08:03:14.522: INFO: Pod "execpod-affinityzr2ns": Phase="Running", Reason="", readiness=true. Elapsed: 2.009970282s
    Jun 28 08:03:14.522: INFO: Pod "execpod-affinityzr2ns" satisfied condition "running"
    Jun 28 08:03:15.531: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-2723 exec execpod-affinityzr2ns -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
    Jun 28 08:03:15.999: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
    Jun 28 08:03:15.999: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jun 28 08:03:15.999: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-2723 exec execpod-affinityzr2ns -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.20.93.168 80'
    Jun 28 08:03:16.431: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.20.93.168 80\nConnection to 172.20.93.168 80 port [tcp/http] succeeded!\n"
    Jun 28 08:03:16.431: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jun 28 08:03:16.431: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-2723 exec execpod-affinityzr2ns -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.11.3 32069'
    Jun 28 08:03:16.748: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.11.3 32069\nConnection to 192.168.11.3 32069 port [tcp/*] succeeded!\n"
    Jun 28 08:03:16.748: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jun 28 08:03:16.748: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-2723 exec execpod-affinityzr2ns -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.11.4 32069'
    Jun 28 08:03:17.193: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.11.4 32069\nConnection to 192.168.11.4 32069 port [tcp/*] succeeded!\n"
    Jun 28 08:03:17.193: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jun 28 08:03:17.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-2723 exec execpod-affinityzr2ns -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.11.3:32069/ ; done'
    Jun 28 08:03:17.817: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:32069/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:32069/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:32069/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:32069/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:32069/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:32069/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:32069/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:32069/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:32069/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:32069/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:32069/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:32069/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:32069/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:32069/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:32069/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:32069/\n"
    Jun 28 08:03:17.817: INFO: stdout: "\naffinity-nodeport-transition-z2p76\naffinity-nodeport-transition-nqskc\naffinity-nodeport-transition-rm7pv\naffinity-nodeport-transition-z2p76\naffinity-nodeport-transition-nqskc\naffinity-nodeport-transition-nqskc\naffinity-nodeport-transition-z2p76\naffinity-nodeport-transition-z2p76\naffinity-nodeport-transition-rm7pv\naffinity-nodeport-transition-z2p76\naffinity-nodeport-transition-z2p76\naffinity-nodeport-transition-z2p76\naffinity-nodeport-transition-nqskc\naffinity-nodeport-transition-rm7pv\naffinity-nodeport-transition-rm7pv\naffinity-nodeport-transition-rm7pv"
    Jun 28 08:03:17.817: INFO: Received response from host: affinity-nodeport-transition-z2p76
    Jun 28 08:03:17.817: INFO: Received response from host: affinity-nodeport-transition-nqskc
    Jun 28 08:03:17.817: INFO: Received response from host: affinity-nodeport-transition-rm7pv
    Jun 28 08:03:17.817: INFO: Received response from host: affinity-nodeport-transition-z2p76
    Jun 28 08:03:17.817: INFO: Received response from host: affinity-nodeport-transition-nqskc
    Jun 28 08:03:17.817: INFO: Received response from host: affinity-nodeport-transition-nqskc
    Jun 28 08:03:17.817: INFO: Received response from host: affinity-nodeport-transition-z2p76
    Jun 28 08:03:17.817: INFO: Received response from host: affinity-nodeport-transition-z2p76
    Jun 28 08:03:17.817: INFO: Received response from host: affinity-nodeport-transition-rm7pv
    Jun 28 08:03:17.817: INFO: Received response from host: affinity-nodeport-transition-z2p76
    Jun 28 08:03:17.817: INFO: Received response from host: affinity-nodeport-transition-z2p76
    Jun 28 08:03:17.817: INFO: Received response from host: affinity-nodeport-transition-z2p76
    Jun 28 08:03:17.817: INFO: Received response from host: affinity-nodeport-transition-nqskc
    Jun 28 08:03:17.817: INFO: Received response from host: affinity-nodeport-transition-rm7pv
    Jun 28 08:03:17.817: INFO: Received response from host: affinity-nodeport-transition-rm7pv
    Jun 28 08:03:17.817: INFO: Received response from host: affinity-nodeport-transition-rm7pv
    Jun 28 08:03:17.831: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-2723 exec execpod-affinityzr2ns -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.11.3:32069/ ; done'
    Jun 28 08:03:18.347: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:32069/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:32069/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:32069/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:32069/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:32069/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:32069/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:32069/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:32069/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:32069/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:32069/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:32069/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:32069/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:32069/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:32069/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:32069/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.11.3:32069/\n"
    Jun 28 08:03:18.347: INFO: stdout: "\naffinity-nodeport-transition-z2p76\naffinity-nodeport-transition-z2p76\naffinity-nodeport-transition-z2p76\naffinity-nodeport-transition-z2p76\naffinity-nodeport-transition-z2p76\naffinity-nodeport-transition-z2p76\naffinity-nodeport-transition-z2p76\naffinity-nodeport-transition-z2p76\naffinity-nodeport-transition-z2p76\naffinity-nodeport-transition-z2p76\naffinity-nodeport-transition-z2p76\naffinity-nodeport-transition-z2p76\naffinity-nodeport-transition-z2p76\naffinity-nodeport-transition-z2p76\naffinity-nodeport-transition-z2p76\naffinity-nodeport-transition-z2p76"
    Jun 28 08:03:18.347: INFO: Received response from host: affinity-nodeport-transition-z2p76
    Jun 28 08:03:18.347: INFO: Received response from host: affinity-nodeport-transition-z2p76
    Jun 28 08:03:18.347: INFO: Received response from host: affinity-nodeport-transition-z2p76
    Jun 28 08:03:18.347: INFO: Received response from host: affinity-nodeport-transition-z2p76
    Jun 28 08:03:18.347: INFO: Received response from host: affinity-nodeport-transition-z2p76
    Jun 28 08:03:18.347: INFO: Received response from host: affinity-nodeport-transition-z2p76
    Jun 28 08:03:18.347: INFO: Received response from host: affinity-nodeport-transition-z2p76
    Jun 28 08:03:18.347: INFO: Received response from host: affinity-nodeport-transition-z2p76
    Jun 28 08:03:18.347: INFO: Received response from host: affinity-nodeport-transition-z2p76
    Jun 28 08:03:18.347: INFO: Received response from host: affinity-nodeport-transition-z2p76
    Jun 28 08:03:18.347: INFO: Received response from host: affinity-nodeport-transition-z2p76
    Jun 28 08:03:18.347: INFO: Received response from host: affinity-nodeport-transition-z2p76
    Jun 28 08:03:18.347: INFO: Received response from host: affinity-nodeport-transition-z2p76
    Jun 28 08:03:18.347: INFO: Received response from host: affinity-nodeport-transition-z2p76
    Jun 28 08:03:18.347: INFO: Received response from host: affinity-nodeport-transition-z2p76
    Jun 28 08:03:18.347: INFO: Received response from host: affinity-nodeport-transition-z2p76
    Jun 28 08:03:18.347: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-2723, will wait for the garbage collector to delete the pods 06/28/23 08:03:18.36
    Jun 28 08:03:18.422: INFO: Deleting ReplicationController affinity-nodeport-transition took: 8.179757ms
    Jun 28 08:03:18.523: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.655561ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jun 28 08:03:20.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-2723" for this suite. 06/28/23 08:03:20.858
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:03:20.868
Jun 28 08:03:20.868: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename projected 06/28/23 08:03:20.869
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:03:20.89
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:03:20.899
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67
STEP: Creating a pod to test downward API volume plugin 06/28/23 08:03:20.904
Jun 28 08:03:20.915: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cf1a3663-4fdc-4b78-9de5-d770582270dc" in namespace "projected-2426" to be "Succeeded or Failed"
Jun 28 08:03:20.919: INFO: Pod "downwardapi-volume-cf1a3663-4fdc-4b78-9de5-d770582270dc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.481605ms
Jun 28 08:03:22.924: INFO: Pod "downwardapi-volume-cf1a3663-4fdc-4b78-9de5-d770582270dc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009755636s
Jun 28 08:03:24.936: INFO: Pod "downwardapi-volume-cf1a3663-4fdc-4b78-9de5-d770582270dc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021039719s
STEP: Saw pod success 06/28/23 08:03:24.936
Jun 28 08:03:24.936: INFO: Pod "downwardapi-volume-cf1a3663-4fdc-4b78-9de5-d770582270dc" satisfied condition "Succeeded or Failed"
Jun 28 08:03:24.942: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod downwardapi-volume-cf1a3663-4fdc-4b78-9de5-d770582270dc container client-container: <nil>
STEP: delete the pod 06/28/23 08:03:24.993
Jun 28 08:03:25.006: INFO: Waiting for pod downwardapi-volume-cf1a3663-4fdc-4b78-9de5-d770582270dc to disappear
Jun 28 08:03:25.009: INFO: Pod downwardapi-volume-cf1a3663-4fdc-4b78-9de5-d770582270dc no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jun 28 08:03:25.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2426" for this suite. 06/28/23 08:03:25.016
{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","completed":86,"skipped":1580,"failed":0}
------------------------------
â€¢ [4.155 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:03:20.868
    Jun 28 08:03:20.868: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename projected 06/28/23 08:03:20.869
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:03:20.89
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:03:20.899
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:67
    STEP: Creating a pod to test downward API volume plugin 06/28/23 08:03:20.904
    Jun 28 08:03:20.915: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cf1a3663-4fdc-4b78-9de5-d770582270dc" in namespace "projected-2426" to be "Succeeded or Failed"
    Jun 28 08:03:20.919: INFO: Pod "downwardapi-volume-cf1a3663-4fdc-4b78-9de5-d770582270dc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.481605ms
    Jun 28 08:03:22.924: INFO: Pod "downwardapi-volume-cf1a3663-4fdc-4b78-9de5-d770582270dc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009755636s
    Jun 28 08:03:24.936: INFO: Pod "downwardapi-volume-cf1a3663-4fdc-4b78-9de5-d770582270dc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021039719s
    STEP: Saw pod success 06/28/23 08:03:24.936
    Jun 28 08:03:24.936: INFO: Pod "downwardapi-volume-cf1a3663-4fdc-4b78-9de5-d770582270dc" satisfied condition "Succeeded or Failed"
    Jun 28 08:03:24.942: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod downwardapi-volume-cf1a3663-4fdc-4b78-9de5-d770582270dc container client-container: <nil>
    STEP: delete the pod 06/28/23 08:03:24.993
    Jun 28 08:03:25.006: INFO: Waiting for pod downwardapi-volume-cf1a3663-4fdc-4b78-9de5-d770582270dc to disappear
    Jun 28 08:03:25.009: INFO: Pod downwardapi-volume-cf1a3663-4fdc-4b78-9de5-d770582270dc no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jun 28 08:03:25.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-2426" for this suite. 06/28/23 08:03:25.016
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:03:25.024
Jun 28 08:03:25.024: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename proxy 06/28/23 08:03:25.025
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:03:25.04
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:03:25.044
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
Jun 28 08:03:25.048: INFO: Creating pod...
Jun 28 08:03:25.056: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-9999" to be "running"
Jun 28 08:03:25.060: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022267ms
Jun 28 08:03:27.064: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.008546118s
Jun 28 08:03:27.064: INFO: Pod "agnhost" satisfied condition "running"
Jun 28 08:03:27.064: INFO: Creating service...
Jun 28 08:03:27.074: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-9999/pods/agnhost/proxy/some/path/with/DELETE
Jun 28 08:03:27.122: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jun 28 08:03:27.122: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-9999/pods/agnhost/proxy/some/path/with/GET
Jun 28 08:03:27.131: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Jun 28 08:03:27.131: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-9999/pods/agnhost/proxy/some/path/with/HEAD
Jun 28 08:03:27.178: INFO: http.Client request:HEAD | StatusCode:200
Jun 28 08:03:27.178: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-9999/pods/agnhost/proxy/some/path/with/OPTIONS
Jun 28 08:03:27.186: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jun 28 08:03:27.186: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-9999/pods/agnhost/proxy/some/path/with/PATCH
Jun 28 08:03:27.194: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jun 28 08:03:27.194: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-9999/pods/agnhost/proxy/some/path/with/POST
Jun 28 08:03:27.201: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jun 28 08:03:27.201: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-9999/pods/agnhost/proxy/some/path/with/PUT
Jun 28 08:03:27.211: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jun 28 08:03:27.211: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-9999/services/test-service/proxy/some/path/with/DELETE
Jun 28 08:03:27.219: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jun 28 08:03:27.219: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-9999/services/test-service/proxy/some/path/with/GET
Jun 28 08:03:27.229: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Jun 28 08:03:27.229: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-9999/services/test-service/proxy/some/path/with/HEAD
Jun 28 08:03:27.238: INFO: http.Client request:HEAD | StatusCode:200
Jun 28 08:03:27.238: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-9999/services/test-service/proxy/some/path/with/OPTIONS
Jun 28 08:03:27.246: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jun 28 08:03:27.246: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-9999/services/test-service/proxy/some/path/with/PATCH
Jun 28 08:03:27.255: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jun 28 08:03:27.255: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-9999/services/test-service/proxy/some/path/with/POST
Jun 28 08:03:27.263: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jun 28 08:03:27.263: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-9999/services/test-service/proxy/some/path/with/PUT
Jun 28 08:03:27.272: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  test/e2e/framework/framework.go:187
Jun 28 08:03:27.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-9999" for this suite. 06/28/23 08:03:27.279
{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]","completed":87,"skipped":1606,"failed":0}
------------------------------
â€¢ [2.261 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
    test/e2e/network/proxy.go:286

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:03:25.024
    Jun 28 08:03:25.024: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename proxy 06/28/23 08:03:25.025
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:03:25.04
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:03:25.044
    [It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
      test/e2e/network/proxy.go:286
    Jun 28 08:03:25.048: INFO: Creating pod...
    Jun 28 08:03:25.056: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-9999" to be "running"
    Jun 28 08:03:25.060: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022267ms
    Jun 28 08:03:27.064: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.008546118s
    Jun 28 08:03:27.064: INFO: Pod "agnhost" satisfied condition "running"
    Jun 28 08:03:27.064: INFO: Creating service...
    Jun 28 08:03:27.074: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-9999/pods/agnhost/proxy/some/path/with/DELETE
    Jun 28 08:03:27.122: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jun 28 08:03:27.122: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-9999/pods/agnhost/proxy/some/path/with/GET
    Jun 28 08:03:27.131: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Jun 28 08:03:27.131: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-9999/pods/agnhost/proxy/some/path/with/HEAD
    Jun 28 08:03:27.178: INFO: http.Client request:HEAD | StatusCode:200
    Jun 28 08:03:27.178: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-9999/pods/agnhost/proxy/some/path/with/OPTIONS
    Jun 28 08:03:27.186: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jun 28 08:03:27.186: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-9999/pods/agnhost/proxy/some/path/with/PATCH
    Jun 28 08:03:27.194: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jun 28 08:03:27.194: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-9999/pods/agnhost/proxy/some/path/with/POST
    Jun 28 08:03:27.201: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jun 28 08:03:27.201: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-9999/pods/agnhost/proxy/some/path/with/PUT
    Jun 28 08:03:27.211: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Jun 28 08:03:27.211: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-9999/services/test-service/proxy/some/path/with/DELETE
    Jun 28 08:03:27.219: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jun 28 08:03:27.219: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-9999/services/test-service/proxy/some/path/with/GET
    Jun 28 08:03:27.229: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Jun 28 08:03:27.229: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-9999/services/test-service/proxy/some/path/with/HEAD
    Jun 28 08:03:27.238: INFO: http.Client request:HEAD | StatusCode:200
    Jun 28 08:03:27.238: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-9999/services/test-service/proxy/some/path/with/OPTIONS
    Jun 28 08:03:27.246: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jun 28 08:03:27.246: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-9999/services/test-service/proxy/some/path/with/PATCH
    Jun 28 08:03:27.255: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jun 28 08:03:27.255: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-9999/services/test-service/proxy/some/path/with/POST
    Jun 28 08:03:27.263: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jun 28 08:03:27.263: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-9999/services/test-service/proxy/some/path/with/PUT
    Jun 28 08:03:27.272: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    Jun 28 08:03:27.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-9999" for this suite. 06/28/23 08:03:27.279
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2179
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:03:27.286
Jun 28 08:03:27.286: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename services 06/28/23 08:03:27.287
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:03:27.299
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:03:27.302
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2179
STEP: creating service in namespace services-2072 06/28/23 08:03:27.306
STEP: creating service affinity-clusterip-transition in namespace services-2072 06/28/23 08:03:27.307
STEP: creating replication controller affinity-clusterip-transition in namespace services-2072 06/28/23 08:03:27.316
I0628 08:03:27.321159      18 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-2072, replica count: 3
I0628 08:03:30.372011      18 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 28 08:03:30.379: INFO: Creating new exec pod
Jun 28 08:03:30.385: INFO: Waiting up to 5m0s for pod "execpod-affinitygbvl8" in namespace "services-2072" to be "running"
Jun 28 08:03:30.389: INFO: Pod "execpod-affinitygbvl8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.671121ms
Jun 28 08:03:32.393: INFO: Pod "execpod-affinitygbvl8": Phase="Running", Reason="", readiness=true. Elapsed: 2.008090498s
Jun 28 08:03:32.393: INFO: Pod "execpod-affinitygbvl8" satisfied condition "running"
Jun 28 08:03:33.393: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-2072 exec execpod-affinitygbvl8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
Jun 28 08:03:33.830: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Jun 28 08:03:33.831: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun 28 08:03:33.831: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-2072 exec execpod-affinitygbvl8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.20.52.180 80'
Jun 28 08:03:34.221: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.20.52.180 80\nConnection to 172.20.52.180 80 port [tcp/http] succeeded!\n"
Jun 28 08:03:34.221: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun 28 08:03:34.234: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-2072 exec execpod-affinitygbvl8 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.20.52.180:80/ ; done'
Jun 28 08:03:34.771: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.52.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.52.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.52.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.52.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.52.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.52.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.52.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.52.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.52.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.52.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.52.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.52.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.52.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.52.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.52.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.52.180:80/\n"
Jun 28 08:03:34.771: INFO: stdout: "\naffinity-clusterip-transition-msngx\naffinity-clusterip-transition-msngx\naffinity-clusterip-transition-msngx\naffinity-clusterip-transition-msngx\naffinity-clusterip-transition-q7cpq\naffinity-clusterip-transition-q7cpq\naffinity-clusterip-transition-q7cpq\naffinity-clusterip-transition-q7cpq\naffinity-clusterip-transition-5tjdv\naffinity-clusterip-transition-5tjdv\naffinity-clusterip-transition-5tjdv\naffinity-clusterip-transition-msngx\naffinity-clusterip-transition-q7cpq\naffinity-clusterip-transition-msngx\naffinity-clusterip-transition-5tjdv\naffinity-clusterip-transition-q7cpq"
Jun 28 08:03:34.771: INFO: Received response from host: affinity-clusterip-transition-msngx
Jun 28 08:03:34.771: INFO: Received response from host: affinity-clusterip-transition-msngx
Jun 28 08:03:34.771: INFO: Received response from host: affinity-clusterip-transition-msngx
Jun 28 08:03:34.771: INFO: Received response from host: affinity-clusterip-transition-msngx
Jun 28 08:03:34.771: INFO: Received response from host: affinity-clusterip-transition-q7cpq
Jun 28 08:03:34.771: INFO: Received response from host: affinity-clusterip-transition-q7cpq
Jun 28 08:03:34.771: INFO: Received response from host: affinity-clusterip-transition-q7cpq
Jun 28 08:03:34.771: INFO: Received response from host: affinity-clusterip-transition-q7cpq
Jun 28 08:03:34.771: INFO: Received response from host: affinity-clusterip-transition-5tjdv
Jun 28 08:03:34.771: INFO: Received response from host: affinity-clusterip-transition-5tjdv
Jun 28 08:03:34.771: INFO: Received response from host: affinity-clusterip-transition-5tjdv
Jun 28 08:03:34.771: INFO: Received response from host: affinity-clusterip-transition-msngx
Jun 28 08:03:34.771: INFO: Received response from host: affinity-clusterip-transition-q7cpq
Jun 28 08:03:34.771: INFO: Received response from host: affinity-clusterip-transition-msngx
Jun 28 08:03:34.771: INFO: Received response from host: affinity-clusterip-transition-5tjdv
Jun 28 08:03:34.771: INFO: Received response from host: affinity-clusterip-transition-q7cpq
Jun 28 08:03:34.782: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-2072 exec execpod-affinitygbvl8 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.20.52.180:80/ ; done'
Jun 28 08:03:35.213: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.52.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.52.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.52.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.52.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.52.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.52.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.52.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.52.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.52.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.52.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.52.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.52.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.52.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.52.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.52.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.52.180:80/\n"
Jun 28 08:03:35.213: INFO: stdout: "\naffinity-clusterip-transition-5tjdv\naffinity-clusterip-transition-5tjdv\naffinity-clusterip-transition-5tjdv\naffinity-clusterip-transition-5tjdv\naffinity-clusterip-transition-5tjdv\naffinity-clusterip-transition-5tjdv\naffinity-clusterip-transition-5tjdv\naffinity-clusterip-transition-5tjdv\naffinity-clusterip-transition-5tjdv\naffinity-clusterip-transition-5tjdv\naffinity-clusterip-transition-5tjdv\naffinity-clusterip-transition-5tjdv\naffinity-clusterip-transition-5tjdv\naffinity-clusterip-transition-5tjdv\naffinity-clusterip-transition-5tjdv\naffinity-clusterip-transition-5tjdv"
Jun 28 08:03:35.213: INFO: Received response from host: affinity-clusterip-transition-5tjdv
Jun 28 08:03:35.213: INFO: Received response from host: affinity-clusterip-transition-5tjdv
Jun 28 08:03:35.213: INFO: Received response from host: affinity-clusterip-transition-5tjdv
Jun 28 08:03:35.213: INFO: Received response from host: affinity-clusterip-transition-5tjdv
Jun 28 08:03:35.213: INFO: Received response from host: affinity-clusterip-transition-5tjdv
Jun 28 08:03:35.213: INFO: Received response from host: affinity-clusterip-transition-5tjdv
Jun 28 08:03:35.213: INFO: Received response from host: affinity-clusterip-transition-5tjdv
Jun 28 08:03:35.213: INFO: Received response from host: affinity-clusterip-transition-5tjdv
Jun 28 08:03:35.213: INFO: Received response from host: affinity-clusterip-transition-5tjdv
Jun 28 08:03:35.213: INFO: Received response from host: affinity-clusterip-transition-5tjdv
Jun 28 08:03:35.213: INFO: Received response from host: affinity-clusterip-transition-5tjdv
Jun 28 08:03:35.213: INFO: Received response from host: affinity-clusterip-transition-5tjdv
Jun 28 08:03:35.213: INFO: Received response from host: affinity-clusterip-transition-5tjdv
Jun 28 08:03:35.213: INFO: Received response from host: affinity-clusterip-transition-5tjdv
Jun 28 08:03:35.213: INFO: Received response from host: affinity-clusterip-transition-5tjdv
Jun 28 08:03:35.213: INFO: Received response from host: affinity-clusterip-transition-5tjdv
Jun 28 08:03:35.213: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-2072, will wait for the garbage collector to delete the pods 06/28/23 08:03:35.223
Jun 28 08:03:35.285: INFO: Deleting ReplicationController affinity-clusterip-transition took: 7.273402ms
Jun 28 08:03:35.385: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.14758ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jun 28 08:03:37.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2072" for this suite. 06/28/23 08:03:37.708
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","completed":88,"skipped":1617,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.428 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2179

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:03:27.286
    Jun 28 08:03:27.286: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename services 06/28/23 08:03:27.287
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:03:27.299
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:03:27.302
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2179
    STEP: creating service in namespace services-2072 06/28/23 08:03:27.306
    STEP: creating service affinity-clusterip-transition in namespace services-2072 06/28/23 08:03:27.307
    STEP: creating replication controller affinity-clusterip-transition in namespace services-2072 06/28/23 08:03:27.316
    I0628 08:03:27.321159      18 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-2072, replica count: 3
    I0628 08:03:30.372011      18 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jun 28 08:03:30.379: INFO: Creating new exec pod
    Jun 28 08:03:30.385: INFO: Waiting up to 5m0s for pod "execpod-affinitygbvl8" in namespace "services-2072" to be "running"
    Jun 28 08:03:30.389: INFO: Pod "execpod-affinitygbvl8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.671121ms
    Jun 28 08:03:32.393: INFO: Pod "execpod-affinitygbvl8": Phase="Running", Reason="", readiness=true. Elapsed: 2.008090498s
    Jun 28 08:03:32.393: INFO: Pod "execpod-affinitygbvl8" satisfied condition "running"
    Jun 28 08:03:33.393: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-2072 exec execpod-affinitygbvl8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
    Jun 28 08:03:33.830: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
    Jun 28 08:03:33.831: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jun 28 08:03:33.831: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-2072 exec execpod-affinitygbvl8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.20.52.180 80'
    Jun 28 08:03:34.221: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.20.52.180 80\nConnection to 172.20.52.180 80 port [tcp/http] succeeded!\n"
    Jun 28 08:03:34.221: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jun 28 08:03:34.234: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-2072 exec execpod-affinitygbvl8 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.20.52.180:80/ ; done'
    Jun 28 08:03:34.771: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.52.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.52.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.52.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.52.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.52.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.52.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.52.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.52.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.52.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.52.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.52.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.52.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.52.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.52.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.52.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.52.180:80/\n"
    Jun 28 08:03:34.771: INFO: stdout: "\naffinity-clusterip-transition-msngx\naffinity-clusterip-transition-msngx\naffinity-clusterip-transition-msngx\naffinity-clusterip-transition-msngx\naffinity-clusterip-transition-q7cpq\naffinity-clusterip-transition-q7cpq\naffinity-clusterip-transition-q7cpq\naffinity-clusterip-transition-q7cpq\naffinity-clusterip-transition-5tjdv\naffinity-clusterip-transition-5tjdv\naffinity-clusterip-transition-5tjdv\naffinity-clusterip-transition-msngx\naffinity-clusterip-transition-q7cpq\naffinity-clusterip-transition-msngx\naffinity-clusterip-transition-5tjdv\naffinity-clusterip-transition-q7cpq"
    Jun 28 08:03:34.771: INFO: Received response from host: affinity-clusterip-transition-msngx
    Jun 28 08:03:34.771: INFO: Received response from host: affinity-clusterip-transition-msngx
    Jun 28 08:03:34.771: INFO: Received response from host: affinity-clusterip-transition-msngx
    Jun 28 08:03:34.771: INFO: Received response from host: affinity-clusterip-transition-msngx
    Jun 28 08:03:34.771: INFO: Received response from host: affinity-clusterip-transition-q7cpq
    Jun 28 08:03:34.771: INFO: Received response from host: affinity-clusterip-transition-q7cpq
    Jun 28 08:03:34.771: INFO: Received response from host: affinity-clusterip-transition-q7cpq
    Jun 28 08:03:34.771: INFO: Received response from host: affinity-clusterip-transition-q7cpq
    Jun 28 08:03:34.771: INFO: Received response from host: affinity-clusterip-transition-5tjdv
    Jun 28 08:03:34.771: INFO: Received response from host: affinity-clusterip-transition-5tjdv
    Jun 28 08:03:34.771: INFO: Received response from host: affinity-clusterip-transition-5tjdv
    Jun 28 08:03:34.771: INFO: Received response from host: affinity-clusterip-transition-msngx
    Jun 28 08:03:34.771: INFO: Received response from host: affinity-clusterip-transition-q7cpq
    Jun 28 08:03:34.771: INFO: Received response from host: affinity-clusterip-transition-msngx
    Jun 28 08:03:34.771: INFO: Received response from host: affinity-clusterip-transition-5tjdv
    Jun 28 08:03:34.771: INFO: Received response from host: affinity-clusterip-transition-q7cpq
    Jun 28 08:03:34.782: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-2072 exec execpod-affinitygbvl8 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.20.52.180:80/ ; done'
    Jun 28 08:03:35.213: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.52.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.52.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.52.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.52.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.52.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.52.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.52.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.52.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.52.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.52.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.52.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.52.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.52.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.52.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.52.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.52.180:80/\n"
    Jun 28 08:03:35.213: INFO: stdout: "\naffinity-clusterip-transition-5tjdv\naffinity-clusterip-transition-5tjdv\naffinity-clusterip-transition-5tjdv\naffinity-clusterip-transition-5tjdv\naffinity-clusterip-transition-5tjdv\naffinity-clusterip-transition-5tjdv\naffinity-clusterip-transition-5tjdv\naffinity-clusterip-transition-5tjdv\naffinity-clusterip-transition-5tjdv\naffinity-clusterip-transition-5tjdv\naffinity-clusterip-transition-5tjdv\naffinity-clusterip-transition-5tjdv\naffinity-clusterip-transition-5tjdv\naffinity-clusterip-transition-5tjdv\naffinity-clusterip-transition-5tjdv\naffinity-clusterip-transition-5tjdv"
    Jun 28 08:03:35.213: INFO: Received response from host: affinity-clusterip-transition-5tjdv
    Jun 28 08:03:35.213: INFO: Received response from host: affinity-clusterip-transition-5tjdv
    Jun 28 08:03:35.213: INFO: Received response from host: affinity-clusterip-transition-5tjdv
    Jun 28 08:03:35.213: INFO: Received response from host: affinity-clusterip-transition-5tjdv
    Jun 28 08:03:35.213: INFO: Received response from host: affinity-clusterip-transition-5tjdv
    Jun 28 08:03:35.213: INFO: Received response from host: affinity-clusterip-transition-5tjdv
    Jun 28 08:03:35.213: INFO: Received response from host: affinity-clusterip-transition-5tjdv
    Jun 28 08:03:35.213: INFO: Received response from host: affinity-clusterip-transition-5tjdv
    Jun 28 08:03:35.213: INFO: Received response from host: affinity-clusterip-transition-5tjdv
    Jun 28 08:03:35.213: INFO: Received response from host: affinity-clusterip-transition-5tjdv
    Jun 28 08:03:35.213: INFO: Received response from host: affinity-clusterip-transition-5tjdv
    Jun 28 08:03:35.213: INFO: Received response from host: affinity-clusterip-transition-5tjdv
    Jun 28 08:03:35.213: INFO: Received response from host: affinity-clusterip-transition-5tjdv
    Jun 28 08:03:35.213: INFO: Received response from host: affinity-clusterip-transition-5tjdv
    Jun 28 08:03:35.213: INFO: Received response from host: affinity-clusterip-transition-5tjdv
    Jun 28 08:03:35.213: INFO: Received response from host: affinity-clusterip-transition-5tjdv
    Jun 28 08:03:35.213: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-2072, will wait for the garbage collector to delete the pods 06/28/23 08:03:35.223
    Jun 28 08:03:35.285: INFO: Deleting ReplicationController affinity-clusterip-transition took: 7.273402ms
    Jun 28 08:03:35.385: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.14758ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jun 28 08:03:37.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-2072" for this suite. 06/28/23 08:03:37.708
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:03:37.715
Jun 28 08:03:37.715: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename projected 06/28/23 08:03:37.716
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:03:37.727
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:03:37.733
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83
STEP: Creating a pod to test downward API volume plugin 06/28/23 08:03:37.737
Jun 28 08:03:37.746: INFO: Waiting up to 5m0s for pod "downwardapi-volume-526a4159-5dfa-4a6c-a6ef-a203004c8508" in namespace "projected-183" to be "Succeeded or Failed"
Jun 28 08:03:37.750: INFO: Pod "downwardapi-volume-526a4159-5dfa-4a6c-a6ef-a203004c8508": Phase="Pending", Reason="", readiness=false. Elapsed: 4.699896ms
Jun 28 08:03:39.756: INFO: Pod "downwardapi-volume-526a4159-5dfa-4a6c-a6ef-a203004c8508": Phase="Running", Reason="", readiness=false. Elapsed: 2.010450934s
Jun 28 08:03:41.756: INFO: Pod "downwardapi-volume-526a4159-5dfa-4a6c-a6ef-a203004c8508": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010665904s
STEP: Saw pod success 06/28/23 08:03:41.756
Jun 28 08:03:41.757: INFO: Pod "downwardapi-volume-526a4159-5dfa-4a6c-a6ef-a203004c8508" satisfied condition "Succeeded or Failed"
Jun 28 08:03:41.761: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod downwardapi-volume-526a4159-5dfa-4a6c-a6ef-a203004c8508 container client-container: <nil>
STEP: delete the pod 06/28/23 08:03:41.77
Jun 28 08:03:41.779: INFO: Waiting for pod downwardapi-volume-526a4159-5dfa-4a6c-a6ef-a203004c8508 to disappear
Jun 28 08:03:41.783: INFO: Pod downwardapi-volume-526a4159-5dfa-4a6c-a6ef-a203004c8508 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jun 28 08:03:41.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-183" for this suite. 06/28/23 08:03:41.79
{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","completed":89,"skipped":1621,"failed":0}
------------------------------
â€¢ [4.082 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:03:37.715
    Jun 28 08:03:37.715: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename projected 06/28/23 08:03:37.716
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:03:37.727
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:03:37.733
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:83
    STEP: Creating a pod to test downward API volume plugin 06/28/23 08:03:37.737
    Jun 28 08:03:37.746: INFO: Waiting up to 5m0s for pod "downwardapi-volume-526a4159-5dfa-4a6c-a6ef-a203004c8508" in namespace "projected-183" to be "Succeeded or Failed"
    Jun 28 08:03:37.750: INFO: Pod "downwardapi-volume-526a4159-5dfa-4a6c-a6ef-a203004c8508": Phase="Pending", Reason="", readiness=false. Elapsed: 4.699896ms
    Jun 28 08:03:39.756: INFO: Pod "downwardapi-volume-526a4159-5dfa-4a6c-a6ef-a203004c8508": Phase="Running", Reason="", readiness=false. Elapsed: 2.010450934s
    Jun 28 08:03:41.756: INFO: Pod "downwardapi-volume-526a4159-5dfa-4a6c-a6ef-a203004c8508": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010665904s
    STEP: Saw pod success 06/28/23 08:03:41.756
    Jun 28 08:03:41.757: INFO: Pod "downwardapi-volume-526a4159-5dfa-4a6c-a6ef-a203004c8508" satisfied condition "Succeeded or Failed"
    Jun 28 08:03:41.761: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod downwardapi-volume-526a4159-5dfa-4a6c-a6ef-a203004c8508 container client-container: <nil>
    STEP: delete the pod 06/28/23 08:03:41.77
    Jun 28 08:03:41.779: INFO: Waiting for pod downwardapi-volume-526a4159-5dfa-4a6c-a6ef-a203004c8508 to disappear
    Jun 28 08:03:41.783: INFO: Pod downwardapi-volume-526a4159-5dfa-4a6c-a6ef-a203004c8508 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jun 28 08:03:41.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-183" for this suite. 06/28/23 08:03:41.79
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:03:41.797
Jun 28 08:03:41.798: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename daemonsets 06/28/23 08:03:41.799
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:03:41.811
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:03:41.814
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193
Jun 28 08:03:41.845: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes. 06/28/23 08:03:41.851
Jun 28 08:03:41.854: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 28 08:03:41.854: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched. 06/28/23 08:03:41.854
Jun 28 08:03:41.882: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 28 08:03:41.882: INFO: Node ske-rhel-749f7d55c8xdd8b6-srshq is running 0 daemon pod, expected 1
Jun 28 08:03:42.886: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jun 28 08:03:42.886: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled 06/28/23 08:03:42.889
Jun 28 08:03:42.911: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jun 28 08:03:42.911: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
Jun 28 08:03:43.917: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 28 08:03:43.917: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 06/28/23 08:03:43.917
Jun 28 08:03:43.930: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 28 08:03:43.930: INFO: Node ske-rhel-749f7d55c8xdd8b6-srshq is running 0 daemon pod, expected 1
Jun 28 08:03:44.935: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 28 08:03:44.935: INFO: Node ske-rhel-749f7d55c8xdd8b6-srshq is running 0 daemon pod, expected 1
Jun 28 08:03:45.936: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 28 08:03:45.936: INFO: Node ske-rhel-749f7d55c8xdd8b6-srshq is running 0 daemon pod, expected 1
Jun 28 08:03:46.934: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jun 28 08:03:46.934: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 06/28/23 08:03:46.944
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7155, will wait for the garbage collector to delete the pods 06/28/23 08:03:46.944
Jun 28 08:03:47.005: INFO: Deleting DaemonSet.extensions daemon-set took: 6.928993ms
Jun 28 08:03:47.106: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.728898ms
Jun 28 08:03:49.912: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 28 08:03:49.912: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jun 28 08:03:49.915: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"59831"},"items":null}

Jun 28 08:03:49.919: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"59831"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Jun 28 08:03:49.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7155" for this suite. 06/28/23 08:03:49.961
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","completed":90,"skipped":1637,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.171 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:03:41.797
    Jun 28 08:03:41.798: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename daemonsets 06/28/23 08:03:41.799
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:03:41.811
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:03:41.814
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should run and stop complex daemon [Conformance]
      test/e2e/apps/daemon_set.go:193
    Jun 28 08:03:41.845: INFO: Creating daemon "daemon-set" with a node selector
    STEP: Initially, daemon pods should not be running on any nodes. 06/28/23 08:03:41.851
    Jun 28 08:03:41.854: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun 28 08:03:41.854: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Change node label to blue, check that daemon pod is launched. 06/28/23 08:03:41.854
    Jun 28 08:03:41.882: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun 28 08:03:41.882: INFO: Node ske-rhel-749f7d55c8xdd8b6-srshq is running 0 daemon pod, expected 1
    Jun 28 08:03:42.886: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jun 28 08:03:42.886: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    STEP: Update the node label to green, and wait for daemons to be unscheduled 06/28/23 08:03:42.889
    Jun 28 08:03:42.911: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jun 28 08:03:42.911: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
    Jun 28 08:03:43.917: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun 28 08:03:43.917: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 06/28/23 08:03:43.917
    Jun 28 08:03:43.930: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun 28 08:03:43.930: INFO: Node ske-rhel-749f7d55c8xdd8b6-srshq is running 0 daemon pod, expected 1
    Jun 28 08:03:44.935: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun 28 08:03:44.935: INFO: Node ske-rhel-749f7d55c8xdd8b6-srshq is running 0 daemon pod, expected 1
    Jun 28 08:03:45.936: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun 28 08:03:45.936: INFO: Node ske-rhel-749f7d55c8xdd8b6-srshq is running 0 daemon pod, expected 1
    Jun 28 08:03:46.934: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jun 28 08:03:46.934: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 06/28/23 08:03:46.944
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7155, will wait for the garbage collector to delete the pods 06/28/23 08:03:46.944
    Jun 28 08:03:47.005: INFO: Deleting DaemonSet.extensions daemon-set took: 6.928993ms
    Jun 28 08:03:47.106: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.728898ms
    Jun 28 08:03:49.912: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun 28 08:03:49.912: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jun 28 08:03:49.915: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"59831"},"items":null}

    Jun 28 08:03:49.919: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"59831"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Jun 28 08:03:49.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-7155" for this suite. 06/28/23 08:03:49.961
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] CronJob
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:03:49.968
Jun 28 08:03:49.969: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename cronjob 06/28/23 08:03:49.969
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:03:49.982
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:03:49.986
[It] should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
STEP: Creating a suspended cronjob 06/28/23 08:03:49.992
STEP: Ensuring no jobs are scheduled 06/28/23 08:03:49.998
STEP: Ensuring no job exists by listing jobs explicitly 06/28/23 08:08:50.007
STEP: Removing cronjob 06/28/23 08:08:50.011
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Jun 28 08:08:50.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-8764" for this suite. 06/28/23 08:08:50.026
{"msg":"PASSED [sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]","completed":91,"skipped":1642,"failed":0}
------------------------------
â€¢ [SLOW TEST] [300.065 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:03:49.968
    Jun 28 08:03:49.969: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename cronjob 06/28/23 08:03:49.969
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:03:49.982
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:03:49.986
    [It] should not schedule jobs when suspended [Slow] [Conformance]
      test/e2e/apps/cronjob.go:96
    STEP: Creating a suspended cronjob 06/28/23 08:03:49.992
    STEP: Ensuring no jobs are scheduled 06/28/23 08:03:49.998
    STEP: Ensuring no job exists by listing jobs explicitly 06/28/23 08:08:50.007
    STEP: Removing cronjob 06/28/23 08:08:50.011
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Jun 28 08:08:50.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-8764" for this suite. 06/28/23 08:08:50.026
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Secrets
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:08:50.034
Jun 28 08:08:50.034: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename secrets 06/28/23 08:08:50.035
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:08:50.058
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:08:50.063
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jun 28 08:08:50.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8066" for this suite. 06/28/23 08:08:50.121
{"msg":"PASSED [sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]","completed":92,"skipped":1649,"failed":0}
------------------------------
â€¢ [0.093 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:08:50.034
    Jun 28 08:08:50.034: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename secrets 06/28/23 08:08:50.035
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:08:50.058
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:08:50.063
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/secrets_volume.go:385
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jun 28 08:08:50.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-8066" for this suite. 06/28/23 08:08:50.121
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:08:50.132
Jun 28 08:08:50.132: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename downward-api 06/28/23 08:08:50.133
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:08:50.145
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:08:50.15
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248
STEP: Creating a pod to test downward API volume plugin 06/28/23 08:08:50.154
Jun 28 08:08:50.162: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c8ea79dd-a992-4c3e-8234-f5a0f0b3873c" in namespace "downward-api-3658" to be "Succeeded or Failed"
Jun 28 08:08:50.168: INFO: Pod "downwardapi-volume-c8ea79dd-a992-4c3e-8234-f5a0f0b3873c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.684346ms
Jun 28 08:08:52.174: INFO: Pod "downwardapi-volume-c8ea79dd-a992-4c3e-8234-f5a0f0b3873c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01149664s
Jun 28 08:08:54.174: INFO: Pod "downwardapi-volume-c8ea79dd-a992-4c3e-8234-f5a0f0b3873c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011400206s
STEP: Saw pod success 06/28/23 08:08:54.174
Jun 28 08:08:54.174: INFO: Pod "downwardapi-volume-c8ea79dd-a992-4c3e-8234-f5a0f0b3873c" satisfied condition "Succeeded or Failed"
Jun 28 08:08:54.178: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod downwardapi-volume-c8ea79dd-a992-4c3e-8234-f5a0f0b3873c container client-container: <nil>
STEP: delete the pod 06/28/23 08:08:54.229
Jun 28 08:08:54.240: INFO: Waiting for pod downwardapi-volume-c8ea79dd-a992-4c3e-8234-f5a0f0b3873c to disappear
Jun 28 08:08:54.244: INFO: Pod downwardapi-volume-c8ea79dd-a992-4c3e-8234-f5a0f0b3873c no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jun 28 08:08:54.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3658" for this suite. 06/28/23 08:08:54.253
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","completed":93,"skipped":1698,"failed":0}
------------------------------
â€¢ [4.128 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:08:50.132
    Jun 28 08:08:50.132: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename downward-api 06/28/23 08:08:50.133
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:08:50.145
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:08:50.15
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:248
    STEP: Creating a pod to test downward API volume plugin 06/28/23 08:08:50.154
    Jun 28 08:08:50.162: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c8ea79dd-a992-4c3e-8234-f5a0f0b3873c" in namespace "downward-api-3658" to be "Succeeded or Failed"
    Jun 28 08:08:50.168: INFO: Pod "downwardapi-volume-c8ea79dd-a992-4c3e-8234-f5a0f0b3873c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.684346ms
    Jun 28 08:08:52.174: INFO: Pod "downwardapi-volume-c8ea79dd-a992-4c3e-8234-f5a0f0b3873c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01149664s
    Jun 28 08:08:54.174: INFO: Pod "downwardapi-volume-c8ea79dd-a992-4c3e-8234-f5a0f0b3873c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011400206s
    STEP: Saw pod success 06/28/23 08:08:54.174
    Jun 28 08:08:54.174: INFO: Pod "downwardapi-volume-c8ea79dd-a992-4c3e-8234-f5a0f0b3873c" satisfied condition "Succeeded or Failed"
    Jun 28 08:08:54.178: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod downwardapi-volume-c8ea79dd-a992-4c3e-8234-f5a0f0b3873c container client-container: <nil>
    STEP: delete the pod 06/28/23 08:08:54.229
    Jun 28 08:08:54.240: INFO: Waiting for pod downwardapi-volume-c8ea79dd-a992-4c3e-8234-f5a0f0b3873c to disappear
    Jun 28 08:08:54.244: INFO: Pod downwardapi-volume-c8ea79dd-a992-4c3e-8234-f5a0f0b3873c no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jun 28 08:08:54.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-3658" for this suite. 06/28/23 08:08:54.253
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:08:54.263
Jun 28 08:08:54.263: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename replication-controller 06/28/23 08:08:54.263
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:08:54.277
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:08:54.282
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109
STEP: creating a ReplicationController 06/28/23 08:08:54.291
STEP: waiting for RC to be added 06/28/23 08:08:54.297
STEP: waiting for available Replicas 06/28/23 08:08:54.297
STEP: patching ReplicationController 06/28/23 08:08:55.237
STEP: waiting for RC to be modified 06/28/23 08:08:55.247
STEP: patching ReplicationController status 06/28/23 08:08:55.247
STEP: waiting for RC to be modified 06/28/23 08:08:55.255
STEP: waiting for available Replicas 06/28/23 08:08:55.255
STEP: fetching ReplicationController status 06/28/23 08:08:55.259
STEP: patching ReplicationController scale 06/28/23 08:08:55.263
STEP: waiting for RC to be modified 06/28/23 08:08:55.271
STEP: waiting for ReplicationController's scale to be the max amount 06/28/23 08:08:55.271
STEP: fetching ReplicationController; ensuring that it's patched 06/28/23 08:08:56.271
STEP: updating ReplicationController status 06/28/23 08:08:56.275
STEP: waiting for RC to be modified 06/28/23 08:08:56.282
STEP: listing all ReplicationControllers 06/28/23 08:08:56.282
STEP: checking that ReplicationController has expected values 06/28/23 08:08:56.286
STEP: deleting ReplicationControllers by collection 06/28/23 08:08:56.286
STEP: waiting for ReplicationController to have a DELETED watchEvent 06/28/23 08:08:56.294
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Jun 28 08:08:56.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1895" for this suite. 06/28/23 08:08:56.358
{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","completed":94,"skipped":1759,"failed":0}
------------------------------
â€¢ [2.103 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:08:54.263
    Jun 28 08:08:54.263: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename replication-controller 06/28/23 08:08:54.263
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:08:54.277
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:08:54.282
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should test the lifecycle of a ReplicationController [Conformance]
      test/e2e/apps/rc.go:109
    STEP: creating a ReplicationController 06/28/23 08:08:54.291
    STEP: waiting for RC to be added 06/28/23 08:08:54.297
    STEP: waiting for available Replicas 06/28/23 08:08:54.297
    STEP: patching ReplicationController 06/28/23 08:08:55.237
    STEP: waiting for RC to be modified 06/28/23 08:08:55.247
    STEP: patching ReplicationController status 06/28/23 08:08:55.247
    STEP: waiting for RC to be modified 06/28/23 08:08:55.255
    STEP: waiting for available Replicas 06/28/23 08:08:55.255
    STEP: fetching ReplicationController status 06/28/23 08:08:55.259
    STEP: patching ReplicationController scale 06/28/23 08:08:55.263
    STEP: waiting for RC to be modified 06/28/23 08:08:55.271
    STEP: waiting for ReplicationController's scale to be the max amount 06/28/23 08:08:55.271
    STEP: fetching ReplicationController; ensuring that it's patched 06/28/23 08:08:56.271
    STEP: updating ReplicationController status 06/28/23 08:08:56.275
    STEP: waiting for RC to be modified 06/28/23 08:08:56.282
    STEP: listing all ReplicationControllers 06/28/23 08:08:56.282
    STEP: checking that ReplicationController has expected values 06/28/23 08:08:56.286
    STEP: deleting ReplicationControllers by collection 06/28/23 08:08:56.286
    STEP: waiting for ReplicationController to have a DELETED watchEvent 06/28/23 08:08:56.294
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Jun 28 08:08:56.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-1895" for this suite. 06/28/23 08:08:56.358
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Services
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3185
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:08:56.366
Jun 28 08:08:56.366: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename services 06/28/23 08:08:56.367
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:08:56.384
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:08:56.389
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3185
STEP: fetching services 06/28/23 08:08:56.394
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jun 28 08:08:56.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6452" for this suite. 06/28/23 08:08:56.41
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","completed":95,"skipped":1768,"failed":0}
------------------------------
â€¢ [0.051 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:08:56.366
    Jun 28 08:08:56.366: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename services 06/28/23 08:08:56.367
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:08:56.384
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:08:56.389
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should find a service from listing all namespaces [Conformance]
      test/e2e/network/service.go:3185
    STEP: fetching services 06/28/23 08:08:56.394
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jun 28 08:08:56.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-6452" for this suite. 06/28/23 08:08:56.41
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:08:56.418
Jun 28 08:08:56.418: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename containers 06/28/23 08:08:56.419
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:08:56.432
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:08:56.437
[It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58
STEP: Creating a pod to test override arguments 06/28/23 08:08:56.441
Jun 28 08:08:56.450: INFO: Waiting up to 5m0s for pod "client-containers-2e3516be-c543-47bb-a9d4-7aaaf8da898a" in namespace "containers-6130" to be "Succeeded or Failed"
Jun 28 08:08:56.455: INFO: Pod "client-containers-2e3516be-c543-47bb-a9d4-7aaaf8da898a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.961618ms
Jun 28 08:08:58.459: INFO: Pod "client-containers-2e3516be-c543-47bb-a9d4-7aaaf8da898a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009629591s
Jun 28 08:09:00.462: INFO: Pod "client-containers-2e3516be-c543-47bb-a9d4-7aaaf8da898a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011877337s
STEP: Saw pod success 06/28/23 08:09:00.462
Jun 28 08:09:00.462: INFO: Pod "client-containers-2e3516be-c543-47bb-a9d4-7aaaf8da898a" satisfied condition "Succeeded or Failed"
Jun 28 08:09:00.466: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod client-containers-2e3516be-c543-47bb-a9d4-7aaaf8da898a container agnhost-container: <nil>
STEP: delete the pod 06/28/23 08:09:00.476
Jun 28 08:09:00.489: INFO: Waiting for pod client-containers-2e3516be-c543-47bb-a9d4-7aaaf8da898a to disappear
Jun 28 08:09:00.492: INFO: Pod client-containers-2e3516be-c543-47bb-a9d4-7aaaf8da898a no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Jun 28 08:09:00.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-6130" for this suite. 06/28/23 08:09:00.5
{"msg":"PASSED [sig-node] Containers should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]","completed":96,"skipped":1784,"failed":0}
------------------------------
â€¢ [4.088 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:08:56.418
    Jun 28 08:08:56.418: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename containers 06/28/23 08:08:56.419
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:08:56.432
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:08:56.437
    [It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:58
    STEP: Creating a pod to test override arguments 06/28/23 08:08:56.441
    Jun 28 08:08:56.450: INFO: Waiting up to 5m0s for pod "client-containers-2e3516be-c543-47bb-a9d4-7aaaf8da898a" in namespace "containers-6130" to be "Succeeded or Failed"
    Jun 28 08:08:56.455: INFO: Pod "client-containers-2e3516be-c543-47bb-a9d4-7aaaf8da898a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.961618ms
    Jun 28 08:08:58.459: INFO: Pod "client-containers-2e3516be-c543-47bb-a9d4-7aaaf8da898a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009629591s
    Jun 28 08:09:00.462: INFO: Pod "client-containers-2e3516be-c543-47bb-a9d4-7aaaf8da898a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011877337s
    STEP: Saw pod success 06/28/23 08:09:00.462
    Jun 28 08:09:00.462: INFO: Pod "client-containers-2e3516be-c543-47bb-a9d4-7aaaf8da898a" satisfied condition "Succeeded or Failed"
    Jun 28 08:09:00.466: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod client-containers-2e3516be-c543-47bb-a9d4-7aaaf8da898a container agnhost-container: <nil>
    STEP: delete the pod 06/28/23 08:09:00.476
    Jun 28 08:09:00.489: INFO: Waiting for pod client-containers-2e3516be-c543-47bb-a9d4-7aaaf8da898a to disappear
    Jun 28 08:09:00.492: INFO: Pod client-containers-2e3516be-c543-47bb-a9d4-7aaaf8da898a no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Jun 28 08:09:00.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-6130" for this suite. 06/28/23 08:09:00.5
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Services
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:781
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:09:00.506
Jun 28 08:09:00.507: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename services 06/28/23 08:09:00.508
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:09:00.523
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:09:00.527
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should provide secure master service  [Conformance]
  test/e2e/network/service.go:781
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jun 28 08:09:00.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3408" for this suite. 06/28/23 08:09:00.543
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","completed":97,"skipped":1793,"failed":0}
------------------------------
â€¢ [0.044 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:781

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:09:00.506
    Jun 28 08:09:00.507: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename services 06/28/23 08:09:00.508
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:09:00.523
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:09:00.527
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should provide secure master service  [Conformance]
      test/e2e/network/service.go:781
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jun 28 08:09:00.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-3408" for this suite. 06/28/23 08:09:00.543
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run
  should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:960
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:09:00.55
Jun 28 08:09:00.551: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename kubectl 06/28/23 08:09:00.551
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:09:00.566
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:09:00.569
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:960
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 06/28/23 08:09:00.574
Jun 28 08:09:00.574: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-7851 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Jun 28 08:09:00.686: INFO: stderr: ""
Jun 28 08:09:00.686: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run 06/28/23 08:09:00.686
Jun 28 08:09:00.686: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-7851 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-2"}]}} --dry-run=server'
Jun 28 08:09:00.912: INFO: stderr: ""
Jun 28 08:09:00.912: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 06/28/23 08:09:00.912
Jun 28 08:09:00.917: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-7851 delete pods e2e-test-httpd-pod'
Jun 28 08:09:03.285: INFO: stderr: ""
Jun 28 08:09:03.285: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jun 28 08:09:03.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7851" for this suite. 06/28/23 08:09:03.293
{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","completed":98,"skipped":1800,"failed":0}
------------------------------
â€¢ [2.750 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  test/e2e/kubectl/kubectl.go:954
    should check if kubectl can dry-run update Pods [Conformance]
    test/e2e/kubectl/kubectl.go:960

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:09:00.55
    Jun 28 08:09:00.551: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename kubectl 06/28/23 08:09:00.551
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:09:00.566
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:09:00.569
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl can dry-run update Pods [Conformance]
      test/e2e/kubectl/kubectl.go:960
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 06/28/23 08:09:00.574
    Jun 28 08:09:00.574: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-7851 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Jun 28 08:09:00.686: INFO: stderr: ""
    Jun 28 08:09:00.686: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: replace the image in the pod with server-side dry-run 06/28/23 08:09:00.686
    Jun 28 08:09:00.686: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-7851 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-2"}]}} --dry-run=server'
    Jun 28 08:09:00.912: INFO: stderr: ""
    Jun 28 08:09:00.912: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 06/28/23 08:09:00.912
    Jun 28 08:09:00.917: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-7851 delete pods e2e-test-httpd-pod'
    Jun 28 08:09:03.285: INFO: stderr: ""
    Jun 28 08:09:03.285: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jun 28 08:09:03.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-7851" for this suite. 06/28/23 08:09:03.293
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:09:03.301
Jun 28 08:09:03.301: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename gc 06/28/23 08:09:03.302
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:09:03.314
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:09:03.318
[It] should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
STEP: create the rc 06/28/23 08:09:03.329
STEP: delete the rc 06/28/23 08:09:08.343
STEP: wait for the rc to be deleted 06/28/23 08:09:08.35
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 06/28/23 08:09:13.355
STEP: Gathering metrics 06/28/23 08:09:43.371
W0628 08:09:43.382702      18 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jun 28 08:09:43.382: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Jun 28 08:09:43.382: INFO: Deleting pod "simpletest.rc-2727n" in namespace "gc-50"
Jun 28 08:09:43.395: INFO: Deleting pod "simpletest.rc-2xqt4" in namespace "gc-50"
Jun 28 08:09:43.407: INFO: Deleting pod "simpletest.rc-4295v" in namespace "gc-50"
Jun 28 08:09:43.417: INFO: Deleting pod "simpletest.rc-4mlmv" in namespace "gc-50"
Jun 28 08:09:43.433: INFO: Deleting pod "simpletest.rc-4mrrh" in namespace "gc-50"
Jun 28 08:09:43.444: INFO: Deleting pod "simpletest.rc-54wp6" in namespace "gc-50"
Jun 28 08:09:43.459: INFO: Deleting pod "simpletest.rc-57b2j" in namespace "gc-50"
Jun 28 08:09:43.472: INFO: Deleting pod "simpletest.rc-5ggw5" in namespace "gc-50"
Jun 28 08:09:43.487: INFO: Deleting pod "simpletest.rc-5mbb6" in namespace "gc-50"
Jun 28 08:09:43.519: INFO: Deleting pod "simpletest.rc-5sht9" in namespace "gc-50"
Jun 28 08:09:43.533: INFO: Deleting pod "simpletest.rc-62bhn" in namespace "gc-50"
Jun 28 08:09:43.545: INFO: Deleting pod "simpletest.rc-654p4" in namespace "gc-50"
Jun 28 08:09:43.559: INFO: Deleting pod "simpletest.rc-6bzjm" in namespace "gc-50"
Jun 28 08:09:43.572: INFO: Deleting pod "simpletest.rc-6dcjg" in namespace "gc-50"
Jun 28 08:09:43.583: INFO: Deleting pod "simpletest.rc-7gh8t" in namespace "gc-50"
Jun 28 08:09:43.593: INFO: Deleting pod "simpletest.rc-7h94x" in namespace "gc-50"
Jun 28 08:09:43.634: INFO: Deleting pod "simpletest.rc-7hzfc" in namespace "gc-50"
Jun 28 08:09:43.681: INFO: Deleting pod "simpletest.rc-7lr9z" in namespace "gc-50"
Jun 28 08:09:43.696: INFO: Deleting pod "simpletest.rc-7vknk" in namespace "gc-50"
Jun 28 08:09:43.708: INFO: Deleting pod "simpletest.rc-7vvtj" in namespace "gc-50"
Jun 28 08:09:43.720: INFO: Deleting pod "simpletest.rc-7vzpx" in namespace "gc-50"
Jun 28 08:09:43.732: INFO: Deleting pod "simpletest.rc-7x4tv" in namespace "gc-50"
Jun 28 08:09:43.744: INFO: Deleting pod "simpletest.rc-8bj7n" in namespace "gc-50"
Jun 28 08:09:43.759: INFO: Deleting pod "simpletest.rc-8qqvb" in namespace "gc-50"
Jun 28 08:09:43.770: INFO: Deleting pod "simpletest.rc-8zjtm" in namespace "gc-50"
Jun 28 08:09:43.781: INFO: Deleting pod "simpletest.rc-97tff" in namespace "gc-50"
Jun 28 08:09:43.794: INFO: Deleting pod "simpletest.rc-9cm92" in namespace "gc-50"
Jun 28 08:09:43.805: INFO: Deleting pod "simpletest.rc-9mbcs" in namespace "gc-50"
Jun 28 08:09:43.814: INFO: Deleting pod "simpletest.rc-9phqd" in namespace "gc-50"
Jun 28 08:09:43.825: INFO: Deleting pod "simpletest.rc-9r2jx" in namespace "gc-50"
Jun 28 08:09:43.834: INFO: Deleting pod "simpletest.rc-b4v5x" in namespace "gc-50"
Jun 28 08:09:43.856: INFO: Deleting pod "simpletest.rc-b9gbs" in namespace "gc-50"
Jun 28 08:09:43.869: INFO: Deleting pod "simpletest.rc-bnkd7" in namespace "gc-50"
Jun 28 08:09:43.881: INFO: Deleting pod "simpletest.rc-brdgv" in namespace "gc-50"
Jun 28 08:09:43.912: INFO: Deleting pod "simpletest.rc-c6t4l" in namespace "gc-50"
Jun 28 08:09:43.943: INFO: Deleting pod "simpletest.rc-cgfrn" in namespace "gc-50"
Jun 28 08:09:43.973: INFO: Deleting pod "simpletest.rc-chfkp" in namespace "gc-50"
Jun 28 08:09:43.982: INFO: Deleting pod "simpletest.rc-cvjwl" in namespace "gc-50"
Jun 28 08:09:44.006: INFO: Deleting pod "simpletest.rc-cw8zs" in namespace "gc-50"
Jun 28 08:09:44.028: INFO: Deleting pod "simpletest.rc-czt8p" in namespace "gc-50"
Jun 28 08:09:44.060: INFO: Deleting pod "simpletest.rc-dg5rx" in namespace "gc-50"
Jun 28 08:09:44.070: INFO: Deleting pod "simpletest.rc-dhtgs" in namespace "gc-50"
Jun 28 08:09:44.108: INFO: Deleting pod "simpletest.rc-dvw59" in namespace "gc-50"
Jun 28 08:09:44.148: INFO: Deleting pod "simpletest.rc-f2shd" in namespace "gc-50"
Jun 28 08:09:44.172: INFO: Deleting pod "simpletest.rc-fzsxt" in namespace "gc-50"
Jun 28 08:09:44.188: INFO: Deleting pod "simpletest.rc-g5jxn" in namespace "gc-50"
Jun 28 08:09:44.199: INFO: Deleting pod "simpletest.rc-gs22l" in namespace "gc-50"
Jun 28 08:09:44.209: INFO: Deleting pod "simpletest.rc-gtgbw" in namespace "gc-50"
Jun 28 08:09:44.223: INFO: Deleting pod "simpletest.rc-h6vtt" in namespace "gc-50"
Jun 28 08:09:44.240: INFO: Deleting pod "simpletest.rc-h7r89" in namespace "gc-50"
Jun 28 08:09:44.253: INFO: Deleting pod "simpletest.rc-hlj2n" in namespace "gc-50"
Jun 28 08:09:44.265: INFO: Deleting pod "simpletest.rc-j7h5b" in namespace "gc-50"
Jun 28 08:09:44.279: INFO: Deleting pod "simpletest.rc-j9c6r" in namespace "gc-50"
Jun 28 08:09:44.290: INFO: Deleting pod "simpletest.rc-jllg8" in namespace "gc-50"
Jun 28 08:09:44.306: INFO: Deleting pod "simpletest.rc-jmhd4" in namespace "gc-50"
Jun 28 08:09:44.339: INFO: Deleting pod "simpletest.rc-jqcc9" in namespace "gc-50"
Jun 28 08:09:44.352: INFO: Deleting pod "simpletest.rc-jrxm6" in namespace "gc-50"
Jun 28 08:09:44.366: INFO: Deleting pod "simpletest.rc-kbs6n" in namespace "gc-50"
Jun 28 08:09:44.431: INFO: Deleting pod "simpletest.rc-kskmr" in namespace "gc-50"
Jun 28 08:09:44.468: INFO: Deleting pod "simpletest.rc-kv94c" in namespace "gc-50"
Jun 28 08:09:44.484: INFO: Deleting pod "simpletest.rc-l2s8t" in namespace "gc-50"
Jun 28 08:09:44.495: INFO: Deleting pod "simpletest.rc-lc2mv" in namespace "gc-50"
Jun 28 08:09:44.537: INFO: Deleting pod "simpletest.rc-lgw6g" in namespace "gc-50"
Jun 28 08:09:44.552: INFO: Deleting pod "simpletest.rc-lt6zn" in namespace "gc-50"
Jun 28 08:09:44.568: INFO: Deleting pod "simpletest.rc-m8s9g" in namespace "gc-50"
Jun 28 08:09:44.586: INFO: Deleting pod "simpletest.rc-m9hnx" in namespace "gc-50"
Jun 28 08:09:44.603: INFO: Deleting pod "simpletest.rc-mmgl2" in namespace "gc-50"
Jun 28 08:09:44.616: INFO: Deleting pod "simpletest.rc-n8snm" in namespace "gc-50"
Jun 28 08:09:44.628: INFO: Deleting pod "simpletest.rc-nbxc4" in namespace "gc-50"
Jun 28 08:09:44.655: INFO: Deleting pod "simpletest.rc-nkhlw" in namespace "gc-50"
Jun 28 08:09:44.669: INFO: Deleting pod "simpletest.rc-nwhw2" in namespace "gc-50"
Jun 28 08:09:44.684: INFO: Deleting pod "simpletest.rc-p4k9x" in namespace "gc-50"
Jun 28 08:09:44.698: INFO: Deleting pod "simpletest.rc-p984j" in namespace "gc-50"
Jun 28 08:09:44.707: INFO: Deleting pod "simpletest.rc-ps7kz" in namespace "gc-50"
Jun 28 08:09:44.719: INFO: Deleting pod "simpletest.rc-ptwkm" in namespace "gc-50"
Jun 28 08:09:44.731: INFO: Deleting pod "simpletest.rc-q4lbh" in namespace "gc-50"
Jun 28 08:09:44.778: INFO: Deleting pod "simpletest.rc-q8687" in namespace "gc-50"
Jun 28 08:09:44.821: INFO: Deleting pod "simpletest.rc-q8dc9" in namespace "gc-50"
Jun 28 08:09:44.886: INFO: Deleting pod "simpletest.rc-qjtkr" in namespace "gc-50"
Jun 28 08:09:44.956: INFO: Deleting pod "simpletest.rc-qz5jj" in namespace "gc-50"
Jun 28 08:09:44.986: INFO: Deleting pod "simpletest.rc-rb6z8" in namespace "gc-50"
Jun 28 08:09:45.027: INFO: Deleting pod "simpletest.rc-rl8dn" in namespace "gc-50"
Jun 28 08:09:45.072: INFO: Deleting pod "simpletest.rc-s2wwv" in namespace "gc-50"
Jun 28 08:09:45.118: INFO: Deleting pod "simpletest.rc-sb49v" in namespace "gc-50"
Jun 28 08:09:45.170: INFO: Deleting pod "simpletest.rc-slgmn" in namespace "gc-50"
Jun 28 08:09:45.223: INFO: Deleting pod "simpletest.rc-tcdhp" in namespace "gc-50"
Jun 28 08:09:45.268: INFO: Deleting pod "simpletest.rc-tkmsn" in namespace "gc-50"
Jun 28 08:09:45.324: INFO: Deleting pod "simpletest.rc-txpv8" in namespace "gc-50"
Jun 28 08:09:45.368: INFO: Deleting pod "simpletest.rc-v7hh5" in namespace "gc-50"
Jun 28 08:09:45.419: INFO: Deleting pod "simpletest.rc-vl5pr" in namespace "gc-50"
Jun 28 08:09:45.477: INFO: Deleting pod "simpletest.rc-w8ph5" in namespace "gc-50"
Jun 28 08:09:45.526: INFO: Deleting pod "simpletest.rc-wm4cg" in namespace "gc-50"
Jun 28 08:09:45.569: INFO: Deleting pod "simpletest.rc-x2hdr" in namespace "gc-50"
Jun 28 08:09:45.623: INFO: Deleting pod "simpletest.rc-xl975" in namespace "gc-50"
Jun 28 08:09:45.669: INFO: Deleting pod "simpletest.rc-xp79c" in namespace "gc-50"
Jun 28 08:09:45.719: INFO: Deleting pod "simpletest.rc-xr4g6" in namespace "gc-50"
Jun 28 08:09:45.777: INFO: Deleting pod "simpletest.rc-xt4gc" in namespace "gc-50"
Jun 28 08:09:45.819: INFO: Deleting pod "simpletest.rc-xz7j9" in namespace "gc-50"
Jun 28 08:09:45.873: INFO: Deleting pod "simpletest.rc-z8qkq" in namespace "gc-50"
Jun 28 08:09:45.919: INFO: Deleting pod "simpletest.rc-zkvmd" in namespace "gc-50"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Jun 28 08:09:45.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-50" for this suite. 06/28/23 08:09:46.018
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","completed":99,"skipped":1823,"failed":0}
------------------------------
â€¢ [SLOW TEST] [42.762 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:09:03.301
    Jun 28 08:09:03.301: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename gc 06/28/23 08:09:03.302
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:09:03.314
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:09:03.318
    [It] should orphan pods created by rc if delete options say so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:370
    STEP: create the rc 06/28/23 08:09:03.329
    STEP: delete the rc 06/28/23 08:09:08.343
    STEP: wait for the rc to be deleted 06/28/23 08:09:08.35
    STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 06/28/23 08:09:13.355
    STEP: Gathering metrics 06/28/23 08:09:43.371
    W0628 08:09:43.382702      18 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Jun 28 08:09:43.382: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Jun 28 08:09:43.382: INFO: Deleting pod "simpletest.rc-2727n" in namespace "gc-50"
    Jun 28 08:09:43.395: INFO: Deleting pod "simpletest.rc-2xqt4" in namespace "gc-50"
    Jun 28 08:09:43.407: INFO: Deleting pod "simpletest.rc-4295v" in namespace "gc-50"
    Jun 28 08:09:43.417: INFO: Deleting pod "simpletest.rc-4mlmv" in namespace "gc-50"
    Jun 28 08:09:43.433: INFO: Deleting pod "simpletest.rc-4mrrh" in namespace "gc-50"
    Jun 28 08:09:43.444: INFO: Deleting pod "simpletest.rc-54wp6" in namespace "gc-50"
    Jun 28 08:09:43.459: INFO: Deleting pod "simpletest.rc-57b2j" in namespace "gc-50"
    Jun 28 08:09:43.472: INFO: Deleting pod "simpletest.rc-5ggw5" in namespace "gc-50"
    Jun 28 08:09:43.487: INFO: Deleting pod "simpletest.rc-5mbb6" in namespace "gc-50"
    Jun 28 08:09:43.519: INFO: Deleting pod "simpletest.rc-5sht9" in namespace "gc-50"
    Jun 28 08:09:43.533: INFO: Deleting pod "simpletest.rc-62bhn" in namespace "gc-50"
    Jun 28 08:09:43.545: INFO: Deleting pod "simpletest.rc-654p4" in namespace "gc-50"
    Jun 28 08:09:43.559: INFO: Deleting pod "simpletest.rc-6bzjm" in namespace "gc-50"
    Jun 28 08:09:43.572: INFO: Deleting pod "simpletest.rc-6dcjg" in namespace "gc-50"
    Jun 28 08:09:43.583: INFO: Deleting pod "simpletest.rc-7gh8t" in namespace "gc-50"
    Jun 28 08:09:43.593: INFO: Deleting pod "simpletest.rc-7h94x" in namespace "gc-50"
    Jun 28 08:09:43.634: INFO: Deleting pod "simpletest.rc-7hzfc" in namespace "gc-50"
    Jun 28 08:09:43.681: INFO: Deleting pod "simpletest.rc-7lr9z" in namespace "gc-50"
    Jun 28 08:09:43.696: INFO: Deleting pod "simpletest.rc-7vknk" in namespace "gc-50"
    Jun 28 08:09:43.708: INFO: Deleting pod "simpletest.rc-7vvtj" in namespace "gc-50"
    Jun 28 08:09:43.720: INFO: Deleting pod "simpletest.rc-7vzpx" in namespace "gc-50"
    Jun 28 08:09:43.732: INFO: Deleting pod "simpletest.rc-7x4tv" in namespace "gc-50"
    Jun 28 08:09:43.744: INFO: Deleting pod "simpletest.rc-8bj7n" in namespace "gc-50"
    Jun 28 08:09:43.759: INFO: Deleting pod "simpletest.rc-8qqvb" in namespace "gc-50"
    Jun 28 08:09:43.770: INFO: Deleting pod "simpletest.rc-8zjtm" in namespace "gc-50"
    Jun 28 08:09:43.781: INFO: Deleting pod "simpletest.rc-97tff" in namespace "gc-50"
    Jun 28 08:09:43.794: INFO: Deleting pod "simpletest.rc-9cm92" in namespace "gc-50"
    Jun 28 08:09:43.805: INFO: Deleting pod "simpletest.rc-9mbcs" in namespace "gc-50"
    Jun 28 08:09:43.814: INFO: Deleting pod "simpletest.rc-9phqd" in namespace "gc-50"
    Jun 28 08:09:43.825: INFO: Deleting pod "simpletest.rc-9r2jx" in namespace "gc-50"
    Jun 28 08:09:43.834: INFO: Deleting pod "simpletest.rc-b4v5x" in namespace "gc-50"
    Jun 28 08:09:43.856: INFO: Deleting pod "simpletest.rc-b9gbs" in namespace "gc-50"
    Jun 28 08:09:43.869: INFO: Deleting pod "simpletest.rc-bnkd7" in namespace "gc-50"
    Jun 28 08:09:43.881: INFO: Deleting pod "simpletest.rc-brdgv" in namespace "gc-50"
    Jun 28 08:09:43.912: INFO: Deleting pod "simpletest.rc-c6t4l" in namespace "gc-50"
    Jun 28 08:09:43.943: INFO: Deleting pod "simpletest.rc-cgfrn" in namespace "gc-50"
    Jun 28 08:09:43.973: INFO: Deleting pod "simpletest.rc-chfkp" in namespace "gc-50"
    Jun 28 08:09:43.982: INFO: Deleting pod "simpletest.rc-cvjwl" in namespace "gc-50"
    Jun 28 08:09:44.006: INFO: Deleting pod "simpletest.rc-cw8zs" in namespace "gc-50"
    Jun 28 08:09:44.028: INFO: Deleting pod "simpletest.rc-czt8p" in namespace "gc-50"
    Jun 28 08:09:44.060: INFO: Deleting pod "simpletest.rc-dg5rx" in namespace "gc-50"
    Jun 28 08:09:44.070: INFO: Deleting pod "simpletest.rc-dhtgs" in namespace "gc-50"
    Jun 28 08:09:44.108: INFO: Deleting pod "simpletest.rc-dvw59" in namespace "gc-50"
    Jun 28 08:09:44.148: INFO: Deleting pod "simpletest.rc-f2shd" in namespace "gc-50"
    Jun 28 08:09:44.172: INFO: Deleting pod "simpletest.rc-fzsxt" in namespace "gc-50"
    Jun 28 08:09:44.188: INFO: Deleting pod "simpletest.rc-g5jxn" in namespace "gc-50"
    Jun 28 08:09:44.199: INFO: Deleting pod "simpletest.rc-gs22l" in namespace "gc-50"
    Jun 28 08:09:44.209: INFO: Deleting pod "simpletest.rc-gtgbw" in namespace "gc-50"
    Jun 28 08:09:44.223: INFO: Deleting pod "simpletest.rc-h6vtt" in namespace "gc-50"
    Jun 28 08:09:44.240: INFO: Deleting pod "simpletest.rc-h7r89" in namespace "gc-50"
    Jun 28 08:09:44.253: INFO: Deleting pod "simpletest.rc-hlj2n" in namespace "gc-50"
    Jun 28 08:09:44.265: INFO: Deleting pod "simpletest.rc-j7h5b" in namespace "gc-50"
    Jun 28 08:09:44.279: INFO: Deleting pod "simpletest.rc-j9c6r" in namespace "gc-50"
    Jun 28 08:09:44.290: INFO: Deleting pod "simpletest.rc-jllg8" in namespace "gc-50"
    Jun 28 08:09:44.306: INFO: Deleting pod "simpletest.rc-jmhd4" in namespace "gc-50"
    Jun 28 08:09:44.339: INFO: Deleting pod "simpletest.rc-jqcc9" in namespace "gc-50"
    Jun 28 08:09:44.352: INFO: Deleting pod "simpletest.rc-jrxm6" in namespace "gc-50"
    Jun 28 08:09:44.366: INFO: Deleting pod "simpletest.rc-kbs6n" in namespace "gc-50"
    Jun 28 08:09:44.431: INFO: Deleting pod "simpletest.rc-kskmr" in namespace "gc-50"
    Jun 28 08:09:44.468: INFO: Deleting pod "simpletest.rc-kv94c" in namespace "gc-50"
    Jun 28 08:09:44.484: INFO: Deleting pod "simpletest.rc-l2s8t" in namespace "gc-50"
    Jun 28 08:09:44.495: INFO: Deleting pod "simpletest.rc-lc2mv" in namespace "gc-50"
    Jun 28 08:09:44.537: INFO: Deleting pod "simpletest.rc-lgw6g" in namespace "gc-50"
    Jun 28 08:09:44.552: INFO: Deleting pod "simpletest.rc-lt6zn" in namespace "gc-50"
    Jun 28 08:09:44.568: INFO: Deleting pod "simpletest.rc-m8s9g" in namespace "gc-50"
    Jun 28 08:09:44.586: INFO: Deleting pod "simpletest.rc-m9hnx" in namespace "gc-50"
    Jun 28 08:09:44.603: INFO: Deleting pod "simpletest.rc-mmgl2" in namespace "gc-50"
    Jun 28 08:09:44.616: INFO: Deleting pod "simpletest.rc-n8snm" in namespace "gc-50"
    Jun 28 08:09:44.628: INFO: Deleting pod "simpletest.rc-nbxc4" in namespace "gc-50"
    Jun 28 08:09:44.655: INFO: Deleting pod "simpletest.rc-nkhlw" in namespace "gc-50"
    Jun 28 08:09:44.669: INFO: Deleting pod "simpletest.rc-nwhw2" in namespace "gc-50"
    Jun 28 08:09:44.684: INFO: Deleting pod "simpletest.rc-p4k9x" in namespace "gc-50"
    Jun 28 08:09:44.698: INFO: Deleting pod "simpletest.rc-p984j" in namespace "gc-50"
    Jun 28 08:09:44.707: INFO: Deleting pod "simpletest.rc-ps7kz" in namespace "gc-50"
    Jun 28 08:09:44.719: INFO: Deleting pod "simpletest.rc-ptwkm" in namespace "gc-50"
    Jun 28 08:09:44.731: INFO: Deleting pod "simpletest.rc-q4lbh" in namespace "gc-50"
    Jun 28 08:09:44.778: INFO: Deleting pod "simpletest.rc-q8687" in namespace "gc-50"
    Jun 28 08:09:44.821: INFO: Deleting pod "simpletest.rc-q8dc9" in namespace "gc-50"
    Jun 28 08:09:44.886: INFO: Deleting pod "simpletest.rc-qjtkr" in namespace "gc-50"
    Jun 28 08:09:44.956: INFO: Deleting pod "simpletest.rc-qz5jj" in namespace "gc-50"
    Jun 28 08:09:44.986: INFO: Deleting pod "simpletest.rc-rb6z8" in namespace "gc-50"
    Jun 28 08:09:45.027: INFO: Deleting pod "simpletest.rc-rl8dn" in namespace "gc-50"
    Jun 28 08:09:45.072: INFO: Deleting pod "simpletest.rc-s2wwv" in namespace "gc-50"
    Jun 28 08:09:45.118: INFO: Deleting pod "simpletest.rc-sb49v" in namespace "gc-50"
    Jun 28 08:09:45.170: INFO: Deleting pod "simpletest.rc-slgmn" in namespace "gc-50"
    Jun 28 08:09:45.223: INFO: Deleting pod "simpletest.rc-tcdhp" in namespace "gc-50"
    Jun 28 08:09:45.268: INFO: Deleting pod "simpletest.rc-tkmsn" in namespace "gc-50"
    Jun 28 08:09:45.324: INFO: Deleting pod "simpletest.rc-txpv8" in namespace "gc-50"
    Jun 28 08:09:45.368: INFO: Deleting pod "simpletest.rc-v7hh5" in namespace "gc-50"
    Jun 28 08:09:45.419: INFO: Deleting pod "simpletest.rc-vl5pr" in namespace "gc-50"
    Jun 28 08:09:45.477: INFO: Deleting pod "simpletest.rc-w8ph5" in namespace "gc-50"
    Jun 28 08:09:45.526: INFO: Deleting pod "simpletest.rc-wm4cg" in namespace "gc-50"
    Jun 28 08:09:45.569: INFO: Deleting pod "simpletest.rc-x2hdr" in namespace "gc-50"
    Jun 28 08:09:45.623: INFO: Deleting pod "simpletest.rc-xl975" in namespace "gc-50"
    Jun 28 08:09:45.669: INFO: Deleting pod "simpletest.rc-xp79c" in namespace "gc-50"
    Jun 28 08:09:45.719: INFO: Deleting pod "simpletest.rc-xr4g6" in namespace "gc-50"
    Jun 28 08:09:45.777: INFO: Deleting pod "simpletest.rc-xt4gc" in namespace "gc-50"
    Jun 28 08:09:45.819: INFO: Deleting pod "simpletest.rc-xz7j9" in namespace "gc-50"
    Jun 28 08:09:45.873: INFO: Deleting pod "simpletest.rc-z8qkq" in namespace "gc-50"
    Jun 28 08:09:45.919: INFO: Deleting pod "simpletest.rc-zkvmd" in namespace "gc-50"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Jun 28 08:09:45.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-50" for this suite. 06/28/23 08:09:46.018
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:09:46.064
Jun 28 08:09:46.064: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename downward-api 06/28/23 08:09:46.065
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:09:46.078
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:09:46.086
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165
STEP: Creating a pod to test downward api env vars 06/28/23 08:09:46.095
Jun 28 08:09:46.104: INFO: Waiting up to 5m0s for pod "downward-api-742266f1-1baf-46e0-816f-0e67c6c188a0" in namespace "downward-api-5556" to be "Succeeded or Failed"
Jun 28 08:09:46.109: INFO: Pod "downward-api-742266f1-1baf-46e0-816f-0e67c6c188a0": Phase="Pending", Reason="", readiness=false. Elapsed: 5.159671ms
Jun 28 08:09:48.116: INFO: Pod "downward-api-742266f1-1baf-46e0-816f-0e67c6c188a0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01244508s
Jun 28 08:09:50.115: INFO: Pod "downward-api-742266f1-1baf-46e0-816f-0e67c6c188a0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011301422s
Jun 28 08:09:52.115: INFO: Pod "downward-api-742266f1-1baf-46e0-816f-0e67c6c188a0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011002862s
STEP: Saw pod success 06/28/23 08:09:52.115
Jun 28 08:09:52.115: INFO: Pod "downward-api-742266f1-1baf-46e0-816f-0e67c6c188a0" satisfied condition "Succeeded or Failed"
Jun 28 08:09:52.120: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod downward-api-742266f1-1baf-46e0-816f-0e67c6c188a0 container dapi-container: <nil>
STEP: delete the pod 06/28/23 08:09:52.152
Jun 28 08:09:52.164: INFO: Waiting for pod downward-api-742266f1-1baf-46e0-816f-0e67c6c188a0 to disappear
Jun 28 08:09:52.167: INFO: Pod downward-api-742266f1-1baf-46e0-816f-0e67c6c188a0 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Jun 28 08:09:52.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5556" for this suite. 06/28/23 08:09:52.175
{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","completed":100,"skipped":1834,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.117 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:09:46.064
    Jun 28 08:09:46.064: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename downward-api 06/28/23 08:09:46.065
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:09:46.078
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:09:46.086
    [It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:165
    STEP: Creating a pod to test downward api env vars 06/28/23 08:09:46.095
    Jun 28 08:09:46.104: INFO: Waiting up to 5m0s for pod "downward-api-742266f1-1baf-46e0-816f-0e67c6c188a0" in namespace "downward-api-5556" to be "Succeeded or Failed"
    Jun 28 08:09:46.109: INFO: Pod "downward-api-742266f1-1baf-46e0-816f-0e67c6c188a0": Phase="Pending", Reason="", readiness=false. Elapsed: 5.159671ms
    Jun 28 08:09:48.116: INFO: Pod "downward-api-742266f1-1baf-46e0-816f-0e67c6c188a0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01244508s
    Jun 28 08:09:50.115: INFO: Pod "downward-api-742266f1-1baf-46e0-816f-0e67c6c188a0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011301422s
    Jun 28 08:09:52.115: INFO: Pod "downward-api-742266f1-1baf-46e0-816f-0e67c6c188a0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011002862s
    STEP: Saw pod success 06/28/23 08:09:52.115
    Jun 28 08:09:52.115: INFO: Pod "downward-api-742266f1-1baf-46e0-816f-0e67c6c188a0" satisfied condition "Succeeded or Failed"
    Jun 28 08:09:52.120: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod downward-api-742266f1-1baf-46e0-816f-0e67c6c188a0 container dapi-container: <nil>
    STEP: delete the pod 06/28/23 08:09:52.152
    Jun 28 08:09:52.164: INFO: Waiting for pod downward-api-742266f1-1baf-46e0-816f-0e67c6c188a0 to disappear
    Jun 28 08:09:52.167: INFO: Pod downward-api-742266f1-1baf-46e0-816f-0e67c6c188a0 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Jun 28 08:09:52.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-5556" for this suite. 06/28/23 08:09:52.175
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose
  should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1413
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:09:52.182
Jun 28 08:09:52.182: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename kubectl 06/28/23 08:09:52.183
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:09:52.195
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:09:52.201
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1413
STEP: creating Agnhost RC 06/28/23 08:09:52.206
Jun 28 08:09:52.206: INFO: namespace kubectl-6998
Jun 28 08:09:52.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-6998 create -f -'
Jun 28 08:09:52.446: INFO: stderr: ""
Jun 28 08:09:52.446: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 06/28/23 08:09:52.446
Jun 28 08:09:53.450: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 28 08:09:53.450: INFO: Found 0 / 1
Jun 28 08:09:54.451: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 28 08:09:54.451: INFO: Found 0 / 1
Jun 28 08:09:55.451: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 28 08:09:55.451: INFO: Found 0 / 1
Jun 28 08:09:56.451: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 28 08:09:56.451: INFO: Found 0 / 1
Jun 28 08:09:57.455: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 28 08:09:57.455: INFO: Found 1 / 1
Jun 28 08:09:57.455: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jun 28 08:09:57.462: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 28 08:09:57.462: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jun 28 08:09:57.462: INFO: wait on agnhost-primary startup in kubectl-6998 
Jun 28 08:09:57.463: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-6998 logs agnhost-primary-cdkpq agnhost-primary'
Jun 28 08:09:57.579: INFO: stderr: ""
Jun 28 08:09:57.579: INFO: stdout: "Paused\n"
STEP: exposing RC 06/28/23 08:09:57.579
Jun 28 08:09:57.579: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-6998 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Jun 28 08:09:57.683: INFO: stderr: ""
Jun 28 08:09:57.683: INFO: stdout: "service/rm2 exposed\n"
Jun 28 08:09:57.693: INFO: Service rm2 in namespace kubectl-6998 found.
STEP: exposing service 06/28/23 08:09:59.713
Jun 28 08:09:59.713: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-6998 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Jun 28 08:09:59.821: INFO: stderr: ""
Jun 28 08:09:59.821: INFO: stdout: "service/rm3 exposed\n"
Jun 28 08:09:59.833: INFO: Service rm3 in namespace kubectl-6998 found.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jun 28 08:10:01.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6998" for this suite. 06/28/23 08:10:01.873
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","completed":101,"skipped":1840,"failed":0}
------------------------------
â€¢ [SLOW TEST] [9.701 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl expose
  test/e2e/kubectl/kubectl.go:1407
    should create services for rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1413

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:09:52.182
    Jun 28 08:09:52.182: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename kubectl 06/28/23 08:09:52.183
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:09:52.195
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:09:52.201
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should create services for rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1413
    STEP: creating Agnhost RC 06/28/23 08:09:52.206
    Jun 28 08:09:52.206: INFO: namespace kubectl-6998
    Jun 28 08:09:52.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-6998 create -f -'
    Jun 28 08:09:52.446: INFO: stderr: ""
    Jun 28 08:09:52.446: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 06/28/23 08:09:52.446
    Jun 28 08:09:53.450: INFO: Selector matched 1 pods for map[app:agnhost]
    Jun 28 08:09:53.450: INFO: Found 0 / 1
    Jun 28 08:09:54.451: INFO: Selector matched 1 pods for map[app:agnhost]
    Jun 28 08:09:54.451: INFO: Found 0 / 1
    Jun 28 08:09:55.451: INFO: Selector matched 1 pods for map[app:agnhost]
    Jun 28 08:09:55.451: INFO: Found 0 / 1
    Jun 28 08:09:56.451: INFO: Selector matched 1 pods for map[app:agnhost]
    Jun 28 08:09:56.451: INFO: Found 0 / 1
    Jun 28 08:09:57.455: INFO: Selector matched 1 pods for map[app:agnhost]
    Jun 28 08:09:57.455: INFO: Found 1 / 1
    Jun 28 08:09:57.455: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Jun 28 08:09:57.462: INFO: Selector matched 1 pods for map[app:agnhost]
    Jun 28 08:09:57.462: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Jun 28 08:09:57.462: INFO: wait on agnhost-primary startup in kubectl-6998 
    Jun 28 08:09:57.463: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-6998 logs agnhost-primary-cdkpq agnhost-primary'
    Jun 28 08:09:57.579: INFO: stderr: ""
    Jun 28 08:09:57.579: INFO: stdout: "Paused\n"
    STEP: exposing RC 06/28/23 08:09:57.579
    Jun 28 08:09:57.579: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-6998 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
    Jun 28 08:09:57.683: INFO: stderr: ""
    Jun 28 08:09:57.683: INFO: stdout: "service/rm2 exposed\n"
    Jun 28 08:09:57.693: INFO: Service rm2 in namespace kubectl-6998 found.
    STEP: exposing service 06/28/23 08:09:59.713
    Jun 28 08:09:59.713: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-6998 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
    Jun 28 08:09:59.821: INFO: stderr: ""
    Jun 28 08:09:59.821: INFO: stdout: "service/rm3 exposed\n"
    Jun 28 08:09:59.833: INFO: Service rm3 in namespace kubectl-6998 found.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jun 28 08:10:01.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-6998" for this suite. 06/28/23 08:10:01.873
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:10:01.885
Jun 28 08:10:01.885: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename proxy 06/28/23 08:10:01.886
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:10:01.911
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:10:01.92
[It] A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
Jun 28 08:10:01.937: INFO: Creating pod...
Jun 28 08:10:01.956: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-3524" to be "running"
Jun 28 08:10:01.961: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 5.661941ms
Jun 28 08:10:03.968: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.012226806s
Jun 28 08:10:03.968: INFO: Pod "agnhost" satisfied condition "running"
Jun 28 08:10:03.968: INFO: Creating service...
Jun 28 08:10:03.979: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-3524/pods/agnhost/proxy?method=DELETE
Jun 28 08:10:04.072: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jun 28 08:10:04.072: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-3524/pods/agnhost/proxy?method=OPTIONS
Jun 28 08:10:04.115: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jun 28 08:10:04.115: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-3524/pods/agnhost/proxy?method=PATCH
Jun 28 08:10:04.123: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jun 28 08:10:04.123: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-3524/pods/agnhost/proxy?method=POST
Jun 28 08:10:04.131: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jun 28 08:10:04.131: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-3524/pods/agnhost/proxy?method=PUT
Jun 28 08:10:04.139: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jun 28 08:10:04.139: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-3524/services/e2e-proxy-test-service/proxy?method=DELETE
Jun 28 08:10:04.148: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jun 28 08:10:04.148: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-3524/services/e2e-proxy-test-service/proxy?method=OPTIONS
Jun 28 08:10:04.159: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jun 28 08:10:04.159: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-3524/services/e2e-proxy-test-service/proxy?method=PATCH
Jun 28 08:10:04.169: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jun 28 08:10:04.169: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-3524/services/e2e-proxy-test-service/proxy?method=POST
Jun 28 08:10:04.180: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jun 28 08:10:04.180: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-3524/services/e2e-proxy-test-service/proxy?method=PUT
Jun 28 08:10:04.190: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jun 28 08:10:04.190: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-3524/pods/agnhost/proxy?method=GET
Jun 28 08:10:04.195: INFO: http.Client request:GET StatusCode:301
Jun 28 08:10:04.195: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-3524/services/e2e-proxy-test-service/proxy?method=GET
Jun 28 08:10:04.202: INFO: http.Client request:GET StatusCode:301
Jun 28 08:10:04.202: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-3524/pods/agnhost/proxy?method=HEAD
Jun 28 08:10:04.206: INFO: http.Client request:HEAD StatusCode:301
Jun 28 08:10:04.206: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-3524/services/e2e-proxy-test-service/proxy?method=HEAD
Jun 28 08:10:04.213: INFO: http.Client request:HEAD StatusCode:301
[AfterEach] version v1
  test/e2e/framework/framework.go:187
Jun 28 08:10:04.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-3524" for this suite. 06/28/23 08:10:04.222
{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service Proxy [Conformance]","completed":102,"skipped":1866,"failed":0}
------------------------------
â€¢ [2.345 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service Proxy [Conformance]
    test/e2e/network/proxy.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:10:01.885
    Jun 28 08:10:01.885: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename proxy 06/28/23 08:10:01.886
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:10:01.911
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:10:01.92
    [It] A set of valid responses are returned for both pod and service Proxy [Conformance]
      test/e2e/network/proxy.go:380
    Jun 28 08:10:01.937: INFO: Creating pod...
    Jun 28 08:10:01.956: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-3524" to be "running"
    Jun 28 08:10:01.961: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 5.661941ms
    Jun 28 08:10:03.968: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.012226806s
    Jun 28 08:10:03.968: INFO: Pod "agnhost" satisfied condition "running"
    Jun 28 08:10:03.968: INFO: Creating service...
    Jun 28 08:10:03.979: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-3524/pods/agnhost/proxy?method=DELETE
    Jun 28 08:10:04.072: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jun 28 08:10:04.072: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-3524/pods/agnhost/proxy?method=OPTIONS
    Jun 28 08:10:04.115: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jun 28 08:10:04.115: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-3524/pods/agnhost/proxy?method=PATCH
    Jun 28 08:10:04.123: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jun 28 08:10:04.123: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-3524/pods/agnhost/proxy?method=POST
    Jun 28 08:10:04.131: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jun 28 08:10:04.131: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-3524/pods/agnhost/proxy?method=PUT
    Jun 28 08:10:04.139: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Jun 28 08:10:04.139: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-3524/services/e2e-proxy-test-service/proxy?method=DELETE
    Jun 28 08:10:04.148: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jun 28 08:10:04.148: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-3524/services/e2e-proxy-test-service/proxy?method=OPTIONS
    Jun 28 08:10:04.159: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jun 28 08:10:04.159: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-3524/services/e2e-proxy-test-service/proxy?method=PATCH
    Jun 28 08:10:04.169: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jun 28 08:10:04.169: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-3524/services/e2e-proxy-test-service/proxy?method=POST
    Jun 28 08:10:04.180: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jun 28 08:10:04.180: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-3524/services/e2e-proxy-test-service/proxy?method=PUT
    Jun 28 08:10:04.190: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Jun 28 08:10:04.190: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-3524/pods/agnhost/proxy?method=GET
    Jun 28 08:10:04.195: INFO: http.Client request:GET StatusCode:301
    Jun 28 08:10:04.195: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-3524/services/e2e-proxy-test-service/proxy?method=GET
    Jun 28 08:10:04.202: INFO: http.Client request:GET StatusCode:301
    Jun 28 08:10:04.202: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-3524/pods/agnhost/proxy?method=HEAD
    Jun 28 08:10:04.206: INFO: http.Client request:HEAD StatusCode:301
    Jun 28 08:10:04.206: INFO: Starting http.Client for https://172.20.0.1:443/api/v1/namespaces/proxy-3524/services/e2e-proxy-test-service/proxy?method=HEAD
    Jun 28 08:10:04.213: INFO: http.Client request:HEAD StatusCode:301
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    Jun 28 08:10:04.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-3524" for this suite. 06/28/23 08:10:04.222
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected secret
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:10:04.231
Jun 28 08:10:04.231: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename projected 06/28/23 08:10:04.233
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:10:04.248
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:10:04.255
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214
STEP: Creating secret with name s-test-opt-del-9cd598f1-4d91-4a54-9777-ba079bd1577c 06/28/23 08:10:04.271
STEP: Creating secret with name s-test-opt-upd-76f98438-338f-4629-8e76-95dc56e13ffe 06/28/23 08:10:04.277
STEP: Creating the pod 06/28/23 08:10:04.283
Jun 28 08:10:04.294: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-ccbce68d-698c-4b57-a0a6-eaab48a9076d" in namespace "projected-5250" to be "running and ready"
Jun 28 08:10:04.301: INFO: Pod "pod-projected-secrets-ccbce68d-698c-4b57-a0a6-eaab48a9076d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.559495ms
Jun 28 08:10:04.301: INFO: The phase of Pod pod-projected-secrets-ccbce68d-698c-4b57-a0a6-eaab48a9076d is Pending, waiting for it to be Running (with Ready = true)
Jun 28 08:10:06.307: INFO: Pod "pod-projected-secrets-ccbce68d-698c-4b57-a0a6-eaab48a9076d": Phase="Running", Reason="", readiness=true. Elapsed: 2.012936904s
Jun 28 08:10:06.307: INFO: The phase of Pod pod-projected-secrets-ccbce68d-698c-4b57-a0a6-eaab48a9076d is Running (Ready = true)
Jun 28 08:10:06.307: INFO: Pod "pod-projected-secrets-ccbce68d-698c-4b57-a0a6-eaab48a9076d" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-9cd598f1-4d91-4a54-9777-ba079bd1577c 06/28/23 08:10:06.468
STEP: Updating secret s-test-opt-upd-76f98438-338f-4629-8e76-95dc56e13ffe 06/28/23 08:10:06.476
STEP: Creating secret with name s-test-opt-create-33c976ed-b51d-4cd3-b4b1-f49afc98c946 06/28/23 08:10:06.484
STEP: waiting to observe update in volume 06/28/23 08:10:06.491
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Jun 28 08:10:08.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5250" for this suite. 06/28/23 08:10:08.67
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":103,"skipped":1867,"failed":0}
------------------------------
â€¢ [4.447 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:10:04.231
    Jun 28 08:10:04.231: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename projected 06/28/23 08:10:04.233
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:10:04.248
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:10:04.255
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:214
    STEP: Creating secret with name s-test-opt-del-9cd598f1-4d91-4a54-9777-ba079bd1577c 06/28/23 08:10:04.271
    STEP: Creating secret with name s-test-opt-upd-76f98438-338f-4629-8e76-95dc56e13ffe 06/28/23 08:10:04.277
    STEP: Creating the pod 06/28/23 08:10:04.283
    Jun 28 08:10:04.294: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-ccbce68d-698c-4b57-a0a6-eaab48a9076d" in namespace "projected-5250" to be "running and ready"
    Jun 28 08:10:04.301: INFO: Pod "pod-projected-secrets-ccbce68d-698c-4b57-a0a6-eaab48a9076d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.559495ms
    Jun 28 08:10:04.301: INFO: The phase of Pod pod-projected-secrets-ccbce68d-698c-4b57-a0a6-eaab48a9076d is Pending, waiting for it to be Running (with Ready = true)
    Jun 28 08:10:06.307: INFO: Pod "pod-projected-secrets-ccbce68d-698c-4b57-a0a6-eaab48a9076d": Phase="Running", Reason="", readiness=true. Elapsed: 2.012936904s
    Jun 28 08:10:06.307: INFO: The phase of Pod pod-projected-secrets-ccbce68d-698c-4b57-a0a6-eaab48a9076d is Running (Ready = true)
    Jun 28 08:10:06.307: INFO: Pod "pod-projected-secrets-ccbce68d-698c-4b57-a0a6-eaab48a9076d" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-9cd598f1-4d91-4a54-9777-ba079bd1577c 06/28/23 08:10:06.468
    STEP: Updating secret s-test-opt-upd-76f98438-338f-4629-8e76-95dc56e13ffe 06/28/23 08:10:06.476
    STEP: Creating secret with name s-test-opt-create-33c976ed-b51d-4cd3-b4b1-f49afc98c946 06/28/23 08:10:06.484
    STEP: waiting to observe update in volume 06/28/23 08:10:06.491
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Jun 28 08:10:08.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5250" for this suite. 06/28/23 08:10:08.67
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:733
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:10:08.685
Jun 28 08:10:08.685: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename sched-preemption 06/28/23 08:10:08.687
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:10:08.703
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:10:08.709
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Jun 28 08:10:08.732: INFO: Waiting up to 1m0s for all nodes to be ready
Jun 28 08:11:08.781: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:11:08.785
Jun 28 08:11:08.786: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename sched-preemption-path 06/28/23 08:11:08.786
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:11:08.8
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:11:08.805
[BeforeEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:690
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:733
Jun 28 08:11:08.825: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
Jun 28 08:11:08.829: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  test/e2e/framework/framework.go:187
Jun 28 08:11:08.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-9181" for this suite. 06/28/23 08:11:08.856
[AfterEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:706
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Jun 28 08:11:08.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-2848" for this suite. 06/28/23 08:11:08.88
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","completed":104,"skipped":1933,"failed":0}
------------------------------
â€¢ [SLOW TEST] [60.249 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  test/e2e/scheduling/preemption.go:683
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    test/e2e/scheduling/preemption.go:733

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:10:08.685
    Jun 28 08:10:08.685: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename sched-preemption 06/28/23 08:10:08.687
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:10:08.703
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:10:08.709
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Jun 28 08:10:08.732: INFO: Waiting up to 1m0s for all nodes to be ready
    Jun 28 08:11:08.781: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PriorityClass endpoints
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:11:08.785
    Jun 28 08:11:08.786: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename sched-preemption-path 06/28/23 08:11:08.786
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:11:08.8
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:11:08.805
    [BeforeEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:690
    [It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
      test/e2e/scheduling/preemption.go:733
    Jun 28 08:11:08.825: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
    Jun 28 08:11:08.829: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
    [AfterEach] PriorityClass endpoints
      test/e2e/framework/framework.go:187
    Jun 28 08:11:08.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-path-9181" for this suite. 06/28/23 08:11:08.856
    [AfterEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:706
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Jun 28 08:11:08.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-2848" for this suite. 06/28/23 08:11:08.88
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits
  should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:51
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:11:08.934
Jun 28 08:11:08.934: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename container-runtime 06/28/23 08:11:08.935
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:11:08.948
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:11:08.953
[It] should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:51
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 06/28/23 08:11:08.97
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 06/28/23 08:11:26.074
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 06/28/23 08:11:26.078
STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 06/28/23 08:11:26.087
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 06/28/23 08:11:26.087
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 06/28/23 08:11:26.109
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 06/28/23 08:11:28.124
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 06/28/23 08:11:30.139
STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 06/28/23 08:11:30.149
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 06/28/23 08:11:30.149
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 06/28/23 08:11:30.175
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 06/28/23 08:11:31.186
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 06/28/23 08:11:33.2
STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 06/28/23 08:11:33.208
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 06/28/23 08:11:33.208
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Jun 28 08:11:33.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1416" for this suite. 06/28/23 08:11:33.242
{"msg":"PASSED [sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","completed":105,"skipped":1943,"failed":0}
------------------------------
â€¢ [SLOW TEST] [24.314 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    when starting a container that exits
    test/e2e/common/node/runtime.go:44
      should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:11:08.934
    Jun 28 08:11:08.934: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename container-runtime 06/28/23 08:11:08.935
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:11:08.948
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:11:08.953
    [It] should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:51
    STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 06/28/23 08:11:08.97
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 06/28/23 08:11:26.074
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 06/28/23 08:11:26.078
    STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 06/28/23 08:11:26.087
    STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 06/28/23 08:11:26.087
    STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 06/28/23 08:11:26.109
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 06/28/23 08:11:28.124
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 06/28/23 08:11:30.139
    STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 06/28/23 08:11:30.149
    STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 06/28/23 08:11:30.149
    STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 06/28/23 08:11:30.175
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 06/28/23 08:11:31.186
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 06/28/23 08:11:33.2
    STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 06/28/23 08:11:33.208
    STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 06/28/23 08:11:33.208
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Jun 28 08:11:33.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-1416" for this suite. 06/28/23 08:11:33.242
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:11:33.249
Jun 28 08:11:33.249: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename services 06/28/23 08:11:33.25
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:11:33.262
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:11:33.266
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157
STEP: creating service in namespace services-7928 06/28/23 08:11:33.271
STEP: creating service affinity-clusterip in namespace services-7928 06/28/23 08:11:33.271
STEP: creating replication controller affinity-clusterip in namespace services-7928 06/28/23 08:11:33.29
I0628 08:11:33.297360      18 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-7928, replica count: 3
I0628 08:11:36.348336      18 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 28 08:11:36.358: INFO: Creating new exec pod
Jun 28 08:11:36.366: INFO: Waiting up to 5m0s for pod "execpod-affinity7wbmq" in namespace "services-7928" to be "running"
Jun 28 08:11:36.371: INFO: Pod "execpod-affinity7wbmq": Phase="Pending", Reason="", readiness=false. Elapsed: 4.92235ms
Jun 28 08:11:38.375: INFO: Pod "execpod-affinity7wbmq": Phase="Running", Reason="", readiness=true. Elapsed: 2.009002045s
Jun 28 08:11:38.375: INFO: Pod "execpod-affinity7wbmq" satisfied condition "running"
Jun 28 08:11:39.376: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-7928 exec execpod-affinity7wbmq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
Jun 28 08:11:39.811: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Jun 28 08:11:39.811: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun 28 08:11:39.811: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-7928 exec execpod-affinity7wbmq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.20.9.248 80'
Jun 28 08:11:40.254: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.20.9.248 80\nConnection to 172.20.9.248 80 port [tcp/http] succeeded!\n"
Jun 28 08:11:40.254: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun 28 08:11:40.254: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-7928 exec execpod-affinity7wbmq -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.20.9.248:80/ ; done'
Jun 28 08:11:40.767: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.9.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.9.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.9.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.9.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.9.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.9.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.9.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.9.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.9.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.9.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.9.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.9.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.9.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.9.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.9.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.9.248:80/\n"
Jun 28 08:11:40.767: INFO: stdout: "\naffinity-clusterip-962n5\naffinity-clusterip-962n5\naffinity-clusterip-962n5\naffinity-clusterip-962n5\naffinity-clusterip-962n5\naffinity-clusterip-962n5\naffinity-clusterip-962n5\naffinity-clusterip-962n5\naffinity-clusterip-962n5\naffinity-clusterip-962n5\naffinity-clusterip-962n5\naffinity-clusterip-962n5\naffinity-clusterip-962n5\naffinity-clusterip-962n5\naffinity-clusterip-962n5\naffinity-clusterip-962n5"
Jun 28 08:11:40.767: INFO: Received response from host: affinity-clusterip-962n5
Jun 28 08:11:40.767: INFO: Received response from host: affinity-clusterip-962n5
Jun 28 08:11:40.767: INFO: Received response from host: affinity-clusterip-962n5
Jun 28 08:11:40.767: INFO: Received response from host: affinity-clusterip-962n5
Jun 28 08:11:40.767: INFO: Received response from host: affinity-clusterip-962n5
Jun 28 08:11:40.767: INFO: Received response from host: affinity-clusterip-962n5
Jun 28 08:11:40.767: INFO: Received response from host: affinity-clusterip-962n5
Jun 28 08:11:40.767: INFO: Received response from host: affinity-clusterip-962n5
Jun 28 08:11:40.767: INFO: Received response from host: affinity-clusterip-962n5
Jun 28 08:11:40.767: INFO: Received response from host: affinity-clusterip-962n5
Jun 28 08:11:40.767: INFO: Received response from host: affinity-clusterip-962n5
Jun 28 08:11:40.767: INFO: Received response from host: affinity-clusterip-962n5
Jun 28 08:11:40.767: INFO: Received response from host: affinity-clusterip-962n5
Jun 28 08:11:40.767: INFO: Received response from host: affinity-clusterip-962n5
Jun 28 08:11:40.767: INFO: Received response from host: affinity-clusterip-962n5
Jun 28 08:11:40.767: INFO: Received response from host: affinity-clusterip-962n5
Jun 28 08:11:40.767: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-7928, will wait for the garbage collector to delete the pods 06/28/23 08:11:40.782
Jun 28 08:11:40.845: INFO: Deleting ReplicationController affinity-clusterip took: 7.877251ms
Jun 28 08:11:40.946: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.751844ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jun 28 08:11:43.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7928" for this suite. 06/28/23 08:11:43.175
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","completed":106,"skipped":1955,"failed":0}
------------------------------
â€¢ [SLOW TEST] [9.933 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:11:33.249
    Jun 28 08:11:33.249: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename services 06/28/23 08:11:33.25
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:11:33.262
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:11:33.266
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2157
    STEP: creating service in namespace services-7928 06/28/23 08:11:33.271
    STEP: creating service affinity-clusterip in namespace services-7928 06/28/23 08:11:33.271
    STEP: creating replication controller affinity-clusterip in namespace services-7928 06/28/23 08:11:33.29
    I0628 08:11:33.297360      18 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-7928, replica count: 3
    I0628 08:11:36.348336      18 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jun 28 08:11:36.358: INFO: Creating new exec pod
    Jun 28 08:11:36.366: INFO: Waiting up to 5m0s for pod "execpod-affinity7wbmq" in namespace "services-7928" to be "running"
    Jun 28 08:11:36.371: INFO: Pod "execpod-affinity7wbmq": Phase="Pending", Reason="", readiness=false. Elapsed: 4.92235ms
    Jun 28 08:11:38.375: INFO: Pod "execpod-affinity7wbmq": Phase="Running", Reason="", readiness=true. Elapsed: 2.009002045s
    Jun 28 08:11:38.375: INFO: Pod "execpod-affinity7wbmq" satisfied condition "running"
    Jun 28 08:11:39.376: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-7928 exec execpod-affinity7wbmq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
    Jun 28 08:11:39.811: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
    Jun 28 08:11:39.811: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jun 28 08:11:39.811: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-7928 exec execpod-affinity7wbmq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.20.9.248 80'
    Jun 28 08:11:40.254: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.20.9.248 80\nConnection to 172.20.9.248 80 port [tcp/http] succeeded!\n"
    Jun 28 08:11:40.254: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jun 28 08:11:40.254: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-7928 exec execpod-affinity7wbmq -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.20.9.248:80/ ; done'
    Jun 28 08:11:40.767: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.9.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.9.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.9.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.9.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.9.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.9.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.9.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.9.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.9.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.9.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.9.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.9.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.9.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.9.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.9.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.20.9.248:80/\n"
    Jun 28 08:11:40.767: INFO: stdout: "\naffinity-clusterip-962n5\naffinity-clusterip-962n5\naffinity-clusterip-962n5\naffinity-clusterip-962n5\naffinity-clusterip-962n5\naffinity-clusterip-962n5\naffinity-clusterip-962n5\naffinity-clusterip-962n5\naffinity-clusterip-962n5\naffinity-clusterip-962n5\naffinity-clusterip-962n5\naffinity-clusterip-962n5\naffinity-clusterip-962n5\naffinity-clusterip-962n5\naffinity-clusterip-962n5\naffinity-clusterip-962n5"
    Jun 28 08:11:40.767: INFO: Received response from host: affinity-clusterip-962n5
    Jun 28 08:11:40.767: INFO: Received response from host: affinity-clusterip-962n5
    Jun 28 08:11:40.767: INFO: Received response from host: affinity-clusterip-962n5
    Jun 28 08:11:40.767: INFO: Received response from host: affinity-clusterip-962n5
    Jun 28 08:11:40.767: INFO: Received response from host: affinity-clusterip-962n5
    Jun 28 08:11:40.767: INFO: Received response from host: affinity-clusterip-962n5
    Jun 28 08:11:40.767: INFO: Received response from host: affinity-clusterip-962n5
    Jun 28 08:11:40.767: INFO: Received response from host: affinity-clusterip-962n5
    Jun 28 08:11:40.767: INFO: Received response from host: affinity-clusterip-962n5
    Jun 28 08:11:40.767: INFO: Received response from host: affinity-clusterip-962n5
    Jun 28 08:11:40.767: INFO: Received response from host: affinity-clusterip-962n5
    Jun 28 08:11:40.767: INFO: Received response from host: affinity-clusterip-962n5
    Jun 28 08:11:40.767: INFO: Received response from host: affinity-clusterip-962n5
    Jun 28 08:11:40.767: INFO: Received response from host: affinity-clusterip-962n5
    Jun 28 08:11:40.767: INFO: Received response from host: affinity-clusterip-962n5
    Jun 28 08:11:40.767: INFO: Received response from host: affinity-clusterip-962n5
    Jun 28 08:11:40.767: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip in namespace services-7928, will wait for the garbage collector to delete the pods 06/28/23 08:11:40.782
    Jun 28 08:11:40.845: INFO: Deleting ReplicationController affinity-clusterip took: 7.877251ms
    Jun 28 08:11:40.946: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.751844ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jun 28 08:11:43.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-7928" for this suite. 06/28/23 08:11:43.175
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:11:43.183
Jun 28 08:11:43.183: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename projected 06/28/23 08:11:43.184
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:11:43.198
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:11:43.202
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52
STEP: Creating a pod to test downward API volume plugin 06/28/23 08:11:43.206
Jun 28 08:11:43.217: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3a7f77b2-baff-4059-92a0-3e582801e2d4" in namespace "projected-8008" to be "Succeeded or Failed"
Jun 28 08:11:43.222: INFO: Pod "downwardapi-volume-3a7f77b2-baff-4059-92a0-3e582801e2d4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.782173ms
Jun 28 08:11:45.227: INFO: Pod "downwardapi-volume-3a7f77b2-baff-4059-92a0-3e582801e2d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009680213s
Jun 28 08:11:47.227: INFO: Pod "downwardapi-volume-3a7f77b2-baff-4059-92a0-3e582801e2d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010003524s
STEP: Saw pod success 06/28/23 08:11:47.227
Jun 28 08:11:47.227: INFO: Pod "downwardapi-volume-3a7f77b2-baff-4059-92a0-3e582801e2d4" satisfied condition "Succeeded or Failed"
Jun 28 08:11:47.232: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod downwardapi-volume-3a7f77b2-baff-4059-92a0-3e582801e2d4 container client-container: <nil>
STEP: delete the pod 06/28/23 08:11:47.282
Jun 28 08:11:47.293: INFO: Waiting for pod downwardapi-volume-3a7f77b2-baff-4059-92a0-3e582801e2d4 to disappear
Jun 28 08:11:47.296: INFO: Pod downwardapi-volume-3a7f77b2-baff-4059-92a0-3e582801e2d4 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jun 28 08:11:47.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8008" for this suite. 06/28/23 08:11:47.304
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","completed":107,"skipped":1984,"failed":0}
------------------------------
â€¢ [4.128 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:11:43.183
    Jun 28 08:11:43.183: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename projected 06/28/23 08:11:43.184
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:11:43.198
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:11:43.202
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:52
    STEP: Creating a pod to test downward API volume plugin 06/28/23 08:11:43.206
    Jun 28 08:11:43.217: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3a7f77b2-baff-4059-92a0-3e582801e2d4" in namespace "projected-8008" to be "Succeeded or Failed"
    Jun 28 08:11:43.222: INFO: Pod "downwardapi-volume-3a7f77b2-baff-4059-92a0-3e582801e2d4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.782173ms
    Jun 28 08:11:45.227: INFO: Pod "downwardapi-volume-3a7f77b2-baff-4059-92a0-3e582801e2d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009680213s
    Jun 28 08:11:47.227: INFO: Pod "downwardapi-volume-3a7f77b2-baff-4059-92a0-3e582801e2d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010003524s
    STEP: Saw pod success 06/28/23 08:11:47.227
    Jun 28 08:11:47.227: INFO: Pod "downwardapi-volume-3a7f77b2-baff-4059-92a0-3e582801e2d4" satisfied condition "Succeeded or Failed"
    Jun 28 08:11:47.232: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod downwardapi-volume-3a7f77b2-baff-4059-92a0-3e582801e2d4 container client-container: <nil>
    STEP: delete the pod 06/28/23 08:11:47.282
    Jun 28 08:11:47.293: INFO: Waiting for pod downwardapi-volume-3a7f77b2-baff-4059-92a0-3e582801e2d4 to disappear
    Jun 28 08:11:47.296: INFO: Pod downwardapi-volume-3a7f77b2-baff-4059-92a0-3e582801e2d4 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jun 28 08:11:47.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-8008" for this suite. 06/28/23 08:11:47.304
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:11:47.312
Jun 28 08:11:47.312: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename projected 06/28/23 08:11:47.313
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:11:47.325
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:11:47.33
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98
STEP: Creating configMap with name projected-configmap-test-volume-map-6c3a1c2d-7863-4724-bbf4-a6549563d8b8 06/28/23 08:11:47.334
STEP: Creating a pod to test consume configMaps 06/28/23 08:11:47.34
Jun 28 08:11:47.350: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e622d3e9-5609-4465-b446-1c8963380683" in namespace "projected-1026" to be "Succeeded or Failed"
Jun 28 08:11:47.355: INFO: Pod "pod-projected-configmaps-e622d3e9-5609-4465-b446-1c8963380683": Phase="Pending", Reason="", readiness=false. Elapsed: 4.748872ms
Jun 28 08:11:49.360: INFO: Pod "pod-projected-configmaps-e622d3e9-5609-4465-b446-1c8963380683": Phase="Running", Reason="", readiness=false. Elapsed: 2.009878446s
Jun 28 08:11:51.360: INFO: Pod "pod-projected-configmaps-e622d3e9-5609-4465-b446-1c8963380683": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010190006s
STEP: Saw pod success 06/28/23 08:11:51.36
Jun 28 08:11:51.360: INFO: Pod "pod-projected-configmaps-e622d3e9-5609-4465-b446-1c8963380683" satisfied condition "Succeeded or Failed"
Jun 28 08:11:51.365: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod pod-projected-configmaps-e622d3e9-5609-4465-b446-1c8963380683 container agnhost-container: <nil>
STEP: delete the pod 06/28/23 08:11:51.374
Jun 28 08:11:51.389: INFO: Waiting for pod pod-projected-configmaps-e622d3e9-5609-4465-b446-1c8963380683 to disappear
Jun 28 08:11:51.392: INFO: Pod pod-projected-configmaps-e622d3e9-5609-4465-b446-1c8963380683 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jun 28 08:11:51.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1026" for this suite. 06/28/23 08:11:51.401
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":108,"skipped":1986,"failed":0}
------------------------------
â€¢ [4.096 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:11:47.312
    Jun 28 08:11:47.312: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename projected 06/28/23 08:11:47.313
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:11:47.325
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:11:47.33
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:98
    STEP: Creating configMap with name projected-configmap-test-volume-map-6c3a1c2d-7863-4724-bbf4-a6549563d8b8 06/28/23 08:11:47.334
    STEP: Creating a pod to test consume configMaps 06/28/23 08:11:47.34
    Jun 28 08:11:47.350: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e622d3e9-5609-4465-b446-1c8963380683" in namespace "projected-1026" to be "Succeeded or Failed"
    Jun 28 08:11:47.355: INFO: Pod "pod-projected-configmaps-e622d3e9-5609-4465-b446-1c8963380683": Phase="Pending", Reason="", readiness=false. Elapsed: 4.748872ms
    Jun 28 08:11:49.360: INFO: Pod "pod-projected-configmaps-e622d3e9-5609-4465-b446-1c8963380683": Phase="Running", Reason="", readiness=false. Elapsed: 2.009878446s
    Jun 28 08:11:51.360: INFO: Pod "pod-projected-configmaps-e622d3e9-5609-4465-b446-1c8963380683": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010190006s
    STEP: Saw pod success 06/28/23 08:11:51.36
    Jun 28 08:11:51.360: INFO: Pod "pod-projected-configmaps-e622d3e9-5609-4465-b446-1c8963380683" satisfied condition "Succeeded or Failed"
    Jun 28 08:11:51.365: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod pod-projected-configmaps-e622d3e9-5609-4465-b446-1c8963380683 container agnhost-container: <nil>
    STEP: delete the pod 06/28/23 08:11:51.374
    Jun 28 08:11:51.389: INFO: Waiting for pod pod-projected-configmaps-e622d3e9-5609-4465-b446-1c8963380683 to disappear
    Jun 28 08:11:51.392: INFO: Pod pod-projected-configmaps-e622d3e9-5609-4465-b446-1c8963380683 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jun 28 08:11:51.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1026" for this suite. 06/28/23 08:11:51.401
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod
  should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1711
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:11:51.409
Jun 28 08:11:51.409: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename kubectl 06/28/23 08:11:51.41
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:11:51.425
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:11:51.429
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1698
[It] should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1711
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 06/28/23 08:11:51.434
Jun 28 08:11:51.434: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-8550 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2'
Jun 28 08:11:51.517: INFO: stderr: ""
Jun 28 08:11:51.517: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created 06/28/23 08:11:51.517
[AfterEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1702
Jun 28 08:11:51.522: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-8550 delete pods e2e-test-httpd-pod'
Jun 28 08:11:54.127: INFO: stderr: ""
Jun 28 08:11:54.127: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jun 28 08:11:54.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8550" for this suite. 06/28/23 08:11:54.134
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","completed":109,"skipped":2022,"failed":0}
------------------------------
â€¢ [2.732 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl run pod
  test/e2e/kubectl/kubectl.go:1695
    should create a pod from an image when restart is Never  [Conformance]
    test/e2e/kubectl/kubectl.go:1711

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:11:51.409
    Jun 28 08:11:51.409: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename kubectl 06/28/23 08:11:51.41
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:11:51.425
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:11:51.429
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1698
    [It] should create a pod from an image when restart is Never  [Conformance]
      test/e2e/kubectl/kubectl.go:1711
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 06/28/23 08:11:51.434
    Jun 28 08:11:51.434: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-8550 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2'
    Jun 28 08:11:51.517: INFO: stderr: ""
    Jun 28 08:11:51.517: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod was created 06/28/23 08:11:51.517
    [AfterEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1702
    Jun 28 08:11:51.522: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-8550 delete pods e2e-test-httpd-pod'
    Jun 28 08:11:54.127: INFO: stderr: ""
    Jun 28 08:11:54.127: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jun 28 08:11:54.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-8550" for this suite. 06/28/23 08:11:54.134
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:11:54.141
Jun 28 08:11:54.142: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename projected 06/28/23 08:11:54.142
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:11:54.155
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:11:54.159
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55
STEP: Creating projection with secret that has name projected-secret-test-f18b47a5-7ca9-43a9-9e6e-8236b000d05d 06/28/23 08:11:54.163
STEP: Creating a pod to test consume secrets 06/28/23 08:11:54.168
Jun 28 08:11:54.178: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-9cf8e73f-5c53-41dd-9015-10a4355c926b" in namespace "projected-6550" to be "Succeeded or Failed"
Jun 28 08:11:54.183: INFO: Pod "pod-projected-secrets-9cf8e73f-5c53-41dd-9015-10a4355c926b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.487937ms
Jun 28 08:11:56.189: INFO: Pod "pod-projected-secrets-9cf8e73f-5c53-41dd-9015-10a4355c926b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011512576s
Jun 28 08:11:58.189: INFO: Pod "pod-projected-secrets-9cf8e73f-5c53-41dd-9015-10a4355c926b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010925444s
STEP: Saw pod success 06/28/23 08:11:58.189
Jun 28 08:11:58.189: INFO: Pod "pod-projected-secrets-9cf8e73f-5c53-41dd-9015-10a4355c926b" satisfied condition "Succeeded or Failed"
Jun 28 08:11:58.193: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod pod-projected-secrets-9cf8e73f-5c53-41dd-9015-10a4355c926b container projected-secret-volume-test: <nil>
STEP: delete the pod 06/28/23 08:11:58.205
Jun 28 08:11:58.218: INFO: Waiting for pod pod-projected-secrets-9cf8e73f-5c53-41dd-9015-10a4355c926b to disappear
Jun 28 08:11:58.221: INFO: Pod pod-projected-secrets-9cf8e73f-5c53-41dd-9015-10a4355c926b no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Jun 28 08:11:58.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6550" for this suite. 06/28/23 08:11:58.228
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":110,"skipped":2024,"failed":0}
------------------------------
â€¢ [4.092 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:11:54.141
    Jun 28 08:11:54.142: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename projected 06/28/23 08:11:54.142
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:11:54.155
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:11:54.159
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:55
    STEP: Creating projection with secret that has name projected-secret-test-f18b47a5-7ca9-43a9-9e6e-8236b000d05d 06/28/23 08:11:54.163
    STEP: Creating a pod to test consume secrets 06/28/23 08:11:54.168
    Jun 28 08:11:54.178: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-9cf8e73f-5c53-41dd-9015-10a4355c926b" in namespace "projected-6550" to be "Succeeded or Failed"
    Jun 28 08:11:54.183: INFO: Pod "pod-projected-secrets-9cf8e73f-5c53-41dd-9015-10a4355c926b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.487937ms
    Jun 28 08:11:56.189: INFO: Pod "pod-projected-secrets-9cf8e73f-5c53-41dd-9015-10a4355c926b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011512576s
    Jun 28 08:11:58.189: INFO: Pod "pod-projected-secrets-9cf8e73f-5c53-41dd-9015-10a4355c926b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010925444s
    STEP: Saw pod success 06/28/23 08:11:58.189
    Jun 28 08:11:58.189: INFO: Pod "pod-projected-secrets-9cf8e73f-5c53-41dd-9015-10a4355c926b" satisfied condition "Succeeded or Failed"
    Jun 28 08:11:58.193: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod pod-projected-secrets-9cf8e73f-5c53-41dd-9015-10a4355c926b container projected-secret-volume-test: <nil>
    STEP: delete the pod 06/28/23 08:11:58.205
    Jun 28 08:11:58.218: INFO: Waiting for pod pod-projected-secrets-9cf8e73f-5c53-41dd-9015-10a4355c926b to disappear
    Jun 28 08:11:58.221: INFO: Pod pod-projected-secrets-9cf8e73f-5c53-41dd-9015-10a4355c926b no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Jun 28 08:11:58.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6550" for this suite. 06/28/23 08:11:58.228
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:11:58.234
Jun 28 08:11:58.234: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename crd-publish-openapi 06/28/23 08:11:58.235
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:11:58.247
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:11:58.252
[It] works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68
Jun 28 08:11:58.259: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 06/28/23 08:12:00.384
Jun 28 08:12:00.384: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=crd-publish-openapi-4680 --namespace=crd-publish-openapi-4680 create -f -'
Jun 28 08:12:01.020: INFO: stderr: ""
Jun 28 08:12:01.020: INFO: stdout: "e2e-test-crd-publish-openapi-2707-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jun 28 08:12:01.020: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=crd-publish-openapi-4680 --namespace=crd-publish-openapi-4680 delete e2e-test-crd-publish-openapi-2707-crds test-foo'
Jun 28 08:12:01.096: INFO: stderr: ""
Jun 28 08:12:01.096: INFO: stdout: "e2e-test-crd-publish-openapi-2707-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Jun 28 08:12:01.096: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=crd-publish-openapi-4680 --namespace=crd-publish-openapi-4680 apply -f -'
Jun 28 08:12:01.309: INFO: stderr: ""
Jun 28 08:12:01.309: INFO: stdout: "e2e-test-crd-publish-openapi-2707-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jun 28 08:12:01.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=crd-publish-openapi-4680 --namespace=crd-publish-openapi-4680 delete e2e-test-crd-publish-openapi-2707-crds test-foo'
Jun 28 08:12:01.392: INFO: stderr: ""
Jun 28 08:12:01.392: INFO: stdout: "e2e-test-crd-publish-openapi-2707-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 06/28/23 08:12:01.392
Jun 28 08:12:01.392: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=crd-publish-openapi-4680 --namespace=crd-publish-openapi-4680 create -f -'
Jun 28 08:12:01.604: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 06/28/23 08:12:01.604
Jun 28 08:12:01.604: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=crd-publish-openapi-4680 --namespace=crd-publish-openapi-4680 create -f -'
Jun 28 08:12:01.817: INFO: rc: 1
Jun 28 08:12:01.817: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=crd-publish-openapi-4680 --namespace=crd-publish-openapi-4680 apply -f -'
Jun 28 08:12:02.040: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request without required properties 06/28/23 08:12:02.04
Jun 28 08:12:02.040: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=crd-publish-openapi-4680 --namespace=crd-publish-openapi-4680 create -f -'
Jun 28 08:12:02.249: INFO: rc: 1
Jun 28 08:12:02.249: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=crd-publish-openapi-4680 --namespace=crd-publish-openapi-4680 apply -f -'
Jun 28 08:12:02.458: INFO: rc: 1
STEP: kubectl explain works to explain CR properties 06/28/23 08:12:02.458
Jun 28 08:12:02.458: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=crd-publish-openapi-4680 explain e2e-test-crd-publish-openapi-2707-crds'
Jun 28 08:12:02.664: INFO: stderr: ""
Jun 28 08:12:02.664: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2707-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively 06/28/23 08:12:02.664
Jun 28 08:12:02.664: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=crd-publish-openapi-4680 explain e2e-test-crd-publish-openapi-2707-crds.metadata'
Jun 28 08:12:02.859: INFO: stderr: ""
Jun 28 08:12:02.859: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2707-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Jun 28 08:12:02.859: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=crd-publish-openapi-4680 explain e2e-test-crd-publish-openapi-2707-crds.spec'
Jun 28 08:12:03.056: INFO: stderr: ""
Jun 28 08:12:03.056: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2707-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Jun 28 08:12:03.056: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=crd-publish-openapi-4680 explain e2e-test-crd-publish-openapi-2707-crds.spec.bars'
Jun 28 08:12:03.257: INFO: stderr: ""
Jun 28 08:12:03.257: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2707-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist 06/28/23 08:12:03.257
Jun 28 08:12:03.257: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=crd-publish-openapi-4680 explain e2e-test-crd-publish-openapi-2707-crds.spec.bars2'
Jun 28 08:12:03.457: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jun 28 08:12:05.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4680" for this suite. 06/28/23 08:12:05.642
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","completed":111,"skipped":2029,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.415 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:11:58.234
    Jun 28 08:11:58.234: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename crd-publish-openapi 06/28/23 08:11:58.235
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:11:58.247
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:11:58.252
    [It] works for CRD with validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:68
    Jun 28 08:11:58.259: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 06/28/23 08:12:00.384
    Jun 28 08:12:00.384: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=crd-publish-openapi-4680 --namespace=crd-publish-openapi-4680 create -f -'
    Jun 28 08:12:01.020: INFO: stderr: ""
    Jun 28 08:12:01.020: INFO: stdout: "e2e-test-crd-publish-openapi-2707-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Jun 28 08:12:01.020: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=crd-publish-openapi-4680 --namespace=crd-publish-openapi-4680 delete e2e-test-crd-publish-openapi-2707-crds test-foo'
    Jun 28 08:12:01.096: INFO: stderr: ""
    Jun 28 08:12:01.096: INFO: stdout: "e2e-test-crd-publish-openapi-2707-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    Jun 28 08:12:01.096: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=crd-publish-openapi-4680 --namespace=crd-publish-openapi-4680 apply -f -'
    Jun 28 08:12:01.309: INFO: stderr: ""
    Jun 28 08:12:01.309: INFO: stdout: "e2e-test-crd-publish-openapi-2707-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Jun 28 08:12:01.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=crd-publish-openapi-4680 --namespace=crd-publish-openapi-4680 delete e2e-test-crd-publish-openapi-2707-crds test-foo'
    Jun 28 08:12:01.392: INFO: stderr: ""
    Jun 28 08:12:01.392: INFO: stdout: "e2e-test-crd-publish-openapi-2707-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 06/28/23 08:12:01.392
    Jun 28 08:12:01.392: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=crd-publish-openapi-4680 --namespace=crd-publish-openapi-4680 create -f -'
    Jun 28 08:12:01.604: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 06/28/23 08:12:01.604
    Jun 28 08:12:01.604: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=crd-publish-openapi-4680 --namespace=crd-publish-openapi-4680 create -f -'
    Jun 28 08:12:01.817: INFO: rc: 1
    Jun 28 08:12:01.817: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=crd-publish-openapi-4680 --namespace=crd-publish-openapi-4680 apply -f -'
    Jun 28 08:12:02.040: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request without required properties 06/28/23 08:12:02.04
    Jun 28 08:12:02.040: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=crd-publish-openapi-4680 --namespace=crd-publish-openapi-4680 create -f -'
    Jun 28 08:12:02.249: INFO: rc: 1
    Jun 28 08:12:02.249: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=crd-publish-openapi-4680 --namespace=crd-publish-openapi-4680 apply -f -'
    Jun 28 08:12:02.458: INFO: rc: 1
    STEP: kubectl explain works to explain CR properties 06/28/23 08:12:02.458
    Jun 28 08:12:02.458: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=crd-publish-openapi-4680 explain e2e-test-crd-publish-openapi-2707-crds'
    Jun 28 08:12:02.664: INFO: stderr: ""
    Jun 28 08:12:02.664: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2707-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
    STEP: kubectl explain works to explain CR properties recursively 06/28/23 08:12:02.664
    Jun 28 08:12:02.664: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=crd-publish-openapi-4680 explain e2e-test-crd-publish-openapi-2707-crds.metadata'
    Jun 28 08:12:02.859: INFO: stderr: ""
    Jun 28 08:12:02.859: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2707-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
    Jun 28 08:12:02.859: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=crd-publish-openapi-4680 explain e2e-test-crd-publish-openapi-2707-crds.spec'
    Jun 28 08:12:03.056: INFO: stderr: ""
    Jun 28 08:12:03.056: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2707-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
    Jun 28 08:12:03.056: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=crd-publish-openapi-4680 explain e2e-test-crd-publish-openapi-2707-crds.spec.bars'
    Jun 28 08:12:03.257: INFO: stderr: ""
    Jun 28 08:12:03.257: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2707-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
    STEP: kubectl explain works to return error when explain is called on property that doesn't exist 06/28/23 08:12:03.257
    Jun 28 08:12:03.257: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=crd-publish-openapi-4680 explain e2e-test-crd-publish-openapi-2707-crds.spec.bars2'
    Jun 28 08:12:03.457: INFO: rc: 1
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jun 28 08:12:05.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-4680" for this suite. 06/28/23 08:12:05.642
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:12:05.651
Jun 28 08:12:05.651: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename configmap 06/28/23 08:12:05.652
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:12:05.664
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:12:05.669
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174
STEP: Creating configMap with name configmap-test-upd-a1318054-608e-4b3b-9a77-1cb0a8e005d1 06/28/23 08:12:05.682
STEP: Creating the pod 06/28/23 08:12:05.688
Jun 28 08:12:05.697: INFO: Waiting up to 5m0s for pod "pod-configmaps-7af56057-72cd-4064-85d4-4b8dea4c81cc" in namespace "configmap-5042" to be "running"
Jun 28 08:12:05.702: INFO: Pod "pod-configmaps-7af56057-72cd-4064-85d4-4b8dea4c81cc": Phase="Pending", Reason="", readiness=false. Elapsed: 5.213714ms
Jun 28 08:12:07.708: INFO: Pod "pod-configmaps-7af56057-72cd-4064-85d4-4b8dea4c81cc": Phase="Running", Reason="", readiness=false. Elapsed: 2.010897923s
Jun 28 08:12:07.708: INFO: Pod "pod-configmaps-7af56057-72cd-4064-85d4-4b8dea4c81cc" satisfied condition "running"
STEP: Waiting for pod with text data 06/28/23 08:12:07.708
STEP: Waiting for pod with binary data 06/28/23 08:12:07.719
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jun 28 08:12:07.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5042" for this suite. 06/28/23 08:12:07.812
{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","completed":112,"skipped":2040,"failed":0}
------------------------------
â€¢ [2.168 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:12:05.651
    Jun 28 08:12:05.651: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename configmap 06/28/23 08:12:05.652
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:12:05.664
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:12:05.669
    [It] binary data should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:174
    STEP: Creating configMap with name configmap-test-upd-a1318054-608e-4b3b-9a77-1cb0a8e005d1 06/28/23 08:12:05.682
    STEP: Creating the pod 06/28/23 08:12:05.688
    Jun 28 08:12:05.697: INFO: Waiting up to 5m0s for pod "pod-configmaps-7af56057-72cd-4064-85d4-4b8dea4c81cc" in namespace "configmap-5042" to be "running"
    Jun 28 08:12:05.702: INFO: Pod "pod-configmaps-7af56057-72cd-4064-85d4-4b8dea4c81cc": Phase="Pending", Reason="", readiness=false. Elapsed: 5.213714ms
    Jun 28 08:12:07.708: INFO: Pod "pod-configmaps-7af56057-72cd-4064-85d4-4b8dea4c81cc": Phase="Running", Reason="", readiness=false. Elapsed: 2.010897923s
    Jun 28 08:12:07.708: INFO: Pod "pod-configmaps-7af56057-72cd-4064-85d4-4b8dea4c81cc" satisfied condition "running"
    STEP: Waiting for pod with text data 06/28/23 08:12:07.708
    STEP: Waiting for pod with binary data 06/28/23 08:12:07.719
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jun 28 08:12:07.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-5042" for this suite. 06/28/23 08:12:07.812
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:12:07.819
Jun 28 08:12:07.819: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename emptydir 06/28/23 08:12:07.82
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:12:07.835
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:12:07.84
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206
STEP: Creating a pod to test emptydir 0666 on node default medium 06/28/23 08:12:07.846
Jun 28 08:12:07.855: INFO: Waiting up to 5m0s for pod "pod-542948ee-aa2a-47dd-82cc-779b8faf07c7" in namespace "emptydir-4606" to be "Succeeded or Failed"
Jun 28 08:12:07.860: INFO: Pod "pod-542948ee-aa2a-47dd-82cc-779b8faf07c7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.329741ms
Jun 28 08:12:09.869: INFO: Pod "pod-542948ee-aa2a-47dd-82cc-779b8faf07c7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014339992s
Jun 28 08:12:11.867: INFO: Pod "pod-542948ee-aa2a-47dd-82cc-779b8faf07c7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012087596s
STEP: Saw pod success 06/28/23 08:12:11.867
Jun 28 08:12:11.867: INFO: Pod "pod-542948ee-aa2a-47dd-82cc-779b8faf07c7" satisfied condition "Succeeded or Failed"
Jun 28 08:12:11.873: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-zxlfv pod pod-542948ee-aa2a-47dd-82cc-779b8faf07c7 container test-container: <nil>
STEP: delete the pod 06/28/23 08:12:11.924
Jun 28 08:12:11.940: INFO: Waiting for pod pod-542948ee-aa2a-47dd-82cc-779b8faf07c7 to disappear
Jun 28 08:12:11.945: INFO: Pod pod-542948ee-aa2a-47dd-82cc-779b8faf07c7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jun 28 08:12:11.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4606" for this suite. 06/28/23 08:12:11.953
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":113,"skipped":2049,"failed":0}
------------------------------
â€¢ [4.142 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:12:07.819
    Jun 28 08:12:07.819: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename emptydir 06/28/23 08:12:07.82
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:12:07.835
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:12:07.84
    [It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:206
    STEP: Creating a pod to test emptydir 0666 on node default medium 06/28/23 08:12:07.846
    Jun 28 08:12:07.855: INFO: Waiting up to 5m0s for pod "pod-542948ee-aa2a-47dd-82cc-779b8faf07c7" in namespace "emptydir-4606" to be "Succeeded or Failed"
    Jun 28 08:12:07.860: INFO: Pod "pod-542948ee-aa2a-47dd-82cc-779b8faf07c7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.329741ms
    Jun 28 08:12:09.869: INFO: Pod "pod-542948ee-aa2a-47dd-82cc-779b8faf07c7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014339992s
    Jun 28 08:12:11.867: INFO: Pod "pod-542948ee-aa2a-47dd-82cc-779b8faf07c7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012087596s
    STEP: Saw pod success 06/28/23 08:12:11.867
    Jun 28 08:12:11.867: INFO: Pod "pod-542948ee-aa2a-47dd-82cc-779b8faf07c7" satisfied condition "Succeeded or Failed"
    Jun 28 08:12:11.873: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-zxlfv pod pod-542948ee-aa2a-47dd-82cc-779b8faf07c7 container test-container: <nil>
    STEP: delete the pod 06/28/23 08:12:11.924
    Jun 28 08:12:11.940: INFO: Waiting for pod pod-542948ee-aa2a-47dd-82cc-779b8faf07c7 to disappear
    Jun 28 08:12:11.945: INFO: Pod pod-542948ee-aa2a-47dd-82cc-779b8faf07c7 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jun 28 08:12:11.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-4606" for this suite. 06/28/23 08:12:11.953
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:12:11.961
Jun 28 08:12:11.961: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename namespaces 06/28/23 08:12:11.962
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:12:11.978
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:12:11.987
[It] should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298
STEP: Read namespace status 06/28/23 08:12:11.992
Jun 28 08:12:11.997: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
STEP: Patch namespace status 06/28/23 08:12:11.997
Jun 28 08:12:12.005: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
STEP: Update namespace status 06/28/23 08:12:12.005
Jun 28 08:12:12.016: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Jun 28 08:12:12.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-3298" for this suite. 06/28/23 08:12:12.024
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should apply changes to a namespace status [Conformance]","completed":114,"skipped":2050,"failed":0}
------------------------------
â€¢ [0.070 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:12:11.961
    Jun 28 08:12:11.961: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename namespaces 06/28/23 08:12:11.962
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:12:11.978
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:12:11.987
    [It] should apply changes to a namespace status [Conformance]
      test/e2e/apimachinery/namespace.go:298
    STEP: Read namespace status 06/28/23 08:12:11.992
    Jun 28 08:12:11.997: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
    STEP: Patch namespace status 06/28/23 08:12:11.997
    Jun 28 08:12:12.005: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
    STEP: Update namespace status 06/28/23 08:12:12.005
    Jun 28 08:12:12.016: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Jun 28 08:12:12.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-3298" for this suite. 06/28/23 08:12:12.024
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch
  watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:12:12.032
Jun 28 08:12:12.032: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename crd-watch 06/28/23 08:12:12.033
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:12:12.048
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:12:12.054
[It] watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
Jun 28 08:12:12.059: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Creating first CR  06/28/23 08:12:14.623
Jun 28 08:12:14.629: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-06-28T08:12:14Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-06-28T08:12:14Z]] name:name1 resourceVersion:64715 uid:ec6eb212-bbf6-48e3-b990-0bdbfc5e1044] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR 06/28/23 08:12:24.63
Jun 28 08:12:24.636: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-06-28T08:12:24Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-06-28T08:12:24Z]] name:name2 resourceVersion:64763 uid:1b6e03b6-b85e-4a59-9368-9faf2f314c6c] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR 06/28/23 08:12:34.637
Jun 28 08:12:34.646: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-06-28T08:12:14Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-06-28T08:12:34Z]] name:name1 resourceVersion:64798 uid:ec6eb212-bbf6-48e3-b990-0bdbfc5e1044] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR 06/28/23 08:12:44.647
Jun 28 08:12:44.655: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-06-28T08:12:24Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-06-28T08:12:44Z]] name:name2 resourceVersion:64834 uid:1b6e03b6-b85e-4a59-9368-9faf2f314c6c] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR 06/28/23 08:12:54.657
Jun 28 08:12:54.666: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-06-28T08:12:14Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-06-28T08:12:34Z]] name:name1 resourceVersion:64869 uid:ec6eb212-bbf6-48e3-b990-0bdbfc5e1044] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR 06/28/23 08:13:04.667
Jun 28 08:13:04.676: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-06-28T08:12:24Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-06-28T08:12:44Z]] name:name2 resourceVersion:64904 uid:1b6e03b6-b85e-4a59-9368-9faf2f314c6c] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jun 28 08:13:15.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-7966" for this suite. 06/28/23 08:13:15.201
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","completed":115,"skipped":2067,"failed":0}
------------------------------
â€¢ [SLOW TEST] [63.177 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  test/e2e/apimachinery/crd_watch.go:44
    watch on custom resource definition objects [Conformance]
    test/e2e/apimachinery/crd_watch.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:12:12.032
    Jun 28 08:12:12.032: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename crd-watch 06/28/23 08:12:12.033
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:12:12.048
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:12:12.054
    [It] watch on custom resource definition objects [Conformance]
      test/e2e/apimachinery/crd_watch.go:51
    Jun 28 08:12:12.059: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Creating first CR  06/28/23 08:12:14.623
    Jun 28 08:12:14.629: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-06-28T08:12:14Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-06-28T08:12:14Z]] name:name1 resourceVersion:64715 uid:ec6eb212-bbf6-48e3-b990-0bdbfc5e1044] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Creating second CR 06/28/23 08:12:24.63
    Jun 28 08:12:24.636: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-06-28T08:12:24Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-06-28T08:12:24Z]] name:name2 resourceVersion:64763 uid:1b6e03b6-b85e-4a59-9368-9faf2f314c6c] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying first CR 06/28/23 08:12:34.637
    Jun 28 08:12:34.646: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-06-28T08:12:14Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-06-28T08:12:34Z]] name:name1 resourceVersion:64798 uid:ec6eb212-bbf6-48e3-b990-0bdbfc5e1044] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying second CR 06/28/23 08:12:44.647
    Jun 28 08:12:44.655: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-06-28T08:12:24Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-06-28T08:12:44Z]] name:name2 resourceVersion:64834 uid:1b6e03b6-b85e-4a59-9368-9faf2f314c6c] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting first CR 06/28/23 08:12:54.657
    Jun 28 08:12:54.666: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-06-28T08:12:14Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-06-28T08:12:34Z]] name:name1 resourceVersion:64869 uid:ec6eb212-bbf6-48e3-b990-0bdbfc5e1044] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting second CR 06/28/23 08:13:04.667
    Jun 28 08:13:04.676: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-06-28T08:12:24Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-06-28T08:12:44Z]] name:name2 resourceVersion:64904 uid:1b6e03b6-b85e-4a59-9368-9faf2f314c6c] num:map[num1:9223372036854775807 num2:1000000]]}
    [AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jun 28 08:13:15.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-watch-7966" for this suite. 06/28/23 08:13:15.201
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:13:15.21
Jun 28 08:13:15.210: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename secrets 06/28/23 08:13:15.21
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:13:15.223
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:13:15.229
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78
STEP: Creating secret with name secret-test-map-f862086e-37da-4db5-b61c-f2f23064ee34 06/28/23 08:13:15.233
STEP: Creating a pod to test consume secrets 06/28/23 08:13:15.238
Jun 28 08:13:15.247: INFO: Waiting up to 5m0s for pod "pod-secrets-30953db5-dde5-4022-a4b4-2d2b27dba2ca" in namespace "secrets-1391" to be "Succeeded or Failed"
Jun 28 08:13:15.251: INFO: Pod "pod-secrets-30953db5-dde5-4022-a4b4-2d2b27dba2ca": Phase="Pending", Reason="", readiness=false. Elapsed: 4.270014ms
Jun 28 08:13:17.257: INFO: Pod "pod-secrets-30953db5-dde5-4022-a4b4-2d2b27dba2ca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010271445s
Jun 28 08:13:19.257: INFO: Pod "pod-secrets-30953db5-dde5-4022-a4b4-2d2b27dba2ca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009694514s
STEP: Saw pod success 06/28/23 08:13:19.257
Jun 28 08:13:19.257: INFO: Pod "pod-secrets-30953db5-dde5-4022-a4b4-2d2b27dba2ca" satisfied condition "Succeeded or Failed"
Jun 28 08:13:19.261: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod pod-secrets-30953db5-dde5-4022-a4b4-2d2b27dba2ca container secret-volume-test: <nil>
STEP: delete the pod 06/28/23 08:13:19.31
Jun 28 08:13:19.320: INFO: Waiting for pod pod-secrets-30953db5-dde5-4022-a4b4-2d2b27dba2ca to disappear
Jun 28 08:13:19.323: INFO: Pod pod-secrets-30953db5-dde5-4022-a4b4-2d2b27dba2ca no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jun 28 08:13:19.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1391" for this suite. 06/28/23 08:13:19.33
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":116,"skipped":2073,"failed":0}
------------------------------
â€¢ [4.126 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:13:15.21
    Jun 28 08:13:15.210: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename secrets 06/28/23 08:13:15.21
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:13:15.223
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:13:15.229
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:78
    STEP: Creating secret with name secret-test-map-f862086e-37da-4db5-b61c-f2f23064ee34 06/28/23 08:13:15.233
    STEP: Creating a pod to test consume secrets 06/28/23 08:13:15.238
    Jun 28 08:13:15.247: INFO: Waiting up to 5m0s for pod "pod-secrets-30953db5-dde5-4022-a4b4-2d2b27dba2ca" in namespace "secrets-1391" to be "Succeeded or Failed"
    Jun 28 08:13:15.251: INFO: Pod "pod-secrets-30953db5-dde5-4022-a4b4-2d2b27dba2ca": Phase="Pending", Reason="", readiness=false. Elapsed: 4.270014ms
    Jun 28 08:13:17.257: INFO: Pod "pod-secrets-30953db5-dde5-4022-a4b4-2d2b27dba2ca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010271445s
    Jun 28 08:13:19.257: INFO: Pod "pod-secrets-30953db5-dde5-4022-a4b4-2d2b27dba2ca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009694514s
    STEP: Saw pod success 06/28/23 08:13:19.257
    Jun 28 08:13:19.257: INFO: Pod "pod-secrets-30953db5-dde5-4022-a4b4-2d2b27dba2ca" satisfied condition "Succeeded or Failed"
    Jun 28 08:13:19.261: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod pod-secrets-30953db5-dde5-4022-a4b4-2d2b27dba2ca container secret-volume-test: <nil>
    STEP: delete the pod 06/28/23 08:13:19.31
    Jun 28 08:13:19.320: INFO: Waiting for pod pod-secrets-30953db5-dde5-4022-a4b4-2d2b27dba2ca to disappear
    Jun 28 08:13:19.323: INFO: Pod pod-secrets-30953db5-dde5-4022-a4b4-2d2b27dba2ca no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jun 28 08:13:19.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-1391" for this suite. 06/28/23 08:13:19.33
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:13:19.336
Jun 28 08:13:19.336: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename crd-publish-openapi 06/28/23 08:13:19.337
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:13:19.349
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:13:19.353
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193
Jun 28 08:13:19.357: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 06/28/23 08:13:22.948
Jun 28 08:13:22.949: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=crd-publish-openapi-4807 --namespace=crd-publish-openapi-4807 create -f -'
Jun 28 08:13:23.512: INFO: stderr: ""
Jun 28 08:13:23.512: INFO: stdout: "e2e-test-crd-publish-openapi-5660-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jun 28 08:13:23.512: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=crd-publish-openapi-4807 --namespace=crd-publish-openapi-4807 delete e2e-test-crd-publish-openapi-5660-crds test-cr'
Jun 28 08:13:23.583: INFO: stderr: ""
Jun 28 08:13:23.583: INFO: stdout: "e2e-test-crd-publish-openapi-5660-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Jun 28 08:13:23.583: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=crd-publish-openapi-4807 --namespace=crd-publish-openapi-4807 apply -f -'
Jun 28 08:13:23.788: INFO: stderr: ""
Jun 28 08:13:23.788: INFO: stdout: "e2e-test-crd-publish-openapi-5660-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jun 28 08:13:23.788: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=crd-publish-openapi-4807 --namespace=crd-publish-openapi-4807 delete e2e-test-crd-publish-openapi-5660-crds test-cr'
Jun 28 08:13:23.869: INFO: stderr: ""
Jun 28 08:13:23.869: INFO: stdout: "e2e-test-crd-publish-openapi-5660-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 06/28/23 08:13:23.869
Jun 28 08:13:23.869: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=crd-publish-openapi-4807 explain e2e-test-crd-publish-openapi-5660-crds'
Jun 28 08:13:24.054: INFO: stderr: ""
Jun 28 08:13:24.054: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5660-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jun 28 08:13:26.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4807" for this suite. 06/28/23 08:13:26.167
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","completed":117,"skipped":2079,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.838 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:13:19.336
    Jun 28 08:13:19.336: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename crd-publish-openapi 06/28/23 08:13:19.337
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:13:19.349
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:13:19.353
    [It] works for CRD preserving unknown fields at the schema root [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:193
    Jun 28 08:13:19.357: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 06/28/23 08:13:22.948
    Jun 28 08:13:22.949: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=crd-publish-openapi-4807 --namespace=crd-publish-openapi-4807 create -f -'
    Jun 28 08:13:23.512: INFO: stderr: ""
    Jun 28 08:13:23.512: INFO: stdout: "e2e-test-crd-publish-openapi-5660-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Jun 28 08:13:23.512: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=crd-publish-openapi-4807 --namespace=crd-publish-openapi-4807 delete e2e-test-crd-publish-openapi-5660-crds test-cr'
    Jun 28 08:13:23.583: INFO: stderr: ""
    Jun 28 08:13:23.583: INFO: stdout: "e2e-test-crd-publish-openapi-5660-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    Jun 28 08:13:23.583: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=crd-publish-openapi-4807 --namespace=crd-publish-openapi-4807 apply -f -'
    Jun 28 08:13:23.788: INFO: stderr: ""
    Jun 28 08:13:23.788: INFO: stdout: "e2e-test-crd-publish-openapi-5660-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Jun 28 08:13:23.788: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=crd-publish-openapi-4807 --namespace=crd-publish-openapi-4807 delete e2e-test-crd-publish-openapi-5660-crds test-cr'
    Jun 28 08:13:23.869: INFO: stderr: ""
    Jun 28 08:13:23.869: INFO: stdout: "e2e-test-crd-publish-openapi-5660-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 06/28/23 08:13:23.869
    Jun 28 08:13:23.869: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=crd-publish-openapi-4807 explain e2e-test-crd-publish-openapi-5660-crds'
    Jun 28 08:13:24.054: INFO: stderr: ""
    Jun 28 08:13:24.054: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5660-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jun 28 08:13:26.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-4807" for this suite. 06/28/23 08:13:26.167
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:13:26.175
Jun 28 08:13:26.175: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename sched-preemption 06/28/23 08:13:26.176
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:13:26.188
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:13:26.193
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Jun 28 08:13:26.213: INFO: Waiting up to 1m0s for all nodes to be ready
Jun 28 08:14:26.262: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125
STEP: Create pods that use 4/5 of node resources. 06/28/23 08:14:26.266
Jun 28 08:14:26.295: INFO: Created pod: pod0-0-sched-preemption-low-priority
Jun 28 08:14:26.305: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Jun 28 08:14:26.328: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Jun 28 08:14:26.335: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Jun 28 08:14:26.356: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Jun 28 08:14:26.363: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 06/28/23 08:14:26.363
Jun 28 08:14:26.363: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-5221" to be "running"
Jun 28 08:14:26.367: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 3.840572ms
Jun 28 08:14:28.372: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009464751s
Jun 28 08:14:30.373: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.009578974s
Jun 28 08:14:30.373: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Jun 28 08:14:30.373: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-5221" to be "running"
Jun 28 08:14:30.377: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.389004ms
Jun 28 08:14:30.377: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Jun 28 08:14:30.377: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-5221" to be "running"
Jun 28 08:14:30.383: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 5.515712ms
Jun 28 08:14:32.388: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011169528s
Jun 28 08:14:34.389: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.011685277s
Jun 28 08:14:34.389: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Jun 28 08:14:34.389: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-5221" to be "running"
Jun 28 08:14:34.399: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 9.974668ms
Jun 28 08:14:34.399: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
Jun 28 08:14:34.399: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-5221" to be "running"
Jun 28 08:14:34.404: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.998319ms
Jun 28 08:14:36.409: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.010254472s
Jun 28 08:14:36.409: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
Jun 28 08:14:36.409: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-5221" to be "running"
Jun 28 08:14:36.414: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.860643ms
Jun 28 08:14:36.414: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a high priority pod that has same requirements as that of lower priority pod 06/28/23 08:14:36.414
Jun 28 08:14:36.421: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-5221" to be "running"
Jun 28 08:14:36.424: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.496743ms
Jun 28 08:14:38.429: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00798927s
Jun 28 08:14:40.430: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.00915293s
Jun 28 08:14:40.430: INFO: Pod "preemptor-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Jun 28 08:14:40.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-5221" for this suite. 06/28/23 08:14:40.465
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","completed":118,"skipped":2082,"failed":0}
------------------------------
â€¢ [SLOW TEST] [74.350 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:13:26.175
    Jun 28 08:13:26.175: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename sched-preemption 06/28/23 08:13:26.176
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:13:26.188
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:13:26.193
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Jun 28 08:13:26.213: INFO: Waiting up to 1m0s for all nodes to be ready
    Jun 28 08:14:26.262: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates basic preemption works [Conformance]
      test/e2e/scheduling/preemption.go:125
    STEP: Create pods that use 4/5 of node resources. 06/28/23 08:14:26.266
    Jun 28 08:14:26.295: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Jun 28 08:14:26.305: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Jun 28 08:14:26.328: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Jun 28 08:14:26.335: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    Jun 28 08:14:26.356: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    Jun 28 08:14:26.363: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 06/28/23 08:14:26.363
    Jun 28 08:14:26.363: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-5221" to be "running"
    Jun 28 08:14:26.367: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 3.840572ms
    Jun 28 08:14:28.372: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009464751s
    Jun 28 08:14:30.373: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.009578974s
    Jun 28 08:14:30.373: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Jun 28 08:14:30.373: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-5221" to be "running"
    Jun 28 08:14:30.377: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.389004ms
    Jun 28 08:14:30.377: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Jun 28 08:14:30.377: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-5221" to be "running"
    Jun 28 08:14:30.383: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 5.515712ms
    Jun 28 08:14:32.388: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011169528s
    Jun 28 08:14:34.389: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.011685277s
    Jun 28 08:14:34.389: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Jun 28 08:14:34.389: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-5221" to be "running"
    Jun 28 08:14:34.399: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 9.974668ms
    Jun 28 08:14:34.399: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    Jun 28 08:14:34.399: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-5221" to be "running"
    Jun 28 08:14:34.404: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.998319ms
    Jun 28 08:14:36.409: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.010254472s
    Jun 28 08:14:36.409: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    Jun 28 08:14:36.409: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-5221" to be "running"
    Jun 28 08:14:36.414: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.860643ms
    Jun 28 08:14:36.414: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a high priority pod that has same requirements as that of lower priority pod 06/28/23 08:14:36.414
    Jun 28 08:14:36.421: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-5221" to be "running"
    Jun 28 08:14:36.424: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.496743ms
    Jun 28 08:14:38.429: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00798927s
    Jun 28 08:14:40.430: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.00915293s
    Jun 28 08:14:40.430: INFO: Pod "preemptor-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Jun 28 08:14:40.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-5221" for this suite. 06/28/23 08:14:40.465
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] IngressClass API
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
[BeforeEach] [sig-network] IngressClass API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:14:40.526
Jun 28 08:14:40.526: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename ingressclass 06/28/23 08:14:40.527
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:14:40.54
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:14:40.545
[BeforeEach] [sig-network] IngressClass API
  test/e2e/network/ingressclass.go:211
[It]  should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
STEP: getting /apis 06/28/23 08:14:40.549
STEP: getting /apis/networking.k8s.io 06/28/23 08:14:40.553
STEP: getting /apis/networking.k8s.iov1 06/28/23 08:14:40.557
STEP: creating 06/28/23 08:14:40.559
STEP: getting 06/28/23 08:14:40.577
STEP: listing 06/28/23 08:14:40.581
STEP: watching 06/28/23 08:14:40.588
Jun 28 08:14:40.588: INFO: starting watch
STEP: patching 06/28/23 08:14:40.59
STEP: updating 06/28/23 08:14:40.601
Jun 28 08:14:40.614: INFO: waiting for watch events with expected annotations
Jun 28 08:14:40.614: INFO: saw patched and updated annotations
STEP: deleting 06/28/23 08:14:40.614
STEP: deleting a collection 06/28/23 08:14:40.641
[AfterEach] [sig-network] IngressClass API
  test/e2e/framework/framework.go:187
Jun 28 08:14:40.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-4003" for this suite. 06/28/23 08:14:40.665
{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","completed":119,"skipped":2095,"failed":0}
------------------------------
â€¢ [0.145 seconds]
[sig-network] IngressClass API
test/e2e/network/common/framework.go:23
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:14:40.526
    Jun 28 08:14:40.526: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename ingressclass 06/28/23 08:14:40.527
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:14:40.54
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:14:40.545
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/network/ingressclass.go:211
    [It]  should support creating IngressClass API operations [Conformance]
      test/e2e/network/ingressclass.go:223
    STEP: getting /apis 06/28/23 08:14:40.549
    STEP: getting /apis/networking.k8s.io 06/28/23 08:14:40.553
    STEP: getting /apis/networking.k8s.iov1 06/28/23 08:14:40.557
    STEP: creating 06/28/23 08:14:40.559
    STEP: getting 06/28/23 08:14:40.577
    STEP: listing 06/28/23 08:14:40.581
    STEP: watching 06/28/23 08:14:40.588
    Jun 28 08:14:40.588: INFO: starting watch
    STEP: patching 06/28/23 08:14:40.59
    STEP: updating 06/28/23 08:14:40.601
    Jun 28 08:14:40.614: INFO: waiting for watch events with expected annotations
    Jun 28 08:14:40.614: INFO: saw patched and updated annotations
    STEP: deleting 06/28/23 08:14:40.614
    STEP: deleting a collection 06/28/23 08:14:40.641
    [AfterEach] [sig-network] IngressClass API
      test/e2e/framework/framework.go:187
    Jun 28 08:14:40.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ingressclass-4003" for this suite. 06/28/23 08:14:40.665
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:14:40.671
Jun 28 08:14:40.671: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename resourcequota 06/28/23 08:14:40.672
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:14:40.685
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:14:40.69
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438
STEP: Counting existing ResourceQuota 06/28/23 08:14:40.696
STEP: Creating a ResourceQuota 06/28/23 08:14:45.701
STEP: Ensuring resource quota status is calculated 06/28/23 08:14:45.708
STEP: Creating a ReplicaSet 06/28/23 08:14:47.713
STEP: Ensuring resource quota status captures replicaset creation 06/28/23 08:14:47.724
STEP: Deleting a ReplicaSet 06/28/23 08:14:49.731
STEP: Ensuring resource quota status released usage 06/28/23 08:14:49.738
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jun 28 08:14:51.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8807" for this suite. 06/28/23 08:14:51.751
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","completed":120,"skipped":2099,"failed":0}
------------------------------
â€¢ [SLOW TEST] [11.087 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:14:40.671
    Jun 28 08:14:40.671: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename resourcequota 06/28/23 08:14:40.672
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:14:40.685
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:14:40.69
    [It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
      test/e2e/apimachinery/resource_quota.go:438
    STEP: Counting existing ResourceQuota 06/28/23 08:14:40.696
    STEP: Creating a ResourceQuota 06/28/23 08:14:45.701
    STEP: Ensuring resource quota status is calculated 06/28/23 08:14:45.708
    STEP: Creating a ReplicaSet 06/28/23 08:14:47.713
    STEP: Ensuring resource quota status captures replicaset creation 06/28/23 08:14:47.724
    STEP: Deleting a ReplicaSet 06/28/23 08:14:49.731
    STEP: Ensuring resource quota status released usage 06/28/23 08:14:49.738
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jun 28 08:14:51.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-8807" for this suite. 06/28/23 08:14:51.751
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Containers
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:14:51.759
Jun 28 08:14:51.759: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename containers 06/28/23 08:14:51.76
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:14:51.772
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:14:51.777
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38
Jun 28 08:14:51.790: INFO: Waiting up to 5m0s for pod "client-containers-6355e7e9-0986-4ddb-928b-3efb2bfd322a" in namespace "containers-2058" to be "running"
Jun 28 08:14:51.795: INFO: Pod "client-containers-6355e7e9-0986-4ddb-928b-3efb2bfd322a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.435437ms
Jun 28 08:14:53.799: INFO: Pod "client-containers-6355e7e9-0986-4ddb-928b-3efb2bfd322a": Phase="Running", Reason="", readiness=true. Elapsed: 2.009019564s
Jun 28 08:14:53.799: INFO: Pod "client-containers-6355e7e9-0986-4ddb-928b-3efb2bfd322a" satisfied condition "running"
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Jun 28 08:14:53.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-2058" for this suite. 06/28/23 08:14:53.857
{"msg":"PASSED [sig-node] Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","completed":121,"skipped":2103,"failed":0}
------------------------------
â€¢ [2.105 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:14:51.759
    Jun 28 08:14:51.759: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename containers 06/28/23 08:14:51.76
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:14:51.772
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:14:51.777
    [It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:38
    Jun 28 08:14:51.790: INFO: Waiting up to 5m0s for pod "client-containers-6355e7e9-0986-4ddb-928b-3efb2bfd322a" in namespace "containers-2058" to be "running"
    Jun 28 08:14:51.795: INFO: Pod "client-containers-6355e7e9-0986-4ddb-928b-3efb2bfd322a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.435437ms
    Jun 28 08:14:53.799: INFO: Pod "client-containers-6355e7e9-0986-4ddb-928b-3efb2bfd322a": Phase="Running", Reason="", readiness=true. Elapsed: 2.009019564s
    Jun 28 08:14:53.799: INFO: Pod "client-containers-6355e7e9-0986-4ddb-928b-3efb2bfd322a" satisfied condition "running"
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Jun 28 08:14:53.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-2058" for this suite. 06/28/23 08:14:53.857
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:14:53.864
Jun 28 08:14:53.865: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename configmap 06/28/23 08:14:53.866
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:14:53.877
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:14:53.881
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46
STEP: Creating configMap with name configmap-test-volume-d34bbf4d-909d-439c-8037-85527702f782 06/28/23 08:14:53.885
STEP: Creating a pod to test consume configMaps 06/28/23 08:14:53.891
Jun 28 08:14:53.899: INFO: Waiting up to 5m0s for pod "pod-configmaps-ea21bcd2-c817-4b6d-8b29-4ce86c54378a" in namespace "configmap-9532" to be "Succeeded or Failed"
Jun 28 08:14:53.907: INFO: Pod "pod-configmaps-ea21bcd2-c817-4b6d-8b29-4ce86c54378a": Phase="Pending", Reason="", readiness=false. Elapsed: 7.245814ms
Jun 28 08:14:55.913: INFO: Pod "pod-configmaps-ea21bcd2-c817-4b6d-8b29-4ce86c54378a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013544461s
Jun 28 08:14:57.912: INFO: Pod "pod-configmaps-ea21bcd2-c817-4b6d-8b29-4ce86c54378a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013102257s
STEP: Saw pod success 06/28/23 08:14:57.912
Jun 28 08:14:57.913: INFO: Pod "pod-configmaps-ea21bcd2-c817-4b6d-8b29-4ce86c54378a" satisfied condition "Succeeded or Failed"
Jun 28 08:14:57.917: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-zxlfv pod pod-configmaps-ea21bcd2-c817-4b6d-8b29-4ce86c54378a container agnhost-container: <nil>
STEP: delete the pod 06/28/23 08:14:57.965
Jun 28 08:14:57.979: INFO: Waiting for pod pod-configmaps-ea21bcd2-c817-4b6d-8b29-4ce86c54378a to disappear
Jun 28 08:14:57.983: INFO: Pod pod-configmaps-ea21bcd2-c817-4b6d-8b29-4ce86c54378a no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jun 28 08:14:57.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9532" for this suite. 06/28/23 08:14:57.992
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","completed":122,"skipped":2113,"failed":0}
------------------------------
â€¢ [4.135 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:14:53.864
    Jun 28 08:14:53.865: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename configmap 06/28/23 08:14:53.866
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:14:53.877
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:14:53.881
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:46
    STEP: Creating configMap with name configmap-test-volume-d34bbf4d-909d-439c-8037-85527702f782 06/28/23 08:14:53.885
    STEP: Creating a pod to test consume configMaps 06/28/23 08:14:53.891
    Jun 28 08:14:53.899: INFO: Waiting up to 5m0s for pod "pod-configmaps-ea21bcd2-c817-4b6d-8b29-4ce86c54378a" in namespace "configmap-9532" to be "Succeeded or Failed"
    Jun 28 08:14:53.907: INFO: Pod "pod-configmaps-ea21bcd2-c817-4b6d-8b29-4ce86c54378a": Phase="Pending", Reason="", readiness=false. Elapsed: 7.245814ms
    Jun 28 08:14:55.913: INFO: Pod "pod-configmaps-ea21bcd2-c817-4b6d-8b29-4ce86c54378a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013544461s
    Jun 28 08:14:57.912: INFO: Pod "pod-configmaps-ea21bcd2-c817-4b6d-8b29-4ce86c54378a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013102257s
    STEP: Saw pod success 06/28/23 08:14:57.912
    Jun 28 08:14:57.913: INFO: Pod "pod-configmaps-ea21bcd2-c817-4b6d-8b29-4ce86c54378a" satisfied condition "Succeeded or Failed"
    Jun 28 08:14:57.917: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-zxlfv pod pod-configmaps-ea21bcd2-c817-4b6d-8b29-4ce86c54378a container agnhost-container: <nil>
    STEP: delete the pod 06/28/23 08:14:57.965
    Jun 28 08:14:57.979: INFO: Waiting for pod pod-configmaps-ea21bcd2-c817-4b6d-8b29-4ce86c54378a to disappear
    Jun 28 08:14:57.983: INFO: Pod pod-configmaps-ea21bcd2-c817-4b6d-8b29-4ce86c54378a no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jun 28 08:14:57.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-9532" for this suite. 06/28/23 08:14:57.992
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] EndpointSliceMirroring
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:14:57.999
Jun 28 08:14:57.999: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename endpointslicemirroring 06/28/23 08:14:58
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:14:58.013
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:14:58.018
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/network/endpointslicemirroring.go:41
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
STEP: mirroring a new custom Endpoint 06/28/23 08:14:58.033
Jun 28 08:14:58.043: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint 06/28/23 08:15:00.048
STEP: mirroring deletion of a custom Endpoint 06/28/23 08:15:00.058
Jun 28 08:15:00.068: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/framework.go:187
Jun 28 08:15:02.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslicemirroring-3633" for this suite. 06/28/23 08:15:02.083
{"msg":"PASSED [sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]","completed":123,"skipped":2113,"failed":0}
------------------------------
â€¢ [4.093 seconds]
[sig-network] EndpointSliceMirroring
test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:14:57.999
    Jun 28 08:14:57.999: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename endpointslicemirroring 06/28/23 08:14:58
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:14:58.013
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:14:58.018
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/network/endpointslicemirroring.go:41
    [It] should mirror a custom Endpoints resource through create update and delete [Conformance]
      test/e2e/network/endpointslicemirroring.go:53
    STEP: mirroring a new custom Endpoint 06/28/23 08:14:58.033
    Jun 28 08:14:58.043: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
    STEP: mirroring an update to a custom Endpoint 06/28/23 08:15:00.048
    STEP: mirroring deletion of a custom Endpoint 06/28/23 08:15:00.058
    Jun 28 08:15:00.068: INFO: Waiting for 0 EndpointSlices to exist, got 1
    [AfterEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/framework.go:187
    Jun 28 08:15:02.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslicemirroring-3633" for this suite. 06/28/23 08:15:02.083
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:15:02.092
Jun 28 08:15:02.092: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename projected 06/28/23 08:15:02.093
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:15:02.108
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:15:02.114
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87
STEP: Creating projection with secret that has name projected-secret-test-map-b15febc0-a26c-45cb-a46a-23fd97010a62 06/28/23 08:15:02.121
STEP: Creating a pod to test consume secrets 06/28/23 08:15:02.128
Jun 28 08:15:02.138: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-9c6d6b11-65eb-4a9d-9c00-d55285832ec0" in namespace "projected-3802" to be "Succeeded or Failed"
Jun 28 08:15:02.143: INFO: Pod "pod-projected-secrets-9c6d6b11-65eb-4a9d-9c00-d55285832ec0": Phase="Pending", Reason="", readiness=false. Elapsed: 5.285236ms
Jun 28 08:15:04.149: INFO: Pod "pod-projected-secrets-9c6d6b11-65eb-4a9d-9c00-d55285832ec0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011033699s
Jun 28 08:15:06.148: INFO: Pod "pod-projected-secrets-9c6d6b11-65eb-4a9d-9c00-d55285832ec0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010277859s
STEP: Saw pod success 06/28/23 08:15:06.148
Jun 28 08:15:06.148: INFO: Pod "pod-projected-secrets-9c6d6b11-65eb-4a9d-9c00-d55285832ec0" satisfied condition "Succeeded or Failed"
Jun 28 08:15:06.153: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod pod-projected-secrets-9c6d6b11-65eb-4a9d-9c00-d55285832ec0 container projected-secret-volume-test: <nil>
STEP: delete the pod 06/28/23 08:15:06.163
Jun 28 08:15:06.174: INFO: Waiting for pod pod-projected-secrets-9c6d6b11-65eb-4a9d-9c00-d55285832ec0 to disappear
Jun 28 08:15:06.178: INFO: Pod pod-projected-secrets-9c6d6b11-65eb-4a9d-9c00-d55285832ec0 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Jun 28 08:15:06.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3802" for this suite. 06/28/23 08:15:06.186
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":124,"skipped":2125,"failed":0}
------------------------------
â€¢ [4.101 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:15:02.092
    Jun 28 08:15:02.092: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename projected 06/28/23 08:15:02.093
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:15:02.108
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:15:02.114
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:87
    STEP: Creating projection with secret that has name projected-secret-test-map-b15febc0-a26c-45cb-a46a-23fd97010a62 06/28/23 08:15:02.121
    STEP: Creating a pod to test consume secrets 06/28/23 08:15:02.128
    Jun 28 08:15:02.138: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-9c6d6b11-65eb-4a9d-9c00-d55285832ec0" in namespace "projected-3802" to be "Succeeded or Failed"
    Jun 28 08:15:02.143: INFO: Pod "pod-projected-secrets-9c6d6b11-65eb-4a9d-9c00-d55285832ec0": Phase="Pending", Reason="", readiness=false. Elapsed: 5.285236ms
    Jun 28 08:15:04.149: INFO: Pod "pod-projected-secrets-9c6d6b11-65eb-4a9d-9c00-d55285832ec0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011033699s
    Jun 28 08:15:06.148: INFO: Pod "pod-projected-secrets-9c6d6b11-65eb-4a9d-9c00-d55285832ec0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010277859s
    STEP: Saw pod success 06/28/23 08:15:06.148
    Jun 28 08:15:06.148: INFO: Pod "pod-projected-secrets-9c6d6b11-65eb-4a9d-9c00-d55285832ec0" satisfied condition "Succeeded or Failed"
    Jun 28 08:15:06.153: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod pod-projected-secrets-9c6d6b11-65eb-4a9d-9c00-d55285832ec0 container projected-secret-volume-test: <nil>
    STEP: delete the pod 06/28/23 08:15:06.163
    Jun 28 08:15:06.174: INFO: Waiting for pod pod-projected-secrets-9c6d6b11-65eb-4a9d-9c00-d55285832ec0 to disappear
    Jun 28 08:15:06.178: INFO: Pod pod-projected-secrets-9c6d6b11-65eb-4a9d-9c00-d55285832ec0 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Jun 28 08:15:06.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3802" for this suite. 06/28/23 08:15:06.186
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected configMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:15:06.194
Jun 28 08:15:06.194: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename projected 06/28/23 08:15:06.194
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:15:06.206
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:15:06.211
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123
STEP: Creating projection with configMap that has name projected-configmap-test-upd-36b008a4-5b8c-4d3d-acd2-1d19a676d0e6 06/28/23 08:15:06.223
STEP: Creating the pod 06/28/23 08:15:06.228
Jun 28 08:15:06.237: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e2aa02fa-01fc-49fa-84a9-64008d2e3b8c" in namespace "projected-1613" to be "running and ready"
Jun 28 08:15:06.241: INFO: Pod "pod-projected-configmaps-e2aa02fa-01fc-49fa-84a9-64008d2e3b8c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.084469ms
Jun 28 08:15:06.241: INFO: The phase of Pod pod-projected-configmaps-e2aa02fa-01fc-49fa-84a9-64008d2e3b8c is Pending, waiting for it to be Running (with Ready = true)
Jun 28 08:15:08.247: INFO: Pod "pod-projected-configmaps-e2aa02fa-01fc-49fa-84a9-64008d2e3b8c": Phase="Running", Reason="", readiness=true. Elapsed: 2.010008333s
Jun 28 08:15:08.247: INFO: The phase of Pod pod-projected-configmaps-e2aa02fa-01fc-49fa-84a9-64008d2e3b8c is Running (Ready = true)
Jun 28 08:15:08.247: INFO: Pod "pod-projected-configmaps-e2aa02fa-01fc-49fa-84a9-64008d2e3b8c" satisfied condition "running and ready"
STEP: Updating configmap projected-configmap-test-upd-36b008a4-5b8c-4d3d-acd2-1d19a676d0e6 06/28/23 08:15:08.261
STEP: waiting to observe update in volume 06/28/23 08:15:08.267
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jun 28 08:15:10.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1613" for this suite. 06/28/23 08:15:10.366
{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","completed":125,"skipped":2133,"failed":0}
------------------------------
â€¢ [4.181 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:15:06.194
    Jun 28 08:15:06.194: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename projected 06/28/23 08:15:06.194
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:15:06.206
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:15:06.211
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:123
    STEP: Creating projection with configMap that has name projected-configmap-test-upd-36b008a4-5b8c-4d3d-acd2-1d19a676d0e6 06/28/23 08:15:06.223
    STEP: Creating the pod 06/28/23 08:15:06.228
    Jun 28 08:15:06.237: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e2aa02fa-01fc-49fa-84a9-64008d2e3b8c" in namespace "projected-1613" to be "running and ready"
    Jun 28 08:15:06.241: INFO: Pod "pod-projected-configmaps-e2aa02fa-01fc-49fa-84a9-64008d2e3b8c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.084469ms
    Jun 28 08:15:06.241: INFO: The phase of Pod pod-projected-configmaps-e2aa02fa-01fc-49fa-84a9-64008d2e3b8c is Pending, waiting for it to be Running (with Ready = true)
    Jun 28 08:15:08.247: INFO: Pod "pod-projected-configmaps-e2aa02fa-01fc-49fa-84a9-64008d2e3b8c": Phase="Running", Reason="", readiness=true. Elapsed: 2.010008333s
    Jun 28 08:15:08.247: INFO: The phase of Pod pod-projected-configmaps-e2aa02fa-01fc-49fa-84a9-64008d2e3b8c is Running (Ready = true)
    Jun 28 08:15:08.247: INFO: Pod "pod-projected-configmaps-e2aa02fa-01fc-49fa-84a9-64008d2e3b8c" satisfied condition "running and ready"
    STEP: Updating configmap projected-configmap-test-upd-36b008a4-5b8c-4d3d-acd2-1d19a676d0e6 06/28/23 08:15:08.261
    STEP: waiting to observe update in volume 06/28/23 08:15:08.267
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jun 28 08:15:10.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1613" for this suite. 06/28/23 08:15:10.366
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:15:10.375
Jun 28 08:15:10.375: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename custom-resource-definition 06/28/23 08:15:10.377
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:15:10.391
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:15:10.396
[It] creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
Jun 28 08:15:10.400: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jun 28 08:15:11.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-5679" for this suite. 06/28/23 08:15:11.439
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","completed":126,"skipped":2137,"failed":0}
------------------------------
â€¢ [1.073 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    creating/deleting custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:15:10.375
    Jun 28 08:15:10.375: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename custom-resource-definition 06/28/23 08:15:10.377
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:15:10.391
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:15:10.396
    [It] creating/deleting custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:58
    Jun 28 08:15:10.400: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jun 28 08:15:11.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-5679" for this suite. 06/28/23 08:15:11.439
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:15:11.449
Jun 28 08:15:11.449: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename pods 06/28/23 08:15:11.45
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:15:11.465
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:15:11.472
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082
STEP: Create a pod 06/28/23 08:15:11.476
Jun 28 08:15:11.484: INFO: Waiting up to 5m0s for pod "pod-lptxg" in namespace "pods-2036" to be "running"
Jun 28 08:15:11.489: INFO: Pod "pod-lptxg": Phase="Pending", Reason="", readiness=false. Elapsed: 4.61944ms
Jun 28 08:15:13.494: INFO: Pod "pod-lptxg": Phase="Running", Reason="", readiness=true. Elapsed: 2.009588293s
Jun 28 08:15:13.494: INFO: Pod "pod-lptxg" satisfied condition "running"
STEP: patching /status 06/28/23 08:15:13.494
Jun 28 08:15:13.503: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jun 28 08:15:13.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2036" for this suite. 06/28/23 08:15:13.511
{"msg":"PASSED [sig-node] Pods should patch a pod status [Conformance]","completed":127,"skipped":2149,"failed":0}
------------------------------
â€¢ [2.069 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:15:11.449
    Jun 28 08:15:11.449: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename pods 06/28/23 08:15:11.45
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:15:11.465
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:15:11.472
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should patch a pod status [Conformance]
      test/e2e/common/node/pods.go:1082
    STEP: Create a pod 06/28/23 08:15:11.476
    Jun 28 08:15:11.484: INFO: Waiting up to 5m0s for pod "pod-lptxg" in namespace "pods-2036" to be "running"
    Jun 28 08:15:11.489: INFO: Pod "pod-lptxg": Phase="Pending", Reason="", readiness=false. Elapsed: 4.61944ms
    Jun 28 08:15:13.494: INFO: Pod "pod-lptxg": Phase="Running", Reason="", readiness=true. Elapsed: 2.009588293s
    Jun 28 08:15:13.494: INFO: Pod "pod-lptxg" satisfied condition "running"
    STEP: patching /status 06/28/23 08:15:13.494
    Jun 28 08:15:13.503: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jun 28 08:15:13.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-2036" for this suite. 06/28/23 08:15:13.511
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] PreStop
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
[BeforeEach] [sig-node] PreStop
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:15:13.518
Jun 28 08:15:13.518: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename prestop 06/28/23 08:15:13.519
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:15:13.53
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:15:13.534
[BeforeEach] [sig-node] PreStop
  test/e2e/node/pre_stop.go:159
[It] should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
STEP: Creating server pod server in namespace prestop-1731 06/28/23 08:15:13.539
STEP: Waiting for pods to come up. 06/28/23 08:15:13.547
Jun 28 08:15:13.548: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-1731" to be "running"
Jun 28 08:15:13.553: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 5.448458ms
Jun 28 08:15:15.558: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.010397751s
Jun 28 08:15:15.558: INFO: Pod "server" satisfied condition "running"
STEP: Creating tester pod tester in namespace prestop-1731 06/28/23 08:15:15.562
Jun 28 08:15:15.569: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-1731" to be "running"
Jun 28 08:15:15.573: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 4.528312ms
Jun 28 08:15:17.581: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.012578864s
Jun 28 08:15:17.581: INFO: Pod "tester" satisfied condition "running"
STEP: Deleting pre-stop pod 06/28/23 08:15:17.581
Jun 28 08:15:22.684: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod 06/28/23 08:15:22.684
[AfterEach] [sig-node] PreStop
  test/e2e/framework/framework.go:187
Jun 28 08:15:22.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-1731" for this suite. 06/28/23 08:15:22.701
{"msg":"PASSED [sig-node] PreStop should call prestop when killing a pod  [Conformance]","completed":128,"skipped":2154,"failed":0}
------------------------------
â€¢ [SLOW TEST] [9.190 seconds]
[sig-node] PreStop
test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PreStop
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:15:13.518
    Jun 28 08:15:13.518: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename prestop 06/28/23 08:15:13.519
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:15:13.53
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:15:13.534
    [BeforeEach] [sig-node] PreStop
      test/e2e/node/pre_stop.go:159
    [It] should call prestop when killing a pod  [Conformance]
      test/e2e/node/pre_stop.go:168
    STEP: Creating server pod server in namespace prestop-1731 06/28/23 08:15:13.539
    STEP: Waiting for pods to come up. 06/28/23 08:15:13.547
    Jun 28 08:15:13.548: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-1731" to be "running"
    Jun 28 08:15:13.553: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 5.448458ms
    Jun 28 08:15:15.558: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.010397751s
    Jun 28 08:15:15.558: INFO: Pod "server" satisfied condition "running"
    STEP: Creating tester pod tester in namespace prestop-1731 06/28/23 08:15:15.562
    Jun 28 08:15:15.569: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-1731" to be "running"
    Jun 28 08:15:15.573: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 4.528312ms
    Jun 28 08:15:17.581: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.012578864s
    Jun 28 08:15:17.581: INFO: Pod "tester" satisfied condition "running"
    STEP: Deleting pre-stop pod 06/28/23 08:15:17.581
    Jun 28 08:15:22.684: INFO: Saw: {
    	"Hostname": "server",
    	"Sent": null,
    	"Received": {
    		"prestop": 1
    	},
    	"Errors": null,
    	"Log": [
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
    	],
    	"StillContactingPeers": true
    }
    STEP: Deleting the server pod 06/28/23 08:15:22.684
    [AfterEach] [sig-node] PreStop
      test/e2e/framework/framework.go:187
    Jun 28 08:15:22.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "prestop-1731" for this suite. 06/28/23 08:15:22.701
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:15:22.711
Jun 28 08:15:22.711: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename init-container 06/28/23 08:15:22.712
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:15:22.724
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:15:22.73
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457
STEP: creating the pod 06/28/23 08:15:22.734
Jun 28 08:15:22.734: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Jun 28 08:15:26.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4249" for this suite. 06/28/23 08:15:26.619
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","completed":129,"skipped":2183,"failed":0}
------------------------------
â€¢ [3.916 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:15:22.711
    Jun 28 08:15:22.711: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename init-container 06/28/23 08:15:22.712
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:15:22.724
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:15:22.73
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:457
    STEP: creating the pod 06/28/23 08:15:22.734
    Jun 28 08:15:22.734: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Jun 28 08:15:26.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-4249" for this suite. 06/28/23 08:15:26.619
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-auth] ServiceAccounts
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:15:26.628
Jun 28 08:15:26.628: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename svcaccounts 06/28/23 08:15:26.629
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:15:26.644
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:15:26.648
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646
STEP: creating a ServiceAccount 06/28/23 08:15:26.653
STEP: watching for the ServiceAccount to be added 06/28/23 08:15:26.664
STEP: patching the ServiceAccount 06/28/23 08:15:26.666
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 06/28/23 08:15:26.673
STEP: deleting the ServiceAccount 06/28/23 08:15:26.678
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Jun 28 08:15:26.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-5292" for this suite. 06/28/23 08:15:26.698
{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","completed":130,"skipped":2187,"failed":0}
------------------------------
â€¢ [0.077 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:15:26.628
    Jun 28 08:15:26.628: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename svcaccounts 06/28/23 08:15:26.629
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:15:26.644
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:15:26.648
    [It] should run through the lifecycle of a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:646
    STEP: creating a ServiceAccount 06/28/23 08:15:26.653
    STEP: watching for the ServiceAccount to be added 06/28/23 08:15:26.664
    STEP: patching the ServiceAccount 06/28/23 08:15:26.666
    STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 06/28/23 08:15:26.673
    STEP: deleting the ServiceAccount 06/28/23 08:15:26.678
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Jun 28 08:15:26.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-5292" for this suite. 06/28/23 08:15:26.698
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:15:26.706
Jun 28 08:15:26.706: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename events 06/28/23 08:15:26.707
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:15:26.72
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:15:26.724
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
STEP: creating a test event 06/28/23 08:15:26.729
STEP: listing events in all namespaces 06/28/23 08:15:26.738
STEP: listing events in test namespace 06/28/23 08:15:26.744
STEP: listing events with field selection filtering on source 06/28/23 08:15:26.749
STEP: listing events with field selection filtering on reportingController 06/28/23 08:15:26.753
STEP: getting the test event 06/28/23 08:15:26.757
STEP: patching the test event 06/28/23 08:15:26.762
STEP: getting the test event 06/28/23 08:15:26.771
STEP: updating the test event 06/28/23 08:15:26.776
STEP: getting the test event 06/28/23 08:15:26.786
STEP: deleting the test event 06/28/23 08:15:26.79
STEP: listing events in all namespaces 06/28/23 08:15:26.799
STEP: listing events in test namespace 06/28/23 08:15:26.804
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:187
Jun 28 08:15:26.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-4803" for this suite. 06/28/23 08:15:26.816
{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","completed":131,"skipped":2198,"failed":0}
------------------------------
â€¢ [0.117 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:15:26.706
    Jun 28 08:15:26.706: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename events 06/28/23 08:15:26.707
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:15:26.72
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:15:26.724
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
      test/e2e/instrumentation/events.go:98
    STEP: creating a test event 06/28/23 08:15:26.729
    STEP: listing events in all namespaces 06/28/23 08:15:26.738
    STEP: listing events in test namespace 06/28/23 08:15:26.744
    STEP: listing events with field selection filtering on source 06/28/23 08:15:26.749
    STEP: listing events with field selection filtering on reportingController 06/28/23 08:15:26.753
    STEP: getting the test event 06/28/23 08:15:26.757
    STEP: patching the test event 06/28/23 08:15:26.762
    STEP: getting the test event 06/28/23 08:15:26.771
    STEP: updating the test event 06/28/23 08:15:26.776
    STEP: getting the test event 06/28/23 08:15:26.786
    STEP: deleting the test event 06/28/23 08:15:26.79
    STEP: listing events in all namespaces 06/28/23 08:15:26.799
    STEP: listing events in test namespace 06/28/23 08:15:26.804
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:187
    Jun 28 08:15:26.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-4803" for this suite. 06/28/23 08:15:26.816
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:15:26.824
Jun 28 08:15:26.824: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename tables 06/28/23 08:15:26.825
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:15:26.837
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:15:26.842
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/apimachinery/table_conversion.go:49
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/framework.go:187
Jun 28 08:15:26.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-3316" for this suite. 06/28/23 08:15:26.859
{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","completed":132,"skipped":2224,"failed":0}
------------------------------
â€¢ [0.041 seconds]
[sig-api-machinery] Servers with support for Table transformation
test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:15:26.824
    Jun 28 08:15:26.824: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename tables 06/28/23 08:15:26.825
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:15:26.837
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:15:26.842
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/apimachinery/table_conversion.go:49
    [It] should return a 406 for a backend which does not implement metadata [Conformance]
      test/e2e/apimachinery/table_conversion.go:154
    [AfterEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/framework.go:187
    Jun 28 08:15:26.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "tables-3316" for this suite. 06/28/23 08:15:26.859
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:15:26.866
Jun 28 08:15:26.866: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename runtimeclass 06/28/23 08:15:26.867
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:15:26.879
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:15:26.882
[It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
Jun 28 08:15:26.900: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-7129 to be scheduled
Jun 28 08:15:26.904: INFO: 1 pods are not scheduled: [runtimeclass-7129/test-runtimeclass-runtimeclass-7129-preconfigured-handler-br884(f15f339c-e953-449c-99cb-f93ba381c8ca)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Jun 28 08:15:28.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-7129" for this suite. 06/28/23 08:15:28.926
{"msg":"PASSED [sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]","completed":133,"skipped":2244,"failed":0}
------------------------------
â€¢ [2.068 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:15:26.866
    Jun 28 08:15:26.866: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename runtimeclass 06/28/23 08:15:26.867
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:15:26.879
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:15:26.882
    [It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:104
    Jun 28 08:15:26.900: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-7129 to be scheduled
    Jun 28 08:15:26.904: INFO: 1 pods are not scheduled: [runtimeclass-7129/test-runtimeclass-runtimeclass-7129-preconfigured-handler-br884(f15f339c-e953-449c-99cb-f93ba381c8ca)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Jun 28 08:15:28.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-7129" for this suite. 06/28/23 08:15:28.926
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:15:28.935
Jun 28 08:15:28.935: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename webhook 06/28/23 08:15:28.936
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:15:28.95
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:15:28.955
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 06/28/23 08:15:28.973
STEP: Create role binding to let webhook read extension-apiserver-authentication 06/28/23 08:15:29.361
STEP: Deploying the webhook pod 06/28/23 08:15:29.37
STEP: Wait for the deployment to be ready 06/28/23 08:15:29.382
Jun 28 08:15:29.391: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 06/28/23 08:15:31.41
STEP: Verifying the service has paired with the endpoint 06/28/23 08:15:31.421
Jun 28 08:15:32.422: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307
STEP: Registering the crd webhook via the AdmissionRegistration API 06/28/23 08:15:32.427
STEP: Creating a custom resource definition that should be denied by the webhook 06/28/23 08:15:32.563
Jun 28 08:15:32.564: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jun 28 08:15:32.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5927" for this suite. 06/28/23 08:15:32.71
STEP: Destroying namespace "webhook-5927-markers" for this suite. 06/28/23 08:15:32.718
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","completed":134,"skipped":2273,"failed":0}
------------------------------
â€¢ [3.825 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:15:28.935
    Jun 28 08:15:28.935: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename webhook 06/28/23 08:15:28.936
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:15:28.95
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:15:28.955
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 06/28/23 08:15:28.973
    STEP: Create role binding to let webhook read extension-apiserver-authentication 06/28/23 08:15:29.361
    STEP: Deploying the webhook pod 06/28/23 08:15:29.37
    STEP: Wait for the deployment to be ready 06/28/23 08:15:29.382
    Jun 28 08:15:29.391: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 06/28/23 08:15:31.41
    STEP: Verifying the service has paired with the endpoint 06/28/23 08:15:31.421
    Jun 28 08:15:32.422: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should deny crd creation [Conformance]
      test/e2e/apimachinery/webhook.go:307
    STEP: Registering the crd webhook via the AdmissionRegistration API 06/28/23 08:15:32.427
    STEP: Creating a custom resource definition that should be denied by the webhook 06/28/23 08:15:32.563
    Jun 28 08:15:32.564: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jun 28 08:15:32.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-5927" for this suite. 06/28/23 08:15:32.71
    STEP: Destroying namespace "webhook-5927-markers" for this suite. 06/28/23 08:15:32.718
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Lease
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[BeforeEach] [sig-node] Lease
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:15:32.76
Jun 28 08:15:32.760: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename lease-test 06/28/23 08:15:32.761
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:15:32.774
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:15:32.778
[It] lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[AfterEach] [sig-node] Lease
  test/e2e/framework/framework.go:187
Jun 28 08:15:32.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-2258" for this suite. 06/28/23 08:15:32.851
{"msg":"PASSED [sig-node] Lease lease API should be available [Conformance]","completed":135,"skipped":2277,"failed":0}
------------------------------
â€¢ [0.098 seconds]
[sig-node] Lease
test/e2e/common/node/framework.go:23
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Lease
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:15:32.76
    Jun 28 08:15:32.760: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename lease-test 06/28/23 08:15:32.761
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:15:32.774
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:15:32.778
    [It] lease API should be available [Conformance]
      test/e2e/common/node/lease.go:72
    [AfterEach] [sig-node] Lease
      test/e2e/framework/framework.go:187
    Jun 28 08:15:32.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "lease-test-2258" for this suite. 06/28/23 08:15:32.851
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:15:32.858
Jun 28 08:15:32.858: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename configmap 06/28/23 08:15:32.859
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:15:32.872
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:15:32.877
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56
STEP: Creating configMap with name configmap-test-volume-f41c6fdc-cef6-4c8f-8cab-8b6e75eb36d7 06/28/23 08:15:32.881
STEP: Creating a pod to test consume configMaps 06/28/23 08:15:32.886
Jun 28 08:15:32.896: INFO: Waiting up to 5m0s for pod "pod-configmaps-914291e6-796e-49d7-b88c-840bd1126586" in namespace "configmap-4671" to be "Succeeded or Failed"
Jun 28 08:15:32.903: INFO: Pod "pod-configmaps-914291e6-796e-49d7-b88c-840bd1126586": Phase="Pending", Reason="", readiness=false. Elapsed: 6.61389ms
Jun 28 08:15:34.915: INFO: Pod "pod-configmaps-914291e6-796e-49d7-b88c-840bd1126586": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019280437s
Jun 28 08:15:36.908: INFO: Pod "pod-configmaps-914291e6-796e-49d7-b88c-840bd1126586": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012398999s
STEP: Saw pod success 06/28/23 08:15:36.908
Jun 28 08:15:36.908: INFO: Pod "pod-configmaps-914291e6-796e-49d7-b88c-840bd1126586" satisfied condition "Succeeded or Failed"
Jun 28 08:15:36.913: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-zxlfv pod pod-configmaps-914291e6-796e-49d7-b88c-840bd1126586 container agnhost-container: <nil>
STEP: delete the pod 06/28/23 08:15:36.923
Jun 28 08:15:36.938: INFO: Waiting for pod pod-configmaps-914291e6-796e-49d7-b88c-840bd1126586 to disappear
Jun 28 08:15:36.941: INFO: Pod pod-configmaps-914291e6-796e-49d7-b88c-840bd1126586 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jun 28 08:15:36.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4671" for this suite. 06/28/23 08:15:36.95
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":136,"skipped":2283,"failed":0}
------------------------------
â€¢ [4.099 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:15:32.858
    Jun 28 08:15:32.858: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename configmap 06/28/23 08:15:32.859
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:15:32.872
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:15:32.877
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:56
    STEP: Creating configMap with name configmap-test-volume-f41c6fdc-cef6-4c8f-8cab-8b6e75eb36d7 06/28/23 08:15:32.881
    STEP: Creating a pod to test consume configMaps 06/28/23 08:15:32.886
    Jun 28 08:15:32.896: INFO: Waiting up to 5m0s for pod "pod-configmaps-914291e6-796e-49d7-b88c-840bd1126586" in namespace "configmap-4671" to be "Succeeded or Failed"
    Jun 28 08:15:32.903: INFO: Pod "pod-configmaps-914291e6-796e-49d7-b88c-840bd1126586": Phase="Pending", Reason="", readiness=false. Elapsed: 6.61389ms
    Jun 28 08:15:34.915: INFO: Pod "pod-configmaps-914291e6-796e-49d7-b88c-840bd1126586": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019280437s
    Jun 28 08:15:36.908: INFO: Pod "pod-configmaps-914291e6-796e-49d7-b88c-840bd1126586": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012398999s
    STEP: Saw pod success 06/28/23 08:15:36.908
    Jun 28 08:15:36.908: INFO: Pod "pod-configmaps-914291e6-796e-49d7-b88c-840bd1126586" satisfied condition "Succeeded or Failed"
    Jun 28 08:15:36.913: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-zxlfv pod pod-configmaps-914291e6-796e-49d7-b88c-840bd1126586 container agnhost-container: <nil>
    STEP: delete the pod 06/28/23 08:15:36.923
    Jun 28 08:15:36.938: INFO: Waiting for pod pod-configmaps-914291e6-796e-49d7-b88c-840bd1126586 to disappear
    Jun 28 08:15:36.941: INFO: Pod pod-configmaps-914291e6-796e-49d7-b88c-840bd1126586 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jun 28 08:15:36.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-4671" for this suite. 06/28/23 08:15:36.95
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Security Context
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:15:36.958
Jun 28 08:15:36.958: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename security-context 06/28/23 08:15:36.958
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:15:36.972
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:15:36.976
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 06/28/23 08:15:36.981
Jun 28 08:15:36.989: INFO: Waiting up to 5m0s for pod "security-context-7ea3387d-c3f7-44b4-a145-ae30bc5cd5c3" in namespace "security-context-4957" to be "Succeeded or Failed"
Jun 28 08:15:36.994: INFO: Pod "security-context-7ea3387d-c3f7-44b4-a145-ae30bc5cd5c3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.057877ms
Jun 28 08:15:38.999: INFO: Pod "security-context-7ea3387d-c3f7-44b4-a145-ae30bc5cd5c3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009591867s
Jun 28 08:15:40.999: INFO: Pod "security-context-7ea3387d-c3f7-44b4-a145-ae30bc5cd5c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00931448s
STEP: Saw pod success 06/28/23 08:15:40.999
Jun 28 08:15:40.999: INFO: Pod "security-context-7ea3387d-c3f7-44b4-a145-ae30bc5cd5c3" satisfied condition "Succeeded or Failed"
Jun 28 08:15:41.004: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-zxlfv pod security-context-7ea3387d-c3f7-44b4-a145-ae30bc5cd5c3 container test-container: <nil>
STEP: delete the pod 06/28/23 08:15:41.015
Jun 28 08:15:41.028: INFO: Waiting for pod security-context-7ea3387d-c3f7-44b4-a145-ae30bc5cd5c3 to disappear
Jun 28 08:15:41.033: INFO: Pod security-context-7ea3387d-c3f7-44b4-a145-ae30bc5cd5c3 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Jun 28 08:15:41.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-4957" for this suite. 06/28/23 08:15:41.041
{"msg":"PASSED [sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","completed":137,"skipped":2292,"failed":0}
------------------------------
â€¢ [4.091 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:15:36.958
    Jun 28 08:15:36.958: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename security-context 06/28/23 08:15:36.958
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:15:36.972
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:15:36.976
    [It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:97
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 06/28/23 08:15:36.981
    Jun 28 08:15:36.989: INFO: Waiting up to 5m0s for pod "security-context-7ea3387d-c3f7-44b4-a145-ae30bc5cd5c3" in namespace "security-context-4957" to be "Succeeded or Failed"
    Jun 28 08:15:36.994: INFO: Pod "security-context-7ea3387d-c3f7-44b4-a145-ae30bc5cd5c3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.057877ms
    Jun 28 08:15:38.999: INFO: Pod "security-context-7ea3387d-c3f7-44b4-a145-ae30bc5cd5c3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009591867s
    Jun 28 08:15:40.999: INFO: Pod "security-context-7ea3387d-c3f7-44b4-a145-ae30bc5cd5c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00931448s
    STEP: Saw pod success 06/28/23 08:15:40.999
    Jun 28 08:15:40.999: INFO: Pod "security-context-7ea3387d-c3f7-44b4-a145-ae30bc5cd5c3" satisfied condition "Succeeded or Failed"
    Jun 28 08:15:41.004: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-zxlfv pod security-context-7ea3387d-c3f7-44b4-a145-ae30bc5cd5c3 container test-container: <nil>
    STEP: delete the pod 06/28/23 08:15:41.015
    Jun 28 08:15:41.028: INFO: Waiting for pod security-context-7ea3387d-c3f7-44b4-a145-ae30bc5cd5c3 to disappear
    Jun 28 08:15:41.033: INFO: Pod security-context-7ea3387d-c3f7-44b4-a145-ae30bc5cd5c3 no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Jun 28 08:15:41.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-4957" for this suite. 06/28/23 08:15:41.041
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] Services
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:15:41.049
Jun 28 08:15:41.049: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename services 06/28/23 08:15:41.05
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:15:41.063
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:15:41.068
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791
STEP: creating service endpoint-test2 in namespace services-5477 06/28/23 08:15:41.073
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5477 to expose endpoints map[] 06/28/23 08:15:41.083
Jun 28 08:15:41.088: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
Jun 28 08:15:42.099: INFO: successfully validated that service endpoint-test2 in namespace services-5477 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-5477 06/28/23 08:15:42.099
Jun 28 08:15:42.107: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-5477" to be "running and ready"
Jun 28 08:15:42.112: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.589201ms
Jun 28 08:15:42.112: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jun 28 08:15:44.118: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.010598891s
Jun 28 08:15:44.118: INFO: The phase of Pod pod1 is Running (Ready = true)
Jun 28 08:15:44.118: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5477 to expose endpoints map[pod1:[80]] 06/28/23 08:15:44.123
Jun 28 08:15:44.140: INFO: successfully validated that service endpoint-test2 in namespace services-5477 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1 06/28/23 08:15:44.141
Jun 28 08:15:44.141: INFO: Creating new exec pod
Jun 28 08:15:44.147: INFO: Waiting up to 5m0s for pod "execpod4jskk" in namespace "services-5477" to be "running"
Jun 28 08:15:44.151: INFO: Pod "execpod4jskk": Phase="Pending", Reason="", readiness=false. Elapsed: 4.205839ms
Jun 28 08:15:46.155: INFO: Pod "execpod4jskk": Phase="Running", Reason="", readiness=true. Elapsed: 2.008681266s
Jun 28 08:15:46.155: INFO: Pod "execpod4jskk" satisfied condition "running"
Jun 28 08:15:47.156: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-5477 exec execpod4jskk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Jun 28 08:15:47.600: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jun 28 08:15:47.600: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun 28 08:15:47.600: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-5477 exec execpod4jskk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.20.89.253 80'
Jun 28 08:15:48.037: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.20.89.253 80\nConnection to 172.20.89.253 80 port [tcp/http] succeeded!\n"
Jun 28 08:15:48.037: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Creating pod pod2 in namespace services-5477 06/28/23 08:15:48.037
Jun 28 08:15:48.045: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-5477" to be "running and ready"
Jun 28 08:15:48.051: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.59374ms
Jun 28 08:15:48.051: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jun 28 08:15:50.057: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.011898039s
Jun 28 08:15:50.057: INFO: The phase of Pod pod2 is Running (Ready = true)
Jun 28 08:15:50.057: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5477 to expose endpoints map[pod1:[80] pod2:[80]] 06/28/23 08:15:50.062
Jun 28 08:15:50.081: INFO: successfully validated that service endpoint-test2 in namespace services-5477 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2 06/28/23 08:15:50.081
Jun 28 08:15:51.082: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-5477 exec execpod4jskk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Jun 28 08:15:51.554: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jun 28 08:15:51.554: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun 28 08:15:51.554: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-5477 exec execpod4jskk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.20.89.253 80'
Jun 28 08:15:51.988: INFO: stderr: "+ nc -v -t -w 2 172.20.89.253 80\n+ echo hostName\nConnection to 172.20.89.253 80 port [tcp/http] succeeded!\n"
Jun 28 08:15:51.988: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-5477 06/28/23 08:15:51.988
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5477 to expose endpoints map[pod2:[80]] 06/28/23 08:15:52.012
Jun 28 08:15:52.037: INFO: successfully validated that service endpoint-test2 in namespace services-5477 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2 06/28/23 08:15:52.038
Jun 28 08:15:53.039: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-5477 exec execpod4jskk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Jun 28 08:15:53.474: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jun 28 08:15:53.474: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun 28 08:15:53.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-5477 exec execpod4jskk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.20.89.253 80'
Jun 28 08:15:53.945: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.20.89.253 80\nConnection to 172.20.89.253 80 port [tcp/http] succeeded!\n"
Jun 28 08:15:53.945: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod2 in namespace services-5477 06/28/23 08:15:53.945
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5477 to expose endpoints map[] 06/28/23 08:15:53.956
Jun 28 08:15:54.978: INFO: successfully validated that service endpoint-test2 in namespace services-5477 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jun 28 08:15:54.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5477" for this suite. 06/28/23 08:15:55
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","completed":138,"skipped":2297,"failed":0}
------------------------------
â€¢ [SLOW TEST] [13.957 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:15:41.049
    Jun 28 08:15:41.049: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename services 06/28/23 08:15:41.05
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:15:41.063
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:15:41.068
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should serve a basic endpoint from pods  [Conformance]
      test/e2e/network/service.go:791
    STEP: creating service endpoint-test2 in namespace services-5477 06/28/23 08:15:41.073
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5477 to expose endpoints map[] 06/28/23 08:15:41.083
    Jun 28 08:15:41.088: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
    Jun 28 08:15:42.099: INFO: successfully validated that service endpoint-test2 in namespace services-5477 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-5477 06/28/23 08:15:42.099
    Jun 28 08:15:42.107: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-5477" to be "running and ready"
    Jun 28 08:15:42.112: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.589201ms
    Jun 28 08:15:42.112: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jun 28 08:15:44.118: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.010598891s
    Jun 28 08:15:44.118: INFO: The phase of Pod pod1 is Running (Ready = true)
    Jun 28 08:15:44.118: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5477 to expose endpoints map[pod1:[80]] 06/28/23 08:15:44.123
    Jun 28 08:15:44.140: INFO: successfully validated that service endpoint-test2 in namespace services-5477 exposes endpoints map[pod1:[80]]
    STEP: Checking if the Service forwards traffic to pod1 06/28/23 08:15:44.141
    Jun 28 08:15:44.141: INFO: Creating new exec pod
    Jun 28 08:15:44.147: INFO: Waiting up to 5m0s for pod "execpod4jskk" in namespace "services-5477" to be "running"
    Jun 28 08:15:44.151: INFO: Pod "execpod4jskk": Phase="Pending", Reason="", readiness=false. Elapsed: 4.205839ms
    Jun 28 08:15:46.155: INFO: Pod "execpod4jskk": Phase="Running", Reason="", readiness=true. Elapsed: 2.008681266s
    Jun 28 08:15:46.155: INFO: Pod "execpod4jskk" satisfied condition "running"
    Jun 28 08:15:47.156: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-5477 exec execpod4jskk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    Jun 28 08:15:47.600: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Jun 28 08:15:47.600: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jun 28 08:15:47.600: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-5477 exec execpod4jskk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.20.89.253 80'
    Jun 28 08:15:48.037: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.20.89.253 80\nConnection to 172.20.89.253 80 port [tcp/http] succeeded!\n"
    Jun 28 08:15:48.037: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Creating pod pod2 in namespace services-5477 06/28/23 08:15:48.037
    Jun 28 08:15:48.045: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-5477" to be "running and ready"
    Jun 28 08:15:48.051: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.59374ms
    Jun 28 08:15:48.051: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jun 28 08:15:50.057: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.011898039s
    Jun 28 08:15:50.057: INFO: The phase of Pod pod2 is Running (Ready = true)
    Jun 28 08:15:50.057: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5477 to expose endpoints map[pod1:[80] pod2:[80]] 06/28/23 08:15:50.062
    Jun 28 08:15:50.081: INFO: successfully validated that service endpoint-test2 in namespace services-5477 exposes endpoints map[pod1:[80] pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod1 and pod2 06/28/23 08:15:50.081
    Jun 28 08:15:51.082: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-5477 exec execpod4jskk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    Jun 28 08:15:51.554: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Jun 28 08:15:51.554: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jun 28 08:15:51.554: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-5477 exec execpod4jskk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.20.89.253 80'
    Jun 28 08:15:51.988: INFO: stderr: "+ nc -v -t -w 2 172.20.89.253 80\n+ echo hostName\nConnection to 172.20.89.253 80 port [tcp/http] succeeded!\n"
    Jun 28 08:15:51.988: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod1 in namespace services-5477 06/28/23 08:15:51.988
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5477 to expose endpoints map[pod2:[80]] 06/28/23 08:15:52.012
    Jun 28 08:15:52.037: INFO: successfully validated that service endpoint-test2 in namespace services-5477 exposes endpoints map[pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod2 06/28/23 08:15:52.038
    Jun 28 08:15:53.039: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-5477 exec execpod4jskk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    Jun 28 08:15:53.474: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Jun 28 08:15:53.474: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jun 28 08:15:53.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-5477 exec execpod4jskk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.20.89.253 80'
    Jun 28 08:15:53.945: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.20.89.253 80\nConnection to 172.20.89.253 80 port [tcp/http] succeeded!\n"
    Jun 28 08:15:53.945: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod2 in namespace services-5477 06/28/23 08:15:53.945
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5477 to expose endpoints map[] 06/28/23 08:15:53.956
    Jun 28 08:15:54.978: INFO: successfully validated that service endpoint-test2 in namespace services-5477 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jun 28 08:15:54.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-5477" for this suite. 06/28/23 08:15:55
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:15:55.008
Jun 28 08:15:55.008: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename job 06/28/23 08:15:55.009
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:15:55.024
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:15:55.029
[It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194
STEP: Creating Indexed job 06/28/23 08:15:55.034
STEP: Ensuring job reaches completions 06/28/23 08:15:55.04
STEP: Ensuring pods with index for job exist 06/28/23 08:16:03.046
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Jun 28 08:16:03.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-3011" for this suite. 06/28/23 08:16:03.061
{"msg":"PASSED [sig-apps] Job should create pods for an Indexed job with completion indexes and specified hostname [Conformance]","completed":139,"skipped":2325,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.061 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:15:55.008
    Jun 28 08:15:55.008: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename job 06/28/23 08:15:55.009
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:15:55.024
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:15:55.029
    [It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
      test/e2e/apps/job.go:194
    STEP: Creating Indexed job 06/28/23 08:15:55.034
    STEP: Ensuring job reaches completions 06/28/23 08:15:55.04
    STEP: Ensuring pods with index for job exist 06/28/23 08:16:03.046
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Jun 28 08:16:03.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-3011" for this suite. 06/28/23 08:16:03.061
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:16:03.071
Jun 28 08:16:03.071: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename deployment 06/28/23 08:16:03.072
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:16:03.087
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:16:03.091
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
Jun 28 08:16:03.096: INFO: Creating simple deployment test-new-deployment
Jun 28 08:16:03.116: INFO: deployment "test-new-deployment" doesn't have the required revision set
STEP: getting scale subresource 06/28/23 08:16:05.137
STEP: updating a scale subresource 06/28/23 08:16:05.143
STEP: verifying the deployment Spec.Replicas was modified 06/28/23 08:16:05.15
STEP: Patch a scale subresource 06/28/23 08:16:05.154
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jun 28 08:16:05.173: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-6133  d469d335-81ab-4955-85c8-65c01887c289 66512 3 2023-06-28 08:16:03 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-06-28 08:16:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-28 08:16:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002b06938 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-06-28 08:16:04 +0000 UTC,LastTransitionTime:2023-06-28 08:16:04 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-845c8977d9" has successfully progressed.,LastUpdateTime:2023-06-28 08:16:04 +0000 UTC,LastTransitionTime:2023-06-28 08:16:03 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jun 28 08:16:05.179: INFO: New ReplicaSet "test-new-deployment-845c8977d9" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-845c8977d9  deployment-6133  20b32282-c657-497a-9dee-7a69c62e3bd8 66516 2 2023-06-28 08:16:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment d469d335-81ab-4955-85c8-65c01887c289 0xc003292e97 0xc003292e98}] [] [{kube-controller-manager Update apps/v1 2023-06-28 08:16:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d469d335-81ab-4955-85c8-65c01887c289\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-28 08:16:05 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003292f28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jun 28 08:16:05.186: INFO: Pod "test-new-deployment-845c8977d9-7xmjk" is not available:
&Pod{ObjectMeta:{test-new-deployment-845c8977d9-7xmjk test-new-deployment-845c8977d9- deployment-6133  103f3027-dc9f-49a8-a8e4-7d50f5cc87fd 66517 0 2023-06-28 08:16:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 20b32282-c657-497a-9dee-7a69c62e3bd8 0xc0032932b7 0xc0032932b8}] [] [{kube-controller-manager Update v1 2023-06-28 08:16:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"20b32282-c657-497a-9dee-7a69c62e3bd8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vln9d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vln9d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-rhel-749f7d55c8xdd8b6-zxlfv,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:16:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 28 08:16:05.187: INFO: Pod "test-new-deployment-845c8977d9-dlbsb" is available:
&Pod{ObjectMeta:{test-new-deployment-845c8977d9-dlbsb test-new-deployment-845c8977d9- deployment-6133  75e6e951-afaa-44a1-8321-8a046fb392ba 66504 0 2023-06-28 08:16:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:80803ceefc78cb5944d7cb777d2991c721c6a844b6ef38d8ab520680a9a66a47 cni.projectcalico.org/podIP:172.21.122.33/32 cni.projectcalico.org/podIPs:172.21.122.33/32] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 20b32282-c657-497a-9dee-7a69c62e3bd8 0xc003293430 0xc003293431}] [] [{calico Update v1 2023-06-28 08:16:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-06-28 08:16:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"20b32282-c657-497a-9dee-7a69c62e3bd8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-28 08:16:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.122.33\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mrkrk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mrkrk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-rhel-749f7d55c8xdd8b6-ct4cp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:16:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:16:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:16:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:16:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.11.3,PodIP:172.21.122.33,StartTime:2023-06-28 08:16:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-28 08:16:03 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://54baff99bed8a4c577ba49bc069ad89330d797d93ff36c144b388d67d3a5305a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.21.122.33,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jun 28 08:16:05.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6133" for this suite. 06/28/23 08:16:05.195
{"msg":"PASSED [sig-apps] Deployment Deployment should have a working scale subresource [Conformance]","completed":140,"skipped":2336,"failed":0}
------------------------------
â€¢ [2.133 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:16:03.071
    Jun 28 08:16:03.071: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename deployment 06/28/23 08:16:03.072
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:16:03.087
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:16:03.091
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] Deployment should have a working scale subresource [Conformance]
      test/e2e/apps/deployment.go:150
    Jun 28 08:16:03.096: INFO: Creating simple deployment test-new-deployment
    Jun 28 08:16:03.116: INFO: deployment "test-new-deployment" doesn't have the required revision set
    STEP: getting scale subresource 06/28/23 08:16:05.137
    STEP: updating a scale subresource 06/28/23 08:16:05.143
    STEP: verifying the deployment Spec.Replicas was modified 06/28/23 08:16:05.15
    STEP: Patch a scale subresource 06/28/23 08:16:05.154
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jun 28 08:16:05.173: INFO: Deployment "test-new-deployment":
    &Deployment{ObjectMeta:{test-new-deployment  deployment-6133  d469d335-81ab-4955-85c8-65c01887c289 66512 3 2023-06-28 08:16:03 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-06-28 08:16:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-28 08:16:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002b06938 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-06-28 08:16:04 +0000 UTC,LastTransitionTime:2023-06-28 08:16:04 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-845c8977d9" has successfully progressed.,LastUpdateTime:2023-06-28 08:16:04 +0000 UTC,LastTransitionTime:2023-06-28 08:16:03 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jun 28 08:16:05.179: INFO: New ReplicaSet "test-new-deployment-845c8977d9" of Deployment "test-new-deployment":
    &ReplicaSet{ObjectMeta:{test-new-deployment-845c8977d9  deployment-6133  20b32282-c657-497a-9dee-7a69c62e3bd8 66516 2 2023-06-28 08:16:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment d469d335-81ab-4955-85c8-65c01887c289 0xc003292e97 0xc003292e98}] [] [{kube-controller-manager Update apps/v1 2023-06-28 08:16:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d469d335-81ab-4955-85c8-65c01887c289\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-28 08:16:05 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003292f28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jun 28 08:16:05.186: INFO: Pod "test-new-deployment-845c8977d9-7xmjk" is not available:
    &Pod{ObjectMeta:{test-new-deployment-845c8977d9-7xmjk test-new-deployment-845c8977d9- deployment-6133  103f3027-dc9f-49a8-a8e4-7d50f5cc87fd 66517 0 2023-06-28 08:16:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 20b32282-c657-497a-9dee-7a69c62e3bd8 0xc0032932b7 0xc0032932b8}] [] [{kube-controller-manager Update v1 2023-06-28 08:16:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"20b32282-c657-497a-9dee-7a69c62e3bd8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vln9d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vln9d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-rhel-749f7d55c8xdd8b6-zxlfv,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:16:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 28 08:16:05.187: INFO: Pod "test-new-deployment-845c8977d9-dlbsb" is available:
    &Pod{ObjectMeta:{test-new-deployment-845c8977d9-dlbsb test-new-deployment-845c8977d9- deployment-6133  75e6e951-afaa-44a1-8321-8a046fb392ba 66504 0 2023-06-28 08:16:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:80803ceefc78cb5944d7cb777d2991c721c6a844b6ef38d8ab520680a9a66a47 cni.projectcalico.org/podIP:172.21.122.33/32 cni.projectcalico.org/podIPs:172.21.122.33/32] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 20b32282-c657-497a-9dee-7a69c62e3bd8 0xc003293430 0xc003293431}] [] [{calico Update v1 2023-06-28 08:16:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-06-28 08:16:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"20b32282-c657-497a-9dee-7a69c62e3bd8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-28 08:16:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.122.33\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mrkrk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mrkrk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-rhel-749f7d55c8xdd8b6-ct4cp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:16:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:16:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:16:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:16:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.11.3,PodIP:172.21.122.33,StartTime:2023-06-28 08:16:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-28 08:16:03 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://54baff99bed8a4c577ba49bc069ad89330d797d93ff36c144b388d67d3a5305a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.21.122.33,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jun 28 08:16:05.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-6133" for this suite. 06/28/23 08:16:05.195
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected configMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:16:05.205
Jun 28 08:16:05.205: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename projected 06/28/23 08:16:05.206
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:16:05.219
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:16:05.223
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173
STEP: Creating configMap with name cm-test-opt-del-c0217bf9-cc07-4d39-b4dd-745ce4a37bd4 06/28/23 08:16:05.237
STEP: Creating configMap with name cm-test-opt-upd-4d97d3e3-6093-47db-a1ff-9cef1627c719 06/28/23 08:16:05.244
STEP: Creating the pod 06/28/23 08:16:05.251
Jun 28 08:16:05.261: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1bc2eda5-dcd9-443e-b57b-9a5e2364286b" in namespace "projected-6017" to be "running and ready"
Jun 28 08:16:05.265: INFO: Pod "pod-projected-configmaps-1bc2eda5-dcd9-443e-b57b-9a5e2364286b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.044357ms
Jun 28 08:16:05.265: INFO: The phase of Pod pod-projected-configmaps-1bc2eda5-dcd9-443e-b57b-9a5e2364286b is Pending, waiting for it to be Running (with Ready = true)
Jun 28 08:16:07.270: INFO: Pod "pod-projected-configmaps-1bc2eda5-dcd9-443e-b57b-9a5e2364286b": Phase="Running", Reason="", readiness=true. Elapsed: 2.009082388s
Jun 28 08:16:07.270: INFO: The phase of Pod pod-projected-configmaps-1bc2eda5-dcd9-443e-b57b-9a5e2364286b is Running (Ready = true)
Jun 28 08:16:07.270: INFO: Pod "pod-projected-configmaps-1bc2eda5-dcd9-443e-b57b-9a5e2364286b" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-c0217bf9-cc07-4d39-b4dd-745ce4a37bd4 06/28/23 08:16:07.414
STEP: Updating configmap cm-test-opt-upd-4d97d3e3-6093-47db-a1ff-9cef1627c719 06/28/23 08:16:07.422
STEP: Creating configMap with name cm-test-opt-create-6a3a4998-8a2f-48da-9c9c-f8fa6246c705 06/28/23 08:16:07.428
STEP: waiting to observe update in volume 06/28/23 08:16:07.434
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jun 28 08:16:09.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6017" for this suite. 06/28/23 08:16:09.612
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":141,"skipped":2343,"failed":0}
------------------------------
â€¢ [4.416 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:16:05.205
    Jun 28 08:16:05.205: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename projected 06/28/23 08:16:05.206
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:16:05.219
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:16:05.223
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:173
    STEP: Creating configMap with name cm-test-opt-del-c0217bf9-cc07-4d39-b4dd-745ce4a37bd4 06/28/23 08:16:05.237
    STEP: Creating configMap with name cm-test-opt-upd-4d97d3e3-6093-47db-a1ff-9cef1627c719 06/28/23 08:16:05.244
    STEP: Creating the pod 06/28/23 08:16:05.251
    Jun 28 08:16:05.261: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1bc2eda5-dcd9-443e-b57b-9a5e2364286b" in namespace "projected-6017" to be "running and ready"
    Jun 28 08:16:05.265: INFO: Pod "pod-projected-configmaps-1bc2eda5-dcd9-443e-b57b-9a5e2364286b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.044357ms
    Jun 28 08:16:05.265: INFO: The phase of Pod pod-projected-configmaps-1bc2eda5-dcd9-443e-b57b-9a5e2364286b is Pending, waiting for it to be Running (with Ready = true)
    Jun 28 08:16:07.270: INFO: Pod "pod-projected-configmaps-1bc2eda5-dcd9-443e-b57b-9a5e2364286b": Phase="Running", Reason="", readiness=true. Elapsed: 2.009082388s
    Jun 28 08:16:07.270: INFO: The phase of Pod pod-projected-configmaps-1bc2eda5-dcd9-443e-b57b-9a5e2364286b is Running (Ready = true)
    Jun 28 08:16:07.270: INFO: Pod "pod-projected-configmaps-1bc2eda5-dcd9-443e-b57b-9a5e2364286b" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-c0217bf9-cc07-4d39-b4dd-745ce4a37bd4 06/28/23 08:16:07.414
    STEP: Updating configmap cm-test-opt-upd-4d97d3e3-6093-47db-a1ff-9cef1627c719 06/28/23 08:16:07.422
    STEP: Creating configMap with name cm-test-opt-create-6a3a4998-8a2f-48da-9c9c-f8fa6246c705 06/28/23 08:16:07.428
    STEP: waiting to observe update in volume 06/28/23 08:16:07.434
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jun 28 08:16:09.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6017" for this suite. 06/28/23 08:16:09.612
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:16:09.622
Jun 28 08:16:09.622: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename crd-publish-openapi 06/28/23 08:16:09.623
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:16:09.64
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:16:09.645
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441
STEP: set up a multi version CRD 06/28/23 08:16:09.651
Jun 28 08:16:09.652: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: mark a version not serverd 06/28/23 08:16:15.991
STEP: check the unserved version gets removed 06/28/23 08:16:16.016
STEP: check the other version is not changed 06/28/23 08:16:18.641
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jun 28 08:16:23.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1143" for this suite. 06/28/23 08:16:23.526
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","completed":142,"skipped":2363,"failed":0}
------------------------------
â€¢ [SLOW TEST] [13.910 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:16:09.622
    Jun 28 08:16:09.622: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename crd-publish-openapi 06/28/23 08:16:09.623
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:16:09.64
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:16:09.645
    [It] removes definition from spec when one version gets changed to not be served [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:441
    STEP: set up a multi version CRD 06/28/23 08:16:09.651
    Jun 28 08:16:09.652: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: mark a version not serverd 06/28/23 08:16:15.991
    STEP: check the unserved version gets removed 06/28/23 08:16:16.016
    STEP: check the other version is not changed 06/28/23 08:16:18.641
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jun 28 08:16:23.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-1143" for this suite. 06/28/23 08:16:23.526
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1785
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:16:23.534
Jun 28 08:16:23.534: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename kubectl 06/28/23 08:16:23.534
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:16:23.546
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:16:23.55
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1785
STEP: starting the proxy server 06/28/23 08:16:23.554
Jun 28 08:16:23.555: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-7627 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output 06/28/23 08:16:23.601
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jun 28 08:16:23.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7627" for this suite. 06/28/23 08:16:23.624
{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","completed":143,"skipped":2402,"failed":0}
------------------------------
â€¢ [0.097 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1778
    should support proxy with --port 0  [Conformance]
    test/e2e/kubectl/kubectl.go:1785

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:16:23.534
    Jun 28 08:16:23.534: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename kubectl 06/28/23 08:16:23.534
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:16:23.546
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:16:23.55
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should support proxy with --port 0  [Conformance]
      test/e2e/kubectl/kubectl.go:1785
    STEP: starting the proxy server 06/28/23 08:16:23.554
    Jun 28 08:16:23.555: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-7627 proxy -p 0 --disable-filter'
    STEP: curling proxy /api/ output 06/28/23 08:16:23.601
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jun 28 08:16:23.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-7627" for this suite. 06/28/23 08:16:23.624
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Secrets
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:16:23.631
Jun 28 08:16:23.631: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename secrets 06/28/23 08:16:23.632
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:16:23.646
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:16:23.65
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45
STEP: Creating secret with name secret-test-b4ebb6f3-84d8-4ba9-99f9-2d16e28fef8c 06/28/23 08:16:23.655
STEP: Creating a pod to test consume secrets 06/28/23 08:16:23.659
Jun 28 08:16:23.670: INFO: Waiting up to 5m0s for pod "pod-secrets-b7266b1e-52cd-42e8-945b-70eabd1f071f" in namespace "secrets-4288" to be "Succeeded or Failed"
Jun 28 08:16:23.674: INFO: Pod "pod-secrets-b7266b1e-52cd-42e8-945b-70eabd1f071f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.964556ms
Jun 28 08:16:25.680: INFO: Pod "pod-secrets-b7266b1e-52cd-42e8-945b-70eabd1f071f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009965293s
Jun 28 08:16:27.679: INFO: Pod "pod-secrets-b7266b1e-52cd-42e8-945b-70eabd1f071f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009725158s
STEP: Saw pod success 06/28/23 08:16:27.679
Jun 28 08:16:27.679: INFO: Pod "pod-secrets-b7266b1e-52cd-42e8-945b-70eabd1f071f" satisfied condition "Succeeded or Failed"
Jun 28 08:16:27.684: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod pod-secrets-b7266b1e-52cd-42e8-945b-70eabd1f071f container secret-env-test: <nil>
STEP: delete the pod 06/28/23 08:16:27.735
Jun 28 08:16:27.745: INFO: Waiting for pod pod-secrets-b7266b1e-52cd-42e8-945b-70eabd1f071f to disappear
Jun 28 08:16:27.749: INFO: Pod pod-secrets-b7266b1e-52cd-42e8-945b-70eabd1f071f no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Jun 28 08:16:27.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4288" for this suite. 06/28/23 08:16:27.756
{"msg":"PASSED [sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","completed":144,"skipped":2407,"failed":0}
------------------------------
â€¢ [4.132 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:16:23.631
    Jun 28 08:16:23.631: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename secrets 06/28/23 08:16:23.632
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:16:23.646
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:16:23.65
    [It] should be consumable from pods in env vars [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:45
    STEP: Creating secret with name secret-test-b4ebb6f3-84d8-4ba9-99f9-2d16e28fef8c 06/28/23 08:16:23.655
    STEP: Creating a pod to test consume secrets 06/28/23 08:16:23.659
    Jun 28 08:16:23.670: INFO: Waiting up to 5m0s for pod "pod-secrets-b7266b1e-52cd-42e8-945b-70eabd1f071f" in namespace "secrets-4288" to be "Succeeded or Failed"
    Jun 28 08:16:23.674: INFO: Pod "pod-secrets-b7266b1e-52cd-42e8-945b-70eabd1f071f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.964556ms
    Jun 28 08:16:25.680: INFO: Pod "pod-secrets-b7266b1e-52cd-42e8-945b-70eabd1f071f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009965293s
    Jun 28 08:16:27.679: INFO: Pod "pod-secrets-b7266b1e-52cd-42e8-945b-70eabd1f071f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009725158s
    STEP: Saw pod success 06/28/23 08:16:27.679
    Jun 28 08:16:27.679: INFO: Pod "pod-secrets-b7266b1e-52cd-42e8-945b-70eabd1f071f" satisfied condition "Succeeded or Failed"
    Jun 28 08:16:27.684: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod pod-secrets-b7266b1e-52cd-42e8-945b-70eabd1f071f container secret-env-test: <nil>
    STEP: delete the pod 06/28/23 08:16:27.735
    Jun 28 08:16:27.745: INFO: Waiting for pod pod-secrets-b7266b1e-52cd-42e8-945b-70eabd1f071f to disappear
    Jun 28 08:16:27.749: INFO: Pod pod-secrets-b7266b1e-52cd-42e8-945b-70eabd1f071f no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Jun 28 08:16:27.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-4288" for this suite. 06/28/23 08:16:27.756
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:16:27.765
Jun 28 08:16:27.765: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename daemonsets 06/28/23 08:16:27.766
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:16:27.78
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:16:27.785
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373
Jun 28 08:16:27.822: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster. 06/28/23 08:16:27.828
Jun 28 08:16:27.842: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 28 08:16:27.842: INFO: Node ske-rhel-749f7d55c8xdd8b6-ct4cp is running 0 daemon pod, expected 1
Jun 28 08:16:28.857: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jun 28 08:16:28.857: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Update daemon pods image. 06/28/23 08:16:28.877
STEP: Check that daemon pods images are updated. 06/28/23 08:16:28.893
Jun 28 08:16:28.898: INFO: Wrong image for pod: daemon-set-k5ctb. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jun 28 08:16:28.898: INFO: Wrong image for pod: daemon-set-ps2wb. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jun 28 08:16:28.898: INFO: Wrong image for pod: daemon-set-tjzr8. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jun 28 08:16:29.914: INFO: Wrong image for pod: daemon-set-k5ctb. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jun 28 08:16:29.914: INFO: Wrong image for pod: daemon-set-ps2wb. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jun 28 08:16:30.914: INFO: Wrong image for pod: daemon-set-k5ctb. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jun 28 08:16:30.914: INFO: Wrong image for pod: daemon-set-ps2wb. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jun 28 08:16:31.913: INFO: Pod daemon-set-gjsst is not available
Jun 28 08:16:31.913: INFO: Wrong image for pod: daemon-set-k5ctb. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jun 28 08:16:31.913: INFO: Wrong image for pod: daemon-set-ps2wb. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jun 28 08:16:32.913: INFO: Wrong image for pod: daemon-set-ps2wb. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jun 28 08:16:33.914: INFO: Pod daemon-set-5fpkb is not available
Jun 28 08:16:33.914: INFO: Wrong image for pod: daemon-set-ps2wb. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jun 28 08:16:35.913: INFO: Pod daemon-set-lgblt is not available
STEP: Check that daemon pods are still running on every node of the cluster. 06/28/23 08:16:35.922
Jun 28 08:16:35.935: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jun 28 08:16:35.935: INFO: Node ske-rhel-749f7d55c8xdd8b6-srshq is running 0 daemon pod, expected 1
Jun 28 08:16:36.949: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jun 28 08:16:36.949: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 06/28/23 08:16:36.977
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8963, will wait for the garbage collector to delete the pods 06/28/23 08:16:36.977
Jun 28 08:16:37.040: INFO: Deleting DaemonSet.extensions daemon-set took: 8.08426ms
Jun 28 08:16:37.141: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.960527ms
Jun 28 08:16:39.845: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 28 08:16:39.845: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jun 28 08:16:39.850: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"66979"},"items":null}

Jun 28 08:16:39.854: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"66979"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Jun 28 08:16:39.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8963" for this suite. 06/28/23 08:16:39.884
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","completed":145,"skipped":2436,"failed":0}
------------------------------
â€¢ [SLOW TEST] [12.125 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:16:27.765
    Jun 28 08:16:27.765: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename daemonsets 06/28/23 08:16:27.766
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:16:27.78
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:16:27.785
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
      test/e2e/apps/daemon_set.go:373
    Jun 28 08:16:27.822: INFO: Creating simple daemon set daemon-set
    STEP: Check that daemon pods launch on every node of the cluster. 06/28/23 08:16:27.828
    Jun 28 08:16:27.842: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun 28 08:16:27.842: INFO: Node ske-rhel-749f7d55c8xdd8b6-ct4cp is running 0 daemon pod, expected 1
    Jun 28 08:16:28.857: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jun 28 08:16:28.857: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Update daemon pods image. 06/28/23 08:16:28.877
    STEP: Check that daemon pods images are updated. 06/28/23 08:16:28.893
    Jun 28 08:16:28.898: INFO: Wrong image for pod: daemon-set-k5ctb. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jun 28 08:16:28.898: INFO: Wrong image for pod: daemon-set-ps2wb. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jun 28 08:16:28.898: INFO: Wrong image for pod: daemon-set-tjzr8. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jun 28 08:16:29.914: INFO: Wrong image for pod: daemon-set-k5ctb. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jun 28 08:16:29.914: INFO: Wrong image for pod: daemon-set-ps2wb. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jun 28 08:16:30.914: INFO: Wrong image for pod: daemon-set-k5ctb. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jun 28 08:16:30.914: INFO: Wrong image for pod: daemon-set-ps2wb. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jun 28 08:16:31.913: INFO: Pod daemon-set-gjsst is not available
    Jun 28 08:16:31.913: INFO: Wrong image for pod: daemon-set-k5ctb. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jun 28 08:16:31.913: INFO: Wrong image for pod: daemon-set-ps2wb. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jun 28 08:16:32.913: INFO: Wrong image for pod: daemon-set-ps2wb. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jun 28 08:16:33.914: INFO: Pod daemon-set-5fpkb is not available
    Jun 28 08:16:33.914: INFO: Wrong image for pod: daemon-set-ps2wb. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jun 28 08:16:35.913: INFO: Pod daemon-set-lgblt is not available
    STEP: Check that daemon pods are still running on every node of the cluster. 06/28/23 08:16:35.922
    Jun 28 08:16:35.935: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jun 28 08:16:35.935: INFO: Node ske-rhel-749f7d55c8xdd8b6-srshq is running 0 daemon pod, expected 1
    Jun 28 08:16:36.949: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jun 28 08:16:36.949: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 06/28/23 08:16:36.977
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8963, will wait for the garbage collector to delete the pods 06/28/23 08:16:36.977
    Jun 28 08:16:37.040: INFO: Deleting DaemonSet.extensions daemon-set took: 8.08426ms
    Jun 28 08:16:37.141: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.960527ms
    Jun 28 08:16:39.845: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun 28 08:16:39.845: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jun 28 08:16:39.850: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"66979"},"items":null}

    Jun 28 08:16:39.854: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"66979"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Jun 28 08:16:39.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-8963" for this suite. 06/28/23 08:16:39.884
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:16:39.892
Jun 28 08:16:39.892: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename resourcequota 06/28/23 08:16:39.893
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:16:39.906
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:16:39.91
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65
STEP: Counting existing ResourceQuota 06/28/23 08:16:39.914
STEP: Creating a ResourceQuota 06/28/23 08:16:44.919
STEP: Ensuring resource quota status is calculated 06/28/23 08:16:44.925
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jun 28 08:16:46.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9278" for this suite. 06/28/23 08:16:46.938
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","completed":146,"skipped":2477,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.053 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:16:39.892
    Jun 28 08:16:39.892: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename resourcequota 06/28/23 08:16:39.893
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:16:39.906
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:16:39.91
    [It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
      test/e2e/apimachinery/resource_quota.go:65
    STEP: Counting existing ResourceQuota 06/28/23 08:16:39.914
    STEP: Creating a ResourceQuota 06/28/23 08:16:44.919
    STEP: Ensuring resource quota status is calculated 06/28/23 08:16:44.925
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jun 28 08:16:46.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-9278" for this suite. 06/28/23 08:16:46.938
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:16:46.945
Jun 28 08:16:46.945: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename init-container 06/28/23 08:16:46.946
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:16:46.981
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:16:46.985
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176
STEP: creating the pod 06/28/23 08:16:46.99
Jun 28 08:16:46.990: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Jun 28 08:16:51.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5083" for this suite. 06/28/23 08:16:51.91
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","completed":147,"skipped":2482,"failed":0}
------------------------------
â€¢ [4.972 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:16:46.945
    Jun 28 08:16:46.945: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename init-container 06/28/23 08:16:46.946
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:16:46.981
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:16:46.985
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should invoke init containers on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:176
    STEP: creating the pod 06/28/23 08:16:46.99
    Jun 28 08:16:46.990: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Jun 28 08:16:51.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-5083" for this suite. 06/28/23 08:16:51.91
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:16:51.919
Jun 28 08:16:51.920: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename disruption 06/28/23 08:16:51.921
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:16:51.935
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:16:51.94
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107
STEP: creating the pdb 06/28/23 08:16:51.944
STEP: Waiting for the pdb to be processed 06/28/23 08:16:51.95
STEP: updating the pdb 06/28/23 08:16:53.96
STEP: Waiting for the pdb to be processed 06/28/23 08:16:53.97
STEP: patching the pdb 06/28/23 08:16:53.976
STEP: Waiting for the pdb to be processed 06/28/23 08:16:53.987
STEP: Waiting for the pdb to be deleted 06/28/23 08:16:53.999
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Jun 28 08:16:54.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-9981" for this suite. 06/28/23 08:16:54.01
{"msg":"PASSED [sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]","completed":148,"skipped":2531,"failed":0}
------------------------------
â€¢ [2.097 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:16:51.919
    Jun 28 08:16:51.920: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename disruption 06/28/23 08:16:51.921
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:16:51.935
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:16:51.94
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should create a PodDisruptionBudget [Conformance]
      test/e2e/apps/disruption.go:107
    STEP: creating the pdb 06/28/23 08:16:51.944
    STEP: Waiting for the pdb to be processed 06/28/23 08:16:51.95
    STEP: updating the pdb 06/28/23 08:16:53.96
    STEP: Waiting for the pdb to be processed 06/28/23 08:16:53.97
    STEP: patching the pdb 06/28/23 08:16:53.976
    STEP: Waiting for the pdb to be processed 06/28/23 08:16:53.987
    STEP: Waiting for the pdb to be deleted 06/28/23 08:16:53.999
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Jun 28 08:16:54.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-9981" for this suite. 06/28/23 08:16:54.01
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:16:54.017
Jun 28 08:16:54.017: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename security-context 06/28/23 08:16:54.018
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:16:54.03
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:16:54.034
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 06/28/23 08:16:54.038
Jun 28 08:16:54.045: INFO: Waiting up to 5m0s for pod "security-context-0aec52d1-ac4c-41bb-9df9-4e50df6ef773" in namespace "security-context-2058" to be "Succeeded or Failed"
Jun 28 08:16:54.050: INFO: Pod "security-context-0aec52d1-ac4c-41bb-9df9-4e50df6ef773": Phase="Pending", Reason="", readiness=false. Elapsed: 4.963285ms
Jun 28 08:16:56.055: INFO: Pod "security-context-0aec52d1-ac4c-41bb-9df9-4e50df6ef773": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009433361s
Jun 28 08:16:58.056: INFO: Pod "security-context-0aec52d1-ac4c-41bb-9df9-4e50df6ef773": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01015367s
STEP: Saw pod success 06/28/23 08:16:58.056
Jun 28 08:16:58.056: INFO: Pod "security-context-0aec52d1-ac4c-41bb-9df9-4e50df6ef773" satisfied condition "Succeeded or Failed"
Jun 28 08:16:58.060: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod security-context-0aec52d1-ac4c-41bb-9df9-4e50df6ef773 container test-container: <nil>
STEP: delete the pod 06/28/23 08:16:58.109
Jun 28 08:16:58.119: INFO: Waiting for pod security-context-0aec52d1-ac4c-41bb-9df9-4e50df6ef773 to disappear
Jun 28 08:16:58.122: INFO: Pod security-context-0aec52d1-ac4c-41bb-9df9-4e50df6ef773 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Jun 28 08:16:58.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-2058" for this suite. 06/28/23 08:16:58.129
{"msg":"PASSED [sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","completed":149,"skipped":2560,"failed":0}
------------------------------
â€¢ [4.119 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:16:54.017
    Jun 28 08:16:54.017: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename security-context 06/28/23 08:16:54.018
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:16:54.03
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:16:54.034
    [It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:132
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 06/28/23 08:16:54.038
    Jun 28 08:16:54.045: INFO: Waiting up to 5m0s for pod "security-context-0aec52d1-ac4c-41bb-9df9-4e50df6ef773" in namespace "security-context-2058" to be "Succeeded or Failed"
    Jun 28 08:16:54.050: INFO: Pod "security-context-0aec52d1-ac4c-41bb-9df9-4e50df6ef773": Phase="Pending", Reason="", readiness=false. Elapsed: 4.963285ms
    Jun 28 08:16:56.055: INFO: Pod "security-context-0aec52d1-ac4c-41bb-9df9-4e50df6ef773": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009433361s
    Jun 28 08:16:58.056: INFO: Pod "security-context-0aec52d1-ac4c-41bb-9df9-4e50df6ef773": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01015367s
    STEP: Saw pod success 06/28/23 08:16:58.056
    Jun 28 08:16:58.056: INFO: Pod "security-context-0aec52d1-ac4c-41bb-9df9-4e50df6ef773" satisfied condition "Succeeded or Failed"
    Jun 28 08:16:58.060: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod security-context-0aec52d1-ac4c-41bb-9df9-4e50df6ef773 container test-container: <nil>
    STEP: delete the pod 06/28/23 08:16:58.109
    Jun 28 08:16:58.119: INFO: Waiting for pod security-context-0aec52d1-ac4c-41bb-9df9-4e50df6ef773 to disappear
    Jun 28 08:16:58.122: INFO: Pod security-context-0aec52d1-ac4c-41bb-9df9-4e50df6ef773 no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Jun 28 08:16:58.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-2058" for this suite. 06/28/23 08:16:58.129
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] ConfigMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:16:58.136
Jun 28 08:16:58.136: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename configmap 06/28/23 08:16:58.137
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:16:58.148
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:16:58.152
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422
STEP: Creating configMap with name configmap-test-volume-968719ef-1632-45df-a251-69208fb5dc75 06/28/23 08:16:58.156
STEP: Creating a pod to test consume configMaps 06/28/23 08:16:58.161
Jun 28 08:16:58.169: INFO: Waiting up to 5m0s for pod "pod-configmaps-f2affdd6-a105-4d62-96e2-c10ffe0f1ab5" in namespace "configmap-5133" to be "Succeeded or Failed"
Jun 28 08:16:58.173: INFO: Pod "pod-configmaps-f2affdd6-a105-4d62-96e2-c10ffe0f1ab5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.461789ms
Jun 28 08:17:00.179: INFO: Pod "pod-configmaps-f2affdd6-a105-4d62-96e2-c10ffe0f1ab5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009863546s
Jun 28 08:17:02.179: INFO: Pod "pod-configmaps-f2affdd6-a105-4d62-96e2-c10ffe0f1ab5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010081842s
STEP: Saw pod success 06/28/23 08:17:02.179
Jun 28 08:17:02.179: INFO: Pod "pod-configmaps-f2affdd6-a105-4d62-96e2-c10ffe0f1ab5" satisfied condition "Succeeded or Failed"
Jun 28 08:17:02.184: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod pod-configmaps-f2affdd6-a105-4d62-96e2-c10ffe0f1ab5 container configmap-volume-test: <nil>
STEP: delete the pod 06/28/23 08:17:02.194
Jun 28 08:17:02.206: INFO: Waiting for pod pod-configmaps-f2affdd6-a105-4d62-96e2-c10ffe0f1ab5 to disappear
Jun 28 08:17:02.210: INFO: Pod pod-configmaps-f2affdd6-a105-4d62-96e2-c10ffe0f1ab5 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jun 28 08:17:02.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5133" for this suite. 06/28/23 08:17:02.218
{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","completed":150,"skipped":2562,"failed":0}
------------------------------
â€¢ [4.089 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:16:58.136
    Jun 28 08:16:58.136: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename configmap 06/28/23 08:16:58.137
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:16:58.148
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:16:58.152
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:422
    STEP: Creating configMap with name configmap-test-volume-968719ef-1632-45df-a251-69208fb5dc75 06/28/23 08:16:58.156
    STEP: Creating a pod to test consume configMaps 06/28/23 08:16:58.161
    Jun 28 08:16:58.169: INFO: Waiting up to 5m0s for pod "pod-configmaps-f2affdd6-a105-4d62-96e2-c10ffe0f1ab5" in namespace "configmap-5133" to be "Succeeded or Failed"
    Jun 28 08:16:58.173: INFO: Pod "pod-configmaps-f2affdd6-a105-4d62-96e2-c10ffe0f1ab5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.461789ms
    Jun 28 08:17:00.179: INFO: Pod "pod-configmaps-f2affdd6-a105-4d62-96e2-c10ffe0f1ab5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009863546s
    Jun 28 08:17:02.179: INFO: Pod "pod-configmaps-f2affdd6-a105-4d62-96e2-c10ffe0f1ab5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010081842s
    STEP: Saw pod success 06/28/23 08:17:02.179
    Jun 28 08:17:02.179: INFO: Pod "pod-configmaps-f2affdd6-a105-4d62-96e2-c10ffe0f1ab5" satisfied condition "Succeeded or Failed"
    Jun 28 08:17:02.184: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod pod-configmaps-f2affdd6-a105-4d62-96e2-c10ffe0f1ab5 container configmap-volume-test: <nil>
    STEP: delete the pod 06/28/23 08:17:02.194
    Jun 28 08:17:02.206: INFO: Waiting for pod pod-configmaps-f2affdd6-a105-4d62-96e2-c10ffe0f1ab5 to disappear
    Jun 28 08:17:02.210: INFO: Pod pod-configmaps-f2affdd6-a105-4d62-96e2-c10ffe0f1ab5 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jun 28 08:17:02.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-5133" for this suite. 06/28/23 08:17:02.218
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl version
  should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1683
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:17:02.225
Jun 28 08:17:02.225: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename kubectl 06/28/23 08:17:02.226
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:17:02.238
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:17:02.243
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1683
Jun 28 08:17:02.248: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-4823 version'
Jun 28 08:17:02.311: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
Jun 28 08:17:02.311: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.10\", GitCommit:\"e770bdbb87cccdc2daa790ecd69f40cf4df3cc9d\", GitTreeState:\"clean\", BuildDate:\"2023-05-17T14:12:20Z\", GoVersion:\"go1.19.9\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"25+\", GitVersion:\"v1.25.10-ske.p2\", GitCommit:\"e770bdbb87cccdc2daa790ecd69f40cf4df3cc9d\", GitTreeState:\"clean\", BuildDate:\"2023-06-26T02:33:18Z\", GoVersion:\"go1.19.9\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jun 28 08:17:02.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4823" for this suite. 06/28/23 08:17:02.32
{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","completed":151,"skipped":2566,"failed":0}
------------------------------
â€¢ [0.102 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl version
  test/e2e/kubectl/kubectl.go:1677
    should check is all data is printed  [Conformance]
    test/e2e/kubectl/kubectl.go:1683

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:17:02.225
    Jun 28 08:17:02.225: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename kubectl 06/28/23 08:17:02.226
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:17:02.238
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:17:02.243
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check is all data is printed  [Conformance]
      test/e2e/kubectl/kubectl.go:1683
    Jun 28 08:17:02.248: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-4823 version'
    Jun 28 08:17:02.311: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
    Jun 28 08:17:02.311: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.10\", GitCommit:\"e770bdbb87cccdc2daa790ecd69f40cf4df3cc9d\", GitTreeState:\"clean\", BuildDate:\"2023-05-17T14:12:20Z\", GoVersion:\"go1.19.9\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"25+\", GitVersion:\"v1.25.10-ske.p2\", GitCommit:\"e770bdbb87cccdc2daa790ecd69f40cf4df3cc9d\", GitTreeState:\"clean\", BuildDate:\"2023-06-26T02:33:18Z\", GoVersion:\"go1.19.9\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jun 28 08:17:02.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-4823" for this suite. 06/28/23 08:17:02.32
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:17:02.327
Jun 28 08:17:02.327: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename custom-resource-definition 06/28/23 08:17:02.328
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:17:02.341
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:17:02.346
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
Jun 28 08:17:02.351: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jun 28 08:17:02.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-8487" for this suite. 06/28/23 08:17:02.904
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","completed":152,"skipped":2569,"failed":0}
------------------------------
â€¢ [0.585 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:145

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:17:02.327
    Jun 28 08:17:02.327: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename custom-resource-definition 06/28/23 08:17:02.328
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:17:02.341
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:17:02.346
    [It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:145
    Jun 28 08:17:02.351: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jun 28 08:17:02.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-8487" for this suite. 06/28/23 08:17:02.904
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:17:02.913
Jun 28 08:17:02.913: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename subpath 06/28/23 08:17:02.914
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:17:02.928
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:17:02.932
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 06/28/23 08:17:02.937
[It] should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
STEP: Creating pod pod-subpath-test-secret-lb6z 06/28/23 08:17:02.951
STEP: Creating a pod to test atomic-volume-subpath 06/28/23 08:17:02.951
Jun 28 08:17:02.960: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-lb6z" in namespace "subpath-1253" to be "Succeeded or Failed"
Jun 28 08:17:02.966: INFO: Pod "pod-subpath-test-secret-lb6z": Phase="Pending", Reason="", readiness=false. Elapsed: 5.58953ms
Jun 28 08:17:04.974: INFO: Pod "pod-subpath-test-secret-lb6z": Phase="Running", Reason="", readiness=true. Elapsed: 2.014469825s
Jun 28 08:17:06.971: INFO: Pod "pod-subpath-test-secret-lb6z": Phase="Running", Reason="", readiness=true. Elapsed: 4.010767603s
Jun 28 08:17:08.971: INFO: Pod "pod-subpath-test-secret-lb6z": Phase="Running", Reason="", readiness=true. Elapsed: 6.010906596s
Jun 28 08:17:10.972: INFO: Pod "pod-subpath-test-secret-lb6z": Phase="Running", Reason="", readiness=true. Elapsed: 8.011814341s
Jun 28 08:17:12.971: INFO: Pod "pod-subpath-test-secret-lb6z": Phase="Running", Reason="", readiness=true. Elapsed: 10.011491457s
Jun 28 08:17:14.972: INFO: Pod "pod-subpath-test-secret-lb6z": Phase="Running", Reason="", readiness=true. Elapsed: 12.011816541s
Jun 28 08:17:16.972: INFO: Pod "pod-subpath-test-secret-lb6z": Phase="Running", Reason="", readiness=true. Elapsed: 14.011646852s
Jun 28 08:17:18.971: INFO: Pod "pod-subpath-test-secret-lb6z": Phase="Running", Reason="", readiness=true. Elapsed: 16.010900258s
Jun 28 08:17:20.972: INFO: Pod "pod-subpath-test-secret-lb6z": Phase="Running", Reason="", readiness=true. Elapsed: 18.012004611s
Jun 28 08:17:22.971: INFO: Pod "pod-subpath-test-secret-lb6z": Phase="Running", Reason="", readiness=true. Elapsed: 20.010734534s
Jun 28 08:17:24.973: INFO: Pod "pod-subpath-test-secret-lb6z": Phase="Running", Reason="", readiness=false. Elapsed: 22.012654767s
Jun 28 08:17:26.972: INFO: Pod "pod-subpath-test-secret-lb6z": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.011635182s
STEP: Saw pod success 06/28/23 08:17:26.972
Jun 28 08:17:26.972: INFO: Pod "pod-subpath-test-secret-lb6z" satisfied condition "Succeeded or Failed"
Jun 28 08:17:26.977: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod pod-subpath-test-secret-lb6z container test-container-subpath-secret-lb6z: <nil>
STEP: delete the pod 06/28/23 08:17:27.04
Jun 28 08:17:27.052: INFO: Waiting for pod pod-subpath-test-secret-lb6z to disappear
Jun 28 08:17:27.056: INFO: Pod pod-subpath-test-secret-lb6z no longer exists
STEP: Deleting pod pod-subpath-test-secret-lb6z 06/28/23 08:17:27.056
Jun 28 08:17:27.056: INFO: Deleting pod "pod-subpath-test-secret-lb6z" in namespace "subpath-1253"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Jun 28 08:17:27.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1253" for this suite. 06/28/23 08:17:27.07
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [Conformance]","completed":153,"skipped":2600,"failed":0}
------------------------------
â€¢ [SLOW TEST] [24.164 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with secret pod [Conformance]
    test/e2e/storage/subpath.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:17:02.913
    Jun 28 08:17:02.913: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename subpath 06/28/23 08:17:02.914
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:17:02.928
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:17:02.932
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 06/28/23 08:17:02.937
    [It] should support subpaths with secret pod [Conformance]
      test/e2e/storage/subpath.go:60
    STEP: Creating pod pod-subpath-test-secret-lb6z 06/28/23 08:17:02.951
    STEP: Creating a pod to test atomic-volume-subpath 06/28/23 08:17:02.951
    Jun 28 08:17:02.960: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-lb6z" in namespace "subpath-1253" to be "Succeeded or Failed"
    Jun 28 08:17:02.966: INFO: Pod "pod-subpath-test-secret-lb6z": Phase="Pending", Reason="", readiness=false. Elapsed: 5.58953ms
    Jun 28 08:17:04.974: INFO: Pod "pod-subpath-test-secret-lb6z": Phase="Running", Reason="", readiness=true. Elapsed: 2.014469825s
    Jun 28 08:17:06.971: INFO: Pod "pod-subpath-test-secret-lb6z": Phase="Running", Reason="", readiness=true. Elapsed: 4.010767603s
    Jun 28 08:17:08.971: INFO: Pod "pod-subpath-test-secret-lb6z": Phase="Running", Reason="", readiness=true. Elapsed: 6.010906596s
    Jun 28 08:17:10.972: INFO: Pod "pod-subpath-test-secret-lb6z": Phase="Running", Reason="", readiness=true. Elapsed: 8.011814341s
    Jun 28 08:17:12.971: INFO: Pod "pod-subpath-test-secret-lb6z": Phase="Running", Reason="", readiness=true. Elapsed: 10.011491457s
    Jun 28 08:17:14.972: INFO: Pod "pod-subpath-test-secret-lb6z": Phase="Running", Reason="", readiness=true. Elapsed: 12.011816541s
    Jun 28 08:17:16.972: INFO: Pod "pod-subpath-test-secret-lb6z": Phase="Running", Reason="", readiness=true. Elapsed: 14.011646852s
    Jun 28 08:17:18.971: INFO: Pod "pod-subpath-test-secret-lb6z": Phase="Running", Reason="", readiness=true. Elapsed: 16.010900258s
    Jun 28 08:17:20.972: INFO: Pod "pod-subpath-test-secret-lb6z": Phase="Running", Reason="", readiness=true. Elapsed: 18.012004611s
    Jun 28 08:17:22.971: INFO: Pod "pod-subpath-test-secret-lb6z": Phase="Running", Reason="", readiness=true. Elapsed: 20.010734534s
    Jun 28 08:17:24.973: INFO: Pod "pod-subpath-test-secret-lb6z": Phase="Running", Reason="", readiness=false. Elapsed: 22.012654767s
    Jun 28 08:17:26.972: INFO: Pod "pod-subpath-test-secret-lb6z": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.011635182s
    STEP: Saw pod success 06/28/23 08:17:26.972
    Jun 28 08:17:26.972: INFO: Pod "pod-subpath-test-secret-lb6z" satisfied condition "Succeeded or Failed"
    Jun 28 08:17:26.977: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod pod-subpath-test-secret-lb6z container test-container-subpath-secret-lb6z: <nil>
    STEP: delete the pod 06/28/23 08:17:27.04
    Jun 28 08:17:27.052: INFO: Waiting for pod pod-subpath-test-secret-lb6z to disappear
    Jun 28 08:17:27.056: INFO: Pod pod-subpath-test-secret-lb6z no longer exists
    STEP: Deleting pod pod-subpath-test-secret-lb6z 06/28/23 08:17:27.056
    Jun 28 08:17:27.056: INFO: Deleting pod "pod-subpath-test-secret-lb6z" in namespace "subpath-1253"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Jun 28 08:17:27.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-1253" for this suite. 06/28/23 08:17:27.07
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:17:27.078
Jun 28 08:17:27.078: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename disruption 06/28/23 08:17:27.079
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:17:27.097
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:17:27.102
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140
STEP: Waiting for the pdb to be processed 06/28/23 08:17:27.113
STEP: Waiting for all pods to be running 06/28/23 08:17:29.147
Jun 28 08:17:29.152: INFO: running pods: 0 < 3
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Jun 28 08:17:31.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-5704" for this suite. 06/28/23 08:17:31.174
{"msg":"PASSED [sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]","completed":154,"skipped":2615,"failed":0}
------------------------------
â€¢ [4.104 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:17:27.078
    Jun 28 08:17:27.078: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename disruption 06/28/23 08:17:27.079
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:17:27.097
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:17:27.102
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should observe PodDisruptionBudget status updated [Conformance]
      test/e2e/apps/disruption.go:140
    STEP: Waiting for the pdb to be processed 06/28/23 08:17:27.113
    STEP: Waiting for all pods to be running 06/28/23 08:17:29.147
    Jun 28 08:17:29.152: INFO: running pods: 0 < 3
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Jun 28 08:17:31.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-5704" for this suite. 06/28/23 08:17:31.174
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:17:31.182
Jun 28 08:17:31.182: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename gc 06/28/23 08:17:31.183
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:17:31.198
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:17:31.202
[It] should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
Jun 28 08:17:31.242: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"23927543-b94a-49ce-8788-c8889a03632a", Controller:(*bool)(0xc002a8101e), BlockOwnerDeletion:(*bool)(0xc002a8101f)}}
Jun 28 08:17:31.250: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"1d1adbc6-3d4a-4245-8dad-c99f35dffae2", Controller:(*bool)(0xc0035f066e), BlockOwnerDeletion:(*bool)(0xc0035f066f)}}
Jun 28 08:17:31.259: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"d28f93fa-08ea-4999-9d05-76dc2d2081dc", Controller:(*bool)(0xc002a81386), BlockOwnerDeletion:(*bool)(0xc002a81387)}}
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Jun 28 08:17:36.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4530" for this suite. 06/28/23 08:17:36.28
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","completed":155,"skipped":2629,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.104 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:17:31.182
    Jun 28 08:17:31.182: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename gc 06/28/23 08:17:31.183
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:17:31.198
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:17:31.202
    [It] should not be blocked by dependency circle [Conformance]
      test/e2e/apimachinery/garbage_collector.go:849
    Jun 28 08:17:31.242: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"23927543-b94a-49ce-8788-c8889a03632a", Controller:(*bool)(0xc002a8101e), BlockOwnerDeletion:(*bool)(0xc002a8101f)}}
    Jun 28 08:17:31.250: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"1d1adbc6-3d4a-4245-8dad-c99f35dffae2", Controller:(*bool)(0xc0035f066e), BlockOwnerDeletion:(*bool)(0xc0035f066f)}}
    Jun 28 08:17:31.259: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"d28f93fa-08ea-4999-9d05-76dc2d2081dc", Controller:(*bool)(0xc002a81386), BlockOwnerDeletion:(*bool)(0xc002a81387)}}
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Jun 28 08:17:36.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-4530" for this suite. 06/28/23 08:17:36.28
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:17:36.287
Jun 28 08:17:36.288: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename disruption 06/28/23 08:17:36.288
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:17:36.301
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:17:36.306
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163
STEP: Waiting for the pdb to be processed 06/28/23 08:17:36.32
STEP: Updating PodDisruptionBudget status 06/28/23 08:17:36.325
STEP: Waiting for all pods to be running 06/28/23 08:17:36.358
Jun 28 08:17:36.362: INFO: running pods: 0 < 1
STEP: locating a running pod 06/28/23 08:17:38.367
STEP: Waiting for the pdb to be processed 06/28/23 08:17:38.384
STEP: Patching PodDisruptionBudget status 06/28/23 08:17:38.394
STEP: Waiting for the pdb to be processed 06/28/23 08:17:38.406
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Jun 28 08:17:38.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2719" for this suite. 06/28/23 08:17:38.417
{"msg":"PASSED [sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]","completed":156,"skipped":2648,"failed":0}
------------------------------
â€¢ [2.137 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:17:36.287
    Jun 28 08:17:36.288: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename disruption 06/28/23 08:17:36.288
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:17:36.301
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:17:36.306
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should update/patch PodDisruptionBudget status [Conformance]
      test/e2e/apps/disruption.go:163
    STEP: Waiting for the pdb to be processed 06/28/23 08:17:36.32
    STEP: Updating PodDisruptionBudget status 06/28/23 08:17:36.325
    STEP: Waiting for all pods to be running 06/28/23 08:17:36.358
    Jun 28 08:17:36.362: INFO: running pods: 0 < 1
    STEP: locating a running pod 06/28/23 08:17:38.367
    STEP: Waiting for the pdb to be processed 06/28/23 08:17:38.384
    STEP: Patching PodDisruptionBudget status 06/28/23 08:17:38.394
    STEP: Waiting for the pdb to be processed 06/28/23 08:17:38.406
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Jun 28 08:17:38.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-2719" for this suite. 06/28/23 08:17:38.417
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:17:38.425
Jun 28 08:17:38.425: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename runtimeclass 06/28/23 08:17:38.426
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:17:38.438
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:17:38.442
[It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
STEP: Deleting RuntimeClass runtimeclass-4317-delete-me 06/28/23 08:17:38.452
STEP: Waiting for the RuntimeClass to disappear 06/28/23 08:17:38.462
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Jun 28 08:17:38.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-4317" for this suite. 06/28/23 08:17:38.483
{"msg":"PASSED [sig-node] RuntimeClass should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]","completed":157,"skipped":2662,"failed":0}
------------------------------
â€¢ [0.067 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:17:38.425
    Jun 28 08:17:38.425: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename runtimeclass 06/28/23 08:17:38.426
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:17:38.438
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:17:38.442
    [It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:156
    STEP: Deleting RuntimeClass runtimeclass-4317-delete-me 06/28/23 08:17:38.452
    STEP: Waiting for the RuntimeClass to disappear 06/28/23 08:17:38.462
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Jun 28 08:17:38.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-4317" for this suite. 06/28/23 08:17:38.483
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:17:38.492
Jun 28 08:17:38.492: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename container-probe 06/28/23 08:17:38.493
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:17:38.507
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:17:38.513
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165
STEP: Creating pod liveness-cf0807d3-dee7-4c9c-a3af-074eede379eb in namespace container-probe-7412 06/28/23 08:17:38.52
Jun 28 08:17:38.530: INFO: Waiting up to 5m0s for pod "liveness-cf0807d3-dee7-4c9c-a3af-074eede379eb" in namespace "container-probe-7412" to be "not pending"
Jun 28 08:17:38.534: INFO: Pod "liveness-cf0807d3-dee7-4c9c-a3af-074eede379eb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.026681ms
Jun 28 08:17:40.540: INFO: Pod "liveness-cf0807d3-dee7-4c9c-a3af-074eede379eb": Phase="Running", Reason="", readiness=true. Elapsed: 2.010231195s
Jun 28 08:17:40.540: INFO: Pod "liveness-cf0807d3-dee7-4c9c-a3af-074eede379eb" satisfied condition "not pending"
Jun 28 08:17:40.540: INFO: Started pod liveness-cf0807d3-dee7-4c9c-a3af-074eede379eb in namespace container-probe-7412
STEP: checking the pod's current state and verifying that restartCount is present 06/28/23 08:17:40.54
Jun 28 08:17:40.545: INFO: Initial restart count of pod liveness-cf0807d3-dee7-4c9c-a3af-074eede379eb is 0
Jun 28 08:18:00.619: INFO: Restart count of pod container-probe-7412/liveness-cf0807d3-dee7-4c9c-a3af-074eede379eb is now 1 (20.074189017s elapsed)
STEP: deleting the pod 06/28/23 08:18:00.619
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jun 28 08:18:00.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7412" for this suite. 06/28/23 08:18:00.672
{"msg":"PASSED [sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","completed":158,"skipped":2681,"failed":0}
------------------------------
â€¢ [SLOW TEST] [22.188 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:17:38.492
    Jun 28 08:17:38.492: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename container-probe 06/28/23 08:17:38.493
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:17:38.507
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:17:38.513
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:165
    STEP: Creating pod liveness-cf0807d3-dee7-4c9c-a3af-074eede379eb in namespace container-probe-7412 06/28/23 08:17:38.52
    Jun 28 08:17:38.530: INFO: Waiting up to 5m0s for pod "liveness-cf0807d3-dee7-4c9c-a3af-074eede379eb" in namespace "container-probe-7412" to be "not pending"
    Jun 28 08:17:38.534: INFO: Pod "liveness-cf0807d3-dee7-4c9c-a3af-074eede379eb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.026681ms
    Jun 28 08:17:40.540: INFO: Pod "liveness-cf0807d3-dee7-4c9c-a3af-074eede379eb": Phase="Running", Reason="", readiness=true. Elapsed: 2.010231195s
    Jun 28 08:17:40.540: INFO: Pod "liveness-cf0807d3-dee7-4c9c-a3af-074eede379eb" satisfied condition "not pending"
    Jun 28 08:17:40.540: INFO: Started pod liveness-cf0807d3-dee7-4c9c-a3af-074eede379eb in namespace container-probe-7412
    STEP: checking the pod's current state and verifying that restartCount is present 06/28/23 08:17:40.54
    Jun 28 08:17:40.545: INFO: Initial restart count of pod liveness-cf0807d3-dee7-4c9c-a3af-074eede379eb is 0
    Jun 28 08:18:00.619: INFO: Restart count of pod container-probe-7412/liveness-cf0807d3-dee7-4c9c-a3af-074eede379eb is now 1 (20.074189017s elapsed)
    STEP: deleting the pod 06/28/23 08:18:00.619
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jun 28 08:18:00.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-7412" for this suite. 06/28/23 08:18:00.672
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Secrets
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:18:00.68
Jun 28 08:18:00.680: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename secrets 06/28/23 08:18:00.686
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:18:00.702
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:18:00.706
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204
STEP: Creating secret with name s-test-opt-del-81c08939-7cc1-488f-b74d-bee8711b6d90 06/28/23 08:18:00.721
STEP: Creating secret with name s-test-opt-upd-b0a4168d-5c29-45d6-886b-609ef765665f 06/28/23 08:18:00.726
STEP: Creating the pod 06/28/23 08:18:00.732
Jun 28 08:18:00.741: INFO: Waiting up to 5m0s for pod "pod-secrets-91dc1f2c-a7eb-4458-9104-97faf5a90715" in namespace "secrets-3648" to be "running and ready"
Jun 28 08:18:00.746: INFO: Pod "pod-secrets-91dc1f2c-a7eb-4458-9104-97faf5a90715": Phase="Pending", Reason="", readiness=false. Elapsed: 4.810468ms
Jun 28 08:18:00.746: INFO: The phase of Pod pod-secrets-91dc1f2c-a7eb-4458-9104-97faf5a90715 is Pending, waiting for it to be Running (with Ready = true)
Jun 28 08:18:02.752: INFO: Pod "pod-secrets-91dc1f2c-a7eb-4458-9104-97faf5a90715": Phase="Running", Reason="", readiness=true. Elapsed: 2.011040386s
Jun 28 08:18:02.752: INFO: The phase of Pod pod-secrets-91dc1f2c-a7eb-4458-9104-97faf5a90715 is Running (Ready = true)
Jun 28 08:18:02.752: INFO: Pod "pod-secrets-91dc1f2c-a7eb-4458-9104-97faf5a90715" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-81c08939-7cc1-488f-b74d-bee8711b6d90 06/28/23 08:18:02.971
STEP: Updating secret s-test-opt-upd-b0a4168d-5c29-45d6-886b-609ef765665f 06/28/23 08:18:02.979
STEP: Creating secret with name s-test-opt-create-e496a137-4802-41a7-a71b-0d55eab0447c 06/28/23 08:18:02.985
STEP: waiting to observe update in volume 06/28/23 08:18:02.991
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jun 28 08:18:05.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3648" for this suite. 06/28/23 08:18:05.233
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":159,"skipped":2683,"failed":0}
------------------------------
â€¢ [4.560 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:18:00.68
    Jun 28 08:18:00.680: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename secrets 06/28/23 08:18:00.686
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:18:00.702
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:18:00.706
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:204
    STEP: Creating secret with name s-test-opt-del-81c08939-7cc1-488f-b74d-bee8711b6d90 06/28/23 08:18:00.721
    STEP: Creating secret with name s-test-opt-upd-b0a4168d-5c29-45d6-886b-609ef765665f 06/28/23 08:18:00.726
    STEP: Creating the pod 06/28/23 08:18:00.732
    Jun 28 08:18:00.741: INFO: Waiting up to 5m0s for pod "pod-secrets-91dc1f2c-a7eb-4458-9104-97faf5a90715" in namespace "secrets-3648" to be "running and ready"
    Jun 28 08:18:00.746: INFO: Pod "pod-secrets-91dc1f2c-a7eb-4458-9104-97faf5a90715": Phase="Pending", Reason="", readiness=false. Elapsed: 4.810468ms
    Jun 28 08:18:00.746: INFO: The phase of Pod pod-secrets-91dc1f2c-a7eb-4458-9104-97faf5a90715 is Pending, waiting for it to be Running (with Ready = true)
    Jun 28 08:18:02.752: INFO: Pod "pod-secrets-91dc1f2c-a7eb-4458-9104-97faf5a90715": Phase="Running", Reason="", readiness=true. Elapsed: 2.011040386s
    Jun 28 08:18:02.752: INFO: The phase of Pod pod-secrets-91dc1f2c-a7eb-4458-9104-97faf5a90715 is Running (Ready = true)
    Jun 28 08:18:02.752: INFO: Pod "pod-secrets-91dc1f2c-a7eb-4458-9104-97faf5a90715" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-81c08939-7cc1-488f-b74d-bee8711b6d90 06/28/23 08:18:02.971
    STEP: Updating secret s-test-opt-upd-b0a4168d-5c29-45d6-886b-609ef765665f 06/28/23 08:18:02.979
    STEP: Creating secret with name s-test-opt-create-e496a137-4802-41a7-a71b-0d55eab0447c 06/28/23 08:18:02.985
    STEP: waiting to observe update in volume 06/28/23 08:18:02.991
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jun 28 08:18:05.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-3648" for this suite. 06/28/23 08:18:05.233
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:18:05.24
Jun 28 08:18:05.240: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename taint-multiple-pods 06/28/23 08:18:05.242
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:18:05.254
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:18:05.259
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/node/taints.go:348
Jun 28 08:18:05.264: INFO: Waiting up to 1m0s for all nodes to be ready
Jun 28 08:19:05.304: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420
Jun 28 08:19:05.308: INFO: Starting informer...
STEP: Starting pods... 06/28/23 08:19:05.308
Jun 28 08:19:05.527: INFO: Pod1 is running on ske-rhel-749f7d55c8xdd8b6-ct4cp. Tainting Node
Jun 28 08:19:05.739: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-7267" to be "running"
Jun 28 08:19:05.744: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.508248ms
Jun 28 08:19:07.749: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.010331115s
Jun 28 08:19:07.749: INFO: Pod "taint-eviction-b1" satisfied condition "running"
Jun 28 08:19:07.749: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-7267" to be "running"
Jun 28 08:19:07.753: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 4.073182ms
Jun 28 08:19:07.753: INFO: Pod "taint-eviction-b2" satisfied condition "running"
Jun 28 08:19:07.753: INFO: Pod2 is running on ske-rhel-749f7d55c8xdd8b6-ct4cp. Tainting Node
STEP: Trying to apply a taint on the Node 06/28/23 08:19:07.753
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 06/28/23 08:19:07.769
STEP: Waiting for Pod1 and Pod2 to be deleted 06/28/23 08:19:07.774
Jun 28 08:19:13.263: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Jun 28 08:19:33.304: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 06/28/23 08:19:33.318
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/framework.go:187
Jun 28 08:19:33.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-7267" for this suite. 06/28/23 08:19:33.33
{"msg":"PASSED [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","completed":160,"skipped":2683,"failed":0}
------------------------------
â€¢ [SLOW TEST] [88.096 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:18:05.24
    Jun 28 08:18:05.240: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename taint-multiple-pods 06/28/23 08:18:05.242
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:18:05.254
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:18:05.259
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/node/taints.go:348
    Jun 28 08:18:05.264: INFO: Waiting up to 1m0s for all nodes to be ready
    Jun 28 08:19:05.304: INFO: Waiting for terminating namespaces to be deleted...
    [It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
      test/e2e/node/taints.go:420
    Jun 28 08:19:05.308: INFO: Starting informer...
    STEP: Starting pods... 06/28/23 08:19:05.308
    Jun 28 08:19:05.527: INFO: Pod1 is running on ske-rhel-749f7d55c8xdd8b6-ct4cp. Tainting Node
    Jun 28 08:19:05.739: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-7267" to be "running"
    Jun 28 08:19:05.744: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.508248ms
    Jun 28 08:19:07.749: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.010331115s
    Jun 28 08:19:07.749: INFO: Pod "taint-eviction-b1" satisfied condition "running"
    Jun 28 08:19:07.749: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-7267" to be "running"
    Jun 28 08:19:07.753: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 4.073182ms
    Jun 28 08:19:07.753: INFO: Pod "taint-eviction-b2" satisfied condition "running"
    Jun 28 08:19:07.753: INFO: Pod2 is running on ske-rhel-749f7d55c8xdd8b6-ct4cp. Tainting Node
    STEP: Trying to apply a taint on the Node 06/28/23 08:19:07.753
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 06/28/23 08:19:07.769
    STEP: Waiting for Pod1 and Pod2 to be deleted 06/28/23 08:19:07.774
    Jun 28 08:19:13.263: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
    Jun 28 08:19:33.304: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 06/28/23 08:19:33.318
    [AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/framework.go:187
    Jun 28 08:19:33.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "taint-multiple-pods-7267" for this suite. 06/28/23 08:19:33.33
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:19:33.337
Jun 28 08:19:33.337: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename replication-controller 06/28/23 08:19:33.338
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:19:33.349
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:19:33.353
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91
STEP: Given a Pod with a 'name' label pod-adoption is created 06/28/23 08:19:33.357
Jun 28 08:19:33.365: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-1051" to be "running and ready"
Jun 28 08:19:33.370: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 4.40292ms
Jun 28 08:19:33.370: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Jun 28 08:19:35.375: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.010106555s
Jun 28 08:19:35.375: INFO: The phase of Pod pod-adoption is Running (Ready = true)
Jun 28 08:19:35.375: INFO: Pod "pod-adoption" satisfied condition "running and ready"
STEP: When a replication controller with a matching selector is created 06/28/23 08:19:35.379
STEP: Then the orphan pod is adopted 06/28/23 08:19:35.385
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Jun 28 08:19:36.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1051" for this suite. 06/28/23 08:19:36.401
{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","completed":161,"skipped":2697,"failed":0}
------------------------------
â€¢ [3.070 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:19:33.337
    Jun 28 08:19:33.337: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename replication-controller 06/28/23 08:19:33.338
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:19:33.349
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:19:33.353
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should adopt matching pods on creation [Conformance]
      test/e2e/apps/rc.go:91
    STEP: Given a Pod with a 'name' label pod-adoption is created 06/28/23 08:19:33.357
    Jun 28 08:19:33.365: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-1051" to be "running and ready"
    Jun 28 08:19:33.370: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 4.40292ms
    Jun 28 08:19:33.370: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    Jun 28 08:19:35.375: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.010106555s
    Jun 28 08:19:35.375: INFO: The phase of Pod pod-adoption is Running (Ready = true)
    Jun 28 08:19:35.375: INFO: Pod "pod-adoption" satisfied condition "running and ready"
    STEP: When a replication controller with a matching selector is created 06/28/23 08:19:35.379
    STEP: Then the orphan pod is adopted 06/28/23 08:19:35.385
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Jun 28 08:19:36.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-1051" for this suite. 06/28/23 08:19:36.401
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] Services
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:19:36.408
Jun 28 08:19:36.408: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename services 06/28/23 08:19:36.409
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:19:36.42
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:19:36.424
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268
STEP: creating service nodeport-test with type=NodePort in namespace services-8743 06/28/23 08:19:36.428
STEP: creating replication controller nodeport-test in namespace services-8743 06/28/23 08:19:36.44
I0628 08:19:36.451109      18 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-8743, replica count: 2
I0628 08:19:39.502087      18 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 28 08:19:39.502: INFO: Creating new exec pod
Jun 28 08:19:39.508: INFO: Waiting up to 5m0s for pod "execpodmm9cr" in namespace "services-8743" to be "running"
Jun 28 08:19:39.511: INFO: Pod "execpodmm9cr": Phase="Pending", Reason="", readiness=false. Elapsed: 3.439628ms
Jun 28 08:19:41.516: INFO: Pod "execpodmm9cr": Phase="Running", Reason="", readiness=true. Elapsed: 2.008352472s
Jun 28 08:19:41.516: INFO: Pod "execpodmm9cr" satisfied condition "running"
Jun 28 08:19:42.525: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-8743 exec execpodmm9cr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jun 28 08:19:43.002: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jun 28 08:19:43.002: INFO: stdout: ""
Jun 28 08:19:44.002: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-8743 exec execpodmm9cr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jun 28 08:19:44.480: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jun 28 08:19:44.480: INFO: stdout: ""
Jun 28 08:19:45.003: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-8743 exec execpodmm9cr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jun 28 08:19:45.429: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jun 28 08:19:45.429: INFO: stdout: ""
Jun 28 08:19:46.002: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-8743 exec execpodmm9cr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jun 28 08:19:46.479: INFO: stderr: "+ nc -v -t -w 2 nodeport-test 80\n+ echo hostName\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jun 28 08:19:46.479: INFO: stdout: ""
Jun 28 08:19:47.002: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-8743 exec execpodmm9cr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jun 28 08:19:47.396: INFO: stderr: "+ nc -v -t -w 2 nodeport-test 80\n+ echo hostName\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jun 28 08:19:47.396: INFO: stdout: ""
Jun 28 08:19:48.003: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-8743 exec execpodmm9cr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jun 28 08:19:48.430: INFO: stderr: "+ nc -v -t -w 2 nodeport-test 80\n+ echo hostName\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jun 28 08:19:48.430: INFO: stdout: ""
Jun 28 08:19:49.002: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-8743 exec execpodmm9cr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jun 28 08:19:49.453: INFO: stderr: "+ nc -v -t -w 2 nodeport-test 80\n+ echo hostName\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jun 28 08:19:49.453: INFO: stdout: ""
Jun 28 08:19:50.003: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-8743 exec execpodmm9cr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jun 28 08:19:50.440: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jun 28 08:19:50.440: INFO: stdout: "nodeport-test-jr6nz"
Jun 28 08:19:50.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-8743 exec execpodmm9cr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.20.39.240 80'
Jun 28 08:19:50.871: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.20.39.240 80\nConnection to 172.20.39.240 80 port [tcp/http] succeeded!\n"
Jun 28 08:19:50.871: INFO: stdout: ""
Jun 28 08:19:51.871: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-8743 exec execpodmm9cr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.20.39.240 80'
Jun 28 08:19:52.352: INFO: stderr: "+ nc -v -t -w 2 172.20.39.240 80\n+ echo hostName\nConnection to 172.20.39.240 80 port [tcp/http] succeeded!\n"
Jun 28 08:19:52.352: INFO: stdout: ""
Jun 28 08:19:52.871: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-8743 exec execpodmm9cr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.20.39.240 80'
Jun 28 08:19:53.366: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.20.39.240 80\nConnection to 172.20.39.240 80 port [tcp/http] succeeded!\n"
Jun 28 08:19:53.366: INFO: stdout: ""
Jun 28 08:19:53.872: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-8743 exec execpodmm9cr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.20.39.240 80'
Jun 28 08:19:54.377: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.20.39.240 80\nConnection to 172.20.39.240 80 port [tcp/http] succeeded!\n"
Jun 28 08:19:54.377: INFO: stdout: ""
Jun 28 08:19:54.871: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-8743 exec execpodmm9cr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.20.39.240 80'
Jun 28 08:19:55.376: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.20.39.240 80\nConnection to 172.20.39.240 80 port [tcp/http] succeeded!\n"
Jun 28 08:19:55.376: INFO: stdout: "nodeport-test-jr6nz"
Jun 28 08:19:55.376: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-8743 exec execpodmm9cr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.11.5 31098'
Jun 28 08:19:56.020: INFO: stderr: "+ nc -v -t -w 2 192.168.11.5 31098\nConnection to 192.168.11.5 31098 port [tcp/*] succeeded!\n+ echo hostName\n"
Jun 28 08:19:56.020: INFO: stdout: "nodeport-test-85p6b"
Jun 28 08:19:56.020: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-8743 exec execpodmm9cr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.11.3 31098'
Jun 28 08:19:56.476: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.11.3 31098\nConnection to 192.168.11.3 31098 port [tcp/*] succeeded!\n"
Jun 28 08:19:56.476: INFO: stdout: "nodeport-test-jr6nz"
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jun 28 08:19:56.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8743" for this suite. 06/28/23 08:19:56.484
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","completed":162,"skipped":2698,"failed":0}
------------------------------
â€¢ [SLOW TEST] [20.084 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:19:36.408
    Jun 28 08:19:36.408: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename services 06/28/23 08:19:36.409
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:19:36.42
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:19:36.424
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to create a functioning NodePort service [Conformance]
      test/e2e/network/service.go:1268
    STEP: creating service nodeport-test with type=NodePort in namespace services-8743 06/28/23 08:19:36.428
    STEP: creating replication controller nodeport-test in namespace services-8743 06/28/23 08:19:36.44
    I0628 08:19:36.451109      18 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-8743, replica count: 2
    I0628 08:19:39.502087      18 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jun 28 08:19:39.502: INFO: Creating new exec pod
    Jun 28 08:19:39.508: INFO: Waiting up to 5m0s for pod "execpodmm9cr" in namespace "services-8743" to be "running"
    Jun 28 08:19:39.511: INFO: Pod "execpodmm9cr": Phase="Pending", Reason="", readiness=false. Elapsed: 3.439628ms
    Jun 28 08:19:41.516: INFO: Pod "execpodmm9cr": Phase="Running", Reason="", readiness=true. Elapsed: 2.008352472s
    Jun 28 08:19:41.516: INFO: Pod "execpodmm9cr" satisfied condition "running"
    Jun 28 08:19:42.525: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-8743 exec execpodmm9cr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jun 28 08:19:43.002: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jun 28 08:19:43.002: INFO: stdout: ""
    Jun 28 08:19:44.002: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-8743 exec execpodmm9cr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jun 28 08:19:44.480: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jun 28 08:19:44.480: INFO: stdout: ""
    Jun 28 08:19:45.003: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-8743 exec execpodmm9cr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jun 28 08:19:45.429: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jun 28 08:19:45.429: INFO: stdout: ""
    Jun 28 08:19:46.002: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-8743 exec execpodmm9cr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jun 28 08:19:46.479: INFO: stderr: "+ nc -v -t -w 2 nodeport-test 80\n+ echo hostName\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jun 28 08:19:46.479: INFO: stdout: ""
    Jun 28 08:19:47.002: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-8743 exec execpodmm9cr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jun 28 08:19:47.396: INFO: stderr: "+ nc -v -t -w 2 nodeport-test 80\n+ echo hostName\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jun 28 08:19:47.396: INFO: stdout: ""
    Jun 28 08:19:48.003: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-8743 exec execpodmm9cr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jun 28 08:19:48.430: INFO: stderr: "+ nc -v -t -w 2 nodeport-test 80\n+ echo hostName\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jun 28 08:19:48.430: INFO: stdout: ""
    Jun 28 08:19:49.002: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-8743 exec execpodmm9cr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jun 28 08:19:49.453: INFO: stderr: "+ nc -v -t -w 2 nodeport-test 80\n+ echo hostName\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jun 28 08:19:49.453: INFO: stdout: ""
    Jun 28 08:19:50.003: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-8743 exec execpodmm9cr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jun 28 08:19:50.440: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jun 28 08:19:50.440: INFO: stdout: "nodeport-test-jr6nz"
    Jun 28 08:19:50.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-8743 exec execpodmm9cr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.20.39.240 80'
    Jun 28 08:19:50.871: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.20.39.240 80\nConnection to 172.20.39.240 80 port [tcp/http] succeeded!\n"
    Jun 28 08:19:50.871: INFO: stdout: ""
    Jun 28 08:19:51.871: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-8743 exec execpodmm9cr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.20.39.240 80'
    Jun 28 08:19:52.352: INFO: stderr: "+ nc -v -t -w 2 172.20.39.240 80\n+ echo hostName\nConnection to 172.20.39.240 80 port [tcp/http] succeeded!\n"
    Jun 28 08:19:52.352: INFO: stdout: ""
    Jun 28 08:19:52.871: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-8743 exec execpodmm9cr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.20.39.240 80'
    Jun 28 08:19:53.366: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.20.39.240 80\nConnection to 172.20.39.240 80 port [tcp/http] succeeded!\n"
    Jun 28 08:19:53.366: INFO: stdout: ""
    Jun 28 08:19:53.872: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-8743 exec execpodmm9cr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.20.39.240 80'
    Jun 28 08:19:54.377: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.20.39.240 80\nConnection to 172.20.39.240 80 port [tcp/http] succeeded!\n"
    Jun 28 08:19:54.377: INFO: stdout: ""
    Jun 28 08:19:54.871: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-8743 exec execpodmm9cr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.20.39.240 80'
    Jun 28 08:19:55.376: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.20.39.240 80\nConnection to 172.20.39.240 80 port [tcp/http] succeeded!\n"
    Jun 28 08:19:55.376: INFO: stdout: "nodeport-test-jr6nz"
    Jun 28 08:19:55.376: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-8743 exec execpodmm9cr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.11.5 31098'
    Jun 28 08:19:56.020: INFO: stderr: "+ nc -v -t -w 2 192.168.11.5 31098\nConnection to 192.168.11.5 31098 port [tcp/*] succeeded!\n+ echo hostName\n"
    Jun 28 08:19:56.020: INFO: stdout: "nodeport-test-85p6b"
    Jun 28 08:19:56.020: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-8743 exec execpodmm9cr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.11.3 31098'
    Jun 28 08:19:56.476: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.11.3 31098\nConnection to 192.168.11.3 31098 port [tcp/*] succeeded!\n"
    Jun 28 08:19:56.476: INFO: stdout: "nodeport-test-jr6nz"
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jun 28 08:19:56.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-8743" for this suite. 06/28/23 08:19:56.484
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Downward API
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:19:56.492
Jun 28 08:19:56.492: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename downward-api 06/28/23 08:19:56.493
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:19:56.506
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:19:56.512
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89
STEP: Creating a pod to test downward api env vars 06/28/23 08:19:56.517
Jun 28 08:19:56.527: INFO: Waiting up to 5m0s for pod "downward-api-f489f24f-0643-409d-b615-3e0a94ed5bce" in namespace "downward-api-231" to be "Succeeded or Failed"
Jun 28 08:19:56.531: INFO: Pod "downward-api-f489f24f-0643-409d-b615-3e0a94ed5bce": Phase="Pending", Reason="", readiness=false. Elapsed: 4.206453ms
Jun 28 08:19:58.536: INFO: Pod "downward-api-f489f24f-0643-409d-b615-3e0a94ed5bce": Phase="Running", Reason="", readiness=false. Elapsed: 2.009929953s
Jun 28 08:20:00.558: INFO: Pod "downward-api-f489f24f-0643-409d-b615-3e0a94ed5bce": Phase="Running", Reason="", readiness=false. Elapsed: 4.031491552s
Jun 28 08:20:02.537: INFO: Pod "downward-api-f489f24f-0643-409d-b615-3e0a94ed5bce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010373843s
STEP: Saw pod success 06/28/23 08:20:02.537
Jun 28 08:20:02.537: INFO: Pod "downward-api-f489f24f-0643-409d-b615-3e0a94ed5bce" satisfied condition "Succeeded or Failed"
Jun 28 08:20:02.541: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod downward-api-f489f24f-0643-409d-b615-3e0a94ed5bce container dapi-container: <nil>
STEP: delete the pod 06/28/23 08:20:02.598
Jun 28 08:20:02.610: INFO: Waiting for pod downward-api-f489f24f-0643-409d-b615-3e0a94ed5bce to disappear
Jun 28 08:20:02.614: INFO: Pod downward-api-f489f24f-0643-409d-b615-3e0a94ed5bce no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Jun 28 08:20:02.614: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-231" for this suite. 06/28/23 08:20:02.626
{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","completed":163,"skipped":2703,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.141 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:19:56.492
    Jun 28 08:19:56.492: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename downward-api 06/28/23 08:19:56.493
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:19:56.506
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:19:56.512
    [It] should provide host IP as an env var [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:89
    STEP: Creating a pod to test downward api env vars 06/28/23 08:19:56.517
    Jun 28 08:19:56.527: INFO: Waiting up to 5m0s for pod "downward-api-f489f24f-0643-409d-b615-3e0a94ed5bce" in namespace "downward-api-231" to be "Succeeded or Failed"
    Jun 28 08:19:56.531: INFO: Pod "downward-api-f489f24f-0643-409d-b615-3e0a94ed5bce": Phase="Pending", Reason="", readiness=false. Elapsed: 4.206453ms
    Jun 28 08:19:58.536: INFO: Pod "downward-api-f489f24f-0643-409d-b615-3e0a94ed5bce": Phase="Running", Reason="", readiness=false. Elapsed: 2.009929953s
    Jun 28 08:20:00.558: INFO: Pod "downward-api-f489f24f-0643-409d-b615-3e0a94ed5bce": Phase="Running", Reason="", readiness=false. Elapsed: 4.031491552s
    Jun 28 08:20:02.537: INFO: Pod "downward-api-f489f24f-0643-409d-b615-3e0a94ed5bce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010373843s
    STEP: Saw pod success 06/28/23 08:20:02.537
    Jun 28 08:20:02.537: INFO: Pod "downward-api-f489f24f-0643-409d-b615-3e0a94ed5bce" satisfied condition "Succeeded or Failed"
    Jun 28 08:20:02.541: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod downward-api-f489f24f-0643-409d-b615-3e0a94ed5bce container dapi-container: <nil>
    STEP: delete the pod 06/28/23 08:20:02.598
    Jun 28 08:20:02.610: INFO: Waiting for pod downward-api-f489f24f-0643-409d-b615-3e0a94ed5bce to disappear
    Jun 28 08:20:02.614: INFO: Pod downward-api-f489f24f-0643-409d-b615-3e0a94ed5bce no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Jun 28 08:20:02.614: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-231" for this suite. 06/28/23 08:20:02.626
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:20:02.636
Jun 28 08:20:02.636: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename pods 06/28/23 08:20:02.637
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:20:02.652
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:20:02.658
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535
Jun 28 08:20:02.664: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: creating the pod 06/28/23 08:20:02.665
STEP: submitting the pod to kubernetes 06/28/23 08:20:02.665
Jun 28 08:20:02.673: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-f6b7ed03-e05b-48ab-a201-f1e8eac4f571" in namespace "pods-9908" to be "running and ready"
Jun 28 08:20:02.678: INFO: Pod "pod-exec-websocket-f6b7ed03-e05b-48ab-a201-f1e8eac4f571": Phase="Pending", Reason="", readiness=false. Elapsed: 4.896883ms
Jun 28 08:20:02.678: INFO: The phase of Pod pod-exec-websocket-f6b7ed03-e05b-48ab-a201-f1e8eac4f571 is Pending, waiting for it to be Running (with Ready = true)
Jun 28 08:20:04.685: INFO: Pod "pod-exec-websocket-f6b7ed03-e05b-48ab-a201-f1e8eac4f571": Phase="Running", Reason="", readiness=true. Elapsed: 2.01180624s
Jun 28 08:20:04.685: INFO: The phase of Pod pod-exec-websocket-f6b7ed03-e05b-48ab-a201-f1e8eac4f571 is Running (Ready = true)
Jun 28 08:20:04.685: INFO: Pod "pod-exec-websocket-f6b7ed03-e05b-48ab-a201-f1e8eac4f571" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jun 28 08:20:04.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9908" for this suite. 06/28/23 08:20:04.862
{"msg":"PASSED [sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","completed":164,"skipped":2759,"failed":0}
------------------------------
â€¢ [2.233 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:20:02.636
    Jun 28 08:20:02.636: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename pods 06/28/23 08:20:02.637
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:20:02.652
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:20:02.658
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should support remote command execution over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:535
    Jun 28 08:20:02.664: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: creating the pod 06/28/23 08:20:02.665
    STEP: submitting the pod to kubernetes 06/28/23 08:20:02.665
    Jun 28 08:20:02.673: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-f6b7ed03-e05b-48ab-a201-f1e8eac4f571" in namespace "pods-9908" to be "running and ready"
    Jun 28 08:20:02.678: INFO: Pod "pod-exec-websocket-f6b7ed03-e05b-48ab-a201-f1e8eac4f571": Phase="Pending", Reason="", readiness=false. Elapsed: 4.896883ms
    Jun 28 08:20:02.678: INFO: The phase of Pod pod-exec-websocket-f6b7ed03-e05b-48ab-a201-f1e8eac4f571 is Pending, waiting for it to be Running (with Ready = true)
    Jun 28 08:20:04.685: INFO: Pod "pod-exec-websocket-f6b7ed03-e05b-48ab-a201-f1e8eac4f571": Phase="Running", Reason="", readiness=true. Elapsed: 2.01180624s
    Jun 28 08:20:04.685: INFO: The phase of Pod pod-exec-websocket-f6b7ed03-e05b-48ab-a201-f1e8eac4f571 is Running (Ready = true)
    Jun 28 08:20:04.685: INFO: Pod "pod-exec-websocket-f6b7ed03-e05b-48ab-a201-f1e8eac4f571" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jun 28 08:20:04.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-9908" for this suite. 06/28/23 08:20:04.862
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Variable Expansion
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:20:04.869
Jun 28 08:20:04.869: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename var-expansion 06/28/23 08:20:04.87
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:20:04.884
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:20:04.889
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43
STEP: Creating a pod to test env composition 06/28/23 08:20:04.894
Jun 28 08:20:04.902: INFO: Waiting up to 5m0s for pod "var-expansion-d8865194-c37e-4118-a4ec-e00c80e1a243" in namespace "var-expansion-665" to be "Succeeded or Failed"
Jun 28 08:20:04.906: INFO: Pod "var-expansion-d8865194-c37e-4118-a4ec-e00c80e1a243": Phase="Pending", Reason="", readiness=false. Elapsed: 3.562819ms
Jun 28 08:20:06.912: INFO: Pod "var-expansion-d8865194-c37e-4118-a4ec-e00c80e1a243": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009709536s
Jun 28 08:20:08.911: INFO: Pod "var-expansion-d8865194-c37e-4118-a4ec-e00c80e1a243": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009244859s
STEP: Saw pod success 06/28/23 08:20:08.911
Jun 28 08:20:08.912: INFO: Pod "var-expansion-d8865194-c37e-4118-a4ec-e00c80e1a243" satisfied condition "Succeeded or Failed"
Jun 28 08:20:08.916: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod var-expansion-d8865194-c37e-4118-a4ec-e00c80e1a243 container dapi-container: <nil>
STEP: delete the pod 06/28/23 08:20:08.967
Jun 28 08:20:08.980: INFO: Waiting for pod var-expansion-d8865194-c37e-4118-a4ec-e00c80e1a243 to disappear
Jun 28 08:20:08.983: INFO: Pod var-expansion-d8865194-c37e-4118-a4ec-e00c80e1a243 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jun 28 08:20:08.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-665" for this suite. 06/28/23 08:20:08.993
{"msg":"PASSED [sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","completed":165,"skipped":2763,"failed":0}
------------------------------
â€¢ [4.131 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:20:04.869
    Jun 28 08:20:04.869: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename var-expansion 06/28/23 08:20:04.87
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:20:04.884
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:20:04.889
    [It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:43
    STEP: Creating a pod to test env composition 06/28/23 08:20:04.894
    Jun 28 08:20:04.902: INFO: Waiting up to 5m0s for pod "var-expansion-d8865194-c37e-4118-a4ec-e00c80e1a243" in namespace "var-expansion-665" to be "Succeeded or Failed"
    Jun 28 08:20:04.906: INFO: Pod "var-expansion-d8865194-c37e-4118-a4ec-e00c80e1a243": Phase="Pending", Reason="", readiness=false. Elapsed: 3.562819ms
    Jun 28 08:20:06.912: INFO: Pod "var-expansion-d8865194-c37e-4118-a4ec-e00c80e1a243": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009709536s
    Jun 28 08:20:08.911: INFO: Pod "var-expansion-d8865194-c37e-4118-a4ec-e00c80e1a243": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009244859s
    STEP: Saw pod success 06/28/23 08:20:08.911
    Jun 28 08:20:08.912: INFO: Pod "var-expansion-d8865194-c37e-4118-a4ec-e00c80e1a243" satisfied condition "Succeeded or Failed"
    Jun 28 08:20:08.916: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod var-expansion-d8865194-c37e-4118-a4ec-e00c80e1a243 container dapi-container: <nil>
    STEP: delete the pod 06/28/23 08:20:08.967
    Jun 28 08:20:08.980: INFO: Waiting for pod var-expansion-d8865194-c37e-4118-a4ec-e00c80e1a243 to disappear
    Jun 28 08:20:08.983: INFO: Pod var-expansion-d8865194-c37e-4118-a4ec-e00c80e1a243 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jun 28 08:20:08.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-665" for this suite. 06/28/23 08:20:08.993
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:20:09
Jun 28 08:20:09.000: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename downward-api 06/28/23 08:20:09.001
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:20:09.015
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:20:09.02
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266
STEP: Creating a pod to test downward api env vars 06/28/23 08:20:09.025
Jun 28 08:20:09.034: INFO: Waiting up to 5m0s for pod "downward-api-431e451d-c81c-4223-9cfd-5eb8af22f7c4" in namespace "downward-api-8925" to be "Succeeded or Failed"
Jun 28 08:20:09.039: INFO: Pod "downward-api-431e451d-c81c-4223-9cfd-5eb8af22f7c4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.680642ms
Jun 28 08:20:11.046: INFO: Pod "downward-api-431e451d-c81c-4223-9cfd-5eb8af22f7c4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012307638s
Jun 28 08:20:13.044: INFO: Pod "downward-api-431e451d-c81c-4223-9cfd-5eb8af22f7c4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009732929s
STEP: Saw pod success 06/28/23 08:20:13.044
Jun 28 08:20:13.044: INFO: Pod "downward-api-431e451d-c81c-4223-9cfd-5eb8af22f7c4" satisfied condition "Succeeded or Failed"
Jun 28 08:20:13.048: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-zxlfv pod downward-api-431e451d-c81c-4223-9cfd-5eb8af22f7c4 container dapi-container: <nil>
STEP: delete the pod 06/28/23 08:20:13.13
Jun 28 08:20:13.142: INFO: Waiting for pod downward-api-431e451d-c81c-4223-9cfd-5eb8af22f7c4 to disappear
Jun 28 08:20:13.146: INFO: Pod downward-api-431e451d-c81c-4223-9cfd-5eb8af22f7c4 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Jun 28 08:20:13.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8925" for this suite. 06/28/23 08:20:13.156
{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","completed":166,"skipped":2772,"failed":0}
------------------------------
â€¢ [4.164 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:20:09
    Jun 28 08:20:09.000: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename downward-api 06/28/23 08:20:09.001
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:20:09.015
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:20:09.02
    [It] should provide pod UID as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:266
    STEP: Creating a pod to test downward api env vars 06/28/23 08:20:09.025
    Jun 28 08:20:09.034: INFO: Waiting up to 5m0s for pod "downward-api-431e451d-c81c-4223-9cfd-5eb8af22f7c4" in namespace "downward-api-8925" to be "Succeeded or Failed"
    Jun 28 08:20:09.039: INFO: Pod "downward-api-431e451d-c81c-4223-9cfd-5eb8af22f7c4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.680642ms
    Jun 28 08:20:11.046: INFO: Pod "downward-api-431e451d-c81c-4223-9cfd-5eb8af22f7c4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012307638s
    Jun 28 08:20:13.044: INFO: Pod "downward-api-431e451d-c81c-4223-9cfd-5eb8af22f7c4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009732929s
    STEP: Saw pod success 06/28/23 08:20:13.044
    Jun 28 08:20:13.044: INFO: Pod "downward-api-431e451d-c81c-4223-9cfd-5eb8af22f7c4" satisfied condition "Succeeded or Failed"
    Jun 28 08:20:13.048: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-zxlfv pod downward-api-431e451d-c81c-4223-9cfd-5eb8af22f7c4 container dapi-container: <nil>
    STEP: delete the pod 06/28/23 08:20:13.13
    Jun 28 08:20:13.142: INFO: Waiting for pod downward-api-431e451d-c81c-4223-9cfd-5eb8af22f7c4 to disappear
    Jun 28 08:20:13.146: INFO: Pod downward-api-431e451d-c81c-4223-9cfd-5eb8af22f7c4 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Jun 28 08:20:13.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-8925" for this suite. 06/28/23 08:20:13.156
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:20:13.164
Jun 28 08:20:13.164: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename podtemplate 06/28/23 08:20:13.165
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:20:13.179
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:20:13.184
[It] should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
Jun 28 08:20:13.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-8008" for this suite. 06/28/23 08:20:13.24
{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","completed":167,"skipped":2784,"failed":0}
------------------------------
â€¢ [0.083 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:20:13.164
    Jun 28 08:20:13.164: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename podtemplate 06/28/23 08:20:13.165
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:20:13.179
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:20:13.184
    [It] should run the lifecycle of PodTemplates [Conformance]
      test/e2e/common/node/podtemplates.go:53
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    Jun 28 08:20:13.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-8008" for this suite. 06/28/23 08:20:13.24
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1810
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:20:13.247
Jun 28 08:20:13.248: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename kubectl 06/28/23 08:20:13.248
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:20:13.26
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:20:13.265
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1810
STEP: Starting the proxy 06/28/23 08:20:13.271
Jun 28 08:20:13.271: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-9067 proxy --unix-socket=/tmp/kubectl-proxy-unix4144366520/test'
STEP: retrieving proxy /api/ output 06/28/23 08:20:13.327
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jun 28 08:20:13.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9067" for this suite. 06/28/23 08:20:13.336
{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","completed":168,"skipped":2798,"failed":0}
------------------------------
â€¢ [0.096 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1778
    should support --unix-socket=/path  [Conformance]
    test/e2e/kubectl/kubectl.go:1810

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:20:13.247
    Jun 28 08:20:13.248: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename kubectl 06/28/23 08:20:13.248
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:20:13.26
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:20:13.265
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should support --unix-socket=/path  [Conformance]
      test/e2e/kubectl/kubectl.go:1810
    STEP: Starting the proxy 06/28/23 08:20:13.271
    Jun 28 08:20:13.271: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-9067 proxy --unix-socket=/tmp/kubectl-proxy-unix4144366520/test'
    STEP: retrieving proxy /api/ output 06/28/23 08:20:13.327
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jun 28 08:20:13.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-9067" for this suite. 06/28/23 08:20:13.336
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:194
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:20:13.343
Jun 28 08:20:13.343: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename container-runtime 06/28/23 08:20:13.344
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:20:13.359
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:20:13.364
[It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:194
STEP: create the container 06/28/23 08:20:13.369
STEP: wait for the container to reach Succeeded 06/28/23 08:20:13.377
STEP: get the container status 06/28/23 08:20:16.398
STEP: the container should be terminated 06/28/23 08:20:16.402
STEP: the termination message should be set 06/28/23 08:20:16.402
Jun 28 08:20:16.402: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 06/28/23 08:20:16.402
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Jun 28 08:20:16.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5840" for this suite. 06/28/23 08:20:16.438
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","completed":169,"skipped":2799,"failed":0}
------------------------------
â€¢ [3.102 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:20:13.343
    Jun 28 08:20:13.343: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename container-runtime 06/28/23 08:20:13.344
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:20:13.359
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:20:13.364
    [It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:194
    STEP: create the container 06/28/23 08:20:13.369
    STEP: wait for the container to reach Succeeded 06/28/23 08:20:13.377
    STEP: get the container status 06/28/23 08:20:16.398
    STEP: the container should be terminated 06/28/23 08:20:16.402
    STEP: the termination message should be set 06/28/23 08:20:16.402
    Jun 28 08:20:16.402: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 06/28/23 08:20:16.402
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Jun 28 08:20:16.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-5840" for this suite. 06/28/23 08:20:16.438
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:20:16.446
Jun 28 08:20:16.446: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename namespaces 06/28/23 08:20:16.447
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:20:16.463
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:20:16.468
[It] should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267
STEP: creating a Namespace 06/28/23 08:20:16.474
STEP: patching the Namespace 06/28/23 08:20:16.487
STEP: get the Namespace and ensuring it has the label 06/28/23 08:20:16.493
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Jun 28 08:20:16.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-920" for this suite. 06/28/23 08:20:16.507
STEP: Destroying namespace "nspatchtest-67f28bc6-df02-4c96-a9e9-1242c3d08fe5-5343" for this suite. 06/28/23 08:20:16.515
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","completed":170,"skipped":2816,"failed":0}
------------------------------
â€¢ [0.076 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:20:16.446
    Jun 28 08:20:16.446: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename namespaces 06/28/23 08:20:16.447
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:20:16.463
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:20:16.468
    [It] should patch a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:267
    STEP: creating a Namespace 06/28/23 08:20:16.474
    STEP: patching the Namespace 06/28/23 08:20:16.487
    STEP: get the Namespace and ensuring it has the label 06/28/23 08:20:16.493
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Jun 28 08:20:16.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-920" for this suite. 06/28/23 08:20:16.507
    STEP: Destroying namespace "nspatchtest-67f28bc6-df02-4c96-a9e9-1242c3d08fe5-5343" for this suite. 06/28/23 08:20:16.515
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:20:16.523
Jun 28 08:20:16.524: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename podtemplate 06/28/23 08:20:16.524
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:20:16.541
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:20:16.546
[It] should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
STEP: Create set of pod templates 06/28/23 08:20:16.553
Jun 28 08:20:16.559: INFO: created test-podtemplate-1
Jun 28 08:20:16.565: INFO: created test-podtemplate-2
Jun 28 08:20:16.571: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace 06/28/23 08:20:16.571
STEP: delete collection of pod templates 06/28/23 08:20:16.576
Jun 28 08:20:16.576: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity 06/28/23 08:20:16.593
Jun 28 08:20:16.594: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
Jun 28 08:20:16.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-7132" for this suite. 06/28/23 08:20:16.608
{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","completed":171,"skipped":2836,"failed":0}
------------------------------
â€¢ [0.094 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:20:16.523
    Jun 28 08:20:16.524: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename podtemplate 06/28/23 08:20:16.524
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:20:16.541
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:20:16.546
    [It] should delete a collection of pod templates [Conformance]
      test/e2e/common/node/podtemplates.go:122
    STEP: Create set of pod templates 06/28/23 08:20:16.553
    Jun 28 08:20:16.559: INFO: created test-podtemplate-1
    Jun 28 08:20:16.565: INFO: created test-podtemplate-2
    Jun 28 08:20:16.571: INFO: created test-podtemplate-3
    STEP: get a list of pod templates with a label in the current namespace 06/28/23 08:20:16.571
    STEP: delete collection of pod templates 06/28/23 08:20:16.576
    Jun 28 08:20:16.576: INFO: requesting DeleteCollection of pod templates
    STEP: check that the list of pod templates matches the requested quantity 06/28/23 08:20:16.593
    Jun 28 08:20:16.594: INFO: requesting list of pod templates to confirm quantity
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    Jun 28 08:20:16.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-7132" for this suite. 06/28/23 08:20:16.608
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] Discovery
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:20:16.618
Jun 28 08:20:16.618: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename discovery 06/28/23 08:20:16.619
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:20:16.638
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:20:16.642
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/apimachinery/discovery.go:43
STEP: Setting up server cert 06/28/23 08:20:16.649
[It] should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
Jun 28 08:20:16.914: INFO: Checking APIGroup: apiregistration.k8s.io
Jun 28 08:20:16.916: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Jun 28 08:20:16.916: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Jun 28 08:20:16.916: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Jun 28 08:20:16.916: INFO: Checking APIGroup: apps
Jun 28 08:20:16.918: INFO: PreferredVersion.GroupVersion: apps/v1
Jun 28 08:20:16.918: INFO: Versions found [{apps/v1 v1}]
Jun 28 08:20:16.918: INFO: apps/v1 matches apps/v1
Jun 28 08:20:16.918: INFO: Checking APIGroup: events.k8s.io
Jun 28 08:20:16.920: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Jun 28 08:20:16.920: INFO: Versions found [{events.k8s.io/v1 v1}]
Jun 28 08:20:16.920: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Jun 28 08:20:16.920: INFO: Checking APIGroup: authentication.k8s.io
Jun 28 08:20:16.922: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Jun 28 08:20:16.922: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Jun 28 08:20:16.922: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Jun 28 08:20:16.922: INFO: Checking APIGroup: authorization.k8s.io
Jun 28 08:20:16.924: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Jun 28 08:20:16.924: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Jun 28 08:20:16.924: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Jun 28 08:20:16.924: INFO: Checking APIGroup: autoscaling
Jun 28 08:20:16.926: INFO: PreferredVersion.GroupVersion: autoscaling/v2
Jun 28 08:20:16.926: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1} {autoscaling/v2beta2 v2beta2}]
Jun 28 08:20:16.926: INFO: autoscaling/v2 matches autoscaling/v2
Jun 28 08:20:16.926: INFO: Checking APIGroup: batch
Jun 28 08:20:16.928: INFO: PreferredVersion.GroupVersion: batch/v1
Jun 28 08:20:16.929: INFO: Versions found [{batch/v1 v1}]
Jun 28 08:20:16.929: INFO: batch/v1 matches batch/v1
Jun 28 08:20:16.929: INFO: Checking APIGroup: certificates.k8s.io
Jun 28 08:20:16.931: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Jun 28 08:20:16.931: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Jun 28 08:20:16.931: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Jun 28 08:20:16.931: INFO: Checking APIGroup: networking.k8s.io
Jun 28 08:20:16.933: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Jun 28 08:20:16.933: INFO: Versions found [{networking.k8s.io/v1 v1}]
Jun 28 08:20:16.933: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Jun 28 08:20:16.933: INFO: Checking APIGroup: policy
Jun 28 08:20:16.935: INFO: PreferredVersion.GroupVersion: policy/v1
Jun 28 08:20:16.935: INFO: Versions found [{policy/v1 v1}]
Jun 28 08:20:16.935: INFO: policy/v1 matches policy/v1
Jun 28 08:20:16.935: INFO: Checking APIGroup: rbac.authorization.k8s.io
Jun 28 08:20:16.937: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Jun 28 08:20:16.937: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Jun 28 08:20:16.937: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Jun 28 08:20:16.937: INFO: Checking APIGroup: storage.k8s.io
Jun 28 08:20:16.939: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Jun 28 08:20:16.939: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Jun 28 08:20:16.939: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Jun 28 08:20:16.939: INFO: Checking APIGroup: admissionregistration.k8s.io
Jun 28 08:20:16.941: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Jun 28 08:20:16.941: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Jun 28 08:20:16.941: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Jun 28 08:20:16.941: INFO: Checking APIGroup: apiextensions.k8s.io
Jun 28 08:20:16.944: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Jun 28 08:20:16.944: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Jun 28 08:20:16.944: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Jun 28 08:20:16.944: INFO: Checking APIGroup: scheduling.k8s.io
Jun 28 08:20:16.946: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Jun 28 08:20:16.946: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Jun 28 08:20:16.946: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Jun 28 08:20:16.946: INFO: Checking APIGroup: coordination.k8s.io
Jun 28 08:20:16.948: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Jun 28 08:20:16.948: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Jun 28 08:20:16.948: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Jun 28 08:20:16.948: INFO: Checking APIGroup: node.k8s.io
Jun 28 08:20:16.950: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Jun 28 08:20:16.950: INFO: Versions found [{node.k8s.io/v1 v1}]
Jun 28 08:20:16.950: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Jun 28 08:20:16.950: INFO: Checking APIGroup: discovery.k8s.io
Jun 28 08:20:16.952: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Jun 28 08:20:16.952: INFO: Versions found [{discovery.k8s.io/v1 v1}]
Jun 28 08:20:16.952: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Jun 28 08:20:16.952: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Jun 28 08:20:16.955: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta2
Jun 28 08:20:16.955: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta2 v1beta2} {flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
Jun 28 08:20:16.955: INFO: flowcontrol.apiserver.k8s.io/v1beta2 matches flowcontrol.apiserver.k8s.io/v1beta2
Jun 28 08:20:16.955: INFO: Checking APIGroup: crd.projectcalico.org
Jun 28 08:20:16.957: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Jun 28 08:20:16.957: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Jun 28 08:20:16.957: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
Jun 28 08:20:16.957: INFO: Checking APIGroup: metrics.k8s.io
Jun 28 08:20:16.959: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
Jun 28 08:20:16.959: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
Jun 28 08:20:16.959: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  test/e2e/framework/framework.go:187
Jun 28 08:20:16.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-6131" for this suite. 06/28/23 08:20:16.967
{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","completed":172,"skipped":2840,"failed":0}
------------------------------
â€¢ [0.356 seconds]
[sig-api-machinery] Discovery
test/e2e/apimachinery/framework.go:23
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:20:16.618
    Jun 28 08:20:16.618: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename discovery 06/28/23 08:20:16.619
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:20:16.638
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:20:16.642
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/apimachinery/discovery.go:43
    STEP: Setting up server cert 06/28/23 08:20:16.649
    [It] should validate PreferredVersion for each APIGroup [Conformance]
      test/e2e/apimachinery/discovery.go:122
    Jun 28 08:20:16.914: INFO: Checking APIGroup: apiregistration.k8s.io
    Jun 28 08:20:16.916: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
    Jun 28 08:20:16.916: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
    Jun 28 08:20:16.916: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
    Jun 28 08:20:16.916: INFO: Checking APIGroup: apps
    Jun 28 08:20:16.918: INFO: PreferredVersion.GroupVersion: apps/v1
    Jun 28 08:20:16.918: INFO: Versions found [{apps/v1 v1}]
    Jun 28 08:20:16.918: INFO: apps/v1 matches apps/v1
    Jun 28 08:20:16.918: INFO: Checking APIGroup: events.k8s.io
    Jun 28 08:20:16.920: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
    Jun 28 08:20:16.920: INFO: Versions found [{events.k8s.io/v1 v1}]
    Jun 28 08:20:16.920: INFO: events.k8s.io/v1 matches events.k8s.io/v1
    Jun 28 08:20:16.920: INFO: Checking APIGroup: authentication.k8s.io
    Jun 28 08:20:16.922: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
    Jun 28 08:20:16.922: INFO: Versions found [{authentication.k8s.io/v1 v1}]
    Jun 28 08:20:16.922: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
    Jun 28 08:20:16.922: INFO: Checking APIGroup: authorization.k8s.io
    Jun 28 08:20:16.924: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
    Jun 28 08:20:16.924: INFO: Versions found [{authorization.k8s.io/v1 v1}]
    Jun 28 08:20:16.924: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
    Jun 28 08:20:16.924: INFO: Checking APIGroup: autoscaling
    Jun 28 08:20:16.926: INFO: PreferredVersion.GroupVersion: autoscaling/v2
    Jun 28 08:20:16.926: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1} {autoscaling/v2beta2 v2beta2}]
    Jun 28 08:20:16.926: INFO: autoscaling/v2 matches autoscaling/v2
    Jun 28 08:20:16.926: INFO: Checking APIGroup: batch
    Jun 28 08:20:16.928: INFO: PreferredVersion.GroupVersion: batch/v1
    Jun 28 08:20:16.929: INFO: Versions found [{batch/v1 v1}]
    Jun 28 08:20:16.929: INFO: batch/v1 matches batch/v1
    Jun 28 08:20:16.929: INFO: Checking APIGroup: certificates.k8s.io
    Jun 28 08:20:16.931: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
    Jun 28 08:20:16.931: INFO: Versions found [{certificates.k8s.io/v1 v1}]
    Jun 28 08:20:16.931: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
    Jun 28 08:20:16.931: INFO: Checking APIGroup: networking.k8s.io
    Jun 28 08:20:16.933: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
    Jun 28 08:20:16.933: INFO: Versions found [{networking.k8s.io/v1 v1}]
    Jun 28 08:20:16.933: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
    Jun 28 08:20:16.933: INFO: Checking APIGroup: policy
    Jun 28 08:20:16.935: INFO: PreferredVersion.GroupVersion: policy/v1
    Jun 28 08:20:16.935: INFO: Versions found [{policy/v1 v1}]
    Jun 28 08:20:16.935: INFO: policy/v1 matches policy/v1
    Jun 28 08:20:16.935: INFO: Checking APIGroup: rbac.authorization.k8s.io
    Jun 28 08:20:16.937: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
    Jun 28 08:20:16.937: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
    Jun 28 08:20:16.937: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
    Jun 28 08:20:16.937: INFO: Checking APIGroup: storage.k8s.io
    Jun 28 08:20:16.939: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
    Jun 28 08:20:16.939: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
    Jun 28 08:20:16.939: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
    Jun 28 08:20:16.939: INFO: Checking APIGroup: admissionregistration.k8s.io
    Jun 28 08:20:16.941: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
    Jun 28 08:20:16.941: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
    Jun 28 08:20:16.941: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
    Jun 28 08:20:16.941: INFO: Checking APIGroup: apiextensions.k8s.io
    Jun 28 08:20:16.944: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
    Jun 28 08:20:16.944: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
    Jun 28 08:20:16.944: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
    Jun 28 08:20:16.944: INFO: Checking APIGroup: scheduling.k8s.io
    Jun 28 08:20:16.946: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
    Jun 28 08:20:16.946: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
    Jun 28 08:20:16.946: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
    Jun 28 08:20:16.946: INFO: Checking APIGroup: coordination.k8s.io
    Jun 28 08:20:16.948: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
    Jun 28 08:20:16.948: INFO: Versions found [{coordination.k8s.io/v1 v1}]
    Jun 28 08:20:16.948: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
    Jun 28 08:20:16.948: INFO: Checking APIGroup: node.k8s.io
    Jun 28 08:20:16.950: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
    Jun 28 08:20:16.950: INFO: Versions found [{node.k8s.io/v1 v1}]
    Jun 28 08:20:16.950: INFO: node.k8s.io/v1 matches node.k8s.io/v1
    Jun 28 08:20:16.950: INFO: Checking APIGroup: discovery.k8s.io
    Jun 28 08:20:16.952: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
    Jun 28 08:20:16.952: INFO: Versions found [{discovery.k8s.io/v1 v1}]
    Jun 28 08:20:16.952: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
    Jun 28 08:20:16.952: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
    Jun 28 08:20:16.955: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta2
    Jun 28 08:20:16.955: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta2 v1beta2} {flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
    Jun 28 08:20:16.955: INFO: flowcontrol.apiserver.k8s.io/v1beta2 matches flowcontrol.apiserver.k8s.io/v1beta2
    Jun 28 08:20:16.955: INFO: Checking APIGroup: crd.projectcalico.org
    Jun 28 08:20:16.957: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
    Jun 28 08:20:16.957: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
    Jun 28 08:20:16.957: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
    Jun 28 08:20:16.957: INFO: Checking APIGroup: metrics.k8s.io
    Jun 28 08:20:16.959: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
    Jun 28 08:20:16.959: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
    Jun 28 08:20:16.959: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
    [AfterEach] [sig-api-machinery] Discovery
      test/e2e/framework/framework.go:187
    Jun 28 08:20:16.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "discovery-6131" for this suite. 06/28/23 08:20:16.967
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Pods
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:20:16.974
Jun 28 08:20:16.974: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename pods 06/28/23 08:20:16.975
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:20:16.986
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:20:16.992
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443
Jun 28 08:20:17.009: INFO: Waiting up to 5m0s for pod "server-envvars-dce82082-c309-4c63-9f87-62665f15f980" in namespace "pods-731" to be "running and ready"
Jun 28 08:20:17.013: INFO: Pod "server-envvars-dce82082-c309-4c63-9f87-62665f15f980": Phase="Pending", Reason="", readiness=false. Elapsed: 4.846511ms
Jun 28 08:20:17.014: INFO: The phase of Pod server-envvars-dce82082-c309-4c63-9f87-62665f15f980 is Pending, waiting for it to be Running (with Ready = true)
Jun 28 08:20:19.019: INFO: Pod "server-envvars-dce82082-c309-4c63-9f87-62665f15f980": Phase="Running", Reason="", readiness=true. Elapsed: 2.010344781s
Jun 28 08:20:19.019: INFO: The phase of Pod server-envvars-dce82082-c309-4c63-9f87-62665f15f980 is Running (Ready = true)
Jun 28 08:20:19.019: INFO: Pod "server-envvars-dce82082-c309-4c63-9f87-62665f15f980" satisfied condition "running and ready"
Jun 28 08:20:19.040: INFO: Waiting up to 5m0s for pod "client-envvars-3ef20eac-486f-4f77-9951-3d78956924dc" in namespace "pods-731" to be "Succeeded or Failed"
Jun 28 08:20:19.044: INFO: Pod "client-envvars-3ef20eac-486f-4f77-9951-3d78956924dc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.031561ms
Jun 28 08:20:21.051: INFO: Pod "client-envvars-3ef20eac-486f-4f77-9951-3d78956924dc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010275538s
Jun 28 08:20:23.050: INFO: Pod "client-envvars-3ef20eac-486f-4f77-9951-3d78956924dc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00994553s
STEP: Saw pod success 06/28/23 08:20:23.05
Jun 28 08:20:23.050: INFO: Pod "client-envvars-3ef20eac-486f-4f77-9951-3d78956924dc" satisfied condition "Succeeded or Failed"
Jun 28 08:20:23.054: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod client-envvars-3ef20eac-486f-4f77-9951-3d78956924dc container env3cont: <nil>
STEP: delete the pod 06/28/23 08:20:23.117
Jun 28 08:20:23.127: INFO: Waiting for pod client-envvars-3ef20eac-486f-4f77-9951-3d78956924dc to disappear
Jun 28 08:20:23.131: INFO: Pod client-envvars-3ef20eac-486f-4f77-9951-3d78956924dc no longer exists
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jun 28 08:20:23.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-731" for this suite. 06/28/23 08:20:23.14
{"msg":"PASSED [sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]","completed":173,"skipped":2850,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.174 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:20:16.974
    Jun 28 08:20:16.974: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename pods 06/28/23 08:20:16.975
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:20:16.986
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:20:16.992
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should contain environment variables for services [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:443
    Jun 28 08:20:17.009: INFO: Waiting up to 5m0s for pod "server-envvars-dce82082-c309-4c63-9f87-62665f15f980" in namespace "pods-731" to be "running and ready"
    Jun 28 08:20:17.013: INFO: Pod "server-envvars-dce82082-c309-4c63-9f87-62665f15f980": Phase="Pending", Reason="", readiness=false. Elapsed: 4.846511ms
    Jun 28 08:20:17.014: INFO: The phase of Pod server-envvars-dce82082-c309-4c63-9f87-62665f15f980 is Pending, waiting for it to be Running (with Ready = true)
    Jun 28 08:20:19.019: INFO: Pod "server-envvars-dce82082-c309-4c63-9f87-62665f15f980": Phase="Running", Reason="", readiness=true. Elapsed: 2.010344781s
    Jun 28 08:20:19.019: INFO: The phase of Pod server-envvars-dce82082-c309-4c63-9f87-62665f15f980 is Running (Ready = true)
    Jun 28 08:20:19.019: INFO: Pod "server-envvars-dce82082-c309-4c63-9f87-62665f15f980" satisfied condition "running and ready"
    Jun 28 08:20:19.040: INFO: Waiting up to 5m0s for pod "client-envvars-3ef20eac-486f-4f77-9951-3d78956924dc" in namespace "pods-731" to be "Succeeded or Failed"
    Jun 28 08:20:19.044: INFO: Pod "client-envvars-3ef20eac-486f-4f77-9951-3d78956924dc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.031561ms
    Jun 28 08:20:21.051: INFO: Pod "client-envvars-3ef20eac-486f-4f77-9951-3d78956924dc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010275538s
    Jun 28 08:20:23.050: INFO: Pod "client-envvars-3ef20eac-486f-4f77-9951-3d78956924dc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00994553s
    STEP: Saw pod success 06/28/23 08:20:23.05
    Jun 28 08:20:23.050: INFO: Pod "client-envvars-3ef20eac-486f-4f77-9951-3d78956924dc" satisfied condition "Succeeded or Failed"
    Jun 28 08:20:23.054: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod client-envvars-3ef20eac-486f-4f77-9951-3d78956924dc container env3cont: <nil>
    STEP: delete the pod 06/28/23 08:20:23.117
    Jun 28 08:20:23.127: INFO: Waiting for pod client-envvars-3ef20eac-486f-4f77-9951-3d78956924dc to disappear
    Jun 28 08:20:23.131: INFO: Pod client-envvars-3ef20eac-486f-4f77-9951-3d78956924dc no longer exists
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jun 28 08:20:23.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-731" for this suite. 06/28/23 08:20:23.14
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:20:23.148
Jun 28 08:20:23.148: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename emptydir 06/28/23 08:20:23.149
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:20:23.163
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:20:23.168
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216
STEP: Creating a pod to test emptydir 0777 on node default medium 06/28/23 08:20:23.173
Jun 28 08:20:23.182: INFO: Waiting up to 5m0s for pod "pod-8bb238fa-0d3f-4142-b8c6-667b93ca9dfe" in namespace "emptydir-8850" to be "Succeeded or Failed"
Jun 28 08:20:23.187: INFO: Pod "pod-8bb238fa-0d3f-4142-b8c6-667b93ca9dfe": Phase="Pending", Reason="", readiness=false. Elapsed: 4.76973ms
Jun 28 08:20:25.194: INFO: Pod "pod-8bb238fa-0d3f-4142-b8c6-667b93ca9dfe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011547348s
Jun 28 08:20:27.193: INFO: Pod "pod-8bb238fa-0d3f-4142-b8c6-667b93ca9dfe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011040008s
STEP: Saw pod success 06/28/23 08:20:27.193
Jun 28 08:20:27.193: INFO: Pod "pod-8bb238fa-0d3f-4142-b8c6-667b93ca9dfe" satisfied condition "Succeeded or Failed"
Jun 28 08:20:27.198: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod pod-8bb238fa-0d3f-4142-b8c6-667b93ca9dfe container test-container: <nil>
STEP: delete the pod 06/28/23 08:20:27.21
Jun 28 08:20:27.222: INFO: Waiting for pod pod-8bb238fa-0d3f-4142-b8c6-667b93ca9dfe to disappear
Jun 28 08:20:27.226: INFO: Pod pod-8bb238fa-0d3f-4142-b8c6-667b93ca9dfe no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jun 28 08:20:27.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8850" for this suite. 06/28/23 08:20:27.235
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":174,"skipped":2850,"failed":0}
------------------------------
â€¢ [4.094 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:20:23.148
    Jun 28 08:20:23.148: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename emptydir 06/28/23 08:20:23.149
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:20:23.163
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:20:23.168
    [It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:216
    STEP: Creating a pod to test emptydir 0777 on node default medium 06/28/23 08:20:23.173
    Jun 28 08:20:23.182: INFO: Waiting up to 5m0s for pod "pod-8bb238fa-0d3f-4142-b8c6-667b93ca9dfe" in namespace "emptydir-8850" to be "Succeeded or Failed"
    Jun 28 08:20:23.187: INFO: Pod "pod-8bb238fa-0d3f-4142-b8c6-667b93ca9dfe": Phase="Pending", Reason="", readiness=false. Elapsed: 4.76973ms
    Jun 28 08:20:25.194: INFO: Pod "pod-8bb238fa-0d3f-4142-b8c6-667b93ca9dfe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011547348s
    Jun 28 08:20:27.193: INFO: Pod "pod-8bb238fa-0d3f-4142-b8c6-667b93ca9dfe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011040008s
    STEP: Saw pod success 06/28/23 08:20:27.193
    Jun 28 08:20:27.193: INFO: Pod "pod-8bb238fa-0d3f-4142-b8c6-667b93ca9dfe" satisfied condition "Succeeded or Failed"
    Jun 28 08:20:27.198: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod pod-8bb238fa-0d3f-4142-b8c6-667b93ca9dfe container test-container: <nil>
    STEP: delete the pod 06/28/23 08:20:27.21
    Jun 28 08:20:27.222: INFO: Waiting for pod pod-8bb238fa-0d3f-4142-b8c6-667b93ca9dfe to disappear
    Jun 28 08:20:27.226: INFO: Pod pod-8bb238fa-0d3f-4142-b8c6-667b93ca9dfe no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jun 28 08:20:27.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-8850" for this suite. 06/28/23 08:20:27.235
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:20:27.243
Jun 28 08:20:27.243: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename sched-pred 06/28/23 08:20:27.244
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:20:27.258
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:20:27.264
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Jun 28 08:20:27.269: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun 28 08:20:27.286: INFO: Waiting for terminating namespaces to be deleted...
Jun 28 08:20:27.291: INFO: 
Logging pods the apiserver thinks is on node ske-rhel-749f7d55c8xdd8b6-ct4cp before test
Jun 28 08:20:27.304: INFO: calico-node-h6www from kube-system started at 2023-06-28 05:12:45 +0000 UTC (1 container statuses recorded)
Jun 28 08:20:27.304: INFO: 	Container calico-node ready: true, restart count 0
Jun 28 08:20:27.304: INFO: csi-smb-node-7hhpt from kube-system started at 2023-06-28 05:12:45 +0000 UTC (3 container statuses recorded)
Jun 28 08:20:27.304: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 28 08:20:27.304: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jun 28 08:20:27.304: INFO: 	Container smb ready: true, restart count 0
Jun 28 08:20:27.304: INFO: kube-proxy-78kbb from kube-system started at 2023-06-28 05:12:45 +0000 UTC (1 container statuses recorded)
Jun 28 08:20:27.304: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 28 08:20:27.304: INFO: node-exporter-fkq2r from kube-system started at 2023-06-28 05:12:45 +0000 UTC (1 container statuses recorded)
Jun 28 08:20:27.304: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Jun 28 08:20:27.304: INFO: pod-exec-websocket-f6b7ed03-e05b-48ab-a201-f1e8eac4f571 from pods-9908 started at 2023-06-28 08:20:02 +0000 UTC (1 container statuses recorded)
Jun 28 08:20:27.304: INFO: 	Container main ready: true, restart count 0
Jun 28 08:20:27.304: INFO: sonobuoy from sonobuoy started at 2023-06-28 07:40:00 +0000 UTC (1 container statuses recorded)
Jun 28 08:20:27.304: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun 28 08:20:27.304: INFO: sonobuoy-systemd-logs-daemon-set-747dcc3d7bcc492c-v77g9 from sonobuoy started at 2023-06-28 07:40:01 +0000 UTC (2 container statuses recorded)
Jun 28 08:20:27.304: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 28 08:20:27.304: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 28 08:20:27.304: INFO: 
Logging pods the apiserver thinks is on node ske-rhel-749f7d55c8xdd8b6-srshq before test
Jun 28 08:20:27.320: INFO: calico-kube-controllers-77cc457ff7-jfwrp from kube-system started at 2023-06-28 05:13:50 +0000 UTC (1 container statuses recorded)
Jun 28 08:20:27.320: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jun 28 08:20:27.320: INFO: calico-node-xkhs5 from kube-system started at 2023-06-28 05:12:45 +0000 UTC (1 container statuses recorded)
Jun 28 08:20:27.320: INFO: 	Container calico-node ready: true, restart count 0
Jun 28 08:20:27.320: INFO: coredns-7b4f76cbb6-gkxll from kube-system started at 2023-06-28 05:13:50 +0000 UTC (1 container statuses recorded)
Jun 28 08:20:27.320: INFO: 	Container coredns ready: true, restart count 0
Jun 28 08:20:27.320: INFO: coredns-7b4f76cbb6-ztck5 from kube-system started at 2023-06-28 05:13:50 +0000 UTC (1 container statuses recorded)
Jun 28 08:20:27.320: INFO: 	Container coredns ready: true, restart count 0
Jun 28 08:20:27.320: INFO: csi-smb-controller-7ffdfbf4b6-pfh4n from kube-system started at 2023-06-28 05:13:50 +0000 UTC (3 container statuses recorded)
Jun 28 08:20:27.320: INFO: 	Container csi-provisioner ready: true, restart count 1
Jun 28 08:20:27.320: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 28 08:20:27.320: INFO: 	Container smb ready: true, restart count 0
Jun 28 08:20:27.320: INFO: csi-smb-node-v64b5 from kube-system started at 2023-06-28 05:12:45 +0000 UTC (3 container statuses recorded)
Jun 28 08:20:27.320: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 28 08:20:27.320: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jun 28 08:20:27.320: INFO: 	Container smb ready: true, restart count 0
Jun 28 08:20:27.320: INFO: kube-proxy-l8rq8 from kube-system started at 2023-06-28 05:12:45 +0000 UTC (1 container statuses recorded)
Jun 28 08:20:27.320: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 28 08:20:27.320: INFO: metrics-server-54cd6dc7f5-8j8ss from kube-system started at 2023-06-28 05:13:50 +0000 UTC (1 container statuses recorded)
Jun 28 08:20:27.320: INFO: 	Container metrics-server ready: true, restart count 0
Jun 28 08:20:27.320: INFO: nfs-subdir-external-provisioner-548dcf4dc4-z87tg from kube-system started at 2023-06-28 05:13:50 +0000 UTC (1 container statuses recorded)
Jun 28 08:20:27.320: INFO: 	Container nfs-subdir-external-provisioner ready: true, restart count 1
Jun 28 08:20:27.320: INFO: node-exporter-hcfhb from kube-system started at 2023-06-28 05:12:45 +0000 UTC (1 container statuses recorded)
Jun 28 08:20:27.320: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Jun 28 08:20:27.320: INFO: vpn-target-bcf545797-bfhjg from kube-system started at 2023-06-28 05:13:50 +0000 UTC (1 container statuses recorded)
Jun 28 08:20:27.320: INFO: 	Container vpn-target ready: true, restart count 0
Jun 28 08:20:27.320: INFO: sonobuoy-systemd-logs-daemon-set-747dcc3d7bcc492c-gmpnw from sonobuoy started at 2023-06-28 07:40:01 +0000 UTC (2 container statuses recorded)
Jun 28 08:20:27.320: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 28 08:20:27.320: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 28 08:20:27.320: INFO: 
Logging pods the apiserver thinks is on node ske-rhel-749f7d55c8xdd8b6-zxlfv before test
Jun 28 08:20:27.335: INFO: calico-node-6xpbr from kube-system started at 2023-06-28 05:12:49 +0000 UTC (1 container statuses recorded)
Jun 28 08:20:27.335: INFO: 	Container calico-node ready: true, restart count 0
Jun 28 08:20:27.335: INFO: csi-smb-node-t6cfj from kube-system started at 2023-06-28 05:12:49 +0000 UTC (3 container statuses recorded)
Jun 28 08:20:27.335: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 28 08:20:27.335: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jun 28 08:20:27.335: INFO: 	Container smb ready: true, restart count 0
Jun 28 08:20:27.335: INFO: kube-proxy-l6xjw from kube-system started at 2023-06-28 05:12:49 +0000 UTC (1 container statuses recorded)
Jun 28 08:20:27.335: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 28 08:20:27.335: INFO: node-exporter-w4bjx from kube-system started at 2023-06-28 05:12:49 +0000 UTC (1 container statuses recorded)
Jun 28 08:20:27.335: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Jun 28 08:20:27.335: INFO: server-envvars-dce82082-c309-4c63-9f87-62665f15f980 from pods-731 started at 2023-06-28 08:20:17 +0000 UTC (1 container statuses recorded)
Jun 28 08:20:27.335: INFO: 	Container srv ready: true, restart count 0
Jun 28 08:20:27.335: INFO: sonobuoy-e2e-job-4078a53074bd4828 from sonobuoy started at 2023-06-28 07:40:01 +0000 UTC (2 container statuses recorded)
Jun 28 08:20:27.335: INFO: 	Container e2e ready: true, restart count 0
Jun 28 08:20:27.335: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 28 08:20:27.335: INFO: sonobuoy-systemd-logs-daemon-set-747dcc3d7bcc492c-wmgvv from sonobuoy started at 2023-06-28 07:40:01 +0000 UTC (2 container statuses recorded)
Jun 28 08:20:27.335: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 28 08:20:27.335: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326
STEP: verifying the node has the label node ske-rhel-749f7d55c8xdd8b6-ct4cp 06/28/23 08:20:27.364
STEP: verifying the node has the label node ske-rhel-749f7d55c8xdd8b6-srshq 06/28/23 08:20:27.382
STEP: verifying the node has the label node ske-rhel-749f7d55c8xdd8b6-zxlfv 06/28/23 08:20:27.4
Jun 28 08:20:27.424: INFO: Pod calico-kube-controllers-77cc457ff7-jfwrp requesting resource cpu=0m on Node ske-rhel-749f7d55c8xdd8b6-srshq
Jun 28 08:20:27.424: INFO: Pod calico-node-6xpbr requesting resource cpu=250m on Node ske-rhel-749f7d55c8xdd8b6-zxlfv
Jun 28 08:20:27.424: INFO: Pod calico-node-h6www requesting resource cpu=250m on Node ske-rhel-749f7d55c8xdd8b6-ct4cp
Jun 28 08:20:27.424: INFO: Pod calico-node-xkhs5 requesting resource cpu=250m on Node ske-rhel-749f7d55c8xdd8b6-srshq
Jun 28 08:20:27.424: INFO: Pod coredns-7b4f76cbb6-gkxll requesting resource cpu=100m on Node ske-rhel-749f7d55c8xdd8b6-srshq
Jun 28 08:20:27.424: INFO: Pod coredns-7b4f76cbb6-ztck5 requesting resource cpu=100m on Node ske-rhel-749f7d55c8xdd8b6-srshq
Jun 28 08:20:27.424: INFO: Pod csi-smb-controller-7ffdfbf4b6-pfh4n requesting resource cpu=30m on Node ske-rhel-749f7d55c8xdd8b6-srshq
Jun 28 08:20:27.424: INFO: Pod csi-smb-node-7hhpt requesting resource cpu=30m on Node ske-rhel-749f7d55c8xdd8b6-ct4cp
Jun 28 08:20:27.424: INFO: Pod csi-smb-node-t6cfj requesting resource cpu=30m on Node ske-rhel-749f7d55c8xdd8b6-zxlfv
Jun 28 08:20:27.424: INFO: Pod csi-smb-node-v64b5 requesting resource cpu=30m on Node ske-rhel-749f7d55c8xdd8b6-srshq
Jun 28 08:20:27.424: INFO: Pod kube-proxy-78kbb requesting resource cpu=0m on Node ske-rhel-749f7d55c8xdd8b6-ct4cp
Jun 28 08:20:27.424: INFO: Pod kube-proxy-l6xjw requesting resource cpu=0m on Node ske-rhel-749f7d55c8xdd8b6-zxlfv
Jun 28 08:20:27.424: INFO: Pod kube-proxy-l8rq8 requesting resource cpu=0m on Node ske-rhel-749f7d55c8xdd8b6-srshq
Jun 28 08:20:27.424: INFO: Pod metrics-server-54cd6dc7f5-8j8ss requesting resource cpu=100m on Node ske-rhel-749f7d55c8xdd8b6-srshq
Jun 28 08:20:27.424: INFO: Pod nfs-subdir-external-provisioner-548dcf4dc4-z87tg requesting resource cpu=0m on Node ske-rhel-749f7d55c8xdd8b6-srshq
Jun 28 08:20:27.424: INFO: Pod node-exporter-fkq2r requesting resource cpu=100m on Node ske-rhel-749f7d55c8xdd8b6-ct4cp
Jun 28 08:20:27.424: INFO: Pod node-exporter-hcfhb requesting resource cpu=100m on Node ske-rhel-749f7d55c8xdd8b6-srshq
Jun 28 08:20:27.424: INFO: Pod node-exporter-w4bjx requesting resource cpu=100m on Node ske-rhel-749f7d55c8xdd8b6-zxlfv
Jun 28 08:20:27.424: INFO: Pod vpn-target-bcf545797-bfhjg requesting resource cpu=100m on Node ske-rhel-749f7d55c8xdd8b6-srshq
Jun 28 08:20:27.424: INFO: Pod server-envvars-dce82082-c309-4c63-9f87-62665f15f980 requesting resource cpu=0m on Node ske-rhel-749f7d55c8xdd8b6-zxlfv
Jun 28 08:20:27.424: INFO: Pod pod-exec-websocket-f6b7ed03-e05b-48ab-a201-f1e8eac4f571 requesting resource cpu=0m on Node ske-rhel-749f7d55c8xdd8b6-ct4cp
Jun 28 08:20:27.424: INFO: Pod sonobuoy requesting resource cpu=0m on Node ske-rhel-749f7d55c8xdd8b6-ct4cp
Jun 28 08:20:27.424: INFO: Pod sonobuoy-e2e-job-4078a53074bd4828 requesting resource cpu=0m on Node ske-rhel-749f7d55c8xdd8b6-zxlfv
Jun 28 08:20:27.424: INFO: Pod sonobuoy-systemd-logs-daemon-set-747dcc3d7bcc492c-gmpnw requesting resource cpu=0m on Node ske-rhel-749f7d55c8xdd8b6-srshq
Jun 28 08:20:27.424: INFO: Pod sonobuoy-systemd-logs-daemon-set-747dcc3d7bcc492c-v77g9 requesting resource cpu=0m on Node ske-rhel-749f7d55c8xdd8b6-ct4cp
Jun 28 08:20:27.424: INFO: Pod sonobuoy-systemd-logs-daemon-set-747dcc3d7bcc492c-wmgvv requesting resource cpu=0m on Node ske-rhel-749f7d55c8xdd8b6-zxlfv
STEP: Starting Pods to consume most of the cluster CPU. 06/28/23 08:20:27.424
Jun 28 08:20:27.425: INFO: Creating a pod which consumes cpu=1085m on Node ske-rhel-749f7d55c8xdd8b6-ct4cp
Jun 28 08:20:27.434: INFO: Creating a pod which consumes cpu=784m on Node ske-rhel-749f7d55c8xdd8b6-srshq
Jun 28 08:20:27.441: INFO: Creating a pod which consumes cpu=1085m on Node ske-rhel-749f7d55c8xdd8b6-zxlfv
Jun 28 08:20:27.447: INFO: Waiting up to 5m0s for pod "filler-pod-45d78dee-9ca3-4d59-9ce2-dbfbaf6902ff" in namespace "sched-pred-3236" to be "running"
Jun 28 08:20:27.455: INFO: Pod "filler-pod-45d78dee-9ca3-4d59-9ce2-dbfbaf6902ff": Phase="Pending", Reason="", readiness=false. Elapsed: 7.652078ms
Jun 28 08:20:29.464: INFO: Pod "filler-pod-45d78dee-9ca3-4d59-9ce2-dbfbaf6902ff": Phase="Running", Reason="", readiness=true. Elapsed: 2.016579287s
Jun 28 08:20:29.464: INFO: Pod "filler-pod-45d78dee-9ca3-4d59-9ce2-dbfbaf6902ff" satisfied condition "running"
Jun 28 08:20:29.464: INFO: Waiting up to 5m0s for pod "filler-pod-f34dce1e-53d2-4f12-a444-578dc3c2e452" in namespace "sched-pred-3236" to be "running"
Jun 28 08:20:29.470: INFO: Pod "filler-pod-f34dce1e-53d2-4f12-a444-578dc3c2e452": Phase="Running", Reason="", readiness=true. Elapsed: 6.036213ms
Jun 28 08:20:29.470: INFO: Pod "filler-pod-f34dce1e-53d2-4f12-a444-578dc3c2e452" satisfied condition "running"
Jun 28 08:20:29.470: INFO: Waiting up to 5m0s for pod "filler-pod-8d4bbd90-e205-45ce-b964-0510888cc3dc" in namespace "sched-pred-3236" to be "running"
Jun 28 08:20:29.478: INFO: Pod "filler-pod-8d4bbd90-e205-45ce-b964-0510888cc3dc": Phase="Running", Reason="", readiness=true. Elapsed: 7.79551ms
Jun 28 08:20:29.478: INFO: Pod "filler-pod-8d4bbd90-e205-45ce-b964-0510888cc3dc" satisfied condition "running"
STEP: Creating another pod that requires unavailable amount of CPU. 06/28/23 08:20:29.478
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-45d78dee-9ca3-4d59-9ce2-dbfbaf6902ff.176cc4e02b2c3744], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3236/filler-pod-45d78dee-9ca3-4d59-9ce2-dbfbaf6902ff to ske-rhel-749f7d55c8xdd8b6-ct4cp] 06/28/23 08:20:29.487
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-45d78dee-9ca3-4d59-9ce2-dbfbaf6902ff.176cc4e05c43174f], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 06/28/23 08:20:29.487
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-45d78dee-9ca3-4d59-9ce2-dbfbaf6902ff.176cc4e05dd9ea5f], Reason = [Created], Message = [Created container filler-pod-45d78dee-9ca3-4d59-9ce2-dbfbaf6902ff] 06/28/23 08:20:29.487
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-45d78dee-9ca3-4d59-9ce2-dbfbaf6902ff.176cc4e06360b9ec], Reason = [Started], Message = [Started container filler-pod-45d78dee-9ca3-4d59-9ce2-dbfbaf6902ff] 06/28/23 08:20:29.487
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8d4bbd90-e205-45ce-b964-0510888cc3dc.176cc4e02c158f1e], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3236/filler-pod-8d4bbd90-e205-45ce-b964-0510888cc3dc to ske-rhel-749f7d55c8xdd8b6-zxlfv] 06/28/23 08:20:29.487
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8d4bbd90-e205-45ce-b964-0510888cc3dc.176cc4e054db54cb], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 06/28/23 08:20:29.487
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8d4bbd90-e205-45ce-b964-0510888cc3dc.176cc4e056129070], Reason = [Created], Message = [Created container filler-pod-8d4bbd90-e205-45ce-b964-0510888cc3dc] 06/28/23 08:20:29.487
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8d4bbd90-e205-45ce-b964-0510888cc3dc.176cc4e05bb1accc], Reason = [Started], Message = [Started container filler-pod-8d4bbd90-e205-45ce-b964-0510888cc3dc] 06/28/23 08:20:29.487
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f34dce1e-53d2-4f12-a444-578dc3c2e452.176cc4e02b74438b], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3236/filler-pod-f34dce1e-53d2-4f12-a444-578dc3c2e452 to ske-rhel-749f7d55c8xdd8b6-srshq] 06/28/23 08:20:29.487
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f34dce1e-53d2-4f12-a444-578dc3c2e452.176cc4e05a4d3513], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 06/28/23 08:20:29.487
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f34dce1e-53d2-4f12-a444-578dc3c2e452.176cc4e05bf9ba06], Reason = [Created], Message = [Created container filler-pod-f34dce1e-53d2-4f12-a444-578dc3c2e452] 06/28/23 08:20:29.488
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f34dce1e-53d2-4f12-a444-578dc3c2e452.176cc4e062039ebc], Reason = [Started], Message = [Started container filler-pod-f34dce1e-53d2-4f12-a444-578dc3c2e452] 06/28/23 08:20:29.488
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.176cc4e0a630318f], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu. preemption: 0/3 nodes are available: 3 No preemption victims found for incoming pod.] 06/28/23 08:20:29.513
STEP: removing the label node off the node ske-rhel-749f7d55c8xdd8b6-ct4cp 06/28/23 08:20:30.551
STEP: verifying the node doesn't have the label node 06/28/23 08:20:30.641
STEP: removing the label node off the node ske-rhel-749f7d55c8xdd8b6-srshq 06/28/23 08:20:30.668
STEP: verifying the node doesn't have the label node 06/28/23 08:20:30.734
STEP: removing the label node off the node ske-rhel-749f7d55c8xdd8b6-zxlfv 06/28/23 08:20:30.767
STEP: verifying the node doesn't have the label node 06/28/23 08:20:30.835
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Jun 28 08:20:30.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-3236" for this suite. 06/28/23 08:20:30.922
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","completed":175,"skipped":2867,"failed":0}
------------------------------
â€¢ [3.717 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:20:27.243
    Jun 28 08:20:27.243: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename sched-pred 06/28/23 08:20:27.244
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:20:27.258
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:20:27.264
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Jun 28 08:20:27.269: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jun 28 08:20:27.286: INFO: Waiting for terminating namespaces to be deleted...
    Jun 28 08:20:27.291: INFO: 
    Logging pods the apiserver thinks is on node ske-rhel-749f7d55c8xdd8b6-ct4cp before test
    Jun 28 08:20:27.304: INFO: calico-node-h6www from kube-system started at 2023-06-28 05:12:45 +0000 UTC (1 container statuses recorded)
    Jun 28 08:20:27.304: INFO: 	Container calico-node ready: true, restart count 0
    Jun 28 08:20:27.304: INFO: csi-smb-node-7hhpt from kube-system started at 2023-06-28 05:12:45 +0000 UTC (3 container statuses recorded)
    Jun 28 08:20:27.304: INFO: 	Container liveness-probe ready: true, restart count 0
    Jun 28 08:20:27.304: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Jun 28 08:20:27.304: INFO: 	Container smb ready: true, restart count 0
    Jun 28 08:20:27.304: INFO: kube-proxy-78kbb from kube-system started at 2023-06-28 05:12:45 +0000 UTC (1 container statuses recorded)
    Jun 28 08:20:27.304: INFO: 	Container kube-proxy ready: true, restart count 0
    Jun 28 08:20:27.304: INFO: node-exporter-fkq2r from kube-system started at 2023-06-28 05:12:45 +0000 UTC (1 container statuses recorded)
    Jun 28 08:20:27.304: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
    Jun 28 08:20:27.304: INFO: pod-exec-websocket-f6b7ed03-e05b-48ab-a201-f1e8eac4f571 from pods-9908 started at 2023-06-28 08:20:02 +0000 UTC (1 container statuses recorded)
    Jun 28 08:20:27.304: INFO: 	Container main ready: true, restart count 0
    Jun 28 08:20:27.304: INFO: sonobuoy from sonobuoy started at 2023-06-28 07:40:00 +0000 UTC (1 container statuses recorded)
    Jun 28 08:20:27.304: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jun 28 08:20:27.304: INFO: sonobuoy-systemd-logs-daemon-set-747dcc3d7bcc492c-v77g9 from sonobuoy started at 2023-06-28 07:40:01 +0000 UTC (2 container statuses recorded)
    Jun 28 08:20:27.304: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jun 28 08:20:27.304: INFO: 	Container systemd-logs ready: true, restart count 0
    Jun 28 08:20:27.304: INFO: 
    Logging pods the apiserver thinks is on node ske-rhel-749f7d55c8xdd8b6-srshq before test
    Jun 28 08:20:27.320: INFO: calico-kube-controllers-77cc457ff7-jfwrp from kube-system started at 2023-06-28 05:13:50 +0000 UTC (1 container statuses recorded)
    Jun 28 08:20:27.320: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Jun 28 08:20:27.320: INFO: calico-node-xkhs5 from kube-system started at 2023-06-28 05:12:45 +0000 UTC (1 container statuses recorded)
    Jun 28 08:20:27.320: INFO: 	Container calico-node ready: true, restart count 0
    Jun 28 08:20:27.320: INFO: coredns-7b4f76cbb6-gkxll from kube-system started at 2023-06-28 05:13:50 +0000 UTC (1 container statuses recorded)
    Jun 28 08:20:27.320: INFO: 	Container coredns ready: true, restart count 0
    Jun 28 08:20:27.320: INFO: coredns-7b4f76cbb6-ztck5 from kube-system started at 2023-06-28 05:13:50 +0000 UTC (1 container statuses recorded)
    Jun 28 08:20:27.320: INFO: 	Container coredns ready: true, restart count 0
    Jun 28 08:20:27.320: INFO: csi-smb-controller-7ffdfbf4b6-pfh4n from kube-system started at 2023-06-28 05:13:50 +0000 UTC (3 container statuses recorded)
    Jun 28 08:20:27.320: INFO: 	Container csi-provisioner ready: true, restart count 1
    Jun 28 08:20:27.320: INFO: 	Container liveness-probe ready: true, restart count 0
    Jun 28 08:20:27.320: INFO: 	Container smb ready: true, restart count 0
    Jun 28 08:20:27.320: INFO: csi-smb-node-v64b5 from kube-system started at 2023-06-28 05:12:45 +0000 UTC (3 container statuses recorded)
    Jun 28 08:20:27.320: INFO: 	Container liveness-probe ready: true, restart count 0
    Jun 28 08:20:27.320: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Jun 28 08:20:27.320: INFO: 	Container smb ready: true, restart count 0
    Jun 28 08:20:27.320: INFO: kube-proxy-l8rq8 from kube-system started at 2023-06-28 05:12:45 +0000 UTC (1 container statuses recorded)
    Jun 28 08:20:27.320: INFO: 	Container kube-proxy ready: true, restart count 0
    Jun 28 08:20:27.320: INFO: metrics-server-54cd6dc7f5-8j8ss from kube-system started at 2023-06-28 05:13:50 +0000 UTC (1 container statuses recorded)
    Jun 28 08:20:27.320: INFO: 	Container metrics-server ready: true, restart count 0
    Jun 28 08:20:27.320: INFO: nfs-subdir-external-provisioner-548dcf4dc4-z87tg from kube-system started at 2023-06-28 05:13:50 +0000 UTC (1 container statuses recorded)
    Jun 28 08:20:27.320: INFO: 	Container nfs-subdir-external-provisioner ready: true, restart count 1
    Jun 28 08:20:27.320: INFO: node-exporter-hcfhb from kube-system started at 2023-06-28 05:12:45 +0000 UTC (1 container statuses recorded)
    Jun 28 08:20:27.320: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
    Jun 28 08:20:27.320: INFO: vpn-target-bcf545797-bfhjg from kube-system started at 2023-06-28 05:13:50 +0000 UTC (1 container statuses recorded)
    Jun 28 08:20:27.320: INFO: 	Container vpn-target ready: true, restart count 0
    Jun 28 08:20:27.320: INFO: sonobuoy-systemd-logs-daemon-set-747dcc3d7bcc492c-gmpnw from sonobuoy started at 2023-06-28 07:40:01 +0000 UTC (2 container statuses recorded)
    Jun 28 08:20:27.320: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jun 28 08:20:27.320: INFO: 	Container systemd-logs ready: true, restart count 0
    Jun 28 08:20:27.320: INFO: 
    Logging pods the apiserver thinks is on node ske-rhel-749f7d55c8xdd8b6-zxlfv before test
    Jun 28 08:20:27.335: INFO: calico-node-6xpbr from kube-system started at 2023-06-28 05:12:49 +0000 UTC (1 container statuses recorded)
    Jun 28 08:20:27.335: INFO: 	Container calico-node ready: true, restart count 0
    Jun 28 08:20:27.335: INFO: csi-smb-node-t6cfj from kube-system started at 2023-06-28 05:12:49 +0000 UTC (3 container statuses recorded)
    Jun 28 08:20:27.335: INFO: 	Container liveness-probe ready: true, restart count 0
    Jun 28 08:20:27.335: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Jun 28 08:20:27.335: INFO: 	Container smb ready: true, restart count 0
    Jun 28 08:20:27.335: INFO: kube-proxy-l6xjw from kube-system started at 2023-06-28 05:12:49 +0000 UTC (1 container statuses recorded)
    Jun 28 08:20:27.335: INFO: 	Container kube-proxy ready: true, restart count 0
    Jun 28 08:20:27.335: INFO: node-exporter-w4bjx from kube-system started at 2023-06-28 05:12:49 +0000 UTC (1 container statuses recorded)
    Jun 28 08:20:27.335: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
    Jun 28 08:20:27.335: INFO: server-envvars-dce82082-c309-4c63-9f87-62665f15f980 from pods-731 started at 2023-06-28 08:20:17 +0000 UTC (1 container statuses recorded)
    Jun 28 08:20:27.335: INFO: 	Container srv ready: true, restart count 0
    Jun 28 08:20:27.335: INFO: sonobuoy-e2e-job-4078a53074bd4828 from sonobuoy started at 2023-06-28 07:40:01 +0000 UTC (2 container statuses recorded)
    Jun 28 08:20:27.335: INFO: 	Container e2e ready: true, restart count 0
    Jun 28 08:20:27.335: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jun 28 08:20:27.335: INFO: sonobuoy-systemd-logs-daemon-set-747dcc3d7bcc492c-wmgvv from sonobuoy started at 2023-06-28 07:40:01 +0000 UTC (2 container statuses recorded)
    Jun 28 08:20:27.335: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jun 28 08:20:27.335: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates resource limits of pods that are allowed to run  [Conformance]
      test/e2e/scheduling/predicates.go:326
    STEP: verifying the node has the label node ske-rhel-749f7d55c8xdd8b6-ct4cp 06/28/23 08:20:27.364
    STEP: verifying the node has the label node ske-rhel-749f7d55c8xdd8b6-srshq 06/28/23 08:20:27.382
    STEP: verifying the node has the label node ske-rhel-749f7d55c8xdd8b6-zxlfv 06/28/23 08:20:27.4
    Jun 28 08:20:27.424: INFO: Pod calico-kube-controllers-77cc457ff7-jfwrp requesting resource cpu=0m on Node ske-rhel-749f7d55c8xdd8b6-srshq
    Jun 28 08:20:27.424: INFO: Pod calico-node-6xpbr requesting resource cpu=250m on Node ske-rhel-749f7d55c8xdd8b6-zxlfv
    Jun 28 08:20:27.424: INFO: Pod calico-node-h6www requesting resource cpu=250m on Node ske-rhel-749f7d55c8xdd8b6-ct4cp
    Jun 28 08:20:27.424: INFO: Pod calico-node-xkhs5 requesting resource cpu=250m on Node ske-rhel-749f7d55c8xdd8b6-srshq
    Jun 28 08:20:27.424: INFO: Pod coredns-7b4f76cbb6-gkxll requesting resource cpu=100m on Node ske-rhel-749f7d55c8xdd8b6-srshq
    Jun 28 08:20:27.424: INFO: Pod coredns-7b4f76cbb6-ztck5 requesting resource cpu=100m on Node ske-rhel-749f7d55c8xdd8b6-srshq
    Jun 28 08:20:27.424: INFO: Pod csi-smb-controller-7ffdfbf4b6-pfh4n requesting resource cpu=30m on Node ske-rhel-749f7d55c8xdd8b6-srshq
    Jun 28 08:20:27.424: INFO: Pod csi-smb-node-7hhpt requesting resource cpu=30m on Node ske-rhel-749f7d55c8xdd8b6-ct4cp
    Jun 28 08:20:27.424: INFO: Pod csi-smb-node-t6cfj requesting resource cpu=30m on Node ske-rhel-749f7d55c8xdd8b6-zxlfv
    Jun 28 08:20:27.424: INFO: Pod csi-smb-node-v64b5 requesting resource cpu=30m on Node ske-rhel-749f7d55c8xdd8b6-srshq
    Jun 28 08:20:27.424: INFO: Pod kube-proxy-78kbb requesting resource cpu=0m on Node ske-rhel-749f7d55c8xdd8b6-ct4cp
    Jun 28 08:20:27.424: INFO: Pod kube-proxy-l6xjw requesting resource cpu=0m on Node ske-rhel-749f7d55c8xdd8b6-zxlfv
    Jun 28 08:20:27.424: INFO: Pod kube-proxy-l8rq8 requesting resource cpu=0m on Node ske-rhel-749f7d55c8xdd8b6-srshq
    Jun 28 08:20:27.424: INFO: Pod metrics-server-54cd6dc7f5-8j8ss requesting resource cpu=100m on Node ske-rhel-749f7d55c8xdd8b6-srshq
    Jun 28 08:20:27.424: INFO: Pod nfs-subdir-external-provisioner-548dcf4dc4-z87tg requesting resource cpu=0m on Node ske-rhel-749f7d55c8xdd8b6-srshq
    Jun 28 08:20:27.424: INFO: Pod node-exporter-fkq2r requesting resource cpu=100m on Node ske-rhel-749f7d55c8xdd8b6-ct4cp
    Jun 28 08:20:27.424: INFO: Pod node-exporter-hcfhb requesting resource cpu=100m on Node ske-rhel-749f7d55c8xdd8b6-srshq
    Jun 28 08:20:27.424: INFO: Pod node-exporter-w4bjx requesting resource cpu=100m on Node ske-rhel-749f7d55c8xdd8b6-zxlfv
    Jun 28 08:20:27.424: INFO: Pod vpn-target-bcf545797-bfhjg requesting resource cpu=100m on Node ske-rhel-749f7d55c8xdd8b6-srshq
    Jun 28 08:20:27.424: INFO: Pod server-envvars-dce82082-c309-4c63-9f87-62665f15f980 requesting resource cpu=0m on Node ske-rhel-749f7d55c8xdd8b6-zxlfv
    Jun 28 08:20:27.424: INFO: Pod pod-exec-websocket-f6b7ed03-e05b-48ab-a201-f1e8eac4f571 requesting resource cpu=0m on Node ske-rhel-749f7d55c8xdd8b6-ct4cp
    Jun 28 08:20:27.424: INFO: Pod sonobuoy requesting resource cpu=0m on Node ske-rhel-749f7d55c8xdd8b6-ct4cp
    Jun 28 08:20:27.424: INFO: Pod sonobuoy-e2e-job-4078a53074bd4828 requesting resource cpu=0m on Node ske-rhel-749f7d55c8xdd8b6-zxlfv
    Jun 28 08:20:27.424: INFO: Pod sonobuoy-systemd-logs-daemon-set-747dcc3d7bcc492c-gmpnw requesting resource cpu=0m on Node ske-rhel-749f7d55c8xdd8b6-srshq
    Jun 28 08:20:27.424: INFO: Pod sonobuoy-systemd-logs-daemon-set-747dcc3d7bcc492c-v77g9 requesting resource cpu=0m on Node ske-rhel-749f7d55c8xdd8b6-ct4cp
    Jun 28 08:20:27.424: INFO: Pod sonobuoy-systemd-logs-daemon-set-747dcc3d7bcc492c-wmgvv requesting resource cpu=0m on Node ske-rhel-749f7d55c8xdd8b6-zxlfv
    STEP: Starting Pods to consume most of the cluster CPU. 06/28/23 08:20:27.424
    Jun 28 08:20:27.425: INFO: Creating a pod which consumes cpu=1085m on Node ske-rhel-749f7d55c8xdd8b6-ct4cp
    Jun 28 08:20:27.434: INFO: Creating a pod which consumes cpu=784m on Node ske-rhel-749f7d55c8xdd8b6-srshq
    Jun 28 08:20:27.441: INFO: Creating a pod which consumes cpu=1085m on Node ske-rhel-749f7d55c8xdd8b6-zxlfv
    Jun 28 08:20:27.447: INFO: Waiting up to 5m0s for pod "filler-pod-45d78dee-9ca3-4d59-9ce2-dbfbaf6902ff" in namespace "sched-pred-3236" to be "running"
    Jun 28 08:20:27.455: INFO: Pod "filler-pod-45d78dee-9ca3-4d59-9ce2-dbfbaf6902ff": Phase="Pending", Reason="", readiness=false. Elapsed: 7.652078ms
    Jun 28 08:20:29.464: INFO: Pod "filler-pod-45d78dee-9ca3-4d59-9ce2-dbfbaf6902ff": Phase="Running", Reason="", readiness=true. Elapsed: 2.016579287s
    Jun 28 08:20:29.464: INFO: Pod "filler-pod-45d78dee-9ca3-4d59-9ce2-dbfbaf6902ff" satisfied condition "running"
    Jun 28 08:20:29.464: INFO: Waiting up to 5m0s for pod "filler-pod-f34dce1e-53d2-4f12-a444-578dc3c2e452" in namespace "sched-pred-3236" to be "running"
    Jun 28 08:20:29.470: INFO: Pod "filler-pod-f34dce1e-53d2-4f12-a444-578dc3c2e452": Phase="Running", Reason="", readiness=true. Elapsed: 6.036213ms
    Jun 28 08:20:29.470: INFO: Pod "filler-pod-f34dce1e-53d2-4f12-a444-578dc3c2e452" satisfied condition "running"
    Jun 28 08:20:29.470: INFO: Waiting up to 5m0s for pod "filler-pod-8d4bbd90-e205-45ce-b964-0510888cc3dc" in namespace "sched-pred-3236" to be "running"
    Jun 28 08:20:29.478: INFO: Pod "filler-pod-8d4bbd90-e205-45ce-b964-0510888cc3dc": Phase="Running", Reason="", readiness=true. Elapsed: 7.79551ms
    Jun 28 08:20:29.478: INFO: Pod "filler-pod-8d4bbd90-e205-45ce-b964-0510888cc3dc" satisfied condition "running"
    STEP: Creating another pod that requires unavailable amount of CPU. 06/28/23 08:20:29.478
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-45d78dee-9ca3-4d59-9ce2-dbfbaf6902ff.176cc4e02b2c3744], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3236/filler-pod-45d78dee-9ca3-4d59-9ce2-dbfbaf6902ff to ske-rhel-749f7d55c8xdd8b6-ct4cp] 06/28/23 08:20:29.487
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-45d78dee-9ca3-4d59-9ce2-dbfbaf6902ff.176cc4e05c43174f], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 06/28/23 08:20:29.487
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-45d78dee-9ca3-4d59-9ce2-dbfbaf6902ff.176cc4e05dd9ea5f], Reason = [Created], Message = [Created container filler-pod-45d78dee-9ca3-4d59-9ce2-dbfbaf6902ff] 06/28/23 08:20:29.487
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-45d78dee-9ca3-4d59-9ce2-dbfbaf6902ff.176cc4e06360b9ec], Reason = [Started], Message = [Started container filler-pod-45d78dee-9ca3-4d59-9ce2-dbfbaf6902ff] 06/28/23 08:20:29.487
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-8d4bbd90-e205-45ce-b964-0510888cc3dc.176cc4e02c158f1e], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3236/filler-pod-8d4bbd90-e205-45ce-b964-0510888cc3dc to ske-rhel-749f7d55c8xdd8b6-zxlfv] 06/28/23 08:20:29.487
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-8d4bbd90-e205-45ce-b964-0510888cc3dc.176cc4e054db54cb], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 06/28/23 08:20:29.487
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-8d4bbd90-e205-45ce-b964-0510888cc3dc.176cc4e056129070], Reason = [Created], Message = [Created container filler-pod-8d4bbd90-e205-45ce-b964-0510888cc3dc] 06/28/23 08:20:29.487
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-8d4bbd90-e205-45ce-b964-0510888cc3dc.176cc4e05bb1accc], Reason = [Started], Message = [Started container filler-pod-8d4bbd90-e205-45ce-b964-0510888cc3dc] 06/28/23 08:20:29.487
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-f34dce1e-53d2-4f12-a444-578dc3c2e452.176cc4e02b74438b], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3236/filler-pod-f34dce1e-53d2-4f12-a444-578dc3c2e452 to ske-rhel-749f7d55c8xdd8b6-srshq] 06/28/23 08:20:29.487
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-f34dce1e-53d2-4f12-a444-578dc3c2e452.176cc4e05a4d3513], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 06/28/23 08:20:29.487
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-f34dce1e-53d2-4f12-a444-578dc3c2e452.176cc4e05bf9ba06], Reason = [Created], Message = [Created container filler-pod-f34dce1e-53d2-4f12-a444-578dc3c2e452] 06/28/23 08:20:29.488
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-f34dce1e-53d2-4f12-a444-578dc3c2e452.176cc4e062039ebc], Reason = [Started], Message = [Started container filler-pod-f34dce1e-53d2-4f12-a444-578dc3c2e452] 06/28/23 08:20:29.488
    STEP: Considering event: 
    Type = [Warning], Name = [additional-pod.176cc4e0a630318f], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu. preemption: 0/3 nodes are available: 3 No preemption victims found for incoming pod.] 06/28/23 08:20:29.513
    STEP: removing the label node off the node ske-rhel-749f7d55c8xdd8b6-ct4cp 06/28/23 08:20:30.551
    STEP: verifying the node doesn't have the label node 06/28/23 08:20:30.641
    STEP: removing the label node off the node ske-rhel-749f7d55c8xdd8b6-srshq 06/28/23 08:20:30.668
    STEP: verifying the node doesn't have the label node 06/28/23 08:20:30.734
    STEP: removing the label node off the node ske-rhel-749f7d55c8xdd8b6-zxlfv 06/28/23 08:20:30.767
    STEP: verifying the node doesn't have the label node 06/28/23 08:20:30.835
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Jun 28 08:20:30.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-3236" for this suite. 06/28/23 08:20:30.922
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:20:30.961
Jun 28 08:20:30.961: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename emptydir 06/28/23 08:20:30.962
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:20:31.057
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:20:31.123
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116
STEP: Creating a pod to test emptydir 0777 on tmpfs 06/28/23 08:20:31.187
Jun 28 08:20:31.233: INFO: Waiting up to 5m0s for pod "pod-84a67464-761f-4f30-8c7e-5071aa69608f" in namespace "emptydir-4975" to be "Succeeded or Failed"
Jun 28 08:20:31.273: INFO: Pod "pod-84a67464-761f-4f30-8c7e-5071aa69608f": Phase="Pending", Reason="", readiness=false. Elapsed: 39.915186ms
Jun 28 08:20:33.331: INFO: Pod "pod-84a67464-761f-4f30-8c7e-5071aa69608f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.097251363s
Jun 28 08:20:35.334: INFO: Pod "pod-84a67464-761f-4f30-8c7e-5071aa69608f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.101021031s
Jun 28 08:20:37.327: INFO: Pod "pod-84a67464-761f-4f30-8c7e-5071aa69608f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.093621279s
STEP: Saw pod success 06/28/23 08:20:37.327
Jun 28 08:20:37.327: INFO: Pod "pod-84a67464-761f-4f30-8c7e-5071aa69608f" satisfied condition "Succeeded or Failed"
Jun 28 08:20:37.380: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-zxlfv pod pod-84a67464-761f-4f30-8c7e-5071aa69608f container test-container: <nil>
STEP: delete the pod 06/28/23 08:20:37.578
Jun 28 08:20:37.643: INFO: Waiting for pod pod-84a67464-761f-4f30-8c7e-5071aa69608f to disappear
Jun 28 08:20:37.670: INFO: Pod pod-84a67464-761f-4f30-8c7e-5071aa69608f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jun 28 08:20:37.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4975" for this suite. 06/28/23 08:20:37.72
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":176,"skipped":2899,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.779 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:20:30.961
    Jun 28 08:20:30.961: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename emptydir 06/28/23 08:20:30.962
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:20:31.057
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:20:31.123
    [It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:116
    STEP: Creating a pod to test emptydir 0777 on tmpfs 06/28/23 08:20:31.187
    Jun 28 08:20:31.233: INFO: Waiting up to 5m0s for pod "pod-84a67464-761f-4f30-8c7e-5071aa69608f" in namespace "emptydir-4975" to be "Succeeded or Failed"
    Jun 28 08:20:31.273: INFO: Pod "pod-84a67464-761f-4f30-8c7e-5071aa69608f": Phase="Pending", Reason="", readiness=false. Elapsed: 39.915186ms
    Jun 28 08:20:33.331: INFO: Pod "pod-84a67464-761f-4f30-8c7e-5071aa69608f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.097251363s
    Jun 28 08:20:35.334: INFO: Pod "pod-84a67464-761f-4f30-8c7e-5071aa69608f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.101021031s
    Jun 28 08:20:37.327: INFO: Pod "pod-84a67464-761f-4f30-8c7e-5071aa69608f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.093621279s
    STEP: Saw pod success 06/28/23 08:20:37.327
    Jun 28 08:20:37.327: INFO: Pod "pod-84a67464-761f-4f30-8c7e-5071aa69608f" satisfied condition "Succeeded or Failed"
    Jun 28 08:20:37.380: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-zxlfv pod pod-84a67464-761f-4f30-8c7e-5071aa69608f container test-container: <nil>
    STEP: delete the pod 06/28/23 08:20:37.578
    Jun 28 08:20:37.643: INFO: Waiting for pod pod-84a67464-761f-4f30-8c7e-5071aa69608f to disappear
    Jun 28 08:20:37.670: INFO: Pod pod-84a67464-761f-4f30-8c7e-5071aa69608f no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jun 28 08:20:37.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-4975" for this suite. 06/28/23 08:20:37.72
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:20:37.74
Jun 28 08:20:37.741: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename subpath 06/28/23 08:20:37.741
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:20:37.807
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:20:37.846
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 06/28/23 08:20:37.886
[It] should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
STEP: Creating pod pod-subpath-test-projected-757j 06/28/23 08:20:37.94
STEP: Creating a pod to test atomic-volume-subpath 06/28/23 08:20:37.94
Jun 28 08:20:37.967: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-757j" in namespace "subpath-7209" to be "Succeeded or Failed"
Jun 28 08:20:37.986: INFO: Pod "pod-subpath-test-projected-757j": Phase="Pending", Reason="", readiness=false. Elapsed: 19.38207ms
Jun 28 08:20:40.044: INFO: Pod "pod-subpath-test-projected-757j": Phase="Pending", Reason="", readiness=false. Elapsed: 2.076992453s
Jun 28 08:20:41.999: INFO: Pod "pod-subpath-test-projected-757j": Phase="Running", Reason="", readiness=true. Elapsed: 4.031649508s
Jun 28 08:20:43.992: INFO: Pod "pod-subpath-test-projected-757j": Phase="Running", Reason="", readiness=true. Elapsed: 6.025170748s
Jun 28 08:20:45.993: INFO: Pod "pod-subpath-test-projected-757j": Phase="Running", Reason="", readiness=true. Elapsed: 8.025720077s
Jun 28 08:20:47.991: INFO: Pod "pod-subpath-test-projected-757j": Phase="Running", Reason="", readiness=true. Elapsed: 10.024515898s
Jun 28 08:20:49.993: INFO: Pod "pod-subpath-test-projected-757j": Phase="Running", Reason="", readiness=true. Elapsed: 12.025849864s
Jun 28 08:20:51.993: INFO: Pod "pod-subpath-test-projected-757j": Phase="Running", Reason="", readiness=true. Elapsed: 14.025652918s
Jun 28 08:20:53.992: INFO: Pod "pod-subpath-test-projected-757j": Phase="Running", Reason="", readiness=true. Elapsed: 16.024791592s
Jun 28 08:20:55.992: INFO: Pod "pod-subpath-test-projected-757j": Phase="Running", Reason="", readiness=true. Elapsed: 18.024920303s
Jun 28 08:20:57.992: INFO: Pod "pod-subpath-test-projected-757j": Phase="Running", Reason="", readiness=true. Elapsed: 20.0252965s
Jun 28 08:20:59.994: INFO: Pod "pod-subpath-test-projected-757j": Phase="Running", Reason="", readiness=true. Elapsed: 22.027171979s
Jun 28 08:21:01.994: INFO: Pod "pod-subpath-test-projected-757j": Phase="Running", Reason="", readiness=false. Elapsed: 24.026989937s
Jun 28 08:21:03.992: INFO: Pod "pod-subpath-test-projected-757j": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.024871894s
STEP: Saw pod success 06/28/23 08:21:03.992
Jun 28 08:21:03.992: INFO: Pod "pod-subpath-test-projected-757j" satisfied condition "Succeeded or Failed"
Jun 28 08:21:03.996: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod pod-subpath-test-projected-757j container test-container-subpath-projected-757j: <nil>
STEP: delete the pod 06/28/23 08:21:04.07
Jun 28 08:21:04.081: INFO: Waiting for pod pod-subpath-test-projected-757j to disappear
Jun 28 08:21:04.086: INFO: Pod pod-subpath-test-projected-757j no longer exists
STEP: Deleting pod pod-subpath-test-projected-757j 06/28/23 08:21:04.086
Jun 28 08:21:04.086: INFO: Deleting pod "pod-subpath-test-projected-757j" in namespace "subpath-7209"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Jun 28 08:21:04.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7209" for this suite. 06/28/23 08:21:04.099
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [Conformance]","completed":177,"skipped":2904,"failed":0}
------------------------------
â€¢ [SLOW TEST] [26.367 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with projected pod [Conformance]
    test/e2e/storage/subpath.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:20:37.74
    Jun 28 08:20:37.741: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename subpath 06/28/23 08:20:37.741
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:20:37.807
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:20:37.846
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 06/28/23 08:20:37.886
    [It] should support subpaths with projected pod [Conformance]
      test/e2e/storage/subpath.go:106
    STEP: Creating pod pod-subpath-test-projected-757j 06/28/23 08:20:37.94
    STEP: Creating a pod to test atomic-volume-subpath 06/28/23 08:20:37.94
    Jun 28 08:20:37.967: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-757j" in namespace "subpath-7209" to be "Succeeded or Failed"
    Jun 28 08:20:37.986: INFO: Pod "pod-subpath-test-projected-757j": Phase="Pending", Reason="", readiness=false. Elapsed: 19.38207ms
    Jun 28 08:20:40.044: INFO: Pod "pod-subpath-test-projected-757j": Phase="Pending", Reason="", readiness=false. Elapsed: 2.076992453s
    Jun 28 08:20:41.999: INFO: Pod "pod-subpath-test-projected-757j": Phase="Running", Reason="", readiness=true. Elapsed: 4.031649508s
    Jun 28 08:20:43.992: INFO: Pod "pod-subpath-test-projected-757j": Phase="Running", Reason="", readiness=true. Elapsed: 6.025170748s
    Jun 28 08:20:45.993: INFO: Pod "pod-subpath-test-projected-757j": Phase="Running", Reason="", readiness=true. Elapsed: 8.025720077s
    Jun 28 08:20:47.991: INFO: Pod "pod-subpath-test-projected-757j": Phase="Running", Reason="", readiness=true. Elapsed: 10.024515898s
    Jun 28 08:20:49.993: INFO: Pod "pod-subpath-test-projected-757j": Phase="Running", Reason="", readiness=true. Elapsed: 12.025849864s
    Jun 28 08:20:51.993: INFO: Pod "pod-subpath-test-projected-757j": Phase="Running", Reason="", readiness=true. Elapsed: 14.025652918s
    Jun 28 08:20:53.992: INFO: Pod "pod-subpath-test-projected-757j": Phase="Running", Reason="", readiness=true. Elapsed: 16.024791592s
    Jun 28 08:20:55.992: INFO: Pod "pod-subpath-test-projected-757j": Phase="Running", Reason="", readiness=true. Elapsed: 18.024920303s
    Jun 28 08:20:57.992: INFO: Pod "pod-subpath-test-projected-757j": Phase="Running", Reason="", readiness=true. Elapsed: 20.0252965s
    Jun 28 08:20:59.994: INFO: Pod "pod-subpath-test-projected-757j": Phase="Running", Reason="", readiness=true. Elapsed: 22.027171979s
    Jun 28 08:21:01.994: INFO: Pod "pod-subpath-test-projected-757j": Phase="Running", Reason="", readiness=false. Elapsed: 24.026989937s
    Jun 28 08:21:03.992: INFO: Pod "pod-subpath-test-projected-757j": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.024871894s
    STEP: Saw pod success 06/28/23 08:21:03.992
    Jun 28 08:21:03.992: INFO: Pod "pod-subpath-test-projected-757j" satisfied condition "Succeeded or Failed"
    Jun 28 08:21:03.996: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod pod-subpath-test-projected-757j container test-container-subpath-projected-757j: <nil>
    STEP: delete the pod 06/28/23 08:21:04.07
    Jun 28 08:21:04.081: INFO: Waiting for pod pod-subpath-test-projected-757j to disappear
    Jun 28 08:21:04.086: INFO: Pod pod-subpath-test-projected-757j no longer exists
    STEP: Deleting pod pod-subpath-test-projected-757j 06/28/23 08:21:04.086
    Jun 28 08:21:04.086: INFO: Deleting pod "pod-subpath-test-projected-757j" in namespace "subpath-7209"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Jun 28 08:21:04.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-7209" for this suite. 06/28/23 08:21:04.099
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:21:04.109
Jun 28 08:21:04.109: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename emptydir 06/28/23 08:21:04.11
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:21:04.123
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:21:04.13
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186
STEP: Creating a pod to test emptydir 0777 on node default medium 06/28/23 08:21:04.135
Jun 28 08:21:04.143: INFO: Waiting up to 5m0s for pod "pod-9008e040-b0bc-4958-9df0-ba118407551b" in namespace "emptydir-3657" to be "Succeeded or Failed"
Jun 28 08:21:04.147: INFO: Pod "pod-9008e040-b0bc-4958-9df0-ba118407551b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.590961ms
Jun 28 08:21:06.153: INFO: Pod "pod-9008e040-b0bc-4958-9df0-ba118407551b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009525355s
Jun 28 08:21:08.152: INFO: Pod "pod-9008e040-b0bc-4958-9df0-ba118407551b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009210969s
STEP: Saw pod success 06/28/23 08:21:08.152
Jun 28 08:21:08.153: INFO: Pod "pod-9008e040-b0bc-4958-9df0-ba118407551b" satisfied condition "Succeeded or Failed"
Jun 28 08:21:08.157: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod pod-9008e040-b0bc-4958-9df0-ba118407551b container test-container: <nil>
STEP: delete the pod 06/28/23 08:21:08.206
Jun 28 08:21:08.217: INFO: Waiting for pod pod-9008e040-b0bc-4958-9df0-ba118407551b to disappear
Jun 28 08:21:08.220: INFO: Pod pod-9008e040-b0bc-4958-9df0-ba118407551b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jun 28 08:21:08.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3657" for this suite. 06/28/23 08:21:08.228
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":178,"skipped":2971,"failed":0}
------------------------------
â€¢ [4.125 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:21:04.109
    Jun 28 08:21:04.109: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename emptydir 06/28/23 08:21:04.11
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:21:04.123
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:21:04.13
    [It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:186
    STEP: Creating a pod to test emptydir 0777 on node default medium 06/28/23 08:21:04.135
    Jun 28 08:21:04.143: INFO: Waiting up to 5m0s for pod "pod-9008e040-b0bc-4958-9df0-ba118407551b" in namespace "emptydir-3657" to be "Succeeded or Failed"
    Jun 28 08:21:04.147: INFO: Pod "pod-9008e040-b0bc-4958-9df0-ba118407551b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.590961ms
    Jun 28 08:21:06.153: INFO: Pod "pod-9008e040-b0bc-4958-9df0-ba118407551b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009525355s
    Jun 28 08:21:08.152: INFO: Pod "pod-9008e040-b0bc-4958-9df0-ba118407551b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009210969s
    STEP: Saw pod success 06/28/23 08:21:08.152
    Jun 28 08:21:08.153: INFO: Pod "pod-9008e040-b0bc-4958-9df0-ba118407551b" satisfied condition "Succeeded or Failed"
    Jun 28 08:21:08.157: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod pod-9008e040-b0bc-4958-9df0-ba118407551b container test-container: <nil>
    STEP: delete the pod 06/28/23 08:21:08.206
    Jun 28 08:21:08.217: INFO: Waiting for pod pod-9008e040-b0bc-4958-9df0-ba118407551b to disappear
    Jun 28 08:21:08.220: INFO: Pod pod-9008e040-b0bc-4958-9df0-ba118407551b no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jun 28 08:21:08.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-3657" for this suite. 06/28/23 08:21:08.228
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:21:08.235
Jun 28 08:21:08.235: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename webhook 06/28/23 08:21:08.236
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:21:08.248
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:21:08.252
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 06/28/23 08:21:08.266
STEP: Create role binding to let webhook read extension-apiserver-authentication 06/28/23 08:21:08.485
STEP: Deploying the webhook pod 06/28/23 08:21:08.494
STEP: Wait for the deployment to be ready 06/28/23 08:21:08.507
Jun 28 08:21:08.516: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 06/28/23 08:21:10.533
STEP: Verifying the service has paired with the endpoint 06/28/23 08:21:10.546
Jun 28 08:21:11.546: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 06/28/23 08:21:11.553
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 06/28/23 08:21:11.7
STEP: Creating a dummy validating-webhook-configuration object 06/28/23 08:21:11.845
STEP: Deleting the validating-webhook-configuration, which should be possible to remove 06/28/23 08:21:11.909
STEP: Creating a dummy mutating-webhook-configuration object 06/28/23 08:21:11.921
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 06/28/23 08:21:11.946
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jun 28 08:21:11.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2551" for this suite. 06/28/23 08:21:11.981
STEP: Destroying namespace "webhook-2551-markers" for this suite. 06/28/23 08:21:11.987
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","completed":179,"skipped":3009,"failed":0}
------------------------------
â€¢ [3.800 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:21:08.235
    Jun 28 08:21:08.235: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename webhook 06/28/23 08:21:08.236
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:21:08.248
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:21:08.252
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 06/28/23 08:21:08.266
    STEP: Create role binding to let webhook read extension-apiserver-authentication 06/28/23 08:21:08.485
    STEP: Deploying the webhook pod 06/28/23 08:21:08.494
    STEP: Wait for the deployment to be ready 06/28/23 08:21:08.507
    Jun 28 08:21:08.516: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 06/28/23 08:21:10.533
    STEP: Verifying the service has paired with the endpoint 06/28/23 08:21:10.546
    Jun 28 08:21:11.546: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
      test/e2e/apimachinery/webhook.go:276
    STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 06/28/23 08:21:11.553
    STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 06/28/23 08:21:11.7
    STEP: Creating a dummy validating-webhook-configuration object 06/28/23 08:21:11.845
    STEP: Deleting the validating-webhook-configuration, which should be possible to remove 06/28/23 08:21:11.909
    STEP: Creating a dummy mutating-webhook-configuration object 06/28/23 08:21:11.921
    STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 06/28/23 08:21:11.946
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jun 28 08:21:11.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-2551" for this suite. 06/28/23 08:21:11.981
    STEP: Destroying namespace "webhook-2551-markers" for this suite. 06/28/23 08:21:11.987
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:21:12.036
Jun 28 08:21:12.036: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename downward-api 06/28/23 08:21:12.037
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:21:12.05
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:21:12.056
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206
STEP: Creating a pod to test downward API volume plugin 06/28/23 08:21:12.061
Jun 28 08:21:12.069: INFO: Waiting up to 5m0s for pod "downwardapi-volume-25771904-17b0-4419-98ef-8829bad2ac08" in namespace "downward-api-3102" to be "Succeeded or Failed"
Jun 28 08:21:12.074: INFO: Pod "downwardapi-volume-25771904-17b0-4419-98ef-8829bad2ac08": Phase="Pending", Reason="", readiness=false. Elapsed: 4.829995ms
Jun 28 08:21:14.080: INFO: Pod "downwardapi-volume-25771904-17b0-4419-98ef-8829bad2ac08": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010047097s
Jun 28 08:21:16.081: INFO: Pod "downwardapi-volume-25771904-17b0-4419-98ef-8829bad2ac08": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01114408s
STEP: Saw pod success 06/28/23 08:21:16.081
Jun 28 08:21:16.081: INFO: Pod "downwardapi-volume-25771904-17b0-4419-98ef-8829bad2ac08" satisfied condition "Succeeded or Failed"
Jun 28 08:21:16.085: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod downwardapi-volume-25771904-17b0-4419-98ef-8829bad2ac08 container client-container: <nil>
STEP: delete the pod 06/28/23 08:21:16.168
Jun 28 08:21:16.179: INFO: Waiting for pod downwardapi-volume-25771904-17b0-4419-98ef-8829bad2ac08 to disappear
Jun 28 08:21:16.184: INFO: Pod downwardapi-volume-25771904-17b0-4419-98ef-8829bad2ac08 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jun 28 08:21:16.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3102" for this suite. 06/28/23 08:21:16.192
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","completed":180,"skipped":3028,"failed":0}
------------------------------
â€¢ [4.163 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:21:12.036
    Jun 28 08:21:12.036: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename downward-api 06/28/23 08:21:12.037
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:21:12.05
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:21:12.056
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:206
    STEP: Creating a pod to test downward API volume plugin 06/28/23 08:21:12.061
    Jun 28 08:21:12.069: INFO: Waiting up to 5m0s for pod "downwardapi-volume-25771904-17b0-4419-98ef-8829bad2ac08" in namespace "downward-api-3102" to be "Succeeded or Failed"
    Jun 28 08:21:12.074: INFO: Pod "downwardapi-volume-25771904-17b0-4419-98ef-8829bad2ac08": Phase="Pending", Reason="", readiness=false. Elapsed: 4.829995ms
    Jun 28 08:21:14.080: INFO: Pod "downwardapi-volume-25771904-17b0-4419-98ef-8829bad2ac08": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010047097s
    Jun 28 08:21:16.081: INFO: Pod "downwardapi-volume-25771904-17b0-4419-98ef-8829bad2ac08": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01114408s
    STEP: Saw pod success 06/28/23 08:21:16.081
    Jun 28 08:21:16.081: INFO: Pod "downwardapi-volume-25771904-17b0-4419-98ef-8829bad2ac08" satisfied condition "Succeeded or Failed"
    Jun 28 08:21:16.085: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod downwardapi-volume-25771904-17b0-4419-98ef-8829bad2ac08 container client-container: <nil>
    STEP: delete the pod 06/28/23 08:21:16.168
    Jun 28 08:21:16.179: INFO: Waiting for pod downwardapi-volume-25771904-17b0-4419-98ef-8829bad2ac08 to disappear
    Jun 28 08:21:16.184: INFO: Pod downwardapi-volume-25771904-17b0-4419-98ef-8829bad2ac08 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jun 28 08:21:16.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-3102" for this suite. 06/28/23 08:21:16.192
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:21:16.2
Jun 28 08:21:16.200: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename downward-api 06/28/23 08:21:16.201
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:21:16.217
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:21:16.221
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43
STEP: Creating a pod to test downward api env vars 06/28/23 08:21:16.227
Jun 28 08:21:16.235: INFO: Waiting up to 5m0s for pod "downward-api-12234962-0261-483d-babc-92985763c607" in namespace "downward-api-9759" to be "Succeeded or Failed"
Jun 28 08:21:16.240: INFO: Pod "downward-api-12234962-0261-483d-babc-92985763c607": Phase="Pending", Reason="", readiness=false. Elapsed: 4.64354ms
Jun 28 08:21:18.246: INFO: Pod "downward-api-12234962-0261-483d-babc-92985763c607": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010466539s
Jun 28 08:21:20.246: INFO: Pod "downward-api-12234962-0261-483d-babc-92985763c607": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010928607s
STEP: Saw pod success 06/28/23 08:21:20.246
Jun 28 08:21:20.246: INFO: Pod "downward-api-12234962-0261-483d-babc-92985763c607" satisfied condition "Succeeded or Failed"
Jun 28 08:21:20.250: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod downward-api-12234962-0261-483d-babc-92985763c607 container dapi-container: <nil>
STEP: delete the pod 06/28/23 08:21:20.301
Jun 28 08:21:20.313: INFO: Waiting for pod downward-api-12234962-0261-483d-babc-92985763c607 to disappear
Jun 28 08:21:20.318: INFO: Pod downward-api-12234962-0261-483d-babc-92985763c607 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Jun 28 08:21:20.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9759" for this suite. 06/28/23 08:21:20.325
{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","completed":181,"skipped":3062,"failed":0}
------------------------------
â€¢ [4.131 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:21:16.2
    Jun 28 08:21:16.200: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename downward-api 06/28/23 08:21:16.201
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:21:16.217
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:21:16.221
    [It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:43
    STEP: Creating a pod to test downward api env vars 06/28/23 08:21:16.227
    Jun 28 08:21:16.235: INFO: Waiting up to 5m0s for pod "downward-api-12234962-0261-483d-babc-92985763c607" in namespace "downward-api-9759" to be "Succeeded or Failed"
    Jun 28 08:21:16.240: INFO: Pod "downward-api-12234962-0261-483d-babc-92985763c607": Phase="Pending", Reason="", readiness=false. Elapsed: 4.64354ms
    Jun 28 08:21:18.246: INFO: Pod "downward-api-12234962-0261-483d-babc-92985763c607": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010466539s
    Jun 28 08:21:20.246: INFO: Pod "downward-api-12234962-0261-483d-babc-92985763c607": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010928607s
    STEP: Saw pod success 06/28/23 08:21:20.246
    Jun 28 08:21:20.246: INFO: Pod "downward-api-12234962-0261-483d-babc-92985763c607" satisfied condition "Succeeded or Failed"
    Jun 28 08:21:20.250: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod downward-api-12234962-0261-483d-babc-92985763c607 container dapi-container: <nil>
    STEP: delete the pod 06/28/23 08:21:20.301
    Jun 28 08:21:20.313: INFO: Waiting for pod downward-api-12234962-0261-483d-babc-92985763c607 to disappear
    Jun 28 08:21:20.318: INFO: Pod downward-api-12234962-0261-483d-babc-92985763c607 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Jun 28 08:21:20.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-9759" for this suite. 06/28/23 08:21:20.325
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:21:20.332
Jun 28 08:21:20.332: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename endpointslice 06/28/23 08:21:20.333
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:21:20.345
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:21:20.35
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65
Jun 28 08:21:20.369: INFO: Endpoints addresses: [198.19.86.25] , ports: [6443]
Jun 28 08:21:20.369: INFO: EndpointSlices addresses: [198.19.86.25] , ports: [6443]
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Jun 28 08:21:20.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-6427" for this suite. 06/28/23 08:21:20.377
{"msg":"PASSED [sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]","completed":182,"skipped":3084,"failed":0}
------------------------------
â€¢ [0.052 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:21:20.332
    Jun 28 08:21:20.332: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename endpointslice 06/28/23 08:21:20.333
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:21:20.345
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:21:20.35
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
      test/e2e/network/endpointslice.go:65
    Jun 28 08:21:20.369: INFO: Endpoints addresses: [198.19.86.25] , ports: [6443]
    Jun 28 08:21:20.369: INFO: EndpointSlices addresses: [198.19.86.25] , ports: [6443]
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Jun 28 08:21:20.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-6427" for this suite. 06/28/23 08:21:20.377
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:21:20.384
Jun 28 08:21:20.384: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename projected 06/28/23 08:21:20.387
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:21:20.399
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:21:20.403
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88
STEP: Creating configMap with name projected-configmap-test-volume-map-040cbc93-eb76-4429-9732-c0c7c2ec6ff6 06/28/23 08:21:20.407
STEP: Creating a pod to test consume configMaps 06/28/23 08:21:20.413
Jun 28 08:21:20.420: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-16edb85f-dc93-42ca-8422-05c34bb8df52" in namespace "projected-1390" to be "Succeeded or Failed"
Jun 28 08:21:20.424: INFO: Pod "pod-projected-configmaps-16edb85f-dc93-42ca-8422-05c34bb8df52": Phase="Pending", Reason="", readiness=false. Elapsed: 4.156717ms
Jun 28 08:21:22.430: INFO: Pod "pod-projected-configmaps-16edb85f-dc93-42ca-8422-05c34bb8df52": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009792074s
Jun 28 08:21:24.429: INFO: Pod "pod-projected-configmaps-16edb85f-dc93-42ca-8422-05c34bb8df52": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008925959s
STEP: Saw pod success 06/28/23 08:21:24.429
Jun 28 08:21:24.429: INFO: Pod "pod-projected-configmaps-16edb85f-dc93-42ca-8422-05c34bb8df52" satisfied condition "Succeeded or Failed"
Jun 28 08:21:24.433: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod pod-projected-configmaps-16edb85f-dc93-42ca-8422-05c34bb8df52 container agnhost-container: <nil>
STEP: delete the pod 06/28/23 08:21:24.52
Jun 28 08:21:24.533: INFO: Waiting for pod pod-projected-configmaps-16edb85f-dc93-42ca-8422-05c34bb8df52 to disappear
Jun 28 08:21:24.537: INFO: Pod pod-projected-configmaps-16edb85f-dc93-42ca-8422-05c34bb8df52 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jun 28 08:21:24.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1390" for this suite. 06/28/23 08:21:24.544
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":183,"skipped":3098,"failed":0}
------------------------------
â€¢ [4.166 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:21:20.384
    Jun 28 08:21:20.384: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename projected 06/28/23 08:21:20.387
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:21:20.399
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:21:20.403
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:88
    STEP: Creating configMap with name projected-configmap-test-volume-map-040cbc93-eb76-4429-9732-c0c7c2ec6ff6 06/28/23 08:21:20.407
    STEP: Creating a pod to test consume configMaps 06/28/23 08:21:20.413
    Jun 28 08:21:20.420: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-16edb85f-dc93-42ca-8422-05c34bb8df52" in namespace "projected-1390" to be "Succeeded or Failed"
    Jun 28 08:21:20.424: INFO: Pod "pod-projected-configmaps-16edb85f-dc93-42ca-8422-05c34bb8df52": Phase="Pending", Reason="", readiness=false. Elapsed: 4.156717ms
    Jun 28 08:21:22.430: INFO: Pod "pod-projected-configmaps-16edb85f-dc93-42ca-8422-05c34bb8df52": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009792074s
    Jun 28 08:21:24.429: INFO: Pod "pod-projected-configmaps-16edb85f-dc93-42ca-8422-05c34bb8df52": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008925959s
    STEP: Saw pod success 06/28/23 08:21:24.429
    Jun 28 08:21:24.429: INFO: Pod "pod-projected-configmaps-16edb85f-dc93-42ca-8422-05c34bb8df52" satisfied condition "Succeeded or Failed"
    Jun 28 08:21:24.433: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod pod-projected-configmaps-16edb85f-dc93-42ca-8422-05c34bb8df52 container agnhost-container: <nil>
    STEP: delete the pod 06/28/23 08:21:24.52
    Jun 28 08:21:24.533: INFO: Waiting for pod pod-projected-configmaps-16edb85f-dc93-42ca-8422-05c34bb8df52 to disappear
    Jun 28 08:21:24.537: INFO: Pod pod-projected-configmaps-16edb85f-dc93-42ca-8422-05c34bb8df52 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jun 28 08:21:24.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1390" for this suite. 06/28/23 08:21:24.544
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:21:24.553
Jun 28 08:21:24.553: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename configmap 06/28/23 08:21:24.554
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:21:24.566
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:21:24.57
[It] should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137
STEP: Creating configMap that has name configmap-test-emptyKey-28fdb1bf-c236-40f9-b7de-1171eee1bc21 06/28/23 08:21:24.574
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Jun 28 08:21:24.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9300" for this suite. 06/28/23 08:21:24.583
{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","completed":184,"skipped":3113,"failed":0}
------------------------------
â€¢ [0.037 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:21:24.553
    Jun 28 08:21:24.553: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename configmap 06/28/23 08:21:24.554
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:21:24.566
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:21:24.57
    [It] should fail to create ConfigMap with empty key [Conformance]
      test/e2e/common/node/configmap.go:137
    STEP: Creating configMap that has name configmap-test-emptyKey-28fdb1bf-c236-40f9-b7de-1171eee1bc21 06/28/23 08:21:24.574
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Jun 28 08:21:24.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-9300" for this suite. 06/28/23 08:21:24.583
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:21:24.59
Jun 28 08:21:24.590: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename crd-publish-openapi 06/28/23 08:21:24.591
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:21:24.602
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:21:24.607
[It] updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390
STEP: set up a multi version CRD 06/28/23 08:21:24.611
Jun 28 08:21:24.611: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: rename a version 06/28/23 08:21:30.932
STEP: check the new version name is served 06/28/23 08:21:30.951
STEP: check the old version name is removed 06/28/23 08:21:33.558
STEP: check the other version is not changed 06/28/23 08:21:34.918
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jun 28 08:21:39.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1036" for this suite. 06/28/23 08:21:39.759
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","completed":185,"skipped":3114,"failed":0}
------------------------------
â€¢ [SLOW TEST] [15.175 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:21:24.59
    Jun 28 08:21:24.590: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename crd-publish-openapi 06/28/23 08:21:24.591
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:21:24.602
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:21:24.607
    [It] updates the published spec when one version gets renamed [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:390
    STEP: set up a multi version CRD 06/28/23 08:21:24.611
    Jun 28 08:21:24.611: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: rename a version 06/28/23 08:21:30.932
    STEP: check the new version name is served 06/28/23 08:21:30.951
    STEP: check the old version name is removed 06/28/23 08:21:33.558
    STEP: check the other version is not changed 06/28/23 08:21:34.918
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jun 28 08:21:39.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-1036" for this suite. 06/28/23 08:21:39.759
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:21:39.766
Jun 28 08:21:39.766: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename webhook 06/28/23 08:21:39.767
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:21:39.78
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:21:39.784
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 06/28/23 08:21:39.801
STEP: Create role binding to let webhook read extension-apiserver-authentication 06/28/23 08:21:40.132
STEP: Deploying the webhook pod 06/28/23 08:21:40.141
STEP: Wait for the deployment to be ready 06/28/23 08:21:40.154
Jun 28 08:21:40.165: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 06/28/23 08:21:42.18
STEP: Verifying the service has paired with the endpoint 06/28/23 08:21:42.191
Jun 28 08:21:43.191: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208
STEP: Registering the webhook via the AdmissionRegistration API 06/28/23 08:21:43.196
STEP: create a pod 06/28/23 08:21:43.375
Jun 28 08:21:43.382: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-9229" to be "running"
Jun 28 08:21:43.386: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.867833ms
Jun 28 08:21:45.392: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.010409809s
Jun 28 08:21:45.392: INFO: Pod "to-be-attached-pod" satisfied condition "running"
STEP: 'kubectl attach' the pod, should be denied by the webhook 06/28/23 08:21:45.392
Jun 28 08:21:45.392: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=webhook-9229 attach --namespace=webhook-9229 to-be-attached-pod -i -c=container1'
Jun 28 08:21:45.567: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jun 28 08:21:45.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9229" for this suite. 06/28/23 08:21:45.581
STEP: Destroying namespace "webhook-9229-markers" for this suite. 06/28/23 08:21:45.588
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","completed":186,"skipped":3132,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.863 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:21:39.766
    Jun 28 08:21:39.766: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename webhook 06/28/23 08:21:39.767
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:21:39.78
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:21:39.784
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 06/28/23 08:21:39.801
    STEP: Create role binding to let webhook read extension-apiserver-authentication 06/28/23 08:21:40.132
    STEP: Deploying the webhook pod 06/28/23 08:21:40.141
    STEP: Wait for the deployment to be ready 06/28/23 08:21:40.154
    Jun 28 08:21:40.165: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 06/28/23 08:21:42.18
    STEP: Verifying the service has paired with the endpoint 06/28/23 08:21:42.191
    Jun 28 08:21:43.191: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny attaching pod [Conformance]
      test/e2e/apimachinery/webhook.go:208
    STEP: Registering the webhook via the AdmissionRegistration API 06/28/23 08:21:43.196
    STEP: create a pod 06/28/23 08:21:43.375
    Jun 28 08:21:43.382: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-9229" to be "running"
    Jun 28 08:21:43.386: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.867833ms
    Jun 28 08:21:45.392: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.010409809s
    Jun 28 08:21:45.392: INFO: Pod "to-be-attached-pod" satisfied condition "running"
    STEP: 'kubectl attach' the pod, should be denied by the webhook 06/28/23 08:21:45.392
    Jun 28 08:21:45.392: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=webhook-9229 attach --namespace=webhook-9229 to-be-attached-pod -i -c=container1'
    Jun 28 08:21:45.567: INFO: rc: 1
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jun 28 08:21:45.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-9229" for this suite. 06/28/23 08:21:45.581
    STEP: Destroying namespace "webhook-9229-markers" for this suite. 06/28/23 08:21:45.588
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:21:45.631
Jun 28 08:21:45.631: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename deployment 06/28/23 08:21:45.632
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:21:45.651
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:21:45.655
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
STEP: creating a Deployment 06/28/23 08:21:45.663
STEP: waiting for Deployment to be created 06/28/23 08:21:45.669
STEP: waiting for all Replicas to be Ready 06/28/23 08:21:45.672
Jun 28 08:21:45.674: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jun 28 08:21:45.674: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jun 28 08:21:45.680: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jun 28 08:21:45.680: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jun 28 08:21:45.692: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jun 28 08:21:45.692: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jun 28 08:21:45.703: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jun 28 08:21:45.703: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jun 28 08:21:46.478: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Jun 28 08:21:46.478: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Jun 28 08:21:46.656: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment 06/28/23 08:21:46.656
W0628 08:21:46.664802      18 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Jun 28 08:21:46.667: INFO: observed event type ADDED
STEP: waiting for Replicas to scale 06/28/23 08:21:46.667
Jun 28 08:21:46.670: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 0
Jun 28 08:21:46.670: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 0
Jun 28 08:21:46.670: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 0
Jun 28 08:21:46.670: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 0
Jun 28 08:21:46.670: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 0
Jun 28 08:21:46.670: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 0
Jun 28 08:21:46.670: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 0
Jun 28 08:21:46.670: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 0
Jun 28 08:21:46.671: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 1
Jun 28 08:21:46.671: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 1
Jun 28 08:21:46.671: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 2
Jun 28 08:21:46.671: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 2
Jun 28 08:21:46.671: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 2
Jun 28 08:21:46.671: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 2
Jun 28 08:21:46.676: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 2
Jun 28 08:21:46.676: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 2
Jun 28 08:21:46.688: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 2
Jun 28 08:21:46.688: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 2
Jun 28 08:21:46.693: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 1
Jun 28 08:21:46.693: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 1
Jun 28 08:21:46.710: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 1
Jun 28 08:21:46.710: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 1
Jun 28 08:21:47.677: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 2
Jun 28 08:21:47.677: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 2
Jun 28 08:21:47.693: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 1
STEP: listing Deployments 06/28/23 08:21:47.693
Jun 28 08:21:47.698: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment 06/28/23 08:21:47.698
Jun 28 08:21:47.712: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 1
STEP: fetching the DeploymentStatus 06/28/23 08:21:47.712
Jun 28 08:21:47.720: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jun 28 08:21:47.723: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jun 28 08:21:47.737: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jun 28 08:21:47.753: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jun 28 08:21:47.758: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jun 28 08:21:47.764: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jun 28 08:21:48.483: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jun 28 08:21:48.688: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
Jun 28 08:21:48.705: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jun 28 08:21:48.718: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jun 28 08:21:49.498: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus 06/28/23 08:21:49.514
STEP: fetching the DeploymentStatus 06/28/23 08:21:49.523
Jun 28 08:21:49.536: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 1
Jun 28 08:21:49.536: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 1
Jun 28 08:21:49.536: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 1
Jun 28 08:21:49.537: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 1
Jun 28 08:21:49.537: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 1
Jun 28 08:21:49.540: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 1
Jun 28 08:21:49.540: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 2
Jun 28 08:21:49.540: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 3
Jun 28 08:21:49.540: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 2
Jun 28 08:21:49.540: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 2
Jun 28 08:21:49.540: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 3
STEP: deleting the Deployment 06/28/23 08:21:49.54
Jun 28 08:21:49.554: INFO: observed event type MODIFIED
Jun 28 08:21:49.554: INFO: observed event type MODIFIED
Jun 28 08:21:49.554: INFO: observed event type MODIFIED
Jun 28 08:21:49.554: INFO: observed event type MODIFIED
Jun 28 08:21:49.554: INFO: observed event type MODIFIED
Jun 28 08:21:49.556: INFO: observed event type MODIFIED
Jun 28 08:21:49.556: INFO: observed event type MODIFIED
Jun 28 08:21:49.556: INFO: observed event type MODIFIED
Jun 28 08:21:49.556: INFO: observed event type MODIFIED
Jun 28 08:21:49.556: INFO: observed event type MODIFIED
Jun 28 08:21:49.556: INFO: observed event type MODIFIED
Jun 28 08:21:49.557: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jun 28 08:21:49.561: INFO: Log out all the ReplicaSets if there is no deployment created
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jun 28 08:21:49.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3275" for this suite. 06/28/23 08:21:49.573
{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","completed":187,"skipped":3163,"failed":0}
------------------------------
â€¢ [3.949 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:21:45.631
    Jun 28 08:21:45.631: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename deployment 06/28/23 08:21:45.632
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:21:45.651
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:21:45.655
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should run the lifecycle of a Deployment [Conformance]
      test/e2e/apps/deployment.go:185
    STEP: creating a Deployment 06/28/23 08:21:45.663
    STEP: waiting for Deployment to be created 06/28/23 08:21:45.669
    STEP: waiting for all Replicas to be Ready 06/28/23 08:21:45.672
    Jun 28 08:21:45.674: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jun 28 08:21:45.674: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jun 28 08:21:45.680: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jun 28 08:21:45.680: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jun 28 08:21:45.692: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jun 28 08:21:45.692: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jun 28 08:21:45.703: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jun 28 08:21:45.703: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jun 28 08:21:46.478: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Jun 28 08:21:46.478: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Jun 28 08:21:46.656: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 2 and labels map[test-deployment-static:true]
    STEP: patching the Deployment 06/28/23 08:21:46.656
    W0628 08:21:46.664802      18 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Jun 28 08:21:46.667: INFO: observed event type ADDED
    STEP: waiting for Replicas to scale 06/28/23 08:21:46.667
    Jun 28 08:21:46.670: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 0
    Jun 28 08:21:46.670: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 0
    Jun 28 08:21:46.670: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 0
    Jun 28 08:21:46.670: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 0
    Jun 28 08:21:46.670: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 0
    Jun 28 08:21:46.670: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 0
    Jun 28 08:21:46.670: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 0
    Jun 28 08:21:46.670: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 0
    Jun 28 08:21:46.671: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 1
    Jun 28 08:21:46.671: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 1
    Jun 28 08:21:46.671: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 2
    Jun 28 08:21:46.671: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 2
    Jun 28 08:21:46.671: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 2
    Jun 28 08:21:46.671: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 2
    Jun 28 08:21:46.676: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 2
    Jun 28 08:21:46.676: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 2
    Jun 28 08:21:46.688: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 2
    Jun 28 08:21:46.688: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 2
    Jun 28 08:21:46.693: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 1
    Jun 28 08:21:46.693: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 1
    Jun 28 08:21:46.710: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 1
    Jun 28 08:21:46.710: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 1
    Jun 28 08:21:47.677: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 2
    Jun 28 08:21:47.677: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 2
    Jun 28 08:21:47.693: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 1
    STEP: listing Deployments 06/28/23 08:21:47.693
    Jun 28 08:21:47.698: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
    STEP: updating the Deployment 06/28/23 08:21:47.698
    Jun 28 08:21:47.712: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 1
    STEP: fetching the DeploymentStatus 06/28/23 08:21:47.712
    Jun 28 08:21:47.720: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jun 28 08:21:47.723: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jun 28 08:21:47.737: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jun 28 08:21:47.753: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jun 28 08:21:47.758: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jun 28 08:21:47.764: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jun 28 08:21:48.483: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Jun 28 08:21:48.688: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    Jun 28 08:21:48.705: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Jun 28 08:21:48.718: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Jun 28 08:21:49.498: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    STEP: patching the DeploymentStatus 06/28/23 08:21:49.514
    STEP: fetching the DeploymentStatus 06/28/23 08:21:49.523
    Jun 28 08:21:49.536: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 1
    Jun 28 08:21:49.536: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 1
    Jun 28 08:21:49.536: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 1
    Jun 28 08:21:49.537: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 1
    Jun 28 08:21:49.537: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 1
    Jun 28 08:21:49.540: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 1
    Jun 28 08:21:49.540: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 2
    Jun 28 08:21:49.540: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 3
    Jun 28 08:21:49.540: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 2
    Jun 28 08:21:49.540: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 2
    Jun 28 08:21:49.540: INFO: observed Deployment test-deployment in namespace deployment-3275 with ReadyReplicas 3
    STEP: deleting the Deployment 06/28/23 08:21:49.54
    Jun 28 08:21:49.554: INFO: observed event type MODIFIED
    Jun 28 08:21:49.554: INFO: observed event type MODIFIED
    Jun 28 08:21:49.554: INFO: observed event type MODIFIED
    Jun 28 08:21:49.554: INFO: observed event type MODIFIED
    Jun 28 08:21:49.554: INFO: observed event type MODIFIED
    Jun 28 08:21:49.556: INFO: observed event type MODIFIED
    Jun 28 08:21:49.556: INFO: observed event type MODIFIED
    Jun 28 08:21:49.556: INFO: observed event type MODIFIED
    Jun 28 08:21:49.556: INFO: observed event type MODIFIED
    Jun 28 08:21:49.556: INFO: observed event type MODIFIED
    Jun 28 08:21:49.556: INFO: observed event type MODIFIED
    Jun 28 08:21:49.557: INFO: observed event type MODIFIED
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jun 28 08:21:49.561: INFO: Log out all the ReplicaSets if there is no deployment created
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jun 28 08:21:49.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-3275" for this suite. 06/28/23 08:21:49.573
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:21:49.581
Jun 28 08:21:49.581: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename resourcequota 06/28/23 08:21:49.581
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:21:49.596
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:21:49.6
[It] should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933
STEP: Creating a ResourceQuota 06/28/23 08:21:49.604
STEP: Getting a ResourceQuota 06/28/23 08:21:49.609
STEP: Listing all ResourceQuotas with LabelSelector 06/28/23 08:21:49.613
STEP: Patching the ResourceQuota 06/28/23 08:21:49.617
STEP: Deleting a Collection of ResourceQuotas 06/28/23 08:21:49.625
STEP: Verifying the deleted ResourceQuota 06/28/23 08:21:49.632
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jun 28 08:21:49.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4938" for this suite. 06/28/23 08:21:49.642
{"msg":"PASSED [sig-api-machinery] ResourceQuota should manage the lifecycle of a ResourceQuota [Conformance]","completed":188,"skipped":3185,"failed":0}
------------------------------
â€¢ [0.068 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:21:49.581
    Jun 28 08:21:49.581: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename resourcequota 06/28/23 08:21:49.581
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:21:49.596
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:21:49.6
    [It] should manage the lifecycle of a ResourceQuota [Conformance]
      test/e2e/apimachinery/resource_quota.go:933
    STEP: Creating a ResourceQuota 06/28/23 08:21:49.604
    STEP: Getting a ResourceQuota 06/28/23 08:21:49.609
    STEP: Listing all ResourceQuotas with LabelSelector 06/28/23 08:21:49.613
    STEP: Patching the ResourceQuota 06/28/23 08:21:49.617
    STEP: Deleting a Collection of ResourceQuotas 06/28/23 08:21:49.625
    STEP: Verifying the deleted ResourceQuota 06/28/23 08:21:49.632
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jun 28 08:21:49.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-4938" for this suite. 06/28/23 08:21:49.642
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:21:49.648
Jun 28 08:21:49.649: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename webhook 06/28/23 08:21:49.649
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:21:49.663
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:21:49.668
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 06/28/23 08:21:49.685
STEP: Create role binding to let webhook read extension-apiserver-authentication 06/28/23 08:21:50.311
STEP: Deploying the webhook pod 06/28/23 08:21:50.317
STEP: Wait for the deployment to be ready 06/28/23 08:21:50.329
Jun 28 08:21:50.339: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 06/28/23 08:21:52.356
STEP: Verifying the service has paired with the endpoint 06/28/23 08:21:52.365
Jun 28 08:21:53.366: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 06/28/23 08:21:53.372
STEP: create a configmap that should be updated by the webhook 06/28/23 08:21:53.521
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jun 28 08:21:53.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9262" for this suite. 06/28/23 08:21:53.765
STEP: Destroying namespace "webhook-9262-markers" for this suite. 06/28/23 08:21:53.776
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","completed":189,"skipped":3189,"failed":0}
------------------------------
â€¢ [4.186 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:21:49.648
    Jun 28 08:21:49.649: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename webhook 06/28/23 08:21:49.649
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:21:49.663
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:21:49.668
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 06/28/23 08:21:49.685
    STEP: Create role binding to let webhook read extension-apiserver-authentication 06/28/23 08:21:50.311
    STEP: Deploying the webhook pod 06/28/23 08:21:50.317
    STEP: Wait for the deployment to be ready 06/28/23 08:21:50.329
    Jun 28 08:21:50.339: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 06/28/23 08:21:52.356
    STEP: Verifying the service has paired with the endpoint 06/28/23 08:21:52.365
    Jun 28 08:21:53.366: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate configmap [Conformance]
      test/e2e/apimachinery/webhook.go:251
    STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 06/28/23 08:21:53.372
    STEP: create a configmap that should be updated by the webhook 06/28/23 08:21:53.521
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jun 28 08:21:53.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-9262" for this suite. 06/28/23 08:21:53.765
    STEP: Destroying namespace "webhook-9262-markers" for this suite. 06/28/23 08:21:53.776
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1274
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:21:53.835
Jun 28 08:21:53.835: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename kubectl 06/28/23 08:21:53.836
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:21:53.86
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:21:53.869
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1274
Jun 28 08:21:53.882: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-7511 create -f -'
Jun 28 08:21:54.417: INFO: stderr: ""
Jun 28 08:21:54.417: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Jun 28 08:21:54.417: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-7511 create -f -'
Jun 28 08:21:54.612: INFO: stderr: ""
Jun 28 08:21:54.612: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 06/28/23 08:21:54.612
Jun 28 08:21:55.619: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 28 08:21:55.619: INFO: Found 0 / 1
Jun 28 08:21:56.619: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 28 08:21:56.619: INFO: Found 1 / 1
Jun 28 08:21:56.619: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jun 28 08:21:56.623: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 28 08:21:56.623: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jun 28 08:21:56.623: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-7511 describe pod agnhost-primary-r4t5c'
Jun 28 08:21:56.710: INFO: stderr: ""
Jun 28 08:21:56.710: INFO: stdout: "Name:             agnhost-primary-r4t5c\nNamespace:        kubectl-7511\nPriority:         0\nService Account:  default\nNode:             ske-rhel-749f7d55c8xdd8b6-ct4cp/192.168.11.3\nStart Time:       Wed, 28 Jun 2023 08:21:54 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: 00a4b65e02e7bfc757401e11c1f3d4240e50de44cf49c9f6f42d8dc83a35e7bc\n                  cni.projectcalico.org/podIP: 172.21.122.35/32\n                  cni.projectcalico.org/podIPs: 172.21.122.35/32\nStatus:           Running\nIP:               172.21.122.35\nIPs:\n  IP:           172.21.122.35\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://3e5fc15ae9f2b25902aecc8ccd05804ce95e9729017aba094906e464107d7003\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.40\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Wed, 28 Jun 2023 08:21:55 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-x7gpq (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-x7gpq:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-7511/agnhost-primary-r4t5c to ske-rhel-749f7d55c8xdd8b6-ct4cp\n  Normal  Pulled     1s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.40\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
Jun 28 08:21:56.710: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-7511 describe rc agnhost-primary'
Jun 28 08:21:56.798: INFO: stderr: ""
Jun 28 08:21:56.798: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-7511\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.40\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-r4t5c\n"
Jun 28 08:21:56.798: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-7511 describe service agnhost-primary'
Jun 28 08:21:56.884: INFO: stderr: ""
Jun 28 08:21:56.884: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-7511\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                172.20.91.179\nIPs:               172.20.91.179\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         172.21.122.35:6379\nSession Affinity:  None\nEvents:            <none>\n"
Jun 28 08:21:56.896: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-7511 describe node ske-rhel-749f7d55c8xdd8b6-ct4cp'
Jun 28 08:21:57.014: INFO: stderr: ""
Jun 28 08:21:57.014: INFO: stdout: "Name:               ske-rhel-749f7d55c8xdd8b6-ct4cp\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=scp\n                    beta.kubernetes.io/os=linux\n                    cloud.samsungsds.com/nodepool=ske-rhel\n                    failure-domain.beta.kubernetes.io/region=KR-EAST-1\n                    failure-domain.beta.kubernetes.io/zone=KR-EAST-1\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=ske-rhel-749f7d55c8xdd8b6-ct4cp\n                    kubernetes.io/os=linux\n                    node.kubernetes.io/instance-type=scp\n                    topology.kubernetes.io/region=KR-EAST-1\n                    topology.kubernetes.io/zone=KR-EAST-1\nAnnotations:        cluster.x-k8s.io/cluster-name: conformanse-test-g8auv\n                    cluster.x-k8s.io/cluster-namespace: conformanse-test-g8auv\n                    cluster.x-k8s.io/labels-from-machine: \n                    cluster.x-k8s.io/machine: ske-rhel-749f7d55c8xdd8b6-ct4cp\n                    cluster.x-k8s.io/owner-kind: MachineSet\n                    cluster.x-k8s.io/owner-name: ske-rhel-749f7d55c8xdd8b6\n                    csi.volume.kubernetes.io/nodeid: {\"smb.csi.k8s.io\":\"ske-rhel-749f7d55c8xdd8b6-ct4cp\"}\n                    kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/containerd/containerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 192.168.11.3/24\n                    projectcalico.org/IPv4VXLANTunnelAddr: 172.21.122.0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Wed, 28 Jun 2023 05:12:38 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  ske-rhel-749f7d55c8xdd8b6-ct4cp\n  AcquireTime:     <unset>\n  RenewTime:       Wed, 28 Jun 2023 08:21:56 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Wed, 28 Jun 2023 05:13:35 +0000   Wed, 28 Jun 2023 05:13:35 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Wed, 28 Jun 2023 08:19:32 +0000   Wed, 28 Jun 2023 05:12:38 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Wed, 28 Jun 2023 08:19:32 +0000   Wed, 28 Jun 2023 05:12:38 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Wed, 28 Jun 2023 08:19:32 +0000   Wed, 28 Jun 2023 05:12:38 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Wed, 28 Jun 2023 08:19:32 +0000   Wed, 28 Jun 2023 05:13:40 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  192.168.11.3\n  Hostname:    ske-rhel-749f7d55c8xdd8b6-ct4cp\nCapacity:\n  cpu:                    2\n  ephemeral-storage:      67853956Ki\n  example.com/fakecpu:    1k\n  hugepages-1Gi:          0\n  hugepages-2Mi:          0\n  memory:                 4022940Ki\n  pods:                   110\n  scheduling.k8s.io/foo:  5\nAllocatable:\n  cpu:                    1930m\n  ephemeral-storage:      62534205747\n  example.com/fakecpu:    1k\n  hugepages-1Gi:          0\n  hugepages-2Mi:          0\n  memory:                 2871964Ki\n  pods:                   110\n  scheduling.k8s.io/foo:  5\nSystem Info:\n  Machine ID:                 c98fbd61e2354135840f5aa524a80bc9\n  System UUID:                515b1442-09d7-34a2-fbbd-5e9c2bb2d272\n  Boot ID:                    f5cbfcde-e002-4eb9-b0f1-e9f73129f153\n  Kernel Version:             4.18.0-305.3.1.el8_4.x86_64\n  OS Image:                   Red Hat Enterprise Linux 8.3 (Ootpa)\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.21\n  Kubelet Version:            v1.25.10-ske.p3\n  Kube-Proxy Version:         v1.25.10-ske.p3\nPodCIDR:                      172.21.0.0/24\nPodCIDRs:                     172.21.0.0/24\nProviderID:                   scp://INSTANCE-y6IFguj3rnfNy7DGTPJ8qn\nNon-terminated Pods:          (7 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 calico-node-h6www                                          250m (12%)    0 (0%)      0 (0%)           0 (0%)         3h9m\n  kube-system                 csi-smb-node-7hhpt                                         30m (1%)      0 (0%)      60Mi (2%)        400Mi (14%)    3h9m\n  kube-system                 kube-proxy-78kbb                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         3h9m\n  kube-system                 node-exporter-fkq2r                                        100m (5%)     200m (10%)  50Mi (1%)        200Mi (7%)     3h9m\n  kubectl-7511                agnhost-primary-r4t5c                                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         3s\n  sonobuoy                    sonobuoy                                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         41m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-747dcc3d7bcc492c-v77g9    0 (0%)        0 (0%)      0 (0%)           0 (0%)         41m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource               Requests    Limits\n  --------               --------    ------\n  cpu                    380m (19%)  200m (10%)\n  memory                 110Mi (3%)  600Mi (21%)\n  ephemeral-storage      0 (0%)      0 (0%)\n  hugepages-1Gi          0 (0%)      0 (0%)\n  hugepages-2Mi          0 (0%)      0 (0%)\n  example.com/fakecpu    0           0\n  scheduling.k8s.io/foo  0           0\nEvents:                  <none>\n"
Jun 28 08:21:57.014: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-7511 describe namespace kubectl-7511'
Jun 28 08:21:57.093: INFO: stderr: ""
Jun 28 08:21:57.093: INFO: stdout: "Name:         kubectl-7511\nLabels:       e2e-framework=kubectl\n              e2e-run=2b4cfe99-b7cd-4bd1-9a61-43431200c020\n              kubernetes.io/metadata.name=kubectl-7511\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jun 28 08:21:57.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7511" for this suite. 06/28/23 08:21:57.11
{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","completed":190,"skipped":3212,"failed":0}
------------------------------
â€¢ [3.285 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl describe
  test/e2e/kubectl/kubectl.go:1268
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    test/e2e/kubectl/kubectl.go:1274

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:21:53.835
    Jun 28 08:21:53.835: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename kubectl 06/28/23 08:21:53.836
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:21:53.86
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:21:53.869
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
      test/e2e/kubectl/kubectl.go:1274
    Jun 28 08:21:53.882: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-7511 create -f -'
    Jun 28 08:21:54.417: INFO: stderr: ""
    Jun 28 08:21:54.417: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    Jun 28 08:21:54.417: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-7511 create -f -'
    Jun 28 08:21:54.612: INFO: stderr: ""
    Jun 28 08:21:54.612: INFO: stdout: "service/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 06/28/23 08:21:54.612
    Jun 28 08:21:55.619: INFO: Selector matched 1 pods for map[app:agnhost]
    Jun 28 08:21:55.619: INFO: Found 0 / 1
    Jun 28 08:21:56.619: INFO: Selector matched 1 pods for map[app:agnhost]
    Jun 28 08:21:56.619: INFO: Found 1 / 1
    Jun 28 08:21:56.619: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Jun 28 08:21:56.623: INFO: Selector matched 1 pods for map[app:agnhost]
    Jun 28 08:21:56.623: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Jun 28 08:21:56.623: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-7511 describe pod agnhost-primary-r4t5c'
    Jun 28 08:21:56.710: INFO: stderr: ""
    Jun 28 08:21:56.710: INFO: stdout: "Name:             agnhost-primary-r4t5c\nNamespace:        kubectl-7511\nPriority:         0\nService Account:  default\nNode:             ske-rhel-749f7d55c8xdd8b6-ct4cp/192.168.11.3\nStart Time:       Wed, 28 Jun 2023 08:21:54 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: 00a4b65e02e7bfc757401e11c1f3d4240e50de44cf49c9f6f42d8dc83a35e7bc\n                  cni.projectcalico.org/podIP: 172.21.122.35/32\n                  cni.projectcalico.org/podIPs: 172.21.122.35/32\nStatus:           Running\nIP:               172.21.122.35\nIPs:\n  IP:           172.21.122.35\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://3e5fc15ae9f2b25902aecc8ccd05804ce95e9729017aba094906e464107d7003\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.40\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Wed, 28 Jun 2023 08:21:55 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-x7gpq (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-x7gpq:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-7511/agnhost-primary-r4t5c to ske-rhel-749f7d55c8xdd8b6-ct4cp\n  Normal  Pulled     1s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.40\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
    Jun 28 08:21:56.710: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-7511 describe rc agnhost-primary'
    Jun 28 08:21:56.798: INFO: stderr: ""
    Jun 28 08:21:56.798: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-7511\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.40\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-r4t5c\n"
    Jun 28 08:21:56.798: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-7511 describe service agnhost-primary'
    Jun 28 08:21:56.884: INFO: stderr: ""
    Jun 28 08:21:56.884: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-7511\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                172.20.91.179\nIPs:               172.20.91.179\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         172.21.122.35:6379\nSession Affinity:  None\nEvents:            <none>\n"
    Jun 28 08:21:56.896: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-7511 describe node ske-rhel-749f7d55c8xdd8b6-ct4cp'
    Jun 28 08:21:57.014: INFO: stderr: ""
    Jun 28 08:21:57.014: INFO: stdout: "Name:               ske-rhel-749f7d55c8xdd8b6-ct4cp\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=scp\n                    beta.kubernetes.io/os=linux\n                    cloud.samsungsds.com/nodepool=ske-rhel\n                    failure-domain.beta.kubernetes.io/region=KR-EAST-1\n                    failure-domain.beta.kubernetes.io/zone=KR-EAST-1\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=ske-rhel-749f7d55c8xdd8b6-ct4cp\n                    kubernetes.io/os=linux\n                    node.kubernetes.io/instance-type=scp\n                    topology.kubernetes.io/region=KR-EAST-1\n                    topology.kubernetes.io/zone=KR-EAST-1\nAnnotations:        cluster.x-k8s.io/cluster-name: conformanse-test-g8auv\n                    cluster.x-k8s.io/cluster-namespace: conformanse-test-g8auv\n                    cluster.x-k8s.io/labels-from-machine: \n                    cluster.x-k8s.io/machine: ske-rhel-749f7d55c8xdd8b6-ct4cp\n                    cluster.x-k8s.io/owner-kind: MachineSet\n                    cluster.x-k8s.io/owner-name: ske-rhel-749f7d55c8xdd8b6\n                    csi.volume.kubernetes.io/nodeid: {\"smb.csi.k8s.io\":\"ske-rhel-749f7d55c8xdd8b6-ct4cp\"}\n                    kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/containerd/containerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 192.168.11.3/24\n                    projectcalico.org/IPv4VXLANTunnelAddr: 172.21.122.0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Wed, 28 Jun 2023 05:12:38 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  ske-rhel-749f7d55c8xdd8b6-ct4cp\n  AcquireTime:     <unset>\n  RenewTime:       Wed, 28 Jun 2023 08:21:56 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Wed, 28 Jun 2023 05:13:35 +0000   Wed, 28 Jun 2023 05:13:35 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Wed, 28 Jun 2023 08:19:32 +0000   Wed, 28 Jun 2023 05:12:38 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Wed, 28 Jun 2023 08:19:32 +0000   Wed, 28 Jun 2023 05:12:38 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Wed, 28 Jun 2023 08:19:32 +0000   Wed, 28 Jun 2023 05:12:38 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Wed, 28 Jun 2023 08:19:32 +0000   Wed, 28 Jun 2023 05:13:40 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  192.168.11.3\n  Hostname:    ske-rhel-749f7d55c8xdd8b6-ct4cp\nCapacity:\n  cpu:                    2\n  ephemeral-storage:      67853956Ki\n  example.com/fakecpu:    1k\n  hugepages-1Gi:          0\n  hugepages-2Mi:          0\n  memory:                 4022940Ki\n  pods:                   110\n  scheduling.k8s.io/foo:  5\nAllocatable:\n  cpu:                    1930m\n  ephemeral-storage:      62534205747\n  example.com/fakecpu:    1k\n  hugepages-1Gi:          0\n  hugepages-2Mi:          0\n  memory:                 2871964Ki\n  pods:                   110\n  scheduling.k8s.io/foo:  5\nSystem Info:\n  Machine ID:                 c98fbd61e2354135840f5aa524a80bc9\n  System UUID:                515b1442-09d7-34a2-fbbd-5e9c2bb2d272\n  Boot ID:                    f5cbfcde-e002-4eb9-b0f1-e9f73129f153\n  Kernel Version:             4.18.0-305.3.1.el8_4.x86_64\n  OS Image:                   Red Hat Enterprise Linux 8.3 (Ootpa)\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.21\n  Kubelet Version:            v1.25.10-ske.p3\n  Kube-Proxy Version:         v1.25.10-ske.p3\nPodCIDR:                      172.21.0.0/24\nPodCIDRs:                     172.21.0.0/24\nProviderID:                   scp://INSTANCE-y6IFguj3rnfNy7DGTPJ8qn\nNon-terminated Pods:          (7 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 calico-node-h6www                                          250m (12%)    0 (0%)      0 (0%)           0 (0%)         3h9m\n  kube-system                 csi-smb-node-7hhpt                                         30m (1%)      0 (0%)      60Mi (2%)        400Mi (14%)    3h9m\n  kube-system                 kube-proxy-78kbb                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         3h9m\n  kube-system                 node-exporter-fkq2r                                        100m (5%)     200m (10%)  50Mi (1%)        200Mi (7%)     3h9m\n  kubectl-7511                agnhost-primary-r4t5c                                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         3s\n  sonobuoy                    sonobuoy                                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         41m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-747dcc3d7bcc492c-v77g9    0 (0%)        0 (0%)      0 (0%)           0 (0%)         41m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource               Requests    Limits\n  --------               --------    ------\n  cpu                    380m (19%)  200m (10%)\n  memory                 110Mi (3%)  600Mi (21%)\n  ephemeral-storage      0 (0%)      0 (0%)\n  hugepages-1Gi          0 (0%)      0 (0%)\n  hugepages-2Mi          0 (0%)      0 (0%)\n  example.com/fakecpu    0           0\n  scheduling.k8s.io/foo  0           0\nEvents:                  <none>\n"
    Jun 28 08:21:57.014: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-7511 describe namespace kubectl-7511'
    Jun 28 08:21:57.093: INFO: stderr: ""
    Jun 28 08:21:57.093: INFO: stdout: "Name:         kubectl-7511\nLabels:       e2e-framework=kubectl\n              e2e-run=2b4cfe99-b7cd-4bd1-9a61-43431200c020\n              kubernetes.io/metadata.name=kubectl-7511\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jun 28 08:21:57.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-7511" for this suite. 06/28/23 08:21:57.11
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:21:57.122
Jun 28 08:21:57.122: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename sched-pred 06/28/23 08:21:57.122
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:21:57.145
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:21:57.158
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Jun 28 08:21:57.173: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun 28 08:21:57.202: INFO: Waiting for terminating namespaces to be deleted...
Jun 28 08:21:57.210: INFO: 
Logging pods the apiserver thinks is on node ske-rhel-749f7d55c8xdd8b6-ct4cp before test
Jun 28 08:21:57.236: INFO: calico-node-h6www from kube-system started at 2023-06-28 05:12:45 +0000 UTC (1 container statuses recorded)
Jun 28 08:21:57.236: INFO: 	Container calico-node ready: true, restart count 0
Jun 28 08:21:57.236: INFO: csi-smb-node-7hhpt from kube-system started at 2023-06-28 05:12:45 +0000 UTC (3 container statuses recorded)
Jun 28 08:21:57.236: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 28 08:21:57.236: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jun 28 08:21:57.236: INFO: 	Container smb ready: true, restart count 0
Jun 28 08:21:57.236: INFO: kube-proxy-78kbb from kube-system started at 2023-06-28 05:12:45 +0000 UTC (1 container statuses recorded)
Jun 28 08:21:57.236: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 28 08:21:57.236: INFO: node-exporter-fkq2r from kube-system started at 2023-06-28 05:12:45 +0000 UTC (1 container statuses recorded)
Jun 28 08:21:57.236: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Jun 28 08:21:57.236: INFO: agnhost-primary-r4t5c from kubectl-7511 started at 2023-06-28 08:21:54 +0000 UTC (1 container statuses recorded)
Jun 28 08:21:57.236: INFO: 	Container agnhost-primary ready: true, restart count 0
Jun 28 08:21:57.236: INFO: sonobuoy from sonobuoy started at 2023-06-28 07:40:00 +0000 UTC (1 container statuses recorded)
Jun 28 08:21:57.236: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun 28 08:21:57.236: INFO: sonobuoy-systemd-logs-daemon-set-747dcc3d7bcc492c-v77g9 from sonobuoy started at 2023-06-28 07:40:01 +0000 UTC (2 container statuses recorded)
Jun 28 08:21:57.236: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 28 08:21:57.236: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 28 08:21:57.236: INFO: 
Logging pods the apiserver thinks is on node ske-rhel-749f7d55c8xdd8b6-srshq before test
Jun 28 08:21:57.279: INFO: calico-kube-controllers-77cc457ff7-jfwrp from kube-system started at 2023-06-28 05:13:50 +0000 UTC (1 container statuses recorded)
Jun 28 08:21:57.279: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jun 28 08:21:57.279: INFO: calico-node-xkhs5 from kube-system started at 2023-06-28 05:12:45 +0000 UTC (1 container statuses recorded)
Jun 28 08:21:57.279: INFO: 	Container calico-node ready: true, restart count 0
Jun 28 08:21:57.279: INFO: coredns-7b4f76cbb6-gkxll from kube-system started at 2023-06-28 05:13:50 +0000 UTC (1 container statuses recorded)
Jun 28 08:21:57.279: INFO: 	Container coredns ready: true, restart count 0
Jun 28 08:21:57.279: INFO: coredns-7b4f76cbb6-ztck5 from kube-system started at 2023-06-28 05:13:50 +0000 UTC (1 container statuses recorded)
Jun 28 08:21:57.279: INFO: 	Container coredns ready: true, restart count 0
Jun 28 08:21:57.279: INFO: csi-smb-controller-7ffdfbf4b6-pfh4n from kube-system started at 2023-06-28 05:13:50 +0000 UTC (3 container statuses recorded)
Jun 28 08:21:57.279: INFO: 	Container csi-provisioner ready: true, restart count 1
Jun 28 08:21:57.279: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 28 08:21:57.279: INFO: 	Container smb ready: true, restart count 0
Jun 28 08:21:57.279: INFO: csi-smb-node-v64b5 from kube-system started at 2023-06-28 05:12:45 +0000 UTC (3 container statuses recorded)
Jun 28 08:21:57.279: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 28 08:21:57.279: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jun 28 08:21:57.279: INFO: 	Container smb ready: true, restart count 0
Jun 28 08:21:57.279: INFO: kube-proxy-l8rq8 from kube-system started at 2023-06-28 05:12:45 +0000 UTC (1 container statuses recorded)
Jun 28 08:21:57.279: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 28 08:21:57.279: INFO: metrics-server-54cd6dc7f5-8j8ss from kube-system started at 2023-06-28 05:13:50 +0000 UTC (1 container statuses recorded)
Jun 28 08:21:57.279: INFO: 	Container metrics-server ready: true, restart count 0
Jun 28 08:21:57.279: INFO: nfs-subdir-external-provisioner-548dcf4dc4-z87tg from kube-system started at 2023-06-28 05:13:50 +0000 UTC (1 container statuses recorded)
Jun 28 08:21:57.279: INFO: 	Container nfs-subdir-external-provisioner ready: true, restart count 1
Jun 28 08:21:57.279: INFO: node-exporter-hcfhb from kube-system started at 2023-06-28 05:12:45 +0000 UTC (1 container statuses recorded)
Jun 28 08:21:57.279: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Jun 28 08:21:57.279: INFO: vpn-target-bcf545797-bfhjg from kube-system started at 2023-06-28 05:13:50 +0000 UTC (1 container statuses recorded)
Jun 28 08:21:57.279: INFO: 	Container vpn-target ready: true, restart count 0
Jun 28 08:21:57.279: INFO: sonobuoy-systemd-logs-daemon-set-747dcc3d7bcc492c-gmpnw from sonobuoy started at 2023-06-28 07:40:01 +0000 UTC (2 container statuses recorded)
Jun 28 08:21:57.279: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 28 08:21:57.279: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 28 08:21:57.279: INFO: 
Logging pods the apiserver thinks is on node ske-rhel-749f7d55c8xdd8b6-zxlfv before test
Jun 28 08:21:57.301: INFO: calico-node-6xpbr from kube-system started at 2023-06-28 05:12:49 +0000 UTC (1 container statuses recorded)
Jun 28 08:21:57.301: INFO: 	Container calico-node ready: true, restart count 0
Jun 28 08:21:57.301: INFO: csi-smb-node-t6cfj from kube-system started at 2023-06-28 05:12:49 +0000 UTC (3 container statuses recorded)
Jun 28 08:21:57.301: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 28 08:21:57.301: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jun 28 08:21:57.301: INFO: 	Container smb ready: true, restart count 0
Jun 28 08:21:57.301: INFO: kube-proxy-l6xjw from kube-system started at 2023-06-28 05:12:49 +0000 UTC (1 container statuses recorded)
Jun 28 08:21:57.301: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 28 08:21:57.301: INFO: node-exporter-w4bjx from kube-system started at 2023-06-28 05:12:49 +0000 UTC (1 container statuses recorded)
Jun 28 08:21:57.301: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Jun 28 08:21:57.301: INFO: sonobuoy-e2e-job-4078a53074bd4828 from sonobuoy started at 2023-06-28 07:40:01 +0000 UTC (2 container statuses recorded)
Jun 28 08:21:57.301: INFO: 	Container e2e ready: true, restart count 0
Jun 28 08:21:57.301: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 28 08:21:57.301: INFO: sonobuoy-systemd-logs-daemon-set-747dcc3d7bcc492c-wmgvv from sonobuoy started at 2023-06-28 07:40:01 +0000 UTC (2 container statuses recorded)
Jun 28 08:21:57.301: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 28 08:21:57.301: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461
STEP: Trying to launch a pod without a label to get a node which can launch it. 06/28/23 08:21:57.301
Jun 28 08:21:57.313: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-6284" to be "running"
Jun 28 08:21:57.323: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 9.955242ms
Jun 28 08:21:59.328: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.014660459s
Jun 28 08:21:59.328: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 06/28/23 08:21:59.332
STEP: Trying to apply a random label on the found node. 06/28/23 08:21:59.343
STEP: verifying the node has the label kubernetes.io/e2e-e7c0b727-3839-4e03-b55e-6132c0ed1530 42 06/28/23 08:21:59.353
STEP: Trying to relaunch the pod, now with labels. 06/28/23 08:21:59.358
Jun 28 08:21:59.363: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-6284" to be "not pending"
Jun 28 08:21:59.368: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 4.87576ms
Jun 28 08:22:01.375: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.011916328s
Jun 28 08:22:01.375: INFO: Pod "with-labels" satisfied condition "not pending"
STEP: removing the label kubernetes.io/e2e-e7c0b727-3839-4e03-b55e-6132c0ed1530 off the node ske-rhel-749f7d55c8xdd8b6-ct4cp 06/28/23 08:22:01.381
STEP: verifying the node doesn't have the label kubernetes.io/e2e-e7c0b727-3839-4e03-b55e-6132c0ed1530 06/28/23 08:22:01.4
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Jun 28 08:22:01.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6284" for this suite. 06/28/23 08:22:01.42
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","completed":191,"skipped":3257,"failed":0}
------------------------------
â€¢ [4.309 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:21:57.122
    Jun 28 08:21:57.122: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename sched-pred 06/28/23 08:21:57.122
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:21:57.145
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:21:57.158
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Jun 28 08:21:57.173: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jun 28 08:21:57.202: INFO: Waiting for terminating namespaces to be deleted...
    Jun 28 08:21:57.210: INFO: 
    Logging pods the apiserver thinks is on node ske-rhel-749f7d55c8xdd8b6-ct4cp before test
    Jun 28 08:21:57.236: INFO: calico-node-h6www from kube-system started at 2023-06-28 05:12:45 +0000 UTC (1 container statuses recorded)
    Jun 28 08:21:57.236: INFO: 	Container calico-node ready: true, restart count 0
    Jun 28 08:21:57.236: INFO: csi-smb-node-7hhpt from kube-system started at 2023-06-28 05:12:45 +0000 UTC (3 container statuses recorded)
    Jun 28 08:21:57.236: INFO: 	Container liveness-probe ready: true, restart count 0
    Jun 28 08:21:57.236: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Jun 28 08:21:57.236: INFO: 	Container smb ready: true, restart count 0
    Jun 28 08:21:57.236: INFO: kube-proxy-78kbb from kube-system started at 2023-06-28 05:12:45 +0000 UTC (1 container statuses recorded)
    Jun 28 08:21:57.236: INFO: 	Container kube-proxy ready: true, restart count 0
    Jun 28 08:21:57.236: INFO: node-exporter-fkq2r from kube-system started at 2023-06-28 05:12:45 +0000 UTC (1 container statuses recorded)
    Jun 28 08:21:57.236: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
    Jun 28 08:21:57.236: INFO: agnhost-primary-r4t5c from kubectl-7511 started at 2023-06-28 08:21:54 +0000 UTC (1 container statuses recorded)
    Jun 28 08:21:57.236: INFO: 	Container agnhost-primary ready: true, restart count 0
    Jun 28 08:21:57.236: INFO: sonobuoy from sonobuoy started at 2023-06-28 07:40:00 +0000 UTC (1 container statuses recorded)
    Jun 28 08:21:57.236: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jun 28 08:21:57.236: INFO: sonobuoy-systemd-logs-daemon-set-747dcc3d7bcc492c-v77g9 from sonobuoy started at 2023-06-28 07:40:01 +0000 UTC (2 container statuses recorded)
    Jun 28 08:21:57.236: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jun 28 08:21:57.236: INFO: 	Container systemd-logs ready: true, restart count 0
    Jun 28 08:21:57.236: INFO: 
    Logging pods the apiserver thinks is on node ske-rhel-749f7d55c8xdd8b6-srshq before test
    Jun 28 08:21:57.279: INFO: calico-kube-controllers-77cc457ff7-jfwrp from kube-system started at 2023-06-28 05:13:50 +0000 UTC (1 container statuses recorded)
    Jun 28 08:21:57.279: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Jun 28 08:21:57.279: INFO: calico-node-xkhs5 from kube-system started at 2023-06-28 05:12:45 +0000 UTC (1 container statuses recorded)
    Jun 28 08:21:57.279: INFO: 	Container calico-node ready: true, restart count 0
    Jun 28 08:21:57.279: INFO: coredns-7b4f76cbb6-gkxll from kube-system started at 2023-06-28 05:13:50 +0000 UTC (1 container statuses recorded)
    Jun 28 08:21:57.279: INFO: 	Container coredns ready: true, restart count 0
    Jun 28 08:21:57.279: INFO: coredns-7b4f76cbb6-ztck5 from kube-system started at 2023-06-28 05:13:50 +0000 UTC (1 container statuses recorded)
    Jun 28 08:21:57.279: INFO: 	Container coredns ready: true, restart count 0
    Jun 28 08:21:57.279: INFO: csi-smb-controller-7ffdfbf4b6-pfh4n from kube-system started at 2023-06-28 05:13:50 +0000 UTC (3 container statuses recorded)
    Jun 28 08:21:57.279: INFO: 	Container csi-provisioner ready: true, restart count 1
    Jun 28 08:21:57.279: INFO: 	Container liveness-probe ready: true, restart count 0
    Jun 28 08:21:57.279: INFO: 	Container smb ready: true, restart count 0
    Jun 28 08:21:57.279: INFO: csi-smb-node-v64b5 from kube-system started at 2023-06-28 05:12:45 +0000 UTC (3 container statuses recorded)
    Jun 28 08:21:57.279: INFO: 	Container liveness-probe ready: true, restart count 0
    Jun 28 08:21:57.279: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Jun 28 08:21:57.279: INFO: 	Container smb ready: true, restart count 0
    Jun 28 08:21:57.279: INFO: kube-proxy-l8rq8 from kube-system started at 2023-06-28 05:12:45 +0000 UTC (1 container statuses recorded)
    Jun 28 08:21:57.279: INFO: 	Container kube-proxy ready: true, restart count 0
    Jun 28 08:21:57.279: INFO: metrics-server-54cd6dc7f5-8j8ss from kube-system started at 2023-06-28 05:13:50 +0000 UTC (1 container statuses recorded)
    Jun 28 08:21:57.279: INFO: 	Container metrics-server ready: true, restart count 0
    Jun 28 08:21:57.279: INFO: nfs-subdir-external-provisioner-548dcf4dc4-z87tg from kube-system started at 2023-06-28 05:13:50 +0000 UTC (1 container statuses recorded)
    Jun 28 08:21:57.279: INFO: 	Container nfs-subdir-external-provisioner ready: true, restart count 1
    Jun 28 08:21:57.279: INFO: node-exporter-hcfhb from kube-system started at 2023-06-28 05:12:45 +0000 UTC (1 container statuses recorded)
    Jun 28 08:21:57.279: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
    Jun 28 08:21:57.279: INFO: vpn-target-bcf545797-bfhjg from kube-system started at 2023-06-28 05:13:50 +0000 UTC (1 container statuses recorded)
    Jun 28 08:21:57.279: INFO: 	Container vpn-target ready: true, restart count 0
    Jun 28 08:21:57.279: INFO: sonobuoy-systemd-logs-daemon-set-747dcc3d7bcc492c-gmpnw from sonobuoy started at 2023-06-28 07:40:01 +0000 UTC (2 container statuses recorded)
    Jun 28 08:21:57.279: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jun 28 08:21:57.279: INFO: 	Container systemd-logs ready: true, restart count 0
    Jun 28 08:21:57.279: INFO: 
    Logging pods the apiserver thinks is on node ske-rhel-749f7d55c8xdd8b6-zxlfv before test
    Jun 28 08:21:57.301: INFO: calico-node-6xpbr from kube-system started at 2023-06-28 05:12:49 +0000 UTC (1 container statuses recorded)
    Jun 28 08:21:57.301: INFO: 	Container calico-node ready: true, restart count 0
    Jun 28 08:21:57.301: INFO: csi-smb-node-t6cfj from kube-system started at 2023-06-28 05:12:49 +0000 UTC (3 container statuses recorded)
    Jun 28 08:21:57.301: INFO: 	Container liveness-probe ready: true, restart count 0
    Jun 28 08:21:57.301: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Jun 28 08:21:57.301: INFO: 	Container smb ready: true, restart count 0
    Jun 28 08:21:57.301: INFO: kube-proxy-l6xjw from kube-system started at 2023-06-28 05:12:49 +0000 UTC (1 container statuses recorded)
    Jun 28 08:21:57.301: INFO: 	Container kube-proxy ready: true, restart count 0
    Jun 28 08:21:57.301: INFO: node-exporter-w4bjx from kube-system started at 2023-06-28 05:12:49 +0000 UTC (1 container statuses recorded)
    Jun 28 08:21:57.301: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
    Jun 28 08:21:57.301: INFO: sonobuoy-e2e-job-4078a53074bd4828 from sonobuoy started at 2023-06-28 07:40:01 +0000 UTC (2 container statuses recorded)
    Jun 28 08:21:57.301: INFO: 	Container e2e ready: true, restart count 0
    Jun 28 08:21:57.301: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jun 28 08:21:57.301: INFO: sonobuoy-systemd-logs-daemon-set-747dcc3d7bcc492c-wmgvv from sonobuoy started at 2023-06-28 07:40:01 +0000 UTC (2 container statuses recorded)
    Jun 28 08:21:57.301: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jun 28 08:21:57.301: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if matching  [Conformance]
      test/e2e/scheduling/predicates.go:461
    STEP: Trying to launch a pod without a label to get a node which can launch it. 06/28/23 08:21:57.301
    Jun 28 08:21:57.313: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-6284" to be "running"
    Jun 28 08:21:57.323: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 9.955242ms
    Jun 28 08:21:59.328: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.014660459s
    Jun 28 08:21:59.328: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 06/28/23 08:21:59.332
    STEP: Trying to apply a random label on the found node. 06/28/23 08:21:59.343
    STEP: verifying the node has the label kubernetes.io/e2e-e7c0b727-3839-4e03-b55e-6132c0ed1530 42 06/28/23 08:21:59.353
    STEP: Trying to relaunch the pod, now with labels. 06/28/23 08:21:59.358
    Jun 28 08:21:59.363: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-6284" to be "not pending"
    Jun 28 08:21:59.368: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 4.87576ms
    Jun 28 08:22:01.375: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.011916328s
    Jun 28 08:22:01.375: INFO: Pod "with-labels" satisfied condition "not pending"
    STEP: removing the label kubernetes.io/e2e-e7c0b727-3839-4e03-b55e-6132c0ed1530 off the node ske-rhel-749f7d55c8xdd8b6-ct4cp 06/28/23 08:22:01.381
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-e7c0b727-3839-4e03-b55e-6132c0ed1530 06/28/23 08:22:01.4
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Jun 28 08:22:01.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-6284" for this suite. 06/28/23 08:22:01.42
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:231
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:22:01.433
Jun 28 08:22:01.433: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename container-runtime 06/28/23 08:22:01.434
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:22:01.459
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:22:01.467
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:231
STEP: create the container 06/28/23 08:22:01.478
STEP: wait for the container to reach Succeeded 06/28/23 08:22:01.49
STEP: get the container status 06/28/23 08:22:05.531
STEP: the container should be terminated 06/28/23 08:22:05.535
STEP: the termination message should be set 06/28/23 08:22:05.535
Jun 28 08:22:05.535: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container 06/28/23 08:22:05.535
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Jun 28 08:22:05.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1238" for this suite. 06/28/23 08:22:05.557
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":192,"skipped":3303,"failed":0}
------------------------------
â€¢ [4.130 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:231

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:22:01.433
    Jun 28 08:22:01.433: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename container-runtime 06/28/23 08:22:01.434
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:22:01.459
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:22:01.467
    [It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:231
    STEP: create the container 06/28/23 08:22:01.478
    STEP: wait for the container to reach Succeeded 06/28/23 08:22:01.49
    STEP: get the container status 06/28/23 08:22:05.531
    STEP: the container should be terminated 06/28/23 08:22:05.535
    STEP: the termination message should be set 06/28/23 08:22:05.535
    Jun 28 08:22:05.535: INFO: Expected: &{} to match Container's Termination Message:  --
    STEP: delete the container 06/28/23 08:22:05.535
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Jun 28 08:22:05.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-1238" for this suite. 06/28/23 08:22:05.557
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:22:05.563
Jun 28 08:22:05.564: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename var-expansion 06/28/23 08:22:05.564
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:22:05.576
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:22:05.58
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72
STEP: Creating a pod to test substitution in container's command 06/28/23 08:22:05.584
Jun 28 08:22:05.592: INFO: Waiting up to 5m0s for pod "var-expansion-7f3b00fe-4aa4-412f-929b-ddf89deda9d8" in namespace "var-expansion-5910" to be "Succeeded or Failed"
Jun 28 08:22:05.596: INFO: Pod "var-expansion-7f3b00fe-4aa4-412f-929b-ddf89deda9d8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.097206ms
Jun 28 08:22:07.600: INFO: Pod "var-expansion-7f3b00fe-4aa4-412f-929b-ddf89deda9d8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008127757s
Jun 28 08:22:09.602: INFO: Pod "var-expansion-7f3b00fe-4aa4-412f-929b-ddf89deda9d8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010467762s
STEP: Saw pod success 06/28/23 08:22:09.602
Jun 28 08:22:09.602: INFO: Pod "var-expansion-7f3b00fe-4aa4-412f-929b-ddf89deda9d8" satisfied condition "Succeeded or Failed"
Jun 28 08:22:09.606: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod var-expansion-7f3b00fe-4aa4-412f-929b-ddf89deda9d8 container dapi-container: <nil>
STEP: delete the pod 06/28/23 08:22:09.65
Jun 28 08:22:09.662: INFO: Waiting for pod var-expansion-7f3b00fe-4aa4-412f-929b-ddf89deda9d8 to disappear
Jun 28 08:22:09.665: INFO: Pod var-expansion-7f3b00fe-4aa4-412f-929b-ddf89deda9d8 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jun 28 08:22:09.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5910" for this suite. 06/28/23 08:22:09.672
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","completed":193,"skipped":3306,"failed":0}
------------------------------
â€¢ [4.115 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:22:05.563
    Jun 28 08:22:05.564: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename var-expansion 06/28/23 08:22:05.564
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:22:05.576
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:22:05.58
    [It] should allow substituting values in a container's command [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:72
    STEP: Creating a pod to test substitution in container's command 06/28/23 08:22:05.584
    Jun 28 08:22:05.592: INFO: Waiting up to 5m0s for pod "var-expansion-7f3b00fe-4aa4-412f-929b-ddf89deda9d8" in namespace "var-expansion-5910" to be "Succeeded or Failed"
    Jun 28 08:22:05.596: INFO: Pod "var-expansion-7f3b00fe-4aa4-412f-929b-ddf89deda9d8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.097206ms
    Jun 28 08:22:07.600: INFO: Pod "var-expansion-7f3b00fe-4aa4-412f-929b-ddf89deda9d8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008127757s
    Jun 28 08:22:09.602: INFO: Pod "var-expansion-7f3b00fe-4aa4-412f-929b-ddf89deda9d8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010467762s
    STEP: Saw pod success 06/28/23 08:22:09.602
    Jun 28 08:22:09.602: INFO: Pod "var-expansion-7f3b00fe-4aa4-412f-929b-ddf89deda9d8" satisfied condition "Succeeded or Failed"
    Jun 28 08:22:09.606: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod var-expansion-7f3b00fe-4aa4-412f-929b-ddf89deda9d8 container dapi-container: <nil>
    STEP: delete the pod 06/28/23 08:22:09.65
    Jun 28 08:22:09.662: INFO: Waiting for pod var-expansion-7f3b00fe-4aa4-412f-929b-ddf89deda9d8 to disappear
    Jun 28 08:22:09.665: INFO: Pod var-expansion-7f3b00fe-4aa4-412f-929b-ddf89deda9d8 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jun 28 08:22:09.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-5910" for this suite. 06/28/23 08:22:09.672
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:22:09.68
Jun 28 08:22:09.680: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename kubelet-test 06/28/23 08:22:09.68
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:22:09.694
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:22:09.698
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Jun 28 08:22:13.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5715" for this suite. 06/28/23 08:22:13.727
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","completed":194,"skipped":3331,"failed":0}
------------------------------
â€¢ [4.054 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should have an terminated reason [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:22:09.68
    Jun 28 08:22:09.680: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename kubelet-test 06/28/23 08:22:09.68
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:22:09.694
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:22:09.698
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should have an terminated reason [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:110
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Jun 28 08:22:13.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-5715" for this suite. 06/28/23 08:22:13.727
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:22:13.734
Jun 28 08:22:13.734: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename svcaccounts 06/28/23 08:22:13.735
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:22:13.745
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:22:13.749
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528
Jun 28 08:22:13.766: INFO: created pod
Jun 28 08:22:13.766: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-1886" to be "Succeeded or Failed"
Jun 28 08:22:13.771: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.261061ms
Jun 28 08:22:15.776: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009867332s
Jun 28 08:22:17.776: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009704936s
STEP: Saw pod success 06/28/23 08:22:17.776
Jun 28 08:22:17.776: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Jun 28 08:22:47.777: INFO: polling logs
Jun 28 08:22:47.787: INFO: Pod logs: 
I0628 08:22:14.519703       1 log.go:195] OK: Got token
I0628 08:22:14.519746       1 log.go:195] validating with in-cluster discovery
I0628 08:22:14.520042       1 log.go:195] OK: got issuer https://kubernetes.default.svc.cluster.local
I0628 08:22:14.520068       1 log.go:195] Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-1886:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1687941133, NotBefore:1687940533, IssuedAt:1687940533, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-1886", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"799ee185-ddda-4ced-8bf8-7092c2d28222"}}}
I0628 08:22:14.533828       1 log.go:195] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
I0628 08:22:14.543927       1 log.go:195] OK: Validated signature on JWT
I0628 08:22:14.544036       1 log.go:195] OK: Got valid claims from token!
I0628 08:22:14.544070       1 log.go:195] Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-1886:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1687941133, NotBefore:1687940533, IssuedAt:1687940533, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-1886", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"799ee185-ddda-4ced-8bf8-7092c2d28222"}}}

Jun 28 08:22:47.787: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Jun 28 08:22:47.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-1886" for this suite. 06/28/23 08:22:47.805
{"msg":"PASSED [sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]","completed":195,"skipped":3342,"failed":0}
------------------------------
â€¢ [SLOW TEST] [34.077 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:22:13.734
    Jun 28 08:22:13.734: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename svcaccounts 06/28/23 08:22:13.735
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:22:13.745
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:22:13.749
    [It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
      test/e2e/auth/service_accounts.go:528
    Jun 28 08:22:13.766: INFO: created pod
    Jun 28 08:22:13.766: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-1886" to be "Succeeded or Failed"
    Jun 28 08:22:13.771: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.261061ms
    Jun 28 08:22:15.776: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009867332s
    Jun 28 08:22:17.776: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009704936s
    STEP: Saw pod success 06/28/23 08:22:17.776
    Jun 28 08:22:17.776: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
    Jun 28 08:22:47.777: INFO: polling logs
    Jun 28 08:22:47.787: INFO: Pod logs: 
    I0628 08:22:14.519703       1 log.go:195] OK: Got token
    I0628 08:22:14.519746       1 log.go:195] validating with in-cluster discovery
    I0628 08:22:14.520042       1 log.go:195] OK: got issuer https://kubernetes.default.svc.cluster.local
    I0628 08:22:14.520068       1 log.go:195] Full, not-validated claims: 
    openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-1886:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1687941133, NotBefore:1687940533, IssuedAt:1687940533, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-1886", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"799ee185-ddda-4ced-8bf8-7092c2d28222"}}}
    I0628 08:22:14.533828       1 log.go:195] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
    I0628 08:22:14.543927       1 log.go:195] OK: Validated signature on JWT
    I0628 08:22:14.544036       1 log.go:195] OK: Got valid claims from token!
    I0628 08:22:14.544070       1 log.go:195] Full, validated claims: 
    &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-1886:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1687941133, NotBefore:1687940533, IssuedAt:1687940533, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-1886", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"799ee185-ddda-4ced-8bf8-7092c2d28222"}}}

    Jun 28 08:22:47.787: INFO: completed pod
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Jun 28 08:22:47.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-1886" for this suite. 06/28/23 08:22:47.805
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:22:47.812
Jun 28 08:22:47.812: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename sysctl 06/28/23 08:22:47.813
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:22:47.827
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:22:47.831
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
STEP: Creating a pod with one valid and two invalid sysctls 06/28/23 08:22:47.836
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:187
Jun 28 08:22:47.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-5308" for this suite. 06/28/23 08:22:47.849
{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]","completed":196,"skipped":3366,"failed":0}
------------------------------
â€¢ [0.043 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:22:47.812
    Jun 28 08:22:47.812: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename sysctl 06/28/23 08:22:47.813
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:22:47.827
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:22:47.831
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:123
    STEP: Creating a pod with one valid and two invalid sysctls 06/28/23 08:22:47.836
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:187
    Jun 28 08:22:47.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sysctl-5308" for this suite. 06/28/23 08:22:47.849
  << End Captured GinkgoWriter Output
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:22:47.856
Jun 28 08:22:47.856: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename sched-preemption 06/28/23 08:22:47.856
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:22:47.868
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:22:47.872
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Jun 28 08:22:47.890: INFO: Waiting up to 1m0s for all nodes to be ready
Jun 28 08:23:47.937: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218
STEP: Create pods that use 4/5 of node resources. 06/28/23 08:23:47.944
Jun 28 08:23:47.966: INFO: Created pod: pod0-0-sched-preemption-low-priority
Jun 28 08:23:47.972: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Jun 28 08:23:47.987: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Jun 28 08:23:47.992: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Jun 28 08:23:48.008: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Jun 28 08:23:48.013: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 06/28/23 08:23:48.013
Jun 28 08:23:48.017: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-4134" to be "running"
Jun 28 08:23:48.026: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 9.390087ms
Jun 28 08:23:50.032: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.015382758s
Jun 28 08:23:50.032: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Jun 28 08:23:50.032: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-4134" to be "running"
Jun 28 08:23:50.036: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.124727ms
Jun 28 08:23:50.036: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Jun 28 08:23:50.036: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-4134" to be "running"
Jun 28 08:23:50.040: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.543223ms
Jun 28 08:23:50.040: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Jun 28 08:23:50.040: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-4134" to be "running"
Jun 28 08:23:50.044: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.773273ms
Jun 28 08:23:50.044: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
Jun 28 08:23:50.044: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-4134" to be "running"
Jun 28 08:23:50.049: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.99658ms
Jun 28 08:23:50.049: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
Jun 28 08:23:50.049: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-4134" to be "running"
Jun 28 08:23:50.052: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.628349ms
Jun 28 08:23:50.052: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a critical pod that use same resources as that of a lower priority pod 06/28/23 08:23:50.052
Jun 28 08:23:50.062: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
Jun 28 08:23:50.066: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.587326ms
Jun 28 08:23:52.071: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008369216s
Jun 28 08:23:54.071: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.008989334s
Jun 28 08:23:54.071: INFO: Pod "critical-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Jun 28 08:23:54.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-4134" for this suite. 06/28/23 08:23:54.118
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","completed":197,"skipped":3366,"failed":0}
------------------------------
â€¢ [SLOW TEST] [66.318 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:22:47.856
    Jun 28 08:22:47.856: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename sched-preemption 06/28/23 08:22:47.856
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:22:47.868
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:22:47.872
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Jun 28 08:22:47.890: INFO: Waiting up to 1m0s for all nodes to be ready
    Jun 28 08:23:47.937: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates lower priority pod preemption by critical pod [Conformance]
      test/e2e/scheduling/preemption.go:218
    STEP: Create pods that use 4/5 of node resources. 06/28/23 08:23:47.944
    Jun 28 08:23:47.966: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Jun 28 08:23:47.972: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Jun 28 08:23:47.987: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Jun 28 08:23:47.992: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    Jun 28 08:23:48.008: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    Jun 28 08:23:48.013: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 06/28/23 08:23:48.013
    Jun 28 08:23:48.017: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-4134" to be "running"
    Jun 28 08:23:48.026: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 9.390087ms
    Jun 28 08:23:50.032: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.015382758s
    Jun 28 08:23:50.032: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Jun 28 08:23:50.032: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-4134" to be "running"
    Jun 28 08:23:50.036: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.124727ms
    Jun 28 08:23:50.036: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Jun 28 08:23:50.036: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-4134" to be "running"
    Jun 28 08:23:50.040: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.543223ms
    Jun 28 08:23:50.040: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Jun 28 08:23:50.040: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-4134" to be "running"
    Jun 28 08:23:50.044: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.773273ms
    Jun 28 08:23:50.044: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    Jun 28 08:23:50.044: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-4134" to be "running"
    Jun 28 08:23:50.049: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.99658ms
    Jun 28 08:23:50.049: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    Jun 28 08:23:50.049: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-4134" to be "running"
    Jun 28 08:23:50.052: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.628349ms
    Jun 28 08:23:50.052: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a critical pod that use same resources as that of a lower priority pod 06/28/23 08:23:50.052
    Jun 28 08:23:50.062: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
    Jun 28 08:23:50.066: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.587326ms
    Jun 28 08:23:52.071: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008369216s
    Jun 28 08:23:54.071: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.008989334s
    Jun 28 08:23:54.071: INFO: Pod "critical-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Jun 28 08:23:54.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-4134" for this suite. 06/28/23 08:23:54.118
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:23:54.174
Jun 28 08:23:54.174: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename webhook 06/28/23 08:23:54.176
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:23:54.188
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:23:54.192
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 06/28/23 08:23:54.208
STEP: Create role binding to let webhook read extension-apiserver-authentication 06/28/23 08:23:54.556
STEP: Deploying the webhook pod 06/28/23 08:23:54.563
STEP: Wait for the deployment to be ready 06/28/23 08:23:54.575
Jun 28 08:23:54.586: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 06/28/23 08:23:56.602
STEP: Verifying the service has paired with the endpoint 06/28/23 08:23:56.612
Jun 28 08:23:57.613: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581
STEP: Listing all of the created validation webhooks 06/28/23 08:23:57.671
STEP: Creating a configMap that does not comply to the validation webhook rules 06/28/23 08:23:57.718
STEP: Deleting the collection of validation webhooks 06/28/23 08:23:57.756
STEP: Creating a configMap that does not comply to the validation webhook rules 06/28/23 08:23:57.801
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jun 28 08:23:57.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9099" for this suite. 06/28/23 08:23:57.82
STEP: Destroying namespace "webhook-9099-markers" for this suite. 06/28/23 08:23:57.828
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","completed":198,"skipped":3373,"failed":0}
------------------------------
â€¢ [3.697 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:23:54.174
    Jun 28 08:23:54.174: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename webhook 06/28/23 08:23:54.176
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:23:54.188
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:23:54.192
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 06/28/23 08:23:54.208
    STEP: Create role binding to let webhook read extension-apiserver-authentication 06/28/23 08:23:54.556
    STEP: Deploying the webhook pod 06/28/23 08:23:54.563
    STEP: Wait for the deployment to be ready 06/28/23 08:23:54.575
    Jun 28 08:23:54.586: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 06/28/23 08:23:56.602
    STEP: Verifying the service has paired with the endpoint 06/28/23 08:23:56.612
    Jun 28 08:23:57.613: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing validating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:581
    STEP: Listing all of the created validation webhooks 06/28/23 08:23:57.671
    STEP: Creating a configMap that does not comply to the validation webhook rules 06/28/23 08:23:57.718
    STEP: Deleting the collection of validation webhooks 06/28/23 08:23:57.756
    STEP: Creating a configMap that does not comply to the validation webhook rules 06/28/23 08:23:57.801
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jun 28 08:23:57.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-9099" for this suite. 06/28/23 08:23:57.82
    STEP: Destroying namespace "webhook-9099-markers" for this suite. 06/28/23 08:23:57.828
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:23:57.876
Jun 28 08:23:57.876: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename emptydir 06/28/23 08:23:57.877
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:23:57.889
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:23:57.893
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86
STEP: Creating a pod to test emptydir volume type on tmpfs 06/28/23 08:23:57.897
Jun 28 08:23:57.905: INFO: Waiting up to 5m0s for pod "pod-c6189732-71df-4153-9eb6-e0b7784f8477" in namespace "emptydir-3505" to be "Succeeded or Failed"
Jun 28 08:23:57.909: INFO: Pod "pod-c6189732-71df-4153-9eb6-e0b7784f8477": Phase="Pending", Reason="", readiness=false. Elapsed: 4.519964ms
Jun 28 08:23:59.917: INFO: Pod "pod-c6189732-71df-4153-9eb6-e0b7784f8477": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0122369s
Jun 28 08:24:01.916: INFO: Pod "pod-c6189732-71df-4153-9eb6-e0b7784f8477": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011045098s
STEP: Saw pod success 06/28/23 08:24:01.916
Jun 28 08:24:01.916: INFO: Pod "pod-c6189732-71df-4153-9eb6-e0b7784f8477" satisfied condition "Succeeded or Failed"
Jun 28 08:24:01.921: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod pod-c6189732-71df-4153-9eb6-e0b7784f8477 container test-container: <nil>
STEP: delete the pod 06/28/23 08:24:01.933
Jun 28 08:24:01.947: INFO: Waiting for pod pod-c6189732-71df-4153-9eb6-e0b7784f8477 to disappear
Jun 28 08:24:01.952: INFO: Pod pod-c6189732-71df-4153-9eb6-e0b7784f8477 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jun 28 08:24:01.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3505" for this suite. 06/28/23 08:24:01.96
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","completed":199,"skipped":3398,"failed":0}
------------------------------
â€¢ [4.093 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:23:57.876
    Jun 28 08:23:57.876: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename emptydir 06/28/23 08:23:57.877
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:23:57.889
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:23:57.893
    [It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:86
    STEP: Creating a pod to test emptydir volume type on tmpfs 06/28/23 08:23:57.897
    Jun 28 08:23:57.905: INFO: Waiting up to 5m0s for pod "pod-c6189732-71df-4153-9eb6-e0b7784f8477" in namespace "emptydir-3505" to be "Succeeded or Failed"
    Jun 28 08:23:57.909: INFO: Pod "pod-c6189732-71df-4153-9eb6-e0b7784f8477": Phase="Pending", Reason="", readiness=false. Elapsed: 4.519964ms
    Jun 28 08:23:59.917: INFO: Pod "pod-c6189732-71df-4153-9eb6-e0b7784f8477": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0122369s
    Jun 28 08:24:01.916: INFO: Pod "pod-c6189732-71df-4153-9eb6-e0b7784f8477": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011045098s
    STEP: Saw pod success 06/28/23 08:24:01.916
    Jun 28 08:24:01.916: INFO: Pod "pod-c6189732-71df-4153-9eb6-e0b7784f8477" satisfied condition "Succeeded or Failed"
    Jun 28 08:24:01.921: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod pod-c6189732-71df-4153-9eb6-e0b7784f8477 container test-container: <nil>
    STEP: delete the pod 06/28/23 08:24:01.933
    Jun 28 08:24:01.947: INFO: Waiting for pod pod-c6189732-71df-4153-9eb6-e0b7784f8477 to disappear
    Jun 28 08:24:01.952: INFO: Pod pod-c6189732-71df-4153-9eb6-e0b7784f8477 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jun 28 08:24:01.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-3505" for this suite. 06/28/23 08:24:01.96
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:24:01.968
Jun 28 08:24:01.968: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename var-expansion 06/28/23 08:24:01.969
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:24:01.982
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:24:01.987
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91
STEP: Creating a pod to test substitution in container's args 06/28/23 08:24:01.993
Jun 28 08:24:02.003: INFO: Waiting up to 5m0s for pod "var-expansion-2cfab46e-7994-4765-a293-d84fe2068a7b" in namespace "var-expansion-4229" to be "Succeeded or Failed"
Jun 28 08:24:02.008: INFO: Pod "var-expansion-2cfab46e-7994-4765-a293-d84fe2068a7b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.201878ms
Jun 28 08:24:04.014: INFO: Pod "var-expansion-2cfab46e-7994-4765-a293-d84fe2068a7b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011150937s
Jun 28 08:24:06.013: INFO: Pod "var-expansion-2cfab46e-7994-4765-a293-d84fe2068a7b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010228513s
STEP: Saw pod success 06/28/23 08:24:06.013
Jun 28 08:24:06.014: INFO: Pod "var-expansion-2cfab46e-7994-4765-a293-d84fe2068a7b" satisfied condition "Succeeded or Failed"
Jun 28 08:24:06.018: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod var-expansion-2cfab46e-7994-4765-a293-d84fe2068a7b container dapi-container: <nil>
STEP: delete the pod 06/28/23 08:24:06.028
Jun 28 08:24:06.039: INFO: Waiting for pod var-expansion-2cfab46e-7994-4765-a293-d84fe2068a7b to disappear
Jun 28 08:24:06.043: INFO: Pod var-expansion-2cfab46e-7994-4765-a293-d84fe2068a7b no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jun 28 08:24:06.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4229" for this suite. 06/28/23 08:24:06.05
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","completed":200,"skipped":3401,"failed":0}
------------------------------
â€¢ [4.089 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:24:01.968
    Jun 28 08:24:01.968: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename var-expansion 06/28/23 08:24:01.969
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:24:01.982
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:24:01.987
    [It] should allow substituting values in a container's args [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:91
    STEP: Creating a pod to test substitution in container's args 06/28/23 08:24:01.993
    Jun 28 08:24:02.003: INFO: Waiting up to 5m0s for pod "var-expansion-2cfab46e-7994-4765-a293-d84fe2068a7b" in namespace "var-expansion-4229" to be "Succeeded or Failed"
    Jun 28 08:24:02.008: INFO: Pod "var-expansion-2cfab46e-7994-4765-a293-d84fe2068a7b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.201878ms
    Jun 28 08:24:04.014: INFO: Pod "var-expansion-2cfab46e-7994-4765-a293-d84fe2068a7b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011150937s
    Jun 28 08:24:06.013: INFO: Pod "var-expansion-2cfab46e-7994-4765-a293-d84fe2068a7b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010228513s
    STEP: Saw pod success 06/28/23 08:24:06.013
    Jun 28 08:24:06.014: INFO: Pod "var-expansion-2cfab46e-7994-4765-a293-d84fe2068a7b" satisfied condition "Succeeded or Failed"
    Jun 28 08:24:06.018: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod var-expansion-2cfab46e-7994-4765-a293-d84fe2068a7b container dapi-container: <nil>
    STEP: delete the pod 06/28/23 08:24:06.028
    Jun 28 08:24:06.039: INFO: Waiting for pod var-expansion-2cfab46e-7994-4765-a293-d84fe2068a7b to disappear
    Jun 28 08:24:06.043: INFO: Pod var-expansion-2cfab46e-7994-4765-a293-d84fe2068a7b no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jun 28 08:24:06.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-4229" for this suite. 06/28/23 08:24:06.05
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:24:06.058
Jun 28 08:24:06.058: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename job 06/28/23 08:24:06.059
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:24:06.073
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:24:06.077
[It] should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335
STEP: Creating a job 06/28/23 08:24:06.082
STEP: Ensuring active pods == parallelism 06/28/23 08:24:06.088
STEP: Orphaning one of the Job's Pods 06/28/23 08:24:08.094
Jun 28 08:24:08.615: INFO: Successfully updated pod "adopt-release-tbrxf"
STEP: Checking that the Job readopts the Pod 06/28/23 08:24:08.615
Jun 28 08:24:08.615: INFO: Waiting up to 15m0s for pod "adopt-release-tbrxf" in namespace "job-6314" to be "adopted"
Jun 28 08:24:08.619: INFO: Pod "adopt-release-tbrxf": Phase="Running", Reason="", readiness=true. Elapsed: 3.913837ms
Jun 28 08:24:10.645: INFO: Pod "adopt-release-tbrxf": Phase="Running", Reason="", readiness=true. Elapsed: 2.029444444s
Jun 28 08:24:10.645: INFO: Pod "adopt-release-tbrxf" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod 06/28/23 08:24:10.645
Jun 28 08:24:11.159: INFO: Successfully updated pod "adopt-release-tbrxf"
STEP: Checking that the Job releases the Pod 06/28/23 08:24:11.159
Jun 28 08:24:11.159: INFO: Waiting up to 15m0s for pod "adopt-release-tbrxf" in namespace "job-6314" to be "released"
Jun 28 08:24:11.164: INFO: Pod "adopt-release-tbrxf": Phase="Running", Reason="", readiness=true. Elapsed: 4.5873ms
Jun 28 08:24:13.170: INFO: Pod "adopt-release-tbrxf": Phase="Running", Reason="", readiness=true. Elapsed: 2.010388335s
Jun 28 08:24:13.170: INFO: Pod "adopt-release-tbrxf" satisfied condition "released"
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Jun 28 08:24:13.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-6314" for this suite. 06/28/23 08:24:13.178
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","completed":201,"skipped":3428,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.128 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:24:06.058
    Jun 28 08:24:06.058: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename job 06/28/23 08:24:06.059
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:24:06.073
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:24:06.077
    [It] should adopt matching orphans and release non-matching pods [Conformance]
      test/e2e/apps/job.go:335
    STEP: Creating a job 06/28/23 08:24:06.082
    STEP: Ensuring active pods == parallelism 06/28/23 08:24:06.088
    STEP: Orphaning one of the Job's Pods 06/28/23 08:24:08.094
    Jun 28 08:24:08.615: INFO: Successfully updated pod "adopt-release-tbrxf"
    STEP: Checking that the Job readopts the Pod 06/28/23 08:24:08.615
    Jun 28 08:24:08.615: INFO: Waiting up to 15m0s for pod "adopt-release-tbrxf" in namespace "job-6314" to be "adopted"
    Jun 28 08:24:08.619: INFO: Pod "adopt-release-tbrxf": Phase="Running", Reason="", readiness=true. Elapsed: 3.913837ms
    Jun 28 08:24:10.645: INFO: Pod "adopt-release-tbrxf": Phase="Running", Reason="", readiness=true. Elapsed: 2.029444444s
    Jun 28 08:24:10.645: INFO: Pod "adopt-release-tbrxf" satisfied condition "adopted"
    STEP: Removing the labels from the Job's Pod 06/28/23 08:24:10.645
    Jun 28 08:24:11.159: INFO: Successfully updated pod "adopt-release-tbrxf"
    STEP: Checking that the Job releases the Pod 06/28/23 08:24:11.159
    Jun 28 08:24:11.159: INFO: Waiting up to 15m0s for pod "adopt-release-tbrxf" in namespace "job-6314" to be "released"
    Jun 28 08:24:11.164: INFO: Pod "adopt-release-tbrxf": Phase="Running", Reason="", readiness=true. Elapsed: 4.5873ms
    Jun 28 08:24:13.170: INFO: Pod "adopt-release-tbrxf": Phase="Running", Reason="", readiness=true. Elapsed: 2.010388335s
    Jun 28 08:24:13.170: INFO: Pod "adopt-release-tbrxf" satisfied condition "released"
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Jun 28 08:24:13.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-6314" for this suite. 06/28/23 08:24:13.178
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] Proxy version v1
  should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:24:13.186
Jun 28 08:24:13.186: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename proxy 06/28/23 08:24:13.187
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:24:13.199
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:24:13.204
[It] should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
STEP: starting an echo server on multiple ports 06/28/23 08:24:13.218
STEP: creating replication controller proxy-service-hqlww in namespace proxy-5560 06/28/23 08:24:13.218
I0628 08:24:13.232326      18 runners.go:193] Created replication controller with name: proxy-service-hqlww, namespace: proxy-5560, replica count: 1
I0628 08:24:14.283938      18 runners.go:193] proxy-service-hqlww Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0628 08:24:15.285008      18 runners.go:193] proxy-service-hqlww Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0628 08:24:16.285736      18 runners.go:193] proxy-service-hqlww Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 28 08:24:16.290: INFO: setup took 3.082066338s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts 06/28/23 08:24:16.29
Jun 28 08:24:16.301: INFO: (0) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:162/proxy/: bar (200; 10.501721ms)
Jun 28 08:24:16.311: INFO: (0) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:1080/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:1080/proxy/rewriteme">... (200; 20.46884ms)
Jun 28 08:24:16.311: INFO: (0) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:160/proxy/: foo (200; 20.23305ms)
Jun 28 08:24:16.311: INFO: (0) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl/proxy/rewriteme">test</a> (200; 20.463717ms)
Jun 28 08:24:16.311: INFO: (0) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:160/proxy/: foo (200; 20.187954ms)
Jun 28 08:24:16.311: INFO: (0) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:1080/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:1080/proxy/rewriteme">test<... (200; 20.309769ms)
Jun 28 08:24:16.311: INFO: (0) /api/v1/namespaces/proxy-5560/services/proxy-service-hqlww:portname2/proxy/: bar (200; 20.666161ms)
Jun 28 08:24:16.311: INFO: (0) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:162/proxy/: bar (200; 20.475531ms)
Jun 28 08:24:16.311: INFO: (0) /api/v1/namespaces/proxy-5560/services/http:proxy-service-hqlww:portname2/proxy/: bar (200; 20.272729ms)
Jun 28 08:24:16.313: INFO: (0) /api/v1/namespaces/proxy-5560/services/http:proxy-service-hqlww:portname1/proxy/: foo (200; 22.883788ms)
Jun 28 08:24:16.313: INFO: (0) /api/v1/namespaces/proxy-5560/services/proxy-service-hqlww:portname1/proxy/: foo (200; 22.825187ms)
Jun 28 08:24:16.313: INFO: (0) /api/v1/namespaces/proxy-5560/services/https:proxy-service-hqlww:tlsportname1/proxy/: tls baz (200; 22.808176ms)
Jun 28 08:24:16.313: INFO: (0) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:460/proxy/: tls baz (200; 22.985577ms)
Jun 28 08:24:16.314: INFO: (0) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:462/proxy/: tls qux (200; 22.900418ms)
Jun 28 08:24:16.314: INFO: (0) /api/v1/namespaces/proxy-5560/services/https:proxy-service-hqlww:tlsportname2/proxy/: tls qux (200; 23.182615ms)
Jun 28 08:24:16.441: INFO: (0) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:443/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:443/proxy/tlsrewritem... (200; 150.934426ms)
Jun 28 08:24:16.451: INFO: (1) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:160/proxy/: foo (200; 8.814693ms)
Jun 28 08:24:16.451: INFO: (1) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl/proxy/rewriteme">test</a> (200; 8.839094ms)
Jun 28 08:24:16.451: INFO: (1) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:1080/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:1080/proxy/rewriteme">... (200; 9.441625ms)
Jun 28 08:24:16.451: INFO: (1) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:1080/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:1080/proxy/rewriteme">test<... (200; 9.011237ms)
Jun 28 08:24:16.451: INFO: (1) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:162/proxy/: bar (200; 9.082735ms)
Jun 28 08:24:16.451: INFO: (1) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:460/proxy/: tls baz (200; 9.163091ms)
Jun 28 08:24:16.452: INFO: (1) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:160/proxy/: foo (200; 9.991131ms)
Jun 28 08:24:16.452: INFO: (1) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:443/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:443/proxy/tlsrewritem... (200; 10.121125ms)
Jun 28 08:24:16.453: INFO: (1) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:162/proxy/: bar (200; 10.532954ms)
Jun 28 08:24:16.453: INFO: (1) /api/v1/namespaces/proxy-5560/services/https:proxy-service-hqlww:tlsportname1/proxy/: tls baz (200; 10.752066ms)
Jun 28 08:24:16.453: INFO: (1) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:462/proxy/: tls qux (200; 10.555755ms)
Jun 28 08:24:16.453: INFO: (1) /api/v1/namespaces/proxy-5560/services/https:proxy-service-hqlww:tlsportname2/proxy/: tls qux (200; 10.948467ms)
Jun 28 08:24:16.454: INFO: (1) /api/v1/namespaces/proxy-5560/services/http:proxy-service-hqlww:portname1/proxy/: foo (200; 12.021521ms)
Jun 28 08:24:16.454: INFO: (1) /api/v1/namespaces/proxy-5560/services/proxy-service-hqlww:portname1/proxy/: foo (200; 12.035403ms)
Jun 28 08:24:16.454: INFO: (1) /api/v1/namespaces/proxy-5560/services/http:proxy-service-hqlww:portname2/proxy/: bar (200; 12.403185ms)
Jun 28 08:24:16.455: INFO: (1) /api/v1/namespaces/proxy-5560/services/proxy-service-hqlww:portname2/proxy/: bar (200; 13.029339ms)
Jun 28 08:24:16.462: INFO: (2) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:1080/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:1080/proxy/rewriteme">test<... (200; 6.375247ms)
Jun 28 08:24:16.462: INFO: (2) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:160/proxy/: foo (200; 6.301345ms)
Jun 28 08:24:16.462: INFO: (2) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:162/proxy/: bar (200; 6.386647ms)
Jun 28 08:24:16.462: INFO: (2) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:462/proxy/: tls qux (200; 6.716765ms)
Jun 28 08:24:16.462: INFO: (2) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:1080/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:1080/proxy/rewriteme">... (200; 6.515237ms)
Jun 28 08:24:16.462: INFO: (2) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:160/proxy/: foo (200; 6.648119ms)
Jun 28 08:24:16.462: INFO: (2) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:162/proxy/: bar (200; 7.130012ms)
Jun 28 08:24:16.462: INFO: (2) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:460/proxy/: tls baz (200; 7.090587ms)
Jun 28 08:24:16.462: INFO: (2) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl/proxy/rewriteme">test</a> (200; 6.968814ms)
Jun 28 08:24:16.463: INFO: (2) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:443/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:443/proxy/tlsrewritem... (200; 7.150753ms)
Jun 28 08:24:16.464: INFO: (2) /api/v1/namespaces/proxy-5560/services/https:proxy-service-hqlww:tlsportname1/proxy/: tls baz (200; 8.246987ms)
Jun 28 08:24:16.464: INFO: (2) /api/v1/namespaces/proxy-5560/services/https:proxy-service-hqlww:tlsportname2/proxy/: tls qux (200; 8.287838ms)
Jun 28 08:24:16.465: INFO: (2) /api/v1/namespaces/proxy-5560/services/http:proxy-service-hqlww:portname2/proxy/: bar (200; 9.608772ms)
Jun 28 08:24:16.465: INFO: (2) /api/v1/namespaces/proxy-5560/services/http:proxy-service-hqlww:portname1/proxy/: foo (200; 9.46464ms)
Jun 28 08:24:16.465: INFO: (2) /api/v1/namespaces/proxy-5560/services/proxy-service-hqlww:portname1/proxy/: foo (200; 9.418449ms)
Jun 28 08:24:16.466: INFO: (2) /api/v1/namespaces/proxy-5560/services/proxy-service-hqlww:portname2/proxy/: bar (200; 10.122098ms)
Jun 28 08:24:16.473: INFO: (3) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:462/proxy/: tls qux (200; 7.023347ms)
Jun 28 08:24:16.473: INFO: (3) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:160/proxy/: foo (200; 7.174008ms)
Jun 28 08:24:16.473: INFO: (3) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:460/proxy/: tls baz (200; 7.529482ms)
Jun 28 08:24:16.473: INFO: (3) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:162/proxy/: bar (200; 7.222355ms)
Jun 28 08:24:16.473: INFO: (3) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:160/proxy/: foo (200; 7.429676ms)
Jun 28 08:24:16.473: INFO: (3) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl/proxy/rewriteme">test</a> (200; 7.355834ms)
Jun 28 08:24:16.473: INFO: (3) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:1080/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:1080/proxy/rewriteme">... (200; 7.342064ms)
Jun 28 08:24:16.474: INFO: (3) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:162/proxy/: bar (200; 7.81254ms)
Jun 28 08:24:16.474: INFO: (3) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:443/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:443/proxy/tlsrewritem... (200; 7.857267ms)
Jun 28 08:24:16.474: INFO: (3) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:1080/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:1080/proxy/rewriteme">test<... (200; 8.003615ms)
Jun 28 08:24:16.476: INFO: (3) /api/v1/namespaces/proxy-5560/services/https:proxy-service-hqlww:tlsportname1/proxy/: tls baz (200; 10.079033ms)
Jun 28 08:24:16.476: INFO: (3) /api/v1/namespaces/proxy-5560/services/http:proxy-service-hqlww:portname1/proxy/: foo (200; 10.046904ms)
Jun 28 08:24:16.476: INFO: (3) /api/v1/namespaces/proxy-5560/services/http:proxy-service-hqlww:portname2/proxy/: bar (200; 10.335677ms)
Jun 28 08:24:16.476: INFO: (3) /api/v1/namespaces/proxy-5560/services/proxy-service-hqlww:portname1/proxy/: foo (200; 10.296308ms)
Jun 28 08:24:16.476: INFO: (3) /api/v1/namespaces/proxy-5560/services/https:proxy-service-hqlww:tlsportname2/proxy/: tls qux (200; 10.19573ms)
Jun 28 08:24:16.477: INFO: (3) /api/v1/namespaces/proxy-5560/services/proxy-service-hqlww:portname2/proxy/: bar (200; 11.111189ms)
Jun 28 08:24:16.483: INFO: (4) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:460/proxy/: tls baz (200; 6.354796ms)
Jun 28 08:24:16.486: INFO: (4) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:160/proxy/: foo (200; 8.86261ms)
Jun 28 08:24:16.486: INFO: (4) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:462/proxy/: tls qux (200; 8.875526ms)
Jun 28 08:24:16.486: INFO: (4) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:1080/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:1080/proxy/rewriteme">... (200; 8.737889ms)
Jun 28 08:24:16.486: INFO: (4) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:1080/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:1080/proxy/rewriteme">test<... (200; 8.894559ms)
Jun 28 08:24:16.486: INFO: (4) /api/v1/namespaces/proxy-5560/services/http:proxy-service-hqlww:portname2/proxy/: bar (200; 9.22837ms)
Jun 28 08:24:16.486: INFO: (4) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:160/proxy/: foo (200; 8.89995ms)
Jun 28 08:24:16.486: INFO: (4) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:162/proxy/: bar (200; 8.88527ms)
Jun 28 08:24:16.487: INFO: (4) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl/proxy/rewriteme">test</a> (200; 9.168283ms)
Jun 28 08:24:16.487: INFO: (4) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:443/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:443/proxy/tlsrewritem... (200; 9.756209ms)
Jun 28 08:24:16.487: INFO: (4) /api/v1/namespaces/proxy-5560/services/https:proxy-service-hqlww:tlsportname2/proxy/: tls qux (200; 9.961265ms)
Jun 28 08:24:16.488: INFO: (4) /api/v1/namespaces/proxy-5560/services/https:proxy-service-hqlww:tlsportname1/proxy/: tls baz (200; 10.597611ms)
Jun 28 08:24:16.489: INFO: (4) /api/v1/namespaces/proxy-5560/services/proxy-service-hqlww:portname1/proxy/: foo (200; 11.799832ms)
Jun 28 08:24:16.489: INFO: (4) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:162/proxy/: bar (200; 11.604012ms)
Jun 28 08:24:16.489: INFO: (4) /api/v1/namespaces/proxy-5560/services/proxy-service-hqlww:portname2/proxy/: bar (200; 11.79036ms)
Jun 28 08:24:16.489: INFO: (4) /api/v1/namespaces/proxy-5560/services/http:proxy-service-hqlww:portname1/proxy/: foo (200; 11.600487ms)
Jun 28 08:24:16.497: INFO: (5) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:443/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:443/proxy/tlsrewritem... (200; 7.487687ms)
Jun 28 08:24:16.497: INFO: (5) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:160/proxy/: foo (200; 7.585228ms)
Jun 28 08:24:16.497: INFO: (5) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:162/proxy/: bar (200; 7.705042ms)
Jun 28 08:24:16.497: INFO: (5) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl/proxy/rewriteme">test</a> (200; 7.552973ms)
Jun 28 08:24:16.497: INFO: (5) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:1080/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:1080/proxy/rewriteme">... (200; 7.553729ms)
Jun 28 08:24:16.498: INFO: (5) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:462/proxy/: tls qux (200; 8.386354ms)
Jun 28 08:24:16.498: INFO: (5) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:1080/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:1080/proxy/rewriteme">test<... (200; 8.817699ms)
Jun 28 08:24:16.498: INFO: (5) /api/v1/namespaces/proxy-5560/services/https:proxy-service-hqlww:tlsportname1/proxy/: tls baz (200; 8.938647ms)
Jun 28 08:24:16.498: INFO: (5) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:460/proxy/: tls baz (200; 8.464913ms)
Jun 28 08:24:16.498: INFO: (5) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:162/proxy/: bar (200; 8.765795ms)
Jun 28 08:24:16.498: INFO: (5) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:160/proxy/: foo (200; 8.653208ms)
Jun 28 08:24:16.541: INFO: (5) /api/v1/namespaces/proxy-5560/services/https:proxy-service-hqlww:tlsportname2/proxy/: tls qux (200; 51.296265ms)
Jun 28 08:24:16.541: INFO: (5) /api/v1/namespaces/proxy-5560/services/http:proxy-service-hqlww:portname1/proxy/: foo (200; 51.802983ms)
Jun 28 08:24:16.541: INFO: (5) /api/v1/namespaces/proxy-5560/services/http:proxy-service-hqlww:portname2/proxy/: bar (200; 51.687412ms)
Jun 28 08:24:16.541: INFO: (5) /api/v1/namespaces/proxy-5560/services/proxy-service-hqlww:portname1/proxy/: foo (200; 51.416185ms)
Jun 28 08:24:16.541: INFO: (5) /api/v1/namespaces/proxy-5560/services/proxy-service-hqlww:portname2/proxy/: bar (200; 51.542976ms)
Jun 28 08:24:16.550: INFO: (6) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:160/proxy/: foo (200; 8.916809ms)
Jun 28 08:24:16.550: INFO: (6) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:162/proxy/: bar (200; 9.026739ms)
Jun 28 08:24:16.550: INFO: (6) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl/proxy/rewriteme">test</a> (200; 8.830609ms)
Jun 28 08:24:16.550: INFO: (6) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:462/proxy/: tls qux (200; 8.856219ms)
Jun 28 08:24:16.550: INFO: (6) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:1080/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:1080/proxy/rewriteme">test<... (200; 9.015183ms)
Jun 28 08:24:16.551: INFO: (6) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:160/proxy/: foo (200; 9.612478ms)
Jun 28 08:24:16.551: INFO: (6) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:460/proxy/: tls baz (200; 10.211128ms)
Jun 28 08:24:16.551: INFO: (6) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:162/proxy/: bar (200; 10.271872ms)
Jun 28 08:24:16.551: INFO: (6) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:1080/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:1080/proxy/rewriteme">... (200; 10.571049ms)
Jun 28 08:24:16.551: INFO: (6) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:443/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:443/proxy/tlsrewritem... (200; 10.477384ms)
Jun 28 08:24:16.552: INFO: (6) /api/v1/namespaces/proxy-5560/services/https:proxy-service-hqlww:tlsportname2/proxy/: tls qux (200; 10.938301ms)
Jun 28 08:24:16.552: INFO: (6) /api/v1/namespaces/proxy-5560/services/https:proxy-service-hqlww:tlsportname1/proxy/: tls baz (200; 11.364481ms)
Jun 28 08:24:16.553: INFO: (6) /api/v1/namespaces/proxy-5560/services/http:proxy-service-hqlww:portname1/proxy/: foo (200; 12.217952ms)
Jun 28 08:24:16.553: INFO: (6) /api/v1/namespaces/proxy-5560/services/http:proxy-service-hqlww:portname2/proxy/: bar (200; 12.292847ms)
Jun 28 08:24:16.553: INFO: (6) /api/v1/namespaces/proxy-5560/services/proxy-service-hqlww:portname1/proxy/: foo (200; 12.125883ms)
Jun 28 08:24:16.554: INFO: (6) /api/v1/namespaces/proxy-5560/services/proxy-service-hqlww:portname2/proxy/: bar (200; 13.284844ms)
Jun 28 08:24:16.563: INFO: (7) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:162/proxy/: bar (200; 8.915658ms)
Jun 28 08:24:16.563: INFO: (7) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:162/proxy/: bar (200; 9.096895ms)
Jun 28 08:24:16.563: INFO: (7) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:462/proxy/: tls qux (200; 8.639151ms)
Jun 28 08:24:16.564: INFO: (7) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:160/proxy/: foo (200; 8.871044ms)
Jun 28 08:24:16.564: INFO: (7) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:1080/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:1080/proxy/rewriteme">... (200; 8.828924ms)
Jun 28 08:24:16.564: INFO: (7) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:1080/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:1080/proxy/rewriteme">test<... (200; 8.735025ms)
Jun 28 08:24:16.564: INFO: (7) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:443/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:443/proxy/tlsrewritem... (200; 9.052176ms)
Jun 28 08:24:16.564: INFO: (7) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:460/proxy/: tls baz (200; 9.092716ms)
Jun 28 08:24:16.564: INFO: (7) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl/proxy/rewriteme">test</a> (200; 8.973638ms)
Jun 28 08:24:16.565: INFO: (7) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:160/proxy/: foo (200; 9.885007ms)
Jun 28 08:24:16.565: INFO: (7) /api/v1/namespaces/proxy-5560/services/http:proxy-service-hqlww:portname2/proxy/: bar (200; 10.27763ms)
Jun 28 08:24:16.566: INFO: (7) /api/v1/namespaces/proxy-5560/services/https:proxy-service-hqlww:tlsportname2/proxy/: tls qux (200; 11.569026ms)
Jun 28 08:24:16.566: INFO: (7) /api/v1/namespaces/proxy-5560/services/https:proxy-service-hqlww:tlsportname1/proxy/: tls baz (200; 11.522644ms)
Jun 28 08:24:16.566: INFO: (7) /api/v1/namespaces/proxy-5560/services/proxy-service-hqlww:portname2/proxy/: bar (200; 11.735606ms)
Jun 28 08:24:16.566: INFO: (7) /api/v1/namespaces/proxy-5560/services/proxy-service-hqlww:portname1/proxy/: foo (200; 11.923176ms)
Jun 28 08:24:16.567: INFO: (7) /api/v1/namespaces/proxy-5560/services/http:proxy-service-hqlww:portname1/proxy/: foo (200; 12.809576ms)
Jun 28 08:24:16.575: INFO: (8) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:160/proxy/: foo (200; 6.999045ms)
Jun 28 08:24:16.575: INFO: (8) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:460/proxy/: tls baz (200; 7.293656ms)
Jun 28 08:24:16.575: INFO: (8) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:443/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:443/proxy/tlsrewritem... (200; 6.0273ms)
Jun 28 08:24:16.575: INFO: (8) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl/proxy/rewriteme">test</a> (200; 6.820424ms)
Jun 28 08:24:16.575: INFO: (8) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:462/proxy/: tls qux (200; 7.028341ms)
Jun 28 08:24:16.575: INFO: (8) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:1080/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:1080/proxy/rewriteme">test<... (200; 6.408582ms)
Jun 28 08:24:16.575: INFO: (8) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:1080/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:1080/proxy/rewriteme">... (200; 6.775634ms)
Jun 28 08:24:16.576: INFO: (8) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:162/proxy/: bar (200; 7.549663ms)
Jun 28 08:24:16.576: INFO: (8) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:160/proxy/: foo (200; 7.135731ms)
Jun 28 08:24:16.576: INFO: (8) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:162/proxy/: bar (200; 7.336358ms)
Jun 28 08:24:16.578: INFO: (8) /api/v1/namespaces/proxy-5560/services/https:proxy-service-hqlww:tlsportname2/proxy/: tls qux (200; 9.80997ms)
Jun 28 08:24:16.579: INFO: (8) /api/v1/namespaces/proxy-5560/services/https:proxy-service-hqlww:tlsportname1/proxy/: tls baz (200; 11.122237ms)
Jun 28 08:24:16.579: INFO: (8) /api/v1/namespaces/proxy-5560/services/proxy-service-hqlww:portname2/proxy/: bar (200; 11.600916ms)
Jun 28 08:24:16.579: INFO: (8) /api/v1/namespaces/proxy-5560/services/proxy-service-hqlww:portname1/proxy/: foo (200; 11.680666ms)
Jun 28 08:24:16.579: INFO: (8) /api/v1/namespaces/proxy-5560/services/http:proxy-service-hqlww:portname2/proxy/: bar (200; 11.204719ms)
Jun 28 08:24:16.580: INFO: (8) /api/v1/namespaces/proxy-5560/services/http:proxy-service-hqlww:portname1/proxy/: foo (200; 12.106677ms)
Jun 28 08:24:16.588: INFO: (9) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:1080/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:1080/proxy/rewriteme">... (200; 7.10339ms)
Jun 28 08:24:16.589: INFO: (9) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl/proxy/rewriteme">test</a> (200; 8.753168ms)
Jun 28 08:24:16.589: INFO: (9) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:160/proxy/: foo (200; 8.193452ms)
Jun 28 08:24:16.589: INFO: (9) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:162/proxy/: bar (200; 8.61061ms)
Jun 28 08:24:16.589: INFO: (9) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:1080/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:1080/proxy/rewriteme">test<... (200; 8.538999ms)
Jun 28 08:24:16.589: INFO: (9) /api/v1/namespaces/proxy-5560/services/proxy-service-hqlww:portname1/proxy/: foo (200; 8.981803ms)
Jun 28 08:24:16.591: INFO: (9) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:162/proxy/: bar (200; 9.638205ms)
Jun 28 08:24:16.591: INFO: (9) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:462/proxy/: tls qux (200; 9.376872ms)
Jun 28 08:24:16.591: INFO: (9) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:460/proxy/: tls baz (200; 9.590147ms)
Jun 28 08:24:16.591: INFO: (9) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:443/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:443/proxy/tlsrewritem... (200; 9.6407ms)
Jun 28 08:24:16.592: INFO: (9) /api/v1/namespaces/proxy-5560/services/https:proxy-service-hqlww:tlsportname2/proxy/: tls qux (200; 10.853576ms)
Jun 28 08:24:16.593: INFO: (9) /api/v1/namespaces/proxy-5560/services/https:proxy-service-hqlww:tlsportname1/proxy/: tls baz (200; 11.809247ms)
Jun 28 08:24:16.593: INFO: (9) /api/v1/namespaces/proxy-5560/services/http:proxy-service-hqlww:portname2/proxy/: bar (200; 11.747753ms)
Jun 28 08:24:16.593: INFO: (9) /api/v1/namespaces/proxy-5560/services/http:proxy-service-hqlww:portname1/proxy/: foo (200; 11.51235ms)
Jun 28 08:24:16.593: INFO: (9) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:160/proxy/: foo (200; 11.754015ms)
Jun 28 08:24:16.594: INFO: (9) /api/v1/namespaces/proxy-5560/services/proxy-service-hqlww:portname2/proxy/: bar (200; 12.699808ms)
Jun 28 08:24:16.601: INFO: (10) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:160/proxy/: foo (200; 7.294863ms)
Jun 28 08:24:16.601: INFO: (10) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:160/proxy/: foo (200; 7.364322ms)
Jun 28 08:24:16.601: INFO: (10) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:443/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:443/proxy/tlsrewritem... (200; 7.232656ms)
Jun 28 08:24:16.601: INFO: (10) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:162/proxy/: bar (200; 7.569517ms)
Jun 28 08:24:16.601: INFO: (10) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl/proxy/rewriteme">test</a> (200; 7.03818ms)
Jun 28 08:24:16.601: INFO: (10) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:462/proxy/: tls qux (200; 7.329933ms)
Jun 28 08:24:16.602: INFO: (10) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:1080/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:1080/proxy/rewriteme">test<... (200; 7.598953ms)
Jun 28 08:24:16.603: INFO: (10) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:162/proxy/: bar (200; 8.966634ms)
Jun 28 08:24:16.603: INFO: (10) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:1080/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:1080/proxy/rewriteme">... (200; 8.857852ms)
Jun 28 08:24:16.603: INFO: (10) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:460/proxy/: tls baz (200; 8.931248ms)
Jun 28 08:24:16.647: INFO: (10) /api/v1/namespaces/proxy-5560/services/https:proxy-service-hqlww:tlsportname1/proxy/: tls baz (200; 52.134167ms)
Jun 28 08:24:16.647: INFO: (10) /api/v1/namespaces/proxy-5560/services/https:proxy-service-hqlww:tlsportname2/proxy/: tls qux (200; 52.217391ms)
Jun 28 08:24:16.647: INFO: (10) /api/v1/namespaces/proxy-5560/services/http:proxy-service-hqlww:portname2/proxy/: bar (200; 52.236547ms)
Jun 28 08:24:16.647: INFO: (10) /api/v1/namespaces/proxy-5560/services/proxy-service-hqlww:portname1/proxy/: foo (200; 52.329123ms)
Jun 28 08:24:16.647: INFO: (10) /api/v1/namespaces/proxy-5560/services/proxy-service-hqlww:portname2/proxy/: bar (200; 52.71785ms)
Jun 28 08:24:16.647: INFO: (10) /api/v1/namespaces/proxy-5560/services/http:proxy-service-hqlww:portname1/proxy/: foo (200; 52.65391ms)
Jun 28 08:24:16.656: INFO: (11) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:162/proxy/: bar (200; 9.416199ms)
Jun 28 08:24:16.656: INFO: (11) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:1080/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:1080/proxy/rewriteme">... (200; 8.901772ms)
Jun 28 08:24:16.656: INFO: (11) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:160/proxy/: foo (200; 9.004113ms)
Jun 28 08:24:16.656: INFO: (11) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:160/proxy/: foo (200; 9.222677ms)
Jun 28 08:24:16.657: INFO: (11) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:1080/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:1080/proxy/rewriteme">test<... (200; 9.893888ms)
Jun 28 08:24:16.657: INFO: (11) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:462/proxy/: tls qux (200; 9.875922ms)
Jun 28 08:24:16.657: INFO: (11) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:460/proxy/: tls baz (200; 9.992588ms)
Jun 28 08:24:16.657: INFO: (11) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl/proxy/rewriteme">test</a> (200; 9.97599ms)
Jun 28 08:24:16.657: INFO: (11) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:162/proxy/: bar (200; 10.189196ms)
Jun 28 08:24:16.657: INFO: (11) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:443/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:443/proxy/tlsrewritem... (200; 10.191377ms)
Jun 28 08:24:16.658: INFO: (11) /api/v1/namespaces/proxy-5560/services/proxy-service-hqlww:portname1/proxy/: foo (200; 10.978604ms)
Jun 28 08:24:16.659: INFO: (11) /api/v1/namespaces/proxy-5560/services/https:proxy-service-hqlww:tlsportname1/proxy/: tls baz (200; 12.0913ms)
Jun 28 08:24:16.659: INFO: (11) /api/v1/namespaces/proxy-5560/services/http:proxy-service-hqlww:portname1/proxy/: foo (200; 12.431298ms)
Jun 28 08:24:16.660: INFO: (11) /api/v1/namespaces/proxy-5560/services/http:proxy-service-hqlww:portname2/proxy/: bar (200; 12.34359ms)
Jun 28 08:24:16.660: INFO: (11) /api/v1/namespaces/proxy-5560/services/https:proxy-service-hqlww:tlsportname2/proxy/: tls qux (200; 12.461688ms)
Jun 28 08:24:16.660: INFO: (11) /api/v1/namespaces/proxy-5560/services/proxy-service-hqlww:portname2/proxy/: bar (200; 13.200139ms)
Jun 28 08:24:16.668: INFO: (12) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:1080/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:1080/proxy/rewriteme">test<... (200; 7.343355ms)
Jun 28 08:24:16.668: INFO: (12) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl/proxy/rewriteme">test</a> (200; 7.732006ms)
Jun 28 08:24:16.668: INFO: (12) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:162/proxy/: bar (200; 7.551929ms)
Jun 28 08:24:16.668: INFO: (12) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:160/proxy/: foo (200; 7.875411ms)
Jun 28 08:24:16.668: INFO: (12) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:460/proxy/: tls baz (200; 8.186029ms)
Jun 28 08:24:16.668: INFO: (12) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:1080/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:1080/proxy/rewriteme">... (200; 7.81379ms)
Jun 28 08:24:16.669: INFO: (12) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:160/proxy/: foo (200; 8.09746ms)
Jun 28 08:24:16.669: INFO: (12) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:443/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:443/proxy/tlsrewritem... (200; 8.160946ms)
Jun 28 08:24:16.669: INFO: (12) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:162/proxy/: bar (200; 8.225301ms)
Jun 28 08:24:16.669: INFO: (12) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:462/proxy/: tls qux (200; 8.6972ms)
Jun 28 08:24:16.671: INFO: (12) /api/v1/namespaces/proxy-5560/services/https:proxy-service-hqlww:tlsportname1/proxy/: tls baz (200; 10.542885ms)
Jun 28 08:24:16.671: INFO: (12) /api/v1/namespaces/proxy-5560/services/https:proxy-service-hqlww:tlsportname2/proxy/: tls qux (200; 10.735892ms)
Jun 28 08:24:16.671: INFO: (12) /api/v1/namespaces/proxy-5560/services/proxy-service-hqlww:portname2/proxy/: bar (200; 10.866845ms)
Jun 28 08:24:16.671: INFO: (12) /api/v1/namespaces/proxy-5560/services/proxy-service-hqlww:portname1/proxy/: foo (200; 10.931509ms)
Jun 28 08:24:16.672: INFO: (12) /api/v1/namespaces/proxy-5560/services/http:proxy-service-hqlww:portname1/proxy/: foo (200; 11.669522ms)
Jun 28 08:24:16.673: INFO: (12) /api/v1/namespaces/proxy-5560/services/http:proxy-service-hqlww:portname2/proxy/: bar (200; 12.072104ms)
Jun 28 08:24:16.681: INFO: (13) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:160/proxy/: foo (200; 7.451936ms)
Jun 28 08:24:16.681: INFO: (13) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:160/proxy/: foo (200; 7.756777ms)
Jun 28 08:24:16.681: INFO: (13) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:1080/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:1080/proxy/rewriteme">test<... (200; 7.835766ms)
Jun 28 08:24:16.681: INFO: (13) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:460/proxy/: tls baz (200; 7.999559ms)
Jun 28 08:24:16.681: INFO: (13) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:443/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:443/proxy/tlsrewritem... (200; 8.091578ms)
Jun 28 08:24:16.681: INFO: (13) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:162/proxy/: bar (200; 8.195296ms)
Jun 28 08:24:16.681: INFO: (13) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:162/proxy/: bar (200; 8.349986ms)
Jun 28 08:24:16.681: INFO: (13) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:1080/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:1080/proxy/rewriteme">... (200; 7.757743ms)
Jun 28 08:24:16.681: INFO: (13) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl/proxy/rewriteme">test</a> (200; 7.835115ms)
Jun 28 08:24:16.682: INFO: (13) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:462/proxy/: tls qux (200; 8.377384ms)
Jun 28 08:24:16.683: INFO: (13) /api/v1/namespaces/proxy-5560/services/proxy-service-hqlww:portname2/proxy/: bar (200; 9.307413ms)
Jun 28 08:24:16.683: INFO: (13) /api/v1/namespaces/proxy-5560/services/https:proxy-service-hqlww:tlsportname1/proxy/: tls baz (200; 8.927861ms)
Jun 28 08:24:16.683: INFO: (13) /api/v1/namespaces/proxy-5560/services/https:proxy-service-hqlww:tlsportname2/proxy/: tls qux (200; 9.187906ms)
Jun 28 08:24:16.684: INFO: (13) /api/v1/namespaces/proxy-5560/services/http:proxy-service-hqlww:portname2/proxy/: bar (200; 10.204209ms)
Jun 28 08:24:16.684: INFO: (13) /api/v1/namespaces/proxy-5560/services/proxy-service-hqlww:portname1/proxy/: foo (200; 10.657871ms)
Jun 28 08:24:16.684: INFO: (13) /api/v1/namespaces/proxy-5560/services/http:proxy-service-hqlww:portname1/proxy/: foo (200; 10.934553ms)
Jun 28 08:24:16.691: INFO: (14) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:162/proxy/: bar (200; 7.142478ms)
Jun 28 08:24:16.692: INFO: (14) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:162/proxy/: bar (200; 7.314343ms)
Jun 28 08:24:16.692: INFO: (14) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:160/proxy/: foo (200; 7.405873ms)
Jun 28 08:24:16.692: INFO: (14) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:1080/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:1080/proxy/rewriteme">test<... (200; 7.482996ms)
Jun 28 08:24:16.692: INFO: (14) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:462/proxy/: tls qux (200; 6.917885ms)
Jun 28 08:24:16.692: INFO: (14) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:160/proxy/: foo (200; 6.992827ms)
Jun 28 08:24:16.692: INFO: (14) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:460/proxy/: tls baz (200; 7.874024ms)
Jun 28 08:24:16.692: INFO: (14) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl/proxy/rewriteme">test</a> (200; 7.480256ms)
Jun 28 08:24:16.692: INFO: (14) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:443/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:443/proxy/tlsrewritem... (200; 7.989523ms)
Jun 28 08:24:16.692: INFO: (14) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:1080/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:1080/proxy/rewriteme">... (200; 7.443448ms)
Jun 28 08:24:16.695: INFO: (14) /api/v1/namespaces/proxy-5560/services/proxy-service-hqlww:portname1/proxy/: foo (200; 10.302752ms)
Jun 28 08:24:16.695: INFO: (14) /api/v1/namespaces/proxy-5560/services/http:proxy-service-hqlww:portname2/proxy/: bar (200; 9.903517ms)
Jun 28 08:24:16.695: INFO: (14) /api/v1/namespaces/proxy-5560/services/http:proxy-service-hqlww:portname1/proxy/: foo (200; 10.899568ms)
Jun 28 08:24:16.695: INFO: (14) /api/v1/namespaces/proxy-5560/services/proxy-service-hqlww:portname2/proxy/: bar (200; 10.397146ms)
Jun 28 08:24:16.695: INFO: (14) /api/v1/namespaces/proxy-5560/services/https:proxy-service-hqlww:tlsportname2/proxy/: tls qux (200; 10.223209ms)
Jun 28 08:24:16.695: INFO: (14) /api/v1/namespaces/proxy-5560/services/https:proxy-service-hqlww:tlsportname1/proxy/: tls baz (200; 9.993365ms)
Jun 28 08:24:16.702: INFO: (15) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:1080/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:1080/proxy/rewriteme">... (200; 7.350794ms)
Jun 28 08:24:16.703: INFO: (15) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:1080/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:1080/proxy/rewriteme">test<... (200; 7.114996ms)
Jun 28 08:24:16.703: INFO: (15) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:160/proxy/: foo (200; 7.52074ms)
Jun 28 08:24:16.703: INFO: (15) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:162/proxy/: bar (200; 7.620254ms)
Jun 28 08:24:16.703: INFO: (15) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:160/proxy/: foo (200; 7.700752ms)
Jun 28 08:24:16.704: INFO: (15) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl/proxy/rewriteme">test</a> (200; 8.837115ms)
Jun 28 08:24:16.704: INFO: (15) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:443/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:443/proxy/tlsrewritem... (200; 8.815422ms)
Jun 28 08:24:16.704: INFO: (15) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:460/proxy/: tls baz (200; 8.715186ms)
Jun 28 08:24:16.704: INFO: (15) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:162/proxy/: bar (200; 8.751119ms)
Jun 28 08:24:16.704: INFO: (15) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:462/proxy/: tls qux (200; 8.938948ms)
Jun 28 08:24:16.705: INFO: (15) /api/v1/namespaces/proxy-5560/services/proxy-service-hqlww:portname1/proxy/: foo (200; 9.965582ms)
Jun 28 08:24:16.707: INFO: (15) /api/v1/namespaces/proxy-5560/services/http:proxy-service-hqlww:portname1/proxy/: foo (200; 11.014386ms)
Jun 28 08:24:16.707: INFO: (15) /api/v1/namespaces/proxy-5560/services/http:proxy-service-hqlww:portname2/proxy/: bar (200; 11.154142ms)
Jun 28 08:24:16.707: INFO: (15) /api/v1/namespaces/proxy-5560/services/https:proxy-service-hqlww:tlsportname1/proxy/: tls baz (200; 11.357021ms)
Jun 28 08:24:16.707: INFO: (15) /api/v1/namespaces/proxy-5560/services/https:proxy-service-hqlww:tlsportname2/proxy/: tls qux (200; 11.513348ms)
Jun 28 08:24:16.708: INFO: (15) /api/v1/namespaces/proxy-5560/services/proxy-service-hqlww:portname2/proxy/: bar (200; 12.891789ms)
Jun 28 08:24:16.716: INFO: (16) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:160/proxy/: foo (200; 7.561064ms)
Jun 28 08:24:16.716: INFO: (16) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:162/proxy/: bar (200; 7.107418ms)
Jun 28 08:24:16.716: INFO: (16) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:462/proxy/: tls qux (200; 7.948489ms)
Jun 28 08:24:16.716: INFO: (16) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl/proxy/rewriteme">test</a> (200; 7.51701ms)
Jun 28 08:24:16.716: INFO: (16) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:160/proxy/: foo (200; 8.107799ms)
Jun 28 08:24:16.718: INFO: (16) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:460/proxy/: tls baz (200; 8.219247ms)
Jun 28 08:24:16.718: INFO: (16) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:1080/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:1080/proxy/rewriteme">test<... (200; 8.165155ms)
Jun 28 08:24:16.718: INFO: (16) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:443/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:443/proxy/tlsrewritem... (200; 8.331572ms)
Jun 28 08:24:16.718: INFO: (16) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:162/proxy/: bar (200; 8.388115ms)
Jun 28 08:24:16.718: INFO: (16) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:1080/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:1080/proxy/rewriteme">... (200; 8.796138ms)
Jun 28 08:24:16.718: INFO: (16) /api/v1/namespaces/proxy-5560/services/proxy-service-hqlww:portname2/proxy/: bar (200; 9.823345ms)
Jun 28 08:24:16.718: INFO: (16) /api/v1/namespaces/proxy-5560/services/https:proxy-service-hqlww:tlsportname1/proxy/: tls baz (200; 9.308846ms)
Jun 28 08:24:16.718: INFO: (16) /api/v1/namespaces/proxy-5560/services/https:proxy-service-hqlww:tlsportname2/proxy/: tls qux (200; 9.722361ms)
Jun 28 08:24:16.718: INFO: (16) /api/v1/namespaces/proxy-5560/services/proxy-service-hqlww:portname1/proxy/: foo (200; 10.071405ms)
Jun 28 08:24:16.720: INFO: (16) /api/v1/namespaces/proxy-5560/services/http:proxy-service-hqlww:portname1/proxy/: foo (200; 10.330116ms)
Jun 28 08:24:16.721: INFO: (16) /api/v1/namespaces/proxy-5560/services/http:proxy-service-hqlww:portname2/proxy/: bar (200; 11.690577ms)
Jun 28 08:24:16.728: INFO: (17) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:160/proxy/: foo (200; 6.854204ms)
Jun 28 08:24:16.729: INFO: (17) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:160/proxy/: foo (200; 7.936114ms)
Jun 28 08:24:16.729: INFO: (17) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:1080/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:1080/proxy/rewriteme">... (200; 8.189126ms)
Jun 28 08:24:16.729: INFO: (17) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:1080/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:1080/proxy/rewriteme">test<... (200; 8.265331ms)
Jun 28 08:24:16.729: INFO: (17) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:162/proxy/: bar (200; 8.329418ms)
Jun 28 08:24:16.729: INFO: (17) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:443/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:443/proxy/tlsrewritem... (200; 8.288637ms)
Jun 28 08:24:16.729: INFO: (17) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl/proxy/rewriteme">test</a> (200; 8.445458ms)
Jun 28 08:24:16.729: INFO: (17) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:460/proxy/: tls baz (200; 8.282168ms)
Jun 28 08:24:16.730: INFO: (17) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:462/proxy/: tls qux (200; 9.188227ms)
Jun 28 08:24:16.730: INFO: (17) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:162/proxy/: bar (200; 8.99983ms)
Jun 28 08:24:16.731: INFO: (17) /api/v1/namespaces/proxy-5560/services/https:proxy-service-hqlww:tlsportname2/proxy/: tls qux (200; 10.508812ms)
Jun 28 08:24:16.732: INFO: (17) /api/v1/namespaces/proxy-5560/services/https:proxy-service-hqlww:tlsportname1/proxy/: tls baz (200; 10.81529ms)
Jun 28 08:24:16.732: INFO: (17) /api/v1/namespaces/proxy-5560/services/proxy-service-hqlww:portname1/proxy/: foo (200; 10.739417ms)
Jun 28 08:24:16.732: INFO: (17) /api/v1/namespaces/proxy-5560/services/http:proxy-service-hqlww:portname1/proxy/: foo (200; 10.958615ms)
Jun 28 08:24:16.732: INFO: (17) /api/v1/namespaces/proxy-5560/services/proxy-service-hqlww:portname2/proxy/: bar (200; 11.810183ms)
Jun 28 08:24:16.733: INFO: (17) /api/v1/namespaces/proxy-5560/services/http:proxy-service-hqlww:portname2/proxy/: bar (200; 11.74016ms)
Jun 28 08:24:16.740: INFO: (18) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl/proxy/rewriteme">test</a> (200; 6.991078ms)
Jun 28 08:24:16.742: INFO: (18) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:1080/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:1080/proxy/rewriteme">... (200; 8.819437ms)
Jun 28 08:24:16.742: INFO: (18) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:162/proxy/: bar (200; 8.558216ms)
Jun 28 08:24:16.742: INFO: (18) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:1080/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:1080/proxy/rewriteme">test<... (200; 8.658508ms)
Jun 28 08:24:16.742: INFO: (18) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:162/proxy/: bar (200; 8.750409ms)
Jun 28 08:24:16.742: INFO: (18) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:462/proxy/: tls qux (200; 9.238806ms)
Jun 28 08:24:16.742: INFO: (18) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:160/proxy/: foo (200; 8.686772ms)
Jun 28 08:24:16.742: INFO: (18) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:443/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:443/proxy/tlsrewritem... (200; 8.897137ms)
Jun 28 08:24:16.742: INFO: (18) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:460/proxy/: tls baz (200; 8.869085ms)
Jun 28 08:24:16.742: INFO: (18) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:160/proxy/: foo (200; 9.001408ms)
Jun 28 08:24:16.746: INFO: (18) /api/v1/namespaces/proxy-5560/services/proxy-service-hqlww:portname2/proxy/: bar (200; 12.817112ms)
Jun 28 08:24:16.746: INFO: (18) /api/v1/namespaces/proxy-5560/services/https:proxy-service-hqlww:tlsportname1/proxy/: tls baz (200; 13.216919ms)
Jun 28 08:24:16.746: INFO: (18) /api/v1/namespaces/proxy-5560/services/http:proxy-service-hqlww:portname2/proxy/: bar (200; 13.27527ms)
Jun 28 08:24:16.746: INFO: (18) /api/v1/namespaces/proxy-5560/services/https:proxy-service-hqlww:tlsportname2/proxy/: tls qux (200; 13.430266ms)
Jun 28 08:24:16.747: INFO: (18) /api/v1/namespaces/proxy-5560/services/proxy-service-hqlww:portname1/proxy/: foo (200; 14.107964ms)
Jun 28 08:24:16.748: INFO: (18) /api/v1/namespaces/proxy-5560/services/http:proxy-service-hqlww:portname1/proxy/: foo (200; 14.497743ms)
Jun 28 08:24:16.754: INFO: (19) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:1080/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:1080/proxy/rewriteme">... (200; 6.458826ms)
Jun 28 08:24:16.755: INFO: (19) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:462/proxy/: tls qux (200; 7.026529ms)
Jun 28 08:24:16.755: INFO: (19) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:160/proxy/: foo (200; 7.086592ms)
Jun 28 08:24:16.755: INFO: (19) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:1080/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:1080/proxy/rewriteme">test<... (200; 7.13187ms)
Jun 28 08:24:16.755: INFO: (19) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:162/proxy/: bar (200; 7.186816ms)
Jun 28 08:24:16.755: INFO: (19) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl/proxy/rewriteme">test</a> (200; 7.530752ms)
Jun 28 08:24:16.755: INFO: (19) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:460/proxy/: tls baz (200; 7.375295ms)
Jun 28 08:24:16.755: INFO: (19) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:162/proxy/: bar (200; 7.42175ms)
Jun 28 08:24:16.757: INFO: (19) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:160/proxy/: foo (200; 9.011209ms)
Jun 28 08:24:16.757: INFO: (19) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:443/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:443/proxy/tlsrewritem... (200; 8.788338ms)
Jun 28 08:24:16.758: INFO: (19) /api/v1/namespaces/proxy-5560/services/https:proxy-service-hqlww:tlsportname1/proxy/: tls baz (200; 10.267734ms)
Jun 28 08:24:16.758: INFO: (19) /api/v1/namespaces/proxy-5560/services/http:proxy-service-hqlww:portname1/proxy/: foo (200; 10.5042ms)
Jun 28 08:24:16.758: INFO: (19) /api/v1/namespaces/proxy-5560/services/proxy-service-hqlww:portname2/proxy/: bar (200; 10.440247ms)
Jun 28 08:24:16.758: INFO: (19) /api/v1/namespaces/proxy-5560/services/https:proxy-service-hqlww:tlsportname2/proxy/: tls qux (200; 10.348165ms)
Jun 28 08:24:16.759: INFO: (19) /api/v1/namespaces/proxy-5560/services/http:proxy-service-hqlww:portname2/proxy/: bar (200; 11.348363ms)
Jun 28 08:24:16.761: INFO: (19) /api/v1/namespaces/proxy-5560/services/proxy-service-hqlww:portname1/proxy/: foo (200; 13.209459ms)
STEP: deleting ReplicationController proxy-service-hqlww in namespace proxy-5560, will wait for the garbage collector to delete the pods 06/28/23 08:24:16.761
Jun 28 08:24:16.824: INFO: Deleting ReplicationController proxy-service-hqlww took: 8.413886ms
Jun 28 08:24:16.925: INFO: Terminating ReplicationController proxy-service-hqlww pods took: 101.060165ms
[AfterEach] version v1
  test/e2e/framework/framework.go:187
Jun 28 08:24:19.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-5560" for this suite. 06/28/23 08:24:19.936
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","completed":202,"skipped":3434,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.757 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    test/e2e/network/proxy.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:24:13.186
    Jun 28 08:24:13.186: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename proxy 06/28/23 08:24:13.187
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:24:13.199
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:24:13.204
    [It] should proxy through a service and a pod  [Conformance]
      test/e2e/network/proxy.go:101
    STEP: starting an echo server on multiple ports 06/28/23 08:24:13.218
    STEP: creating replication controller proxy-service-hqlww in namespace proxy-5560 06/28/23 08:24:13.218
    I0628 08:24:13.232326      18 runners.go:193] Created replication controller with name: proxy-service-hqlww, namespace: proxy-5560, replica count: 1
    I0628 08:24:14.283938      18 runners.go:193] proxy-service-hqlww Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0628 08:24:15.285008      18 runners.go:193] proxy-service-hqlww Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
    I0628 08:24:16.285736      18 runners.go:193] proxy-service-hqlww Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jun 28 08:24:16.290: INFO: setup took 3.082066338s, starting test cases
    STEP: running 16 cases, 20 attempts per case, 320 total attempts 06/28/23 08:24:16.29
    Jun 28 08:24:16.301: INFO: (0) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:162/proxy/: bar (200; 10.501721ms)
    Jun 28 08:24:16.311: INFO: (0) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:1080/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:1080/proxy/rewriteme">... (200; 20.46884ms)
    Jun 28 08:24:16.311: INFO: (0) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:160/proxy/: foo (200; 20.23305ms)
    Jun 28 08:24:16.311: INFO: (0) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl/proxy/rewriteme">test</a> (200; 20.463717ms)
    Jun 28 08:24:16.311: INFO: (0) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:160/proxy/: foo (200; 20.187954ms)
    Jun 28 08:24:16.311: INFO: (0) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:1080/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:1080/proxy/rewriteme">test<... (200; 20.309769ms)
    Jun 28 08:24:16.311: INFO: (0) /api/v1/namespaces/proxy-5560/services/proxy-service-hqlww:portname2/proxy/: bar (200; 20.666161ms)
    Jun 28 08:24:16.311: INFO: (0) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:162/proxy/: bar (200; 20.475531ms)
    Jun 28 08:24:16.311: INFO: (0) /api/v1/namespaces/proxy-5560/services/http:proxy-service-hqlww:portname2/proxy/: bar (200; 20.272729ms)
    Jun 28 08:24:16.313: INFO: (0) /api/v1/namespaces/proxy-5560/services/http:proxy-service-hqlww:portname1/proxy/: foo (200; 22.883788ms)
    Jun 28 08:24:16.313: INFO: (0) /api/v1/namespaces/proxy-5560/services/proxy-service-hqlww:portname1/proxy/: foo (200; 22.825187ms)
    Jun 28 08:24:16.313: INFO: (0) /api/v1/namespaces/proxy-5560/services/https:proxy-service-hqlww:tlsportname1/proxy/: tls baz (200; 22.808176ms)
    Jun 28 08:24:16.313: INFO: (0) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:460/proxy/: tls baz (200; 22.985577ms)
    Jun 28 08:24:16.314: INFO: (0) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:462/proxy/: tls qux (200; 22.900418ms)
    Jun 28 08:24:16.314: INFO: (0) /api/v1/namespaces/proxy-5560/services/https:proxy-service-hqlww:tlsportname2/proxy/: tls qux (200; 23.182615ms)
    Jun 28 08:24:16.441: INFO: (0) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:443/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:443/proxy/tlsrewritem... (200; 150.934426ms)
    Jun 28 08:24:16.451: INFO: (1) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:160/proxy/: foo (200; 8.814693ms)
    Jun 28 08:24:16.451: INFO: (1) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl/proxy/rewriteme">test</a> (200; 8.839094ms)
    Jun 28 08:24:16.451: INFO: (1) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:1080/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:1080/proxy/rewriteme">... (200; 9.441625ms)
    Jun 28 08:24:16.451: INFO: (1) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:1080/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:1080/proxy/rewriteme">test<... (200; 9.011237ms)
    Jun 28 08:24:16.451: INFO: (1) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:162/proxy/: bar (200; 9.082735ms)
    Jun 28 08:24:16.451: INFO: (1) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:460/proxy/: tls baz (200; 9.163091ms)
    Jun 28 08:24:16.452: INFO: (1) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:160/proxy/: foo (200; 9.991131ms)
    Jun 28 08:24:16.452: INFO: (1) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:443/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:443/proxy/tlsrewritem... (200; 10.121125ms)
    Jun 28 08:24:16.453: INFO: (1) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:162/proxy/: bar (200; 10.532954ms)
    Jun 28 08:24:16.453: INFO: (1) /api/v1/namespaces/proxy-5560/services/https:proxy-service-hqlww:tlsportname1/proxy/: tls baz (200; 10.752066ms)
    Jun 28 08:24:16.453: INFO: (1) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:462/proxy/: tls qux (200; 10.555755ms)
    Jun 28 08:24:16.453: INFO: (1) /api/v1/namespaces/proxy-5560/services/https:proxy-service-hqlww:tlsportname2/proxy/: tls qux (200; 10.948467ms)
    Jun 28 08:24:16.454: INFO: (1) /api/v1/namespaces/proxy-5560/services/http:proxy-service-hqlww:portname1/proxy/: foo (200; 12.021521ms)
    Jun 28 08:24:16.454: INFO: (1) /api/v1/namespaces/proxy-5560/services/proxy-service-hqlww:portname1/proxy/: foo (200; 12.035403ms)
    Jun 28 08:24:16.454: INFO: (1) /api/v1/namespaces/proxy-5560/services/http:proxy-service-hqlww:portname2/proxy/: bar (200; 12.403185ms)
    Jun 28 08:24:16.455: INFO: (1) /api/v1/namespaces/proxy-5560/services/proxy-service-hqlww:portname2/proxy/: bar (200; 13.029339ms)
    Jun 28 08:24:16.462: INFO: (2) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:1080/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:1080/proxy/rewriteme">test<... (200; 6.375247ms)
    Jun 28 08:24:16.462: INFO: (2) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:160/proxy/: foo (200; 6.301345ms)
    Jun 28 08:24:16.462: INFO: (2) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:162/proxy/: bar (200; 6.386647ms)
    Jun 28 08:24:16.462: INFO: (2) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:462/proxy/: tls qux (200; 6.716765ms)
    Jun 28 08:24:16.462: INFO: (2) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:1080/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:1080/proxy/rewriteme">... (200; 6.515237ms)
    Jun 28 08:24:16.462: INFO: (2) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:160/proxy/: foo (200; 6.648119ms)
    Jun 28 08:24:16.462: INFO: (2) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:162/proxy/: bar (200; 7.130012ms)
    Jun 28 08:24:16.462: INFO: (2) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:460/proxy/: tls baz (200; 7.090587ms)
    Jun 28 08:24:16.462: INFO: (2) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl/proxy/rewriteme">test</a> (200; 6.968814ms)
    Jun 28 08:24:16.463: INFO: (2) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:443/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:443/proxy/tlsrewritem... (200; 7.150753ms)
    Jun 28 08:24:16.464: INFO: (2) /api/v1/namespaces/proxy-5560/services/https:proxy-service-hqlww:tlsportname1/proxy/: tls baz (200; 8.246987ms)
    Jun 28 08:24:16.464: INFO: (2) /api/v1/namespaces/proxy-5560/services/https:proxy-service-hqlww:tlsportname2/proxy/: tls qux (200; 8.287838ms)
    Jun 28 08:24:16.465: INFO: (2) /api/v1/namespaces/proxy-5560/services/http:proxy-service-hqlww:portname2/proxy/: bar (200; 9.608772ms)
    Jun 28 08:24:16.465: INFO: (2) /api/v1/namespaces/proxy-5560/services/http:proxy-service-hqlww:portname1/proxy/: foo (200; 9.46464ms)
    Jun 28 08:24:16.465: INFO: (2) /api/v1/namespaces/proxy-5560/services/proxy-service-hqlww:portname1/proxy/: foo (200; 9.418449ms)
    Jun 28 08:24:16.466: INFO: (2) /api/v1/namespaces/proxy-5560/services/proxy-service-hqlww:portname2/proxy/: bar (200; 10.122098ms)
    Jun 28 08:24:16.473: INFO: (3) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:462/proxy/: tls qux (200; 7.023347ms)
    Jun 28 08:24:16.473: INFO: (3) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:160/proxy/: foo (200; 7.174008ms)
    Jun 28 08:24:16.473: INFO: (3) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:460/proxy/: tls baz (200; 7.529482ms)
    Jun 28 08:24:16.473: INFO: (3) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:162/proxy/: bar (200; 7.222355ms)
    Jun 28 08:24:16.473: INFO: (3) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:160/proxy/: foo (200; 7.429676ms)
    Jun 28 08:24:16.473: INFO: (3) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl/proxy/rewriteme">test</a> (200; 7.355834ms)
    Jun 28 08:24:16.473: INFO: (3) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:1080/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:1080/proxy/rewriteme">... (200; 7.342064ms)
    Jun 28 08:24:16.474: INFO: (3) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:162/proxy/: bar (200; 7.81254ms)
    Jun 28 08:24:16.474: INFO: (3) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:443/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:443/proxy/tlsrewritem... (200; 7.857267ms)
    Jun 28 08:24:16.474: INFO: (3) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:1080/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:1080/proxy/rewriteme">test<... (200; 8.003615ms)
    Jun 28 08:24:16.476: INFO: (3) /api/v1/namespaces/proxy-5560/services/https:proxy-service-hqlww:tlsportname1/proxy/: tls baz (200; 10.079033ms)
    Jun 28 08:24:16.476: INFO: (3) /api/v1/namespaces/proxy-5560/services/http:proxy-service-hqlww:portname1/proxy/: foo (200; 10.046904ms)
    Jun 28 08:24:16.476: INFO: (3) /api/v1/namespaces/proxy-5560/services/http:proxy-service-hqlww:portname2/proxy/: bar (200; 10.335677ms)
    Jun 28 08:24:16.476: INFO: (3) /api/v1/namespaces/proxy-5560/services/proxy-service-hqlww:portname1/proxy/: foo (200; 10.296308ms)
    Jun 28 08:24:16.476: INFO: (3) /api/v1/namespaces/proxy-5560/services/https:proxy-service-hqlww:tlsportname2/proxy/: tls qux (200; 10.19573ms)
    Jun 28 08:24:16.477: INFO: (3) /api/v1/namespaces/proxy-5560/services/proxy-service-hqlww:portname2/proxy/: bar (200; 11.111189ms)
    Jun 28 08:24:16.483: INFO: (4) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:460/proxy/: tls baz (200; 6.354796ms)
    Jun 28 08:24:16.486: INFO: (4) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:160/proxy/: foo (200; 8.86261ms)
    Jun 28 08:24:16.486: INFO: (4) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:462/proxy/: tls qux (200; 8.875526ms)
    Jun 28 08:24:16.486: INFO: (4) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:1080/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:1080/proxy/rewriteme">... (200; 8.737889ms)
    Jun 28 08:24:16.486: INFO: (4) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:1080/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:1080/proxy/rewriteme">test<... (200; 8.894559ms)
    Jun 28 08:24:16.486: INFO: (4) /api/v1/namespaces/proxy-5560/services/http:proxy-service-hqlww:portname2/proxy/: bar (200; 9.22837ms)
    Jun 28 08:24:16.486: INFO: (4) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:160/proxy/: foo (200; 8.89995ms)
    Jun 28 08:24:16.486: INFO: (4) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:162/proxy/: bar (200; 8.88527ms)
    Jun 28 08:24:16.487: INFO: (4) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl/proxy/rewriteme">test</a> (200; 9.168283ms)
    Jun 28 08:24:16.487: INFO: (4) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:443/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:443/proxy/tlsrewritem... (200; 9.756209ms)
    Jun 28 08:24:16.487: INFO: (4) /api/v1/namespaces/proxy-5560/services/https:proxy-service-hqlww:tlsportname2/proxy/: tls qux (200; 9.961265ms)
    Jun 28 08:24:16.488: INFO: (4) /api/v1/namespaces/proxy-5560/services/https:proxy-service-hqlww:tlsportname1/proxy/: tls baz (200; 10.597611ms)
    Jun 28 08:24:16.489: INFO: (4) /api/v1/namespaces/proxy-5560/services/proxy-service-hqlww:portname1/proxy/: foo (200; 11.799832ms)
    Jun 28 08:24:16.489: INFO: (4) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:162/proxy/: bar (200; 11.604012ms)
    Jun 28 08:24:16.489: INFO: (4) /api/v1/namespaces/proxy-5560/services/proxy-service-hqlww:portname2/proxy/: bar (200; 11.79036ms)
    Jun 28 08:24:16.489: INFO: (4) /api/v1/namespaces/proxy-5560/services/http:proxy-service-hqlww:portname1/proxy/: foo (200; 11.600487ms)
    Jun 28 08:24:16.497: INFO: (5) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:443/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:443/proxy/tlsrewritem... (200; 7.487687ms)
    Jun 28 08:24:16.497: INFO: (5) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:160/proxy/: foo (200; 7.585228ms)
    Jun 28 08:24:16.497: INFO: (5) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:162/proxy/: bar (200; 7.705042ms)
    Jun 28 08:24:16.497: INFO: (5) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl/proxy/rewriteme">test</a> (200; 7.552973ms)
    Jun 28 08:24:16.497: INFO: (5) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:1080/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:1080/proxy/rewriteme">... (200; 7.553729ms)
    Jun 28 08:24:16.498: INFO: (5) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:462/proxy/: tls qux (200; 8.386354ms)
    Jun 28 08:24:16.498: INFO: (5) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:1080/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:1080/proxy/rewriteme">test<... (200; 8.817699ms)
    Jun 28 08:24:16.498: INFO: (5) /api/v1/namespaces/proxy-5560/services/https:proxy-service-hqlww:tlsportname1/proxy/: tls baz (200; 8.938647ms)
    Jun 28 08:24:16.498: INFO: (5) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:460/proxy/: tls baz (200; 8.464913ms)
    Jun 28 08:24:16.498: INFO: (5) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:162/proxy/: bar (200; 8.765795ms)
    Jun 28 08:24:16.498: INFO: (5) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:160/proxy/: foo (200; 8.653208ms)
    Jun 28 08:24:16.541: INFO: (5) /api/v1/namespaces/proxy-5560/services/https:proxy-service-hqlww:tlsportname2/proxy/: tls qux (200; 51.296265ms)
    Jun 28 08:24:16.541: INFO: (5) /api/v1/namespaces/proxy-5560/services/http:proxy-service-hqlww:portname1/proxy/: foo (200; 51.802983ms)
    Jun 28 08:24:16.541: INFO: (5) /api/v1/namespaces/proxy-5560/services/http:proxy-service-hqlww:portname2/proxy/: bar (200; 51.687412ms)
    Jun 28 08:24:16.541: INFO: (5) /api/v1/namespaces/proxy-5560/services/proxy-service-hqlww:portname1/proxy/: foo (200; 51.416185ms)
    Jun 28 08:24:16.541: INFO: (5) /api/v1/namespaces/proxy-5560/services/proxy-service-hqlww:portname2/proxy/: bar (200; 51.542976ms)
    Jun 28 08:24:16.550: INFO: (6) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:160/proxy/: foo (200; 8.916809ms)
    Jun 28 08:24:16.550: INFO: (6) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:162/proxy/: bar (200; 9.026739ms)
    Jun 28 08:24:16.550: INFO: (6) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl/proxy/rewriteme">test</a> (200; 8.830609ms)
    Jun 28 08:24:16.550: INFO: (6) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:462/proxy/: tls qux (200; 8.856219ms)
    Jun 28 08:24:16.550: INFO: (6) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:1080/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:1080/proxy/rewriteme">test<... (200; 9.015183ms)
    Jun 28 08:24:16.551: INFO: (6) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:160/proxy/: foo (200; 9.612478ms)
    Jun 28 08:24:16.551: INFO: (6) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:460/proxy/: tls baz (200; 10.211128ms)
    Jun 28 08:24:16.551: INFO: (6) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:162/proxy/: bar (200; 10.271872ms)
    Jun 28 08:24:16.551: INFO: (6) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:1080/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:1080/proxy/rewriteme">... (200; 10.571049ms)
    Jun 28 08:24:16.551: INFO: (6) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:443/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:443/proxy/tlsrewritem... (200; 10.477384ms)
    Jun 28 08:24:16.552: INFO: (6) /api/v1/namespaces/proxy-5560/services/https:proxy-service-hqlww:tlsportname2/proxy/: tls qux (200; 10.938301ms)
    Jun 28 08:24:16.552: INFO: (6) /api/v1/namespaces/proxy-5560/services/https:proxy-service-hqlww:tlsportname1/proxy/: tls baz (200; 11.364481ms)
    Jun 28 08:24:16.553: INFO: (6) /api/v1/namespaces/proxy-5560/services/http:proxy-service-hqlww:portname1/proxy/: foo (200; 12.217952ms)
    Jun 28 08:24:16.553: INFO: (6) /api/v1/namespaces/proxy-5560/services/http:proxy-service-hqlww:portname2/proxy/: bar (200; 12.292847ms)
    Jun 28 08:24:16.553: INFO: (6) /api/v1/namespaces/proxy-5560/services/proxy-service-hqlww:portname1/proxy/: foo (200; 12.125883ms)
    Jun 28 08:24:16.554: INFO: (6) /api/v1/namespaces/proxy-5560/services/proxy-service-hqlww:portname2/proxy/: bar (200; 13.284844ms)
    Jun 28 08:24:16.563: INFO: (7) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:162/proxy/: bar (200; 8.915658ms)
    Jun 28 08:24:16.563: INFO: (7) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:162/proxy/: bar (200; 9.096895ms)
    Jun 28 08:24:16.563: INFO: (7) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:462/proxy/: tls qux (200; 8.639151ms)
    Jun 28 08:24:16.564: INFO: (7) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:160/proxy/: foo (200; 8.871044ms)
    Jun 28 08:24:16.564: INFO: (7) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:1080/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:1080/proxy/rewriteme">... (200; 8.828924ms)
    Jun 28 08:24:16.564: INFO: (7) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:1080/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:1080/proxy/rewriteme">test<... (200; 8.735025ms)
    Jun 28 08:24:16.564: INFO: (7) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:443/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:443/proxy/tlsrewritem... (200; 9.052176ms)
    Jun 28 08:24:16.564: INFO: (7) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:460/proxy/: tls baz (200; 9.092716ms)
    Jun 28 08:24:16.564: INFO: (7) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl/proxy/rewriteme">test</a> (200; 8.973638ms)
    Jun 28 08:24:16.565: INFO: (7) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:160/proxy/: foo (200; 9.885007ms)
    Jun 28 08:24:16.565: INFO: (7) /api/v1/namespaces/proxy-5560/services/http:proxy-service-hqlww:portname2/proxy/: bar (200; 10.27763ms)
    Jun 28 08:24:16.566: INFO: (7) /api/v1/namespaces/proxy-5560/services/https:proxy-service-hqlww:tlsportname2/proxy/: tls qux (200; 11.569026ms)
    Jun 28 08:24:16.566: INFO: (7) /api/v1/namespaces/proxy-5560/services/https:proxy-service-hqlww:tlsportname1/proxy/: tls baz (200; 11.522644ms)
    Jun 28 08:24:16.566: INFO: (7) /api/v1/namespaces/proxy-5560/services/proxy-service-hqlww:portname2/proxy/: bar (200; 11.735606ms)
    Jun 28 08:24:16.566: INFO: (7) /api/v1/namespaces/proxy-5560/services/proxy-service-hqlww:portname1/proxy/: foo (200; 11.923176ms)
    Jun 28 08:24:16.567: INFO: (7) /api/v1/namespaces/proxy-5560/services/http:proxy-service-hqlww:portname1/proxy/: foo (200; 12.809576ms)
    Jun 28 08:24:16.575: INFO: (8) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:160/proxy/: foo (200; 6.999045ms)
    Jun 28 08:24:16.575: INFO: (8) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:460/proxy/: tls baz (200; 7.293656ms)
    Jun 28 08:24:16.575: INFO: (8) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:443/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:443/proxy/tlsrewritem... (200; 6.0273ms)
    Jun 28 08:24:16.575: INFO: (8) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl/proxy/rewriteme">test</a> (200; 6.820424ms)
    Jun 28 08:24:16.575: INFO: (8) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:462/proxy/: tls qux (200; 7.028341ms)
    Jun 28 08:24:16.575: INFO: (8) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:1080/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:1080/proxy/rewriteme">test<... (200; 6.408582ms)
    Jun 28 08:24:16.575: INFO: (8) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:1080/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:1080/proxy/rewriteme">... (200; 6.775634ms)
    Jun 28 08:24:16.576: INFO: (8) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:162/proxy/: bar (200; 7.549663ms)
    Jun 28 08:24:16.576: INFO: (8) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:160/proxy/: foo (200; 7.135731ms)
    Jun 28 08:24:16.576: INFO: (8) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:162/proxy/: bar (200; 7.336358ms)
    Jun 28 08:24:16.578: INFO: (8) /api/v1/namespaces/proxy-5560/services/https:proxy-service-hqlww:tlsportname2/proxy/: tls qux (200; 9.80997ms)
    Jun 28 08:24:16.579: INFO: (8) /api/v1/namespaces/proxy-5560/services/https:proxy-service-hqlww:tlsportname1/proxy/: tls baz (200; 11.122237ms)
    Jun 28 08:24:16.579: INFO: (8) /api/v1/namespaces/proxy-5560/services/proxy-service-hqlww:portname2/proxy/: bar (200; 11.600916ms)
    Jun 28 08:24:16.579: INFO: (8) /api/v1/namespaces/proxy-5560/services/proxy-service-hqlww:portname1/proxy/: foo (200; 11.680666ms)
    Jun 28 08:24:16.579: INFO: (8) /api/v1/namespaces/proxy-5560/services/http:proxy-service-hqlww:portname2/proxy/: bar (200; 11.204719ms)
    Jun 28 08:24:16.580: INFO: (8) /api/v1/namespaces/proxy-5560/services/http:proxy-service-hqlww:portname1/proxy/: foo (200; 12.106677ms)
    Jun 28 08:24:16.588: INFO: (9) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:1080/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:1080/proxy/rewriteme">... (200; 7.10339ms)
    Jun 28 08:24:16.589: INFO: (9) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl/proxy/rewriteme">test</a> (200; 8.753168ms)
    Jun 28 08:24:16.589: INFO: (9) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:160/proxy/: foo (200; 8.193452ms)
    Jun 28 08:24:16.589: INFO: (9) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:162/proxy/: bar (200; 8.61061ms)
    Jun 28 08:24:16.589: INFO: (9) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:1080/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:1080/proxy/rewriteme">test<... (200; 8.538999ms)
    Jun 28 08:24:16.589: INFO: (9) /api/v1/namespaces/proxy-5560/services/proxy-service-hqlww:portname1/proxy/: foo (200; 8.981803ms)
    Jun 28 08:24:16.591: INFO: (9) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:162/proxy/: bar (200; 9.638205ms)
    Jun 28 08:24:16.591: INFO: (9) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:462/proxy/: tls qux (200; 9.376872ms)
    Jun 28 08:24:16.591: INFO: (9) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:460/proxy/: tls baz (200; 9.590147ms)
    Jun 28 08:24:16.591: INFO: (9) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:443/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:443/proxy/tlsrewritem... (200; 9.6407ms)
    Jun 28 08:24:16.592: INFO: (9) /api/v1/namespaces/proxy-5560/services/https:proxy-service-hqlww:tlsportname2/proxy/: tls qux (200; 10.853576ms)
    Jun 28 08:24:16.593: INFO: (9) /api/v1/namespaces/proxy-5560/services/https:proxy-service-hqlww:tlsportname1/proxy/: tls baz (200; 11.809247ms)
    Jun 28 08:24:16.593: INFO: (9) /api/v1/namespaces/proxy-5560/services/http:proxy-service-hqlww:portname2/proxy/: bar (200; 11.747753ms)
    Jun 28 08:24:16.593: INFO: (9) /api/v1/namespaces/proxy-5560/services/http:proxy-service-hqlww:portname1/proxy/: foo (200; 11.51235ms)
    Jun 28 08:24:16.593: INFO: (9) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:160/proxy/: foo (200; 11.754015ms)
    Jun 28 08:24:16.594: INFO: (9) /api/v1/namespaces/proxy-5560/services/proxy-service-hqlww:portname2/proxy/: bar (200; 12.699808ms)
    Jun 28 08:24:16.601: INFO: (10) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:160/proxy/: foo (200; 7.294863ms)
    Jun 28 08:24:16.601: INFO: (10) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:160/proxy/: foo (200; 7.364322ms)
    Jun 28 08:24:16.601: INFO: (10) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:443/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:443/proxy/tlsrewritem... (200; 7.232656ms)
    Jun 28 08:24:16.601: INFO: (10) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:162/proxy/: bar (200; 7.569517ms)
    Jun 28 08:24:16.601: INFO: (10) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl/proxy/rewriteme">test</a> (200; 7.03818ms)
    Jun 28 08:24:16.601: INFO: (10) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:462/proxy/: tls qux (200; 7.329933ms)
    Jun 28 08:24:16.602: INFO: (10) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:1080/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:1080/proxy/rewriteme">test<... (200; 7.598953ms)
    Jun 28 08:24:16.603: INFO: (10) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:162/proxy/: bar (200; 8.966634ms)
    Jun 28 08:24:16.603: INFO: (10) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:1080/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:1080/proxy/rewriteme">... (200; 8.857852ms)
    Jun 28 08:24:16.603: INFO: (10) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:460/proxy/: tls baz (200; 8.931248ms)
    Jun 28 08:24:16.647: INFO: (10) /api/v1/namespaces/proxy-5560/services/https:proxy-service-hqlww:tlsportname1/proxy/: tls baz (200; 52.134167ms)
    Jun 28 08:24:16.647: INFO: (10) /api/v1/namespaces/proxy-5560/services/https:proxy-service-hqlww:tlsportname2/proxy/: tls qux (200; 52.217391ms)
    Jun 28 08:24:16.647: INFO: (10) /api/v1/namespaces/proxy-5560/services/http:proxy-service-hqlww:portname2/proxy/: bar (200; 52.236547ms)
    Jun 28 08:24:16.647: INFO: (10) /api/v1/namespaces/proxy-5560/services/proxy-service-hqlww:portname1/proxy/: foo (200; 52.329123ms)
    Jun 28 08:24:16.647: INFO: (10) /api/v1/namespaces/proxy-5560/services/proxy-service-hqlww:portname2/proxy/: bar (200; 52.71785ms)
    Jun 28 08:24:16.647: INFO: (10) /api/v1/namespaces/proxy-5560/services/http:proxy-service-hqlww:portname1/proxy/: foo (200; 52.65391ms)
    Jun 28 08:24:16.656: INFO: (11) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:162/proxy/: bar (200; 9.416199ms)
    Jun 28 08:24:16.656: INFO: (11) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:1080/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:1080/proxy/rewriteme">... (200; 8.901772ms)
    Jun 28 08:24:16.656: INFO: (11) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:160/proxy/: foo (200; 9.004113ms)
    Jun 28 08:24:16.656: INFO: (11) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:160/proxy/: foo (200; 9.222677ms)
    Jun 28 08:24:16.657: INFO: (11) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:1080/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:1080/proxy/rewriteme">test<... (200; 9.893888ms)
    Jun 28 08:24:16.657: INFO: (11) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:462/proxy/: tls qux (200; 9.875922ms)
    Jun 28 08:24:16.657: INFO: (11) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:460/proxy/: tls baz (200; 9.992588ms)
    Jun 28 08:24:16.657: INFO: (11) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl/proxy/rewriteme">test</a> (200; 9.97599ms)
    Jun 28 08:24:16.657: INFO: (11) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:162/proxy/: bar (200; 10.189196ms)
    Jun 28 08:24:16.657: INFO: (11) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:443/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:443/proxy/tlsrewritem... (200; 10.191377ms)
    Jun 28 08:24:16.658: INFO: (11) /api/v1/namespaces/proxy-5560/services/proxy-service-hqlww:portname1/proxy/: foo (200; 10.978604ms)
    Jun 28 08:24:16.659: INFO: (11) /api/v1/namespaces/proxy-5560/services/https:proxy-service-hqlww:tlsportname1/proxy/: tls baz (200; 12.0913ms)
    Jun 28 08:24:16.659: INFO: (11) /api/v1/namespaces/proxy-5560/services/http:proxy-service-hqlww:portname1/proxy/: foo (200; 12.431298ms)
    Jun 28 08:24:16.660: INFO: (11) /api/v1/namespaces/proxy-5560/services/http:proxy-service-hqlww:portname2/proxy/: bar (200; 12.34359ms)
    Jun 28 08:24:16.660: INFO: (11) /api/v1/namespaces/proxy-5560/services/https:proxy-service-hqlww:tlsportname2/proxy/: tls qux (200; 12.461688ms)
    Jun 28 08:24:16.660: INFO: (11) /api/v1/namespaces/proxy-5560/services/proxy-service-hqlww:portname2/proxy/: bar (200; 13.200139ms)
    Jun 28 08:24:16.668: INFO: (12) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:1080/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:1080/proxy/rewriteme">test<... (200; 7.343355ms)
    Jun 28 08:24:16.668: INFO: (12) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl/proxy/rewriteme">test</a> (200; 7.732006ms)
    Jun 28 08:24:16.668: INFO: (12) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:162/proxy/: bar (200; 7.551929ms)
    Jun 28 08:24:16.668: INFO: (12) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:160/proxy/: foo (200; 7.875411ms)
    Jun 28 08:24:16.668: INFO: (12) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:460/proxy/: tls baz (200; 8.186029ms)
    Jun 28 08:24:16.668: INFO: (12) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:1080/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:1080/proxy/rewriteme">... (200; 7.81379ms)
    Jun 28 08:24:16.669: INFO: (12) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:160/proxy/: foo (200; 8.09746ms)
    Jun 28 08:24:16.669: INFO: (12) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:443/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:443/proxy/tlsrewritem... (200; 8.160946ms)
    Jun 28 08:24:16.669: INFO: (12) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:162/proxy/: bar (200; 8.225301ms)
    Jun 28 08:24:16.669: INFO: (12) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:462/proxy/: tls qux (200; 8.6972ms)
    Jun 28 08:24:16.671: INFO: (12) /api/v1/namespaces/proxy-5560/services/https:proxy-service-hqlww:tlsportname1/proxy/: tls baz (200; 10.542885ms)
    Jun 28 08:24:16.671: INFO: (12) /api/v1/namespaces/proxy-5560/services/https:proxy-service-hqlww:tlsportname2/proxy/: tls qux (200; 10.735892ms)
    Jun 28 08:24:16.671: INFO: (12) /api/v1/namespaces/proxy-5560/services/proxy-service-hqlww:portname2/proxy/: bar (200; 10.866845ms)
    Jun 28 08:24:16.671: INFO: (12) /api/v1/namespaces/proxy-5560/services/proxy-service-hqlww:portname1/proxy/: foo (200; 10.931509ms)
    Jun 28 08:24:16.672: INFO: (12) /api/v1/namespaces/proxy-5560/services/http:proxy-service-hqlww:portname1/proxy/: foo (200; 11.669522ms)
    Jun 28 08:24:16.673: INFO: (12) /api/v1/namespaces/proxy-5560/services/http:proxy-service-hqlww:portname2/proxy/: bar (200; 12.072104ms)
    Jun 28 08:24:16.681: INFO: (13) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:160/proxy/: foo (200; 7.451936ms)
    Jun 28 08:24:16.681: INFO: (13) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:160/proxy/: foo (200; 7.756777ms)
    Jun 28 08:24:16.681: INFO: (13) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:1080/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:1080/proxy/rewriteme">test<... (200; 7.835766ms)
    Jun 28 08:24:16.681: INFO: (13) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:460/proxy/: tls baz (200; 7.999559ms)
    Jun 28 08:24:16.681: INFO: (13) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:443/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:443/proxy/tlsrewritem... (200; 8.091578ms)
    Jun 28 08:24:16.681: INFO: (13) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:162/proxy/: bar (200; 8.195296ms)
    Jun 28 08:24:16.681: INFO: (13) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:162/proxy/: bar (200; 8.349986ms)
    Jun 28 08:24:16.681: INFO: (13) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:1080/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:1080/proxy/rewriteme">... (200; 7.757743ms)
    Jun 28 08:24:16.681: INFO: (13) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl/proxy/rewriteme">test</a> (200; 7.835115ms)
    Jun 28 08:24:16.682: INFO: (13) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:462/proxy/: tls qux (200; 8.377384ms)
    Jun 28 08:24:16.683: INFO: (13) /api/v1/namespaces/proxy-5560/services/proxy-service-hqlww:portname2/proxy/: bar (200; 9.307413ms)
    Jun 28 08:24:16.683: INFO: (13) /api/v1/namespaces/proxy-5560/services/https:proxy-service-hqlww:tlsportname1/proxy/: tls baz (200; 8.927861ms)
    Jun 28 08:24:16.683: INFO: (13) /api/v1/namespaces/proxy-5560/services/https:proxy-service-hqlww:tlsportname2/proxy/: tls qux (200; 9.187906ms)
    Jun 28 08:24:16.684: INFO: (13) /api/v1/namespaces/proxy-5560/services/http:proxy-service-hqlww:portname2/proxy/: bar (200; 10.204209ms)
    Jun 28 08:24:16.684: INFO: (13) /api/v1/namespaces/proxy-5560/services/proxy-service-hqlww:portname1/proxy/: foo (200; 10.657871ms)
    Jun 28 08:24:16.684: INFO: (13) /api/v1/namespaces/proxy-5560/services/http:proxy-service-hqlww:portname1/proxy/: foo (200; 10.934553ms)
    Jun 28 08:24:16.691: INFO: (14) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:162/proxy/: bar (200; 7.142478ms)
    Jun 28 08:24:16.692: INFO: (14) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:162/proxy/: bar (200; 7.314343ms)
    Jun 28 08:24:16.692: INFO: (14) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:160/proxy/: foo (200; 7.405873ms)
    Jun 28 08:24:16.692: INFO: (14) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:1080/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:1080/proxy/rewriteme">test<... (200; 7.482996ms)
    Jun 28 08:24:16.692: INFO: (14) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:462/proxy/: tls qux (200; 6.917885ms)
    Jun 28 08:24:16.692: INFO: (14) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:160/proxy/: foo (200; 6.992827ms)
    Jun 28 08:24:16.692: INFO: (14) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:460/proxy/: tls baz (200; 7.874024ms)
    Jun 28 08:24:16.692: INFO: (14) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl/proxy/rewriteme">test</a> (200; 7.480256ms)
    Jun 28 08:24:16.692: INFO: (14) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:443/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:443/proxy/tlsrewritem... (200; 7.989523ms)
    Jun 28 08:24:16.692: INFO: (14) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:1080/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:1080/proxy/rewriteme">... (200; 7.443448ms)
    Jun 28 08:24:16.695: INFO: (14) /api/v1/namespaces/proxy-5560/services/proxy-service-hqlww:portname1/proxy/: foo (200; 10.302752ms)
    Jun 28 08:24:16.695: INFO: (14) /api/v1/namespaces/proxy-5560/services/http:proxy-service-hqlww:portname2/proxy/: bar (200; 9.903517ms)
    Jun 28 08:24:16.695: INFO: (14) /api/v1/namespaces/proxy-5560/services/http:proxy-service-hqlww:portname1/proxy/: foo (200; 10.899568ms)
    Jun 28 08:24:16.695: INFO: (14) /api/v1/namespaces/proxy-5560/services/proxy-service-hqlww:portname2/proxy/: bar (200; 10.397146ms)
    Jun 28 08:24:16.695: INFO: (14) /api/v1/namespaces/proxy-5560/services/https:proxy-service-hqlww:tlsportname2/proxy/: tls qux (200; 10.223209ms)
    Jun 28 08:24:16.695: INFO: (14) /api/v1/namespaces/proxy-5560/services/https:proxy-service-hqlww:tlsportname1/proxy/: tls baz (200; 9.993365ms)
    Jun 28 08:24:16.702: INFO: (15) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:1080/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:1080/proxy/rewriteme">... (200; 7.350794ms)
    Jun 28 08:24:16.703: INFO: (15) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:1080/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:1080/proxy/rewriteme">test<... (200; 7.114996ms)
    Jun 28 08:24:16.703: INFO: (15) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:160/proxy/: foo (200; 7.52074ms)
    Jun 28 08:24:16.703: INFO: (15) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:162/proxy/: bar (200; 7.620254ms)
    Jun 28 08:24:16.703: INFO: (15) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:160/proxy/: foo (200; 7.700752ms)
    Jun 28 08:24:16.704: INFO: (15) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl/proxy/rewriteme">test</a> (200; 8.837115ms)
    Jun 28 08:24:16.704: INFO: (15) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:443/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:443/proxy/tlsrewritem... (200; 8.815422ms)
    Jun 28 08:24:16.704: INFO: (15) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:460/proxy/: tls baz (200; 8.715186ms)
    Jun 28 08:24:16.704: INFO: (15) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:162/proxy/: bar (200; 8.751119ms)
    Jun 28 08:24:16.704: INFO: (15) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:462/proxy/: tls qux (200; 8.938948ms)
    Jun 28 08:24:16.705: INFO: (15) /api/v1/namespaces/proxy-5560/services/proxy-service-hqlww:portname1/proxy/: foo (200; 9.965582ms)
    Jun 28 08:24:16.707: INFO: (15) /api/v1/namespaces/proxy-5560/services/http:proxy-service-hqlww:portname1/proxy/: foo (200; 11.014386ms)
    Jun 28 08:24:16.707: INFO: (15) /api/v1/namespaces/proxy-5560/services/http:proxy-service-hqlww:portname2/proxy/: bar (200; 11.154142ms)
    Jun 28 08:24:16.707: INFO: (15) /api/v1/namespaces/proxy-5560/services/https:proxy-service-hqlww:tlsportname1/proxy/: tls baz (200; 11.357021ms)
    Jun 28 08:24:16.707: INFO: (15) /api/v1/namespaces/proxy-5560/services/https:proxy-service-hqlww:tlsportname2/proxy/: tls qux (200; 11.513348ms)
    Jun 28 08:24:16.708: INFO: (15) /api/v1/namespaces/proxy-5560/services/proxy-service-hqlww:portname2/proxy/: bar (200; 12.891789ms)
    Jun 28 08:24:16.716: INFO: (16) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:160/proxy/: foo (200; 7.561064ms)
    Jun 28 08:24:16.716: INFO: (16) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:162/proxy/: bar (200; 7.107418ms)
    Jun 28 08:24:16.716: INFO: (16) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:462/proxy/: tls qux (200; 7.948489ms)
    Jun 28 08:24:16.716: INFO: (16) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl/proxy/rewriteme">test</a> (200; 7.51701ms)
    Jun 28 08:24:16.716: INFO: (16) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:160/proxy/: foo (200; 8.107799ms)
    Jun 28 08:24:16.718: INFO: (16) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:460/proxy/: tls baz (200; 8.219247ms)
    Jun 28 08:24:16.718: INFO: (16) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:1080/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:1080/proxy/rewriteme">test<... (200; 8.165155ms)
    Jun 28 08:24:16.718: INFO: (16) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:443/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:443/proxy/tlsrewritem... (200; 8.331572ms)
    Jun 28 08:24:16.718: INFO: (16) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:162/proxy/: bar (200; 8.388115ms)
    Jun 28 08:24:16.718: INFO: (16) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:1080/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:1080/proxy/rewriteme">... (200; 8.796138ms)
    Jun 28 08:24:16.718: INFO: (16) /api/v1/namespaces/proxy-5560/services/proxy-service-hqlww:portname2/proxy/: bar (200; 9.823345ms)
    Jun 28 08:24:16.718: INFO: (16) /api/v1/namespaces/proxy-5560/services/https:proxy-service-hqlww:tlsportname1/proxy/: tls baz (200; 9.308846ms)
    Jun 28 08:24:16.718: INFO: (16) /api/v1/namespaces/proxy-5560/services/https:proxy-service-hqlww:tlsportname2/proxy/: tls qux (200; 9.722361ms)
    Jun 28 08:24:16.718: INFO: (16) /api/v1/namespaces/proxy-5560/services/proxy-service-hqlww:portname1/proxy/: foo (200; 10.071405ms)
    Jun 28 08:24:16.720: INFO: (16) /api/v1/namespaces/proxy-5560/services/http:proxy-service-hqlww:portname1/proxy/: foo (200; 10.330116ms)
    Jun 28 08:24:16.721: INFO: (16) /api/v1/namespaces/proxy-5560/services/http:proxy-service-hqlww:portname2/proxy/: bar (200; 11.690577ms)
    Jun 28 08:24:16.728: INFO: (17) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:160/proxy/: foo (200; 6.854204ms)
    Jun 28 08:24:16.729: INFO: (17) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:160/proxy/: foo (200; 7.936114ms)
    Jun 28 08:24:16.729: INFO: (17) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:1080/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:1080/proxy/rewriteme">... (200; 8.189126ms)
    Jun 28 08:24:16.729: INFO: (17) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:1080/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:1080/proxy/rewriteme">test<... (200; 8.265331ms)
    Jun 28 08:24:16.729: INFO: (17) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:162/proxy/: bar (200; 8.329418ms)
    Jun 28 08:24:16.729: INFO: (17) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:443/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:443/proxy/tlsrewritem... (200; 8.288637ms)
    Jun 28 08:24:16.729: INFO: (17) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl/proxy/rewriteme">test</a> (200; 8.445458ms)
    Jun 28 08:24:16.729: INFO: (17) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:460/proxy/: tls baz (200; 8.282168ms)
    Jun 28 08:24:16.730: INFO: (17) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:462/proxy/: tls qux (200; 9.188227ms)
    Jun 28 08:24:16.730: INFO: (17) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:162/proxy/: bar (200; 8.99983ms)
    Jun 28 08:24:16.731: INFO: (17) /api/v1/namespaces/proxy-5560/services/https:proxy-service-hqlww:tlsportname2/proxy/: tls qux (200; 10.508812ms)
    Jun 28 08:24:16.732: INFO: (17) /api/v1/namespaces/proxy-5560/services/https:proxy-service-hqlww:tlsportname1/proxy/: tls baz (200; 10.81529ms)
    Jun 28 08:24:16.732: INFO: (17) /api/v1/namespaces/proxy-5560/services/proxy-service-hqlww:portname1/proxy/: foo (200; 10.739417ms)
    Jun 28 08:24:16.732: INFO: (17) /api/v1/namespaces/proxy-5560/services/http:proxy-service-hqlww:portname1/proxy/: foo (200; 10.958615ms)
    Jun 28 08:24:16.732: INFO: (17) /api/v1/namespaces/proxy-5560/services/proxy-service-hqlww:portname2/proxy/: bar (200; 11.810183ms)
    Jun 28 08:24:16.733: INFO: (17) /api/v1/namespaces/proxy-5560/services/http:proxy-service-hqlww:portname2/proxy/: bar (200; 11.74016ms)
    Jun 28 08:24:16.740: INFO: (18) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl/proxy/rewriteme">test</a> (200; 6.991078ms)
    Jun 28 08:24:16.742: INFO: (18) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:1080/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:1080/proxy/rewriteme">... (200; 8.819437ms)
    Jun 28 08:24:16.742: INFO: (18) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:162/proxy/: bar (200; 8.558216ms)
    Jun 28 08:24:16.742: INFO: (18) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:1080/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:1080/proxy/rewriteme">test<... (200; 8.658508ms)
    Jun 28 08:24:16.742: INFO: (18) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:162/proxy/: bar (200; 8.750409ms)
    Jun 28 08:24:16.742: INFO: (18) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:462/proxy/: tls qux (200; 9.238806ms)
    Jun 28 08:24:16.742: INFO: (18) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:160/proxy/: foo (200; 8.686772ms)
    Jun 28 08:24:16.742: INFO: (18) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:443/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:443/proxy/tlsrewritem... (200; 8.897137ms)
    Jun 28 08:24:16.742: INFO: (18) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:460/proxy/: tls baz (200; 8.869085ms)
    Jun 28 08:24:16.742: INFO: (18) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:160/proxy/: foo (200; 9.001408ms)
    Jun 28 08:24:16.746: INFO: (18) /api/v1/namespaces/proxy-5560/services/proxy-service-hqlww:portname2/proxy/: bar (200; 12.817112ms)
    Jun 28 08:24:16.746: INFO: (18) /api/v1/namespaces/proxy-5560/services/https:proxy-service-hqlww:tlsportname1/proxy/: tls baz (200; 13.216919ms)
    Jun 28 08:24:16.746: INFO: (18) /api/v1/namespaces/proxy-5560/services/http:proxy-service-hqlww:portname2/proxy/: bar (200; 13.27527ms)
    Jun 28 08:24:16.746: INFO: (18) /api/v1/namespaces/proxy-5560/services/https:proxy-service-hqlww:tlsportname2/proxy/: tls qux (200; 13.430266ms)
    Jun 28 08:24:16.747: INFO: (18) /api/v1/namespaces/proxy-5560/services/proxy-service-hqlww:portname1/proxy/: foo (200; 14.107964ms)
    Jun 28 08:24:16.748: INFO: (18) /api/v1/namespaces/proxy-5560/services/http:proxy-service-hqlww:portname1/proxy/: foo (200; 14.497743ms)
    Jun 28 08:24:16.754: INFO: (19) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:1080/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:1080/proxy/rewriteme">... (200; 6.458826ms)
    Jun 28 08:24:16.755: INFO: (19) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:462/proxy/: tls qux (200; 7.026529ms)
    Jun 28 08:24:16.755: INFO: (19) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:160/proxy/: foo (200; 7.086592ms)
    Jun 28 08:24:16.755: INFO: (19) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:1080/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:1080/proxy/rewriteme">test<... (200; 7.13187ms)
    Jun 28 08:24:16.755: INFO: (19) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:162/proxy/: bar (200; 7.186816ms)
    Jun 28 08:24:16.755: INFO: (19) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl/proxy/rewriteme">test</a> (200; 7.530752ms)
    Jun 28 08:24:16.755: INFO: (19) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:460/proxy/: tls baz (200; 7.375295ms)
    Jun 28 08:24:16.755: INFO: (19) /api/v1/namespaces/proxy-5560/pods/proxy-service-hqlww-dfpzl:162/proxy/: bar (200; 7.42175ms)
    Jun 28 08:24:16.757: INFO: (19) /api/v1/namespaces/proxy-5560/pods/http:proxy-service-hqlww-dfpzl:160/proxy/: foo (200; 9.011209ms)
    Jun 28 08:24:16.757: INFO: (19) /api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:443/proxy/: <a href="/api/v1/namespaces/proxy-5560/pods/https:proxy-service-hqlww-dfpzl:443/proxy/tlsrewritem... (200; 8.788338ms)
    Jun 28 08:24:16.758: INFO: (19) /api/v1/namespaces/proxy-5560/services/https:proxy-service-hqlww:tlsportname1/proxy/: tls baz (200; 10.267734ms)
    Jun 28 08:24:16.758: INFO: (19) /api/v1/namespaces/proxy-5560/services/http:proxy-service-hqlww:portname1/proxy/: foo (200; 10.5042ms)
    Jun 28 08:24:16.758: INFO: (19) /api/v1/namespaces/proxy-5560/services/proxy-service-hqlww:portname2/proxy/: bar (200; 10.440247ms)
    Jun 28 08:24:16.758: INFO: (19) /api/v1/namespaces/proxy-5560/services/https:proxy-service-hqlww:tlsportname2/proxy/: tls qux (200; 10.348165ms)
    Jun 28 08:24:16.759: INFO: (19) /api/v1/namespaces/proxy-5560/services/http:proxy-service-hqlww:portname2/proxy/: bar (200; 11.348363ms)
    Jun 28 08:24:16.761: INFO: (19) /api/v1/namespaces/proxy-5560/services/proxy-service-hqlww:portname1/proxy/: foo (200; 13.209459ms)
    STEP: deleting ReplicationController proxy-service-hqlww in namespace proxy-5560, will wait for the garbage collector to delete the pods 06/28/23 08:24:16.761
    Jun 28 08:24:16.824: INFO: Deleting ReplicationController proxy-service-hqlww took: 8.413886ms
    Jun 28 08:24:16.925: INFO: Terminating ReplicationController proxy-service-hqlww pods took: 101.060165ms
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    Jun 28 08:24:19.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-5560" for this suite. 06/28/23 08:24:19.936
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] CSIStorageCapacity
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
[BeforeEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:24:19.944
Jun 28 08:24:19.944: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename csistoragecapacity 06/28/23 08:24:19.945
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:24:19.959
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:24:19.964
[It]  should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
STEP: getting /apis 06/28/23 08:24:19.968
STEP: getting /apis/storage.k8s.io 06/28/23 08:24:19.972
STEP: getting /apis/storage.k8s.io/v1 06/28/23 08:24:19.974
STEP: creating 06/28/23 08:24:19.976
STEP: watching 06/28/23 08:24:19.992
Jun 28 08:24:19.992: INFO: starting watch
STEP: getting 06/28/23 08:24:20.002
STEP: listing in namespace 06/28/23 08:24:20.006
STEP: listing across namespaces 06/28/23 08:24:20.01
STEP: patching 06/28/23 08:24:20.013
STEP: updating 06/28/23 08:24:20.019
Jun 28 08:24:20.024: INFO: waiting for watch events with expected annotations in namespace
Jun 28 08:24:20.024: INFO: waiting for watch events with expected annotations across namespace
STEP: deleting 06/28/23 08:24:20.024
STEP: deleting a collection 06/28/23 08:24:20.036
[AfterEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/framework.go:187
Jun 28 08:24:20.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "csistoragecapacity-23" for this suite. 06/28/23 08:24:20.058
{"msg":"PASSED [sig-storage] CSIStorageCapacity  should support CSIStorageCapacities API operations [Conformance]","completed":203,"skipped":3437,"failed":0}
------------------------------
â€¢ [0.121 seconds]
[sig-storage] CSIStorageCapacity
test/e2e/storage/utils/framework.go:23
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:24:19.944
    Jun 28 08:24:19.944: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename csistoragecapacity 06/28/23 08:24:19.945
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:24:19.959
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:24:19.964
    [It]  should support CSIStorageCapacities API operations [Conformance]
      test/e2e/storage/csistoragecapacity.go:49
    STEP: getting /apis 06/28/23 08:24:19.968
    STEP: getting /apis/storage.k8s.io 06/28/23 08:24:19.972
    STEP: getting /apis/storage.k8s.io/v1 06/28/23 08:24:19.974
    STEP: creating 06/28/23 08:24:19.976
    STEP: watching 06/28/23 08:24:19.992
    Jun 28 08:24:19.992: INFO: starting watch
    STEP: getting 06/28/23 08:24:20.002
    STEP: listing in namespace 06/28/23 08:24:20.006
    STEP: listing across namespaces 06/28/23 08:24:20.01
    STEP: patching 06/28/23 08:24:20.013
    STEP: updating 06/28/23 08:24:20.019
    Jun 28 08:24:20.024: INFO: waiting for watch events with expected annotations in namespace
    Jun 28 08:24:20.024: INFO: waiting for watch events with expected annotations across namespace
    STEP: deleting 06/28/23 08:24:20.024
    STEP: deleting a collection 06/28/23 08:24:20.036
    [AfterEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/framework.go:187
    Jun 28 08:24:20.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "csistoragecapacity-23" for this suite. 06/28/23 08:24:20.058
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:24:20.069
Jun 28 08:24:20.069: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename resourcequota 06/28/23 08:24:20.07
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:24:20.082
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:24:20.086
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90
STEP: Counting existing ResourceQuota 06/28/23 08:24:20.091
STEP: Creating a ResourceQuota 06/28/23 08:24:25.095
STEP: Ensuring resource quota status is calculated 06/28/23 08:24:25.1
STEP: Creating a Service 06/28/23 08:24:27.105
STEP: Creating a NodePort Service 06/28/23 08:24:27.119
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 06/28/23 08:24:27.136
STEP: Ensuring resource quota status captures service creation 06/28/23 08:24:27.152
STEP: Deleting Services 06/28/23 08:24:29.158
STEP: Ensuring resource quota status released usage 06/28/23 08:24:29.187
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jun 28 08:24:31.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2071" for this suite. 06/28/23 08:24:31.2
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","completed":204,"skipped":3440,"failed":0}
------------------------------
â€¢ [SLOW TEST] [11.137 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:24:20.069
    Jun 28 08:24:20.069: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename resourcequota 06/28/23 08:24:20.07
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:24:20.082
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:24:20.086
    [It] should create a ResourceQuota and capture the life of a service. [Conformance]
      test/e2e/apimachinery/resource_quota.go:90
    STEP: Counting existing ResourceQuota 06/28/23 08:24:20.091
    STEP: Creating a ResourceQuota 06/28/23 08:24:25.095
    STEP: Ensuring resource quota status is calculated 06/28/23 08:24:25.1
    STEP: Creating a Service 06/28/23 08:24:27.105
    STEP: Creating a NodePort Service 06/28/23 08:24:27.119
    STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 06/28/23 08:24:27.136
    STEP: Ensuring resource quota status captures service creation 06/28/23 08:24:27.152
    STEP: Deleting Services 06/28/23 08:24:29.158
    STEP: Ensuring resource quota status released usage 06/28/23 08:24:29.187
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jun 28 08:24:31.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-2071" for this suite. 06/28/23 08:24:31.2
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:24:31.207
Jun 28 08:24:31.207: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename init-container 06/28/23 08:24:31.207
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:24:31.22
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:24:31.224
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254
STEP: creating the pod 06/28/23 08:24:31.227
Jun 28 08:24:31.228: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Jun 28 08:24:34.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7445" for this suite. 06/28/23 08:24:34.893
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","completed":205,"skipped":3445,"failed":0}
------------------------------
â€¢ [3.692 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:24:31.207
    Jun 28 08:24:31.207: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename init-container 06/28/23 08:24:31.207
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:24:31.22
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:24:31.224
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should invoke init containers on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:254
    STEP: creating the pod 06/28/23 08:24:31.227
    Jun 28 08:24:31.228: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Jun 28 08:24:34.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-7445" for this suite. 06/28/23 08:24:34.893
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:24:34.9
Jun 28 08:24:34.900: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename cronjob 06/28/23 08:24:34.9
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:24:34.912
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:24:34.918
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
STEP: Creating a ForbidConcurrent cronjob 06/28/23 08:24:34.922
STEP: Ensuring a job is scheduled 06/28/23 08:24:34.928
STEP: Ensuring exactly one is scheduled 06/28/23 08:25:00.934
STEP: Ensuring exactly one running job exists by listing jobs explicitly 06/28/23 08:25:00.938
STEP: Ensuring no more jobs are scheduled 06/28/23 08:25:00.942
STEP: Removing cronjob 06/28/23 08:30:00.954
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Jun 28 08:30:00.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-3399" for this suite. 06/28/23 08:30:00.976
{"msg":"PASSED [sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]","completed":206,"skipped":3463,"failed":0}
------------------------------
â€¢ [SLOW TEST] [326.083 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:24:34.9
    Jun 28 08:24:34.900: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename cronjob 06/28/23 08:24:34.9
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:24:34.912
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:24:34.918
    [It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
      test/e2e/apps/cronjob.go:124
    STEP: Creating a ForbidConcurrent cronjob 06/28/23 08:24:34.922
    STEP: Ensuring a job is scheduled 06/28/23 08:24:34.928
    STEP: Ensuring exactly one is scheduled 06/28/23 08:25:00.934
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 06/28/23 08:25:00.938
    STEP: Ensuring no more jobs are scheduled 06/28/23 08:25:00.942
    STEP: Removing cronjob 06/28/23 08:30:00.954
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Jun 28 08:30:00.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-3399" for this suite. 06/28/23 08:30:00.976
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Pods
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:30:00.984
Jun 28 08:30:00.984: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename pods 06/28/23 08:30:00.985
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:30:00.998
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:30:01.003
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844
STEP: Create set of pods 06/28/23 08:30:01.008
Jun 28 08:30:01.019: INFO: created test-pod-1
Jun 28 08:30:01.025: INFO: created test-pod-2
Jun 28 08:30:01.031: INFO: created test-pod-3
STEP: waiting for all 3 pods to be running 06/28/23 08:30:01.031
Jun 28 08:30:01.032: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-6585' to be running and ready
Jun 28 08:30:01.054: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jun 28 08:30:01.054: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jun 28 08:30:01.054: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jun 28 08:30:01.054: INFO: 0 / 3 pods in namespace 'pods-6585' are running and ready (0 seconds elapsed)
Jun 28 08:30:01.054: INFO: expected 0 pod replicas in namespace 'pods-6585', 0 are Running and Ready.
Jun 28 08:30:01.054: INFO: POD         NODE                             PHASE    GRACE  CONDITIONS
Jun 28 08:30:01.054: INFO: test-pod-1  ske-rhel-749f7d55c8xdd8b6-zxlfv  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-28 08:30:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-28 08:30:01 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-28 08:30:01 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-28 08:30:01 +0000 UTC  }]
Jun 28 08:30:01.054: INFO: test-pod-2  ske-rhel-749f7d55c8xdd8b6-ct4cp  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-28 08:30:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-28 08:30:01 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-28 08:30:01 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-28 08:30:01 +0000 UTC  }]
Jun 28 08:30:01.054: INFO: test-pod-3  ske-rhel-749f7d55c8xdd8b6-zxlfv  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-28 08:30:01 +0000 UTC  }]
Jun 28 08:30:01.054: INFO: 
Jun 28 08:30:03.072: INFO: 3 / 3 pods in namespace 'pods-6585' are running and ready (2 seconds elapsed)
Jun 28 08:30:03.072: INFO: expected 0 pod replicas in namespace 'pods-6585', 0 are Running and Ready.
STEP: waiting for all pods to be deleted 06/28/23 08:30:03.098
Jun 28 08:30:03.105: INFO: Pod quantity 3 is different from expected quantity 0
Jun 28 08:30:04.114: INFO: Pod quantity 3 is different from expected quantity 0
Jun 28 08:30:05.112: INFO: Pod quantity 3 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jun 28 08:30:06.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6585" for this suite. 06/28/23 08:30:06.119
{"msg":"PASSED [sig-node] Pods should delete a collection of pods [Conformance]","completed":207,"skipped":3468,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.142 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:30:00.984
    Jun 28 08:30:00.984: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename pods 06/28/23 08:30:00.985
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:30:00.998
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:30:01.003
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should delete a collection of pods [Conformance]
      test/e2e/common/node/pods.go:844
    STEP: Create set of pods 06/28/23 08:30:01.008
    Jun 28 08:30:01.019: INFO: created test-pod-1
    Jun 28 08:30:01.025: INFO: created test-pod-2
    Jun 28 08:30:01.031: INFO: created test-pod-3
    STEP: waiting for all 3 pods to be running 06/28/23 08:30:01.031
    Jun 28 08:30:01.032: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-6585' to be running and ready
    Jun 28 08:30:01.054: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jun 28 08:30:01.054: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jun 28 08:30:01.054: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jun 28 08:30:01.054: INFO: 0 / 3 pods in namespace 'pods-6585' are running and ready (0 seconds elapsed)
    Jun 28 08:30:01.054: INFO: expected 0 pod replicas in namespace 'pods-6585', 0 are Running and Ready.
    Jun 28 08:30:01.054: INFO: POD         NODE                             PHASE    GRACE  CONDITIONS
    Jun 28 08:30:01.054: INFO: test-pod-1  ske-rhel-749f7d55c8xdd8b6-zxlfv  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-28 08:30:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-28 08:30:01 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-28 08:30:01 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-28 08:30:01 +0000 UTC  }]
    Jun 28 08:30:01.054: INFO: test-pod-2  ske-rhel-749f7d55c8xdd8b6-ct4cp  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-28 08:30:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-28 08:30:01 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-28 08:30:01 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-28 08:30:01 +0000 UTC  }]
    Jun 28 08:30:01.054: INFO: test-pod-3  ske-rhel-749f7d55c8xdd8b6-zxlfv  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-28 08:30:01 +0000 UTC  }]
    Jun 28 08:30:01.054: INFO: 
    Jun 28 08:30:03.072: INFO: 3 / 3 pods in namespace 'pods-6585' are running and ready (2 seconds elapsed)
    Jun 28 08:30:03.072: INFO: expected 0 pod replicas in namespace 'pods-6585', 0 are Running and Ready.
    STEP: waiting for all pods to be deleted 06/28/23 08:30:03.098
    Jun 28 08:30:03.105: INFO: Pod quantity 3 is different from expected quantity 0
    Jun 28 08:30:04.114: INFO: Pod quantity 3 is different from expected quantity 0
    Jun 28 08:30:05.112: INFO: Pod quantity 3 is different from expected quantity 0
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jun 28 08:30:06.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-6585" for this suite. 06/28/23 08:30:06.119
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:30:06.126
Jun 28 08:30:06.127: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename emptydir 06/28/23 08:30:06.127
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:30:06.141
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:30:06.146
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106
STEP: Creating a pod to test emptydir 0666 on tmpfs 06/28/23 08:30:06.15
Jun 28 08:30:06.158: INFO: Waiting up to 5m0s for pod "pod-3676799c-d759-4f9e-b1fb-41293c13cb95" in namespace "emptydir-9389" to be "Succeeded or Failed"
Jun 28 08:30:06.163: INFO: Pod "pod-3676799c-d759-4f9e-b1fb-41293c13cb95": Phase="Pending", Reason="", readiness=false. Elapsed: 4.309359ms
Jun 28 08:30:08.169: INFO: Pod "pod-3676799c-d759-4f9e-b1fb-41293c13cb95": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010479406s
Jun 28 08:30:10.170: INFO: Pod "pod-3676799c-d759-4f9e-b1fb-41293c13cb95": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011265624s
STEP: Saw pod success 06/28/23 08:30:10.17
Jun 28 08:30:10.170: INFO: Pod "pod-3676799c-d759-4f9e-b1fb-41293c13cb95" satisfied condition "Succeeded or Failed"
Jun 28 08:30:10.174: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod pod-3676799c-d759-4f9e-b1fb-41293c13cb95 container test-container: <nil>
STEP: delete the pod 06/28/23 08:30:10.188
Jun 28 08:30:10.199: INFO: Waiting for pod pod-3676799c-d759-4f9e-b1fb-41293c13cb95 to disappear
Jun 28 08:30:10.203: INFO: Pod pod-3676799c-d759-4f9e-b1fb-41293c13cb95 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jun 28 08:30:10.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9389" for this suite. 06/28/23 08:30:10.213
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":208,"skipped":3490,"failed":0}
------------------------------
â€¢ [4.094 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:30:06.126
    Jun 28 08:30:06.127: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename emptydir 06/28/23 08:30:06.127
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:30:06.141
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:30:06.146
    [It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:106
    STEP: Creating a pod to test emptydir 0666 on tmpfs 06/28/23 08:30:06.15
    Jun 28 08:30:06.158: INFO: Waiting up to 5m0s for pod "pod-3676799c-d759-4f9e-b1fb-41293c13cb95" in namespace "emptydir-9389" to be "Succeeded or Failed"
    Jun 28 08:30:06.163: INFO: Pod "pod-3676799c-d759-4f9e-b1fb-41293c13cb95": Phase="Pending", Reason="", readiness=false. Elapsed: 4.309359ms
    Jun 28 08:30:08.169: INFO: Pod "pod-3676799c-d759-4f9e-b1fb-41293c13cb95": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010479406s
    Jun 28 08:30:10.170: INFO: Pod "pod-3676799c-d759-4f9e-b1fb-41293c13cb95": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011265624s
    STEP: Saw pod success 06/28/23 08:30:10.17
    Jun 28 08:30:10.170: INFO: Pod "pod-3676799c-d759-4f9e-b1fb-41293c13cb95" satisfied condition "Succeeded or Failed"
    Jun 28 08:30:10.174: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod pod-3676799c-d759-4f9e-b1fb-41293c13cb95 container test-container: <nil>
    STEP: delete the pod 06/28/23 08:30:10.188
    Jun 28 08:30:10.199: INFO: Waiting for pod pod-3676799c-d759-4f9e-b1fb-41293c13cb95 to disappear
    Jun 28 08:30:10.203: INFO: Pod pod-3676799c-d759-4f9e-b1fb-41293c13cb95 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jun 28 08:30:10.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-9389" for this suite. 06/28/23 08:30:10.213
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:30:10.221
Jun 28 08:30:10.221: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename resourcequota 06/28/23 08:30:10.222
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:30:10.24
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:30:10.245
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150
STEP: Discovering how many secrets are in namespace by default 06/28/23 08:30:10.25
STEP: Counting existing ResourceQuota 06/28/23 08:30:15.255
STEP: Creating a ResourceQuota 06/28/23 08:30:20.261
STEP: Ensuring resource quota status is calculated 06/28/23 08:30:20.267
STEP: Creating a Secret 06/28/23 08:30:22.273
STEP: Ensuring resource quota status captures secret creation 06/28/23 08:30:22.284
STEP: Deleting a secret 06/28/23 08:30:24.289
STEP: Ensuring resource quota status released usage 06/28/23 08:30:24.295
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jun 28 08:30:26.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-39" for this suite. 06/28/23 08:30:26.309
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","completed":209,"skipped":3496,"failed":0}
------------------------------
â€¢ [SLOW TEST] [16.094 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:30:10.221
    Jun 28 08:30:10.221: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename resourcequota 06/28/23 08:30:10.222
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:30:10.24
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:30:10.245
    [It] should create a ResourceQuota and capture the life of a secret. [Conformance]
      test/e2e/apimachinery/resource_quota.go:150
    STEP: Discovering how many secrets are in namespace by default 06/28/23 08:30:10.25
    STEP: Counting existing ResourceQuota 06/28/23 08:30:15.255
    STEP: Creating a ResourceQuota 06/28/23 08:30:20.261
    STEP: Ensuring resource quota status is calculated 06/28/23 08:30:20.267
    STEP: Creating a Secret 06/28/23 08:30:22.273
    STEP: Ensuring resource quota status captures secret creation 06/28/23 08:30:22.284
    STEP: Deleting a secret 06/28/23 08:30:24.289
    STEP: Ensuring resource quota status released usage 06/28/23 08:30:24.295
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jun 28 08:30:26.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-39" for this suite. 06/28/23 08:30:26.309
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] ReplicationController
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:30:26.316
Jun 28 08:30:26.316: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename replication-controller 06/28/23 08:30:26.317
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:30:26.329
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:30:26.333
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82
Jun 28 08:30:26.336: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 06/28/23 08:30:27.351
STEP: Checking rc "condition-test" has the desired failure condition set 06/28/23 08:30:27.356
STEP: Scaling down rc "condition-test" to satisfy pod quota 06/28/23 08:30:28.367
Jun 28 08:30:28.378: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set 06/28/23 08:30:28.378
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Jun 28 08:30:29.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-748" for this suite. 06/28/23 08:30:29.395
{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","completed":210,"skipped":3501,"failed":0}
------------------------------
â€¢ [3.087 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:30:26.316
    Jun 28 08:30:26.316: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename replication-controller 06/28/23 08:30:26.317
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:30:26.329
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:30:26.333
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should surface a failure condition on a common issue like exceeded quota [Conformance]
      test/e2e/apps/rc.go:82
    Jun 28 08:30:26.336: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
    STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 06/28/23 08:30:27.351
    STEP: Checking rc "condition-test" has the desired failure condition set 06/28/23 08:30:27.356
    STEP: Scaling down rc "condition-test" to satisfy pod quota 06/28/23 08:30:28.367
    Jun 28 08:30:28.378: INFO: Updating replication controller "condition-test"
    STEP: Checking rc "condition-test" has no failure condition set 06/28/23 08:30:28.378
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Jun 28 08:30:29.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-748" for this suite. 06/28/23 08:30:29.395
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] Job
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:30:29.403
Jun 28 08:30:29.403: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename job 06/28/23 08:30:29.404
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:30:29.416
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:30:29.42
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254
STEP: Creating a job 06/28/23 08:30:29.424
STEP: Ensuring job reaches completions 06/28/23 08:30:29.43
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Jun 28 08:30:39.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-3396" for this suite. 06/28/23 08:30:39.444
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","completed":211,"skipped":3504,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.049 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:30:29.403
    Jun 28 08:30:29.403: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename job 06/28/23 08:30:29.404
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:30:29.416
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:30:29.42
    [It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
      test/e2e/apps/job.go:254
    STEP: Creating a job 06/28/23 08:30:29.424
    STEP: Ensuring job reaches completions 06/28/23 08:30:29.43
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Jun 28 08:30:39.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-3396" for this suite. 06/28/23 08:30:39.444
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:30:39.452
Jun 28 08:30:39.452: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename downward-api 06/28/23 08:30:39.452
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:30:39.464
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:30:39.468
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234
STEP: Creating a pod to test downward API volume plugin 06/28/23 08:30:39.472
Jun 28 08:30:39.481: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b255de31-4f23-459b-aad7-dc0311262f87" in namespace "downward-api-3061" to be "Succeeded or Failed"
Jun 28 08:30:39.485: INFO: Pod "downwardapi-volume-b255de31-4f23-459b-aad7-dc0311262f87": Phase="Pending", Reason="", readiness=false. Elapsed: 4.13993ms
Jun 28 08:30:41.491: INFO: Pod "downwardapi-volume-b255de31-4f23-459b-aad7-dc0311262f87": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010269882s
Jun 28 08:30:43.491: INFO: Pod "downwardapi-volume-b255de31-4f23-459b-aad7-dc0311262f87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010310639s
STEP: Saw pod success 06/28/23 08:30:43.491
Jun 28 08:30:43.491: INFO: Pod "downwardapi-volume-b255de31-4f23-459b-aad7-dc0311262f87" satisfied condition "Succeeded or Failed"
Jun 28 08:30:43.496: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod downwardapi-volume-b255de31-4f23-459b-aad7-dc0311262f87 container client-container: <nil>
STEP: delete the pod 06/28/23 08:30:43.546
Jun 28 08:30:43.559: INFO: Waiting for pod downwardapi-volume-b255de31-4f23-459b-aad7-dc0311262f87 to disappear
Jun 28 08:30:43.563: INFO: Pod downwardapi-volume-b255de31-4f23-459b-aad7-dc0311262f87 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jun 28 08:30:43.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3061" for this suite. 06/28/23 08:30:43.571
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","completed":212,"skipped":3506,"failed":0}
------------------------------
â€¢ [4.126 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:30:39.452
    Jun 28 08:30:39.452: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename downward-api 06/28/23 08:30:39.452
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:30:39.464
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:30:39.468
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:234
    STEP: Creating a pod to test downward API volume plugin 06/28/23 08:30:39.472
    Jun 28 08:30:39.481: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b255de31-4f23-459b-aad7-dc0311262f87" in namespace "downward-api-3061" to be "Succeeded or Failed"
    Jun 28 08:30:39.485: INFO: Pod "downwardapi-volume-b255de31-4f23-459b-aad7-dc0311262f87": Phase="Pending", Reason="", readiness=false. Elapsed: 4.13993ms
    Jun 28 08:30:41.491: INFO: Pod "downwardapi-volume-b255de31-4f23-459b-aad7-dc0311262f87": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010269882s
    Jun 28 08:30:43.491: INFO: Pod "downwardapi-volume-b255de31-4f23-459b-aad7-dc0311262f87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010310639s
    STEP: Saw pod success 06/28/23 08:30:43.491
    Jun 28 08:30:43.491: INFO: Pod "downwardapi-volume-b255de31-4f23-459b-aad7-dc0311262f87" satisfied condition "Succeeded or Failed"
    Jun 28 08:30:43.496: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod downwardapi-volume-b255de31-4f23-459b-aad7-dc0311262f87 container client-container: <nil>
    STEP: delete the pod 06/28/23 08:30:43.546
    Jun 28 08:30:43.559: INFO: Waiting for pod downwardapi-volume-b255de31-4f23-459b-aad7-dc0311262f87 to disappear
    Jun 28 08:30:43.563: INFO: Pod downwardapi-volume-b255de31-4f23-459b-aad7-dc0311262f87 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jun 28 08:30:43.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-3061" for this suite. 06/28/23 08:30:43.571
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:30:43.579
Jun 28 08:30:43.579: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename subpath 06/28/23 08:30:43.58
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:30:43.594
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:30:43.599
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 06/28/23 08:30:43.604
[It] should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
STEP: Creating pod pod-subpath-test-downwardapi-th8z 06/28/23 08:30:43.615
STEP: Creating a pod to test atomic-volume-subpath 06/28/23 08:30:43.615
Jun 28 08:30:43.625: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-th8z" in namespace "subpath-6546" to be "Succeeded or Failed"
Jun 28 08:30:43.631: INFO: Pod "pod-subpath-test-downwardapi-th8z": Phase="Pending", Reason="", readiness=false. Elapsed: 5.555713ms
Jun 28 08:30:45.637: INFO: Pod "pod-subpath-test-downwardapi-th8z": Phase="Running", Reason="", readiness=true. Elapsed: 2.012007452s
Jun 28 08:30:47.636: INFO: Pod "pod-subpath-test-downwardapi-th8z": Phase="Running", Reason="", readiness=true. Elapsed: 4.011083872s
Jun 28 08:30:49.636: INFO: Pod "pod-subpath-test-downwardapi-th8z": Phase="Running", Reason="", readiness=true. Elapsed: 6.010568214s
Jun 28 08:30:51.636: INFO: Pod "pod-subpath-test-downwardapi-th8z": Phase="Running", Reason="", readiness=true. Elapsed: 8.010955353s
Jun 28 08:30:53.637: INFO: Pod "pod-subpath-test-downwardapi-th8z": Phase="Running", Reason="", readiness=true. Elapsed: 10.011384975s
Jun 28 08:30:55.636: INFO: Pod "pod-subpath-test-downwardapi-th8z": Phase="Running", Reason="", readiness=true. Elapsed: 12.010703715s
Jun 28 08:30:57.636: INFO: Pod "pod-subpath-test-downwardapi-th8z": Phase="Running", Reason="", readiness=true. Elapsed: 14.010862871s
Jun 28 08:30:59.636: INFO: Pod "pod-subpath-test-downwardapi-th8z": Phase="Running", Reason="", readiness=true. Elapsed: 16.011208952s
Jun 28 08:31:01.638: INFO: Pod "pod-subpath-test-downwardapi-th8z": Phase="Running", Reason="", readiness=true. Elapsed: 18.012647925s
Jun 28 08:31:03.637: INFO: Pod "pod-subpath-test-downwardapi-th8z": Phase="Running", Reason="", readiness=true. Elapsed: 20.011840081s
Jun 28 08:31:05.636: INFO: Pod "pod-subpath-test-downwardapi-th8z": Phase="Running", Reason="", readiness=false. Elapsed: 22.010985356s
Jun 28 08:31:07.636: INFO: Pod "pod-subpath-test-downwardapi-th8z": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.010886486s
STEP: Saw pod success 06/28/23 08:31:07.636
Jun 28 08:31:07.636: INFO: Pod "pod-subpath-test-downwardapi-th8z" satisfied condition "Succeeded or Failed"
Jun 28 08:31:07.640: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod pod-subpath-test-downwardapi-th8z container test-container-subpath-downwardapi-th8z: <nil>
STEP: delete the pod 06/28/23 08:31:07.677
Jun 28 08:31:07.688: INFO: Waiting for pod pod-subpath-test-downwardapi-th8z to disappear
Jun 28 08:31:07.692: INFO: Pod pod-subpath-test-downwardapi-th8z no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-th8z 06/28/23 08:31:07.692
Jun 28 08:31:07.692: INFO: Deleting pod "pod-subpath-test-downwardapi-th8z" in namespace "subpath-6546"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Jun 28 08:31:07.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6546" for this suite. 06/28/23 08:31:07.703
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [Conformance]","completed":213,"skipped":3523,"failed":0}
------------------------------
â€¢ [SLOW TEST] [24.131 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with downward pod [Conformance]
    test/e2e/storage/subpath.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:30:43.579
    Jun 28 08:30:43.579: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename subpath 06/28/23 08:30:43.58
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:30:43.594
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:30:43.599
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 06/28/23 08:30:43.604
    [It] should support subpaths with downward pod [Conformance]
      test/e2e/storage/subpath.go:92
    STEP: Creating pod pod-subpath-test-downwardapi-th8z 06/28/23 08:30:43.615
    STEP: Creating a pod to test atomic-volume-subpath 06/28/23 08:30:43.615
    Jun 28 08:30:43.625: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-th8z" in namespace "subpath-6546" to be "Succeeded or Failed"
    Jun 28 08:30:43.631: INFO: Pod "pod-subpath-test-downwardapi-th8z": Phase="Pending", Reason="", readiness=false. Elapsed: 5.555713ms
    Jun 28 08:30:45.637: INFO: Pod "pod-subpath-test-downwardapi-th8z": Phase="Running", Reason="", readiness=true. Elapsed: 2.012007452s
    Jun 28 08:30:47.636: INFO: Pod "pod-subpath-test-downwardapi-th8z": Phase="Running", Reason="", readiness=true. Elapsed: 4.011083872s
    Jun 28 08:30:49.636: INFO: Pod "pod-subpath-test-downwardapi-th8z": Phase="Running", Reason="", readiness=true. Elapsed: 6.010568214s
    Jun 28 08:30:51.636: INFO: Pod "pod-subpath-test-downwardapi-th8z": Phase="Running", Reason="", readiness=true. Elapsed: 8.010955353s
    Jun 28 08:30:53.637: INFO: Pod "pod-subpath-test-downwardapi-th8z": Phase="Running", Reason="", readiness=true. Elapsed: 10.011384975s
    Jun 28 08:30:55.636: INFO: Pod "pod-subpath-test-downwardapi-th8z": Phase="Running", Reason="", readiness=true. Elapsed: 12.010703715s
    Jun 28 08:30:57.636: INFO: Pod "pod-subpath-test-downwardapi-th8z": Phase="Running", Reason="", readiness=true. Elapsed: 14.010862871s
    Jun 28 08:30:59.636: INFO: Pod "pod-subpath-test-downwardapi-th8z": Phase="Running", Reason="", readiness=true. Elapsed: 16.011208952s
    Jun 28 08:31:01.638: INFO: Pod "pod-subpath-test-downwardapi-th8z": Phase="Running", Reason="", readiness=true. Elapsed: 18.012647925s
    Jun 28 08:31:03.637: INFO: Pod "pod-subpath-test-downwardapi-th8z": Phase="Running", Reason="", readiness=true. Elapsed: 20.011840081s
    Jun 28 08:31:05.636: INFO: Pod "pod-subpath-test-downwardapi-th8z": Phase="Running", Reason="", readiness=false. Elapsed: 22.010985356s
    Jun 28 08:31:07.636: INFO: Pod "pod-subpath-test-downwardapi-th8z": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.010886486s
    STEP: Saw pod success 06/28/23 08:31:07.636
    Jun 28 08:31:07.636: INFO: Pod "pod-subpath-test-downwardapi-th8z" satisfied condition "Succeeded or Failed"
    Jun 28 08:31:07.640: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod pod-subpath-test-downwardapi-th8z container test-container-subpath-downwardapi-th8z: <nil>
    STEP: delete the pod 06/28/23 08:31:07.677
    Jun 28 08:31:07.688: INFO: Waiting for pod pod-subpath-test-downwardapi-th8z to disappear
    Jun 28 08:31:07.692: INFO: Pod pod-subpath-test-downwardapi-th8z no longer exists
    STEP: Deleting pod pod-subpath-test-downwardapi-th8z 06/28/23 08:31:07.692
    Jun 28 08:31:07.692: INFO: Deleting pod "pod-subpath-test-downwardapi-th8z" in namespace "subpath-6546"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Jun 28 08:31:07.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-6546" for this suite. 06/28/23 08:31:07.703
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:31:07.711
Jun 28 08:31:07.711: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename svcaccounts 06/28/23 08:31:07.711
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:31:07.725
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:31:07.73
[It] should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75
Jun 28 08:31:07.749: INFO: Waiting up to 5m0s for pod "pod-service-account-a47d3e56-ce9e-4e60-8c96-6e495d98c167" in namespace "svcaccounts-9967" to be "running"
Jun 28 08:31:07.754: INFO: Pod "pod-service-account-a47d3e56-ce9e-4e60-8c96-6e495d98c167": Phase="Pending", Reason="", readiness=false. Elapsed: 4.715164ms
Jun 28 08:31:09.760: INFO: Pod "pod-service-account-a47d3e56-ce9e-4e60-8c96-6e495d98c167": Phase="Running", Reason="", readiness=true. Elapsed: 2.010556151s
Jun 28 08:31:09.760: INFO: Pod "pod-service-account-a47d3e56-ce9e-4e60-8c96-6e495d98c167" satisfied condition "running"
STEP: reading a file in the container 06/28/23 08:31:09.76
Jun 28 08:31:09.760: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-9967 pod-service-account-a47d3e56-ce9e-4e60-8c96-6e495d98c167 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container 06/28/23 08:31:10.254
Jun 28 08:31:10.255: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-9967 pod-service-account-a47d3e56-ce9e-4e60-8c96-6e495d98c167 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container 06/28/23 08:31:10.726
Jun 28 08:31:10.726: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-9967 pod-service-account-a47d3e56-ce9e-4e60-8c96-6e495d98c167 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
Jun 28 08:31:11.188: INFO: Got root ca configmap in namespace "svcaccounts-9967"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Jun 28 08:31:11.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-9967" for this suite. 06/28/23 08:31:11.202
{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","completed":214,"skipped":3528,"failed":0}
------------------------------
â€¢ [3.500 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:31:07.711
    Jun 28 08:31:07.711: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename svcaccounts 06/28/23 08:31:07.711
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:31:07.725
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:31:07.73
    [It] should mount an API token into pods  [Conformance]
      test/e2e/auth/service_accounts.go:75
    Jun 28 08:31:07.749: INFO: Waiting up to 5m0s for pod "pod-service-account-a47d3e56-ce9e-4e60-8c96-6e495d98c167" in namespace "svcaccounts-9967" to be "running"
    Jun 28 08:31:07.754: INFO: Pod "pod-service-account-a47d3e56-ce9e-4e60-8c96-6e495d98c167": Phase="Pending", Reason="", readiness=false. Elapsed: 4.715164ms
    Jun 28 08:31:09.760: INFO: Pod "pod-service-account-a47d3e56-ce9e-4e60-8c96-6e495d98c167": Phase="Running", Reason="", readiness=true. Elapsed: 2.010556151s
    Jun 28 08:31:09.760: INFO: Pod "pod-service-account-a47d3e56-ce9e-4e60-8c96-6e495d98c167" satisfied condition "running"
    STEP: reading a file in the container 06/28/23 08:31:09.76
    Jun 28 08:31:09.760: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-9967 pod-service-account-a47d3e56-ce9e-4e60-8c96-6e495d98c167 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
    STEP: reading a file in the container 06/28/23 08:31:10.254
    Jun 28 08:31:10.255: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-9967 pod-service-account-a47d3e56-ce9e-4e60-8c96-6e495d98c167 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
    STEP: reading a file in the container 06/28/23 08:31:10.726
    Jun 28 08:31:10.726: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-9967 pod-service-account-a47d3e56-ce9e-4e60-8c96-6e495d98c167 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
    Jun 28 08:31:11.188: INFO: Got root ca configmap in namespace "svcaccounts-9967"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Jun 28 08:31:11.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-9967" for this suite. 06/28/23 08:31:11.202
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:31:11.211
Jun 28 08:31:11.211: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename emptydir 06/28/23 08:31:11.211
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:31:11.225
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:31:11.233
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126
STEP: Creating a pod to test emptydir 0644 on tmpfs 06/28/23 08:31:11.241
Jun 28 08:31:11.251: INFO: Waiting up to 5m0s for pod "pod-6d84c3b1-e0cd-4d4c-9b31-98db9da04587" in namespace "emptydir-5370" to be "Succeeded or Failed"
Jun 28 08:31:11.257: INFO: Pod "pod-6d84c3b1-e0cd-4d4c-9b31-98db9da04587": Phase="Pending", Reason="", readiness=false. Elapsed: 6.847475ms
Jun 28 08:31:13.263: INFO: Pod "pod-6d84c3b1-e0cd-4d4c-9b31-98db9da04587": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012636412s
Jun 28 08:31:15.265: INFO: Pod "pod-6d84c3b1-e0cd-4d4c-9b31-98db9da04587": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014351177s
STEP: Saw pod success 06/28/23 08:31:15.265
Jun 28 08:31:15.265: INFO: Pod "pod-6d84c3b1-e0cd-4d4c-9b31-98db9da04587" satisfied condition "Succeeded or Failed"
Jun 28 08:31:15.269: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-zxlfv pod pod-6d84c3b1-e0cd-4d4c-9b31-98db9da04587 container test-container: <nil>
STEP: delete the pod 06/28/23 08:31:15.321
Jun 28 08:31:15.335: INFO: Waiting for pod pod-6d84c3b1-e0cd-4d4c-9b31-98db9da04587 to disappear
Jun 28 08:31:15.339: INFO: Pod pod-6d84c3b1-e0cd-4d4c-9b31-98db9da04587 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jun 28 08:31:15.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5370" for this suite. 06/28/23 08:31:15.348
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":215,"skipped":3530,"failed":0}
------------------------------
â€¢ [4.144 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:31:11.211
    Jun 28 08:31:11.211: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename emptydir 06/28/23 08:31:11.211
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:31:11.225
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:31:11.233
    [It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:126
    STEP: Creating a pod to test emptydir 0644 on tmpfs 06/28/23 08:31:11.241
    Jun 28 08:31:11.251: INFO: Waiting up to 5m0s for pod "pod-6d84c3b1-e0cd-4d4c-9b31-98db9da04587" in namespace "emptydir-5370" to be "Succeeded or Failed"
    Jun 28 08:31:11.257: INFO: Pod "pod-6d84c3b1-e0cd-4d4c-9b31-98db9da04587": Phase="Pending", Reason="", readiness=false. Elapsed: 6.847475ms
    Jun 28 08:31:13.263: INFO: Pod "pod-6d84c3b1-e0cd-4d4c-9b31-98db9da04587": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012636412s
    Jun 28 08:31:15.265: INFO: Pod "pod-6d84c3b1-e0cd-4d4c-9b31-98db9da04587": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014351177s
    STEP: Saw pod success 06/28/23 08:31:15.265
    Jun 28 08:31:15.265: INFO: Pod "pod-6d84c3b1-e0cd-4d4c-9b31-98db9da04587" satisfied condition "Succeeded or Failed"
    Jun 28 08:31:15.269: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-zxlfv pod pod-6d84c3b1-e0cd-4d4c-9b31-98db9da04587 container test-container: <nil>
    STEP: delete the pod 06/28/23 08:31:15.321
    Jun 28 08:31:15.335: INFO: Waiting for pod pod-6d84c3b1-e0cd-4d4c-9b31-98db9da04587 to disappear
    Jun 28 08:31:15.339: INFO: Pod pod-6d84c3b1-e0cd-4d4c-9b31-98db9da04587 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jun 28 08:31:15.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-5370" for this suite. 06/28/23 08:31:15.348
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:31:15.355
Jun 28 08:31:15.356: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename resourcequota 06/28/23 08:31:15.356
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:31:15.37
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:31:15.374
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220
STEP: Counting existing ResourceQuota 06/28/23 08:31:15.38
STEP: Creating a ResourceQuota 06/28/23 08:31:20.384
STEP: Ensuring resource quota status is calculated 06/28/23 08:31:20.389
STEP: Creating a Pod that fits quota 06/28/23 08:31:22.396
STEP: Ensuring ResourceQuota status captures the pod usage 06/28/23 08:31:22.41
STEP: Not allowing a pod to be created that exceeds remaining quota 06/28/23 08:31:24.417
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 06/28/23 08:31:24.421
STEP: Ensuring a pod cannot update its resource requirements 06/28/23 08:31:24.425
STEP: Ensuring attempts to update pod resource requirements did not change quota usage 06/28/23 08:31:24.434
STEP: Deleting the pod 06/28/23 08:31:26.457
STEP: Ensuring resource quota status released the pod usage 06/28/23 08:31:26.47
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jun 28 08:31:28.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5921" for this suite. 06/28/23 08:31:28.482
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","completed":216,"skipped":3550,"failed":0}
------------------------------
â€¢ [SLOW TEST] [13.134 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:31:15.355
    Jun 28 08:31:15.356: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename resourcequota 06/28/23 08:31:15.356
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:31:15.37
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:31:15.374
    [It] should create a ResourceQuota and capture the life of a pod. [Conformance]
      test/e2e/apimachinery/resource_quota.go:220
    STEP: Counting existing ResourceQuota 06/28/23 08:31:15.38
    STEP: Creating a ResourceQuota 06/28/23 08:31:20.384
    STEP: Ensuring resource quota status is calculated 06/28/23 08:31:20.389
    STEP: Creating a Pod that fits quota 06/28/23 08:31:22.396
    STEP: Ensuring ResourceQuota status captures the pod usage 06/28/23 08:31:22.41
    STEP: Not allowing a pod to be created that exceeds remaining quota 06/28/23 08:31:24.417
    STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 06/28/23 08:31:24.421
    STEP: Ensuring a pod cannot update its resource requirements 06/28/23 08:31:24.425
    STEP: Ensuring attempts to update pod resource requirements did not change quota usage 06/28/23 08:31:24.434
    STEP: Deleting the pod 06/28/23 08:31:26.457
    STEP: Ensuring resource quota status released the pod usage 06/28/23 08:31:26.47
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jun 28 08:31:28.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-5921" for this suite. 06/28/23 08:31:28.482
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:346
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:31:28.49
Jun 28 08:31:28.490: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename security-context-test 06/28/23 08:31:28.491
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:31:28.503
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:31:28.508
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:346
Jun 28 08:31:28.519: INFO: Waiting up to 5m0s for pod "busybox-user-65534-dc043723-1ad0-47d2-8335-08365a05c271" in namespace "security-context-test-8699" to be "Succeeded or Failed"
Jun 28 08:31:28.524: INFO: Pod "busybox-user-65534-dc043723-1ad0-47d2-8335-08365a05c271": Phase="Pending", Reason="", readiness=false. Elapsed: 4.581014ms
Jun 28 08:31:30.530: INFO: Pod "busybox-user-65534-dc043723-1ad0-47d2-8335-08365a05c271": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010312515s
Jun 28 08:31:32.531: INFO: Pod "busybox-user-65534-dc043723-1ad0-47d2-8335-08365a05c271": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01178288s
Jun 28 08:31:32.531: INFO: Pod "busybox-user-65534-dc043723-1ad0-47d2-8335-08365a05c271" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Jun 28 08:31:32.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-8699" for this suite. 06/28/23 08:31:32.54
{"msg":"PASSED [sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","completed":217,"skipped":3554,"failed":0}
------------------------------
â€¢ [4.058 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a container with runAsUser
  test/e2e/common/node/security_context.go:308
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:346

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:31:28.49
    Jun 28 08:31:28.490: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename security-context-test 06/28/23 08:31:28.491
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:31:28.503
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:31:28.508
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:346
    Jun 28 08:31:28.519: INFO: Waiting up to 5m0s for pod "busybox-user-65534-dc043723-1ad0-47d2-8335-08365a05c271" in namespace "security-context-test-8699" to be "Succeeded or Failed"
    Jun 28 08:31:28.524: INFO: Pod "busybox-user-65534-dc043723-1ad0-47d2-8335-08365a05c271": Phase="Pending", Reason="", readiness=false. Elapsed: 4.581014ms
    Jun 28 08:31:30.530: INFO: Pod "busybox-user-65534-dc043723-1ad0-47d2-8335-08365a05c271": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010312515s
    Jun 28 08:31:32.531: INFO: Pod "busybox-user-65534-dc043723-1ad0-47d2-8335-08365a05c271": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01178288s
    Jun 28 08:31:32.531: INFO: Pod "busybox-user-65534-dc043723-1ad0-47d2-8335-08365a05c271" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Jun 28 08:31:32.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-8699" for this suite. 06/28/23 08:31:32.54
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:31:32.549
Jun 28 08:31:32.549: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename configmap 06/28/23 08:31:32.55
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:31:32.565
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:31:32.57
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239
STEP: Creating configMap with name cm-test-opt-del-d997c412-77c3-4bdd-9d75-083e4b42c8d2 06/28/23 08:31:32.585
STEP: Creating configMap with name cm-test-opt-upd-ced7b1b6-147b-4a49-a9f3-1a8ddc434c52 06/28/23 08:31:32.59
STEP: Creating the pod 06/28/23 08:31:32.598
Jun 28 08:31:32.611: INFO: Waiting up to 5m0s for pod "pod-configmaps-be17724a-98b3-4fd6-b50e-64143b6545c8" in namespace "configmap-610" to be "running and ready"
Jun 28 08:31:32.616: INFO: Pod "pod-configmaps-be17724a-98b3-4fd6-b50e-64143b6545c8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.892241ms
Jun 28 08:31:32.616: INFO: The phase of Pod pod-configmaps-be17724a-98b3-4fd6-b50e-64143b6545c8 is Pending, waiting for it to be Running (with Ready = true)
Jun 28 08:31:34.622: INFO: Pod "pod-configmaps-be17724a-98b3-4fd6-b50e-64143b6545c8": Phase="Running", Reason="", readiness=true. Elapsed: 2.011399644s
Jun 28 08:31:34.622: INFO: The phase of Pod pod-configmaps-be17724a-98b3-4fd6-b50e-64143b6545c8 is Running (Ready = true)
Jun 28 08:31:34.622: INFO: Pod "pod-configmaps-be17724a-98b3-4fd6-b50e-64143b6545c8" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-d997c412-77c3-4bdd-9d75-083e4b42c8d2 06/28/23 08:31:34.806
STEP: Updating configmap cm-test-opt-upd-ced7b1b6-147b-4a49-a9f3-1a8ddc434c52 06/28/23 08:31:34.814
STEP: Creating configMap with name cm-test-opt-create-ad8a8bb9-9ac5-4efa-a4d0-8a8c9dd02cfb 06/28/23 08:31:34.82
STEP: waiting to observe update in volume 06/28/23 08:31:34.826
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jun 28 08:31:37.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-610" for this suite. 06/28/23 08:31:37.042
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":218,"skipped":3623,"failed":0}
------------------------------
â€¢ [4.500 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:31:32.549
    Jun 28 08:31:32.549: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename configmap 06/28/23 08:31:32.55
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:31:32.565
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:31:32.57
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:239
    STEP: Creating configMap with name cm-test-opt-del-d997c412-77c3-4bdd-9d75-083e4b42c8d2 06/28/23 08:31:32.585
    STEP: Creating configMap with name cm-test-opt-upd-ced7b1b6-147b-4a49-a9f3-1a8ddc434c52 06/28/23 08:31:32.59
    STEP: Creating the pod 06/28/23 08:31:32.598
    Jun 28 08:31:32.611: INFO: Waiting up to 5m0s for pod "pod-configmaps-be17724a-98b3-4fd6-b50e-64143b6545c8" in namespace "configmap-610" to be "running and ready"
    Jun 28 08:31:32.616: INFO: Pod "pod-configmaps-be17724a-98b3-4fd6-b50e-64143b6545c8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.892241ms
    Jun 28 08:31:32.616: INFO: The phase of Pod pod-configmaps-be17724a-98b3-4fd6-b50e-64143b6545c8 is Pending, waiting for it to be Running (with Ready = true)
    Jun 28 08:31:34.622: INFO: Pod "pod-configmaps-be17724a-98b3-4fd6-b50e-64143b6545c8": Phase="Running", Reason="", readiness=true. Elapsed: 2.011399644s
    Jun 28 08:31:34.622: INFO: The phase of Pod pod-configmaps-be17724a-98b3-4fd6-b50e-64143b6545c8 is Running (Ready = true)
    Jun 28 08:31:34.622: INFO: Pod "pod-configmaps-be17724a-98b3-4fd6-b50e-64143b6545c8" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-d997c412-77c3-4bdd-9d75-083e4b42c8d2 06/28/23 08:31:34.806
    STEP: Updating configmap cm-test-opt-upd-ced7b1b6-147b-4a49-a9f3-1a8ddc434c52 06/28/23 08:31:34.814
    STEP: Creating configMap with name cm-test-opt-create-ad8a8bb9-9ac5-4efa-a4d0-8a8c9dd02cfb 06/28/23 08:31:34.82
    STEP: waiting to observe update in volume 06/28/23 08:31:34.826
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jun 28 08:31:37.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-610" for this suite. 06/28/23 08:31:37.042
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:31:37.049
Jun 28 08:31:37.049: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename projected 06/28/23 08:31:37.05
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:31:37.064
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:31:37.069
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108
STEP: Creating configMap with name projected-configmap-test-volume-map-65411fde-421f-44e0-9d99-3dee05d5ed68 06/28/23 08:31:37.074
STEP: Creating a pod to test consume configMaps 06/28/23 08:31:37.08
Jun 28 08:31:37.089: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-56bdc450-d90f-4a97-8c17-92231672c004" in namespace "projected-7193" to be "Succeeded or Failed"
Jun 28 08:31:37.094: INFO: Pod "pod-projected-configmaps-56bdc450-d90f-4a97-8c17-92231672c004": Phase="Pending", Reason="", readiness=false. Elapsed: 4.627677ms
Jun 28 08:31:39.101: INFO: Pod "pod-projected-configmaps-56bdc450-d90f-4a97-8c17-92231672c004": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011141293s
Jun 28 08:31:41.100: INFO: Pod "pod-projected-configmaps-56bdc450-d90f-4a97-8c17-92231672c004": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010398646s
STEP: Saw pod success 06/28/23 08:31:41.1
Jun 28 08:31:41.100: INFO: Pod "pod-projected-configmaps-56bdc450-d90f-4a97-8c17-92231672c004" satisfied condition "Succeeded or Failed"
Jun 28 08:31:41.105: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-zxlfv pod pod-projected-configmaps-56bdc450-d90f-4a97-8c17-92231672c004 container agnhost-container: <nil>
STEP: delete the pod 06/28/23 08:31:41.116
Jun 28 08:31:41.129: INFO: Waiting for pod pod-projected-configmaps-56bdc450-d90f-4a97-8c17-92231672c004 to disappear
Jun 28 08:31:41.133: INFO: Pod pod-projected-configmaps-56bdc450-d90f-4a97-8c17-92231672c004 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jun 28 08:31:41.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7193" for this suite. 06/28/23 08:31:41.142
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","completed":219,"skipped":3629,"failed":0}
------------------------------
â€¢ [4.101 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:31:37.049
    Jun 28 08:31:37.049: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename projected 06/28/23 08:31:37.05
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:31:37.064
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:31:37.069
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:108
    STEP: Creating configMap with name projected-configmap-test-volume-map-65411fde-421f-44e0-9d99-3dee05d5ed68 06/28/23 08:31:37.074
    STEP: Creating a pod to test consume configMaps 06/28/23 08:31:37.08
    Jun 28 08:31:37.089: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-56bdc450-d90f-4a97-8c17-92231672c004" in namespace "projected-7193" to be "Succeeded or Failed"
    Jun 28 08:31:37.094: INFO: Pod "pod-projected-configmaps-56bdc450-d90f-4a97-8c17-92231672c004": Phase="Pending", Reason="", readiness=false. Elapsed: 4.627677ms
    Jun 28 08:31:39.101: INFO: Pod "pod-projected-configmaps-56bdc450-d90f-4a97-8c17-92231672c004": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011141293s
    Jun 28 08:31:41.100: INFO: Pod "pod-projected-configmaps-56bdc450-d90f-4a97-8c17-92231672c004": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010398646s
    STEP: Saw pod success 06/28/23 08:31:41.1
    Jun 28 08:31:41.100: INFO: Pod "pod-projected-configmaps-56bdc450-d90f-4a97-8c17-92231672c004" satisfied condition "Succeeded or Failed"
    Jun 28 08:31:41.105: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-zxlfv pod pod-projected-configmaps-56bdc450-d90f-4a97-8c17-92231672c004 container agnhost-container: <nil>
    STEP: delete the pod 06/28/23 08:31:41.116
    Jun 28 08:31:41.129: INFO: Waiting for pod pod-projected-configmaps-56bdc450-d90f-4a97-8c17-92231672c004 to disappear
    Jun 28 08:31:41.133: INFO: Pod pod-projected-configmaps-56bdc450-d90f-4a97-8c17-92231672c004 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jun 28 08:31:41.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-7193" for this suite. 06/28/23 08:31:41.142
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:31:41.151
Jun 28 08:31:41.151: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename emptydir 06/28/23 08:31:41.152
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:31:41.166
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:31:41.171
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136
STEP: Creating a pod to test emptydir 0666 on tmpfs 06/28/23 08:31:41.178
Jun 28 08:31:41.188: INFO: Waiting up to 5m0s for pod "pod-1b669b9f-70eb-44d2-b422-b8ad8439c3e2" in namespace "emptydir-4628" to be "Succeeded or Failed"
Jun 28 08:31:41.193: INFO: Pod "pod-1b669b9f-70eb-44d2-b422-b8ad8439c3e2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.23805ms
Jun 28 08:31:43.201: INFO: Pod "pod-1b669b9f-70eb-44d2-b422-b8ad8439c3e2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012567409s
Jun 28 08:31:45.200: INFO: Pod "pod-1b669b9f-70eb-44d2-b422-b8ad8439c3e2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011572037s
STEP: Saw pod success 06/28/23 08:31:45.2
Jun 28 08:31:45.200: INFO: Pod "pod-1b669b9f-70eb-44d2-b422-b8ad8439c3e2" satisfied condition "Succeeded or Failed"
Jun 28 08:31:45.205: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-zxlfv pod pod-1b669b9f-70eb-44d2-b422-b8ad8439c3e2 container test-container: <nil>
STEP: delete the pod 06/28/23 08:31:45.255
Jun 28 08:31:45.268: INFO: Waiting for pod pod-1b669b9f-70eb-44d2-b422-b8ad8439c3e2 to disappear
Jun 28 08:31:45.272: INFO: Pod pod-1b669b9f-70eb-44d2-b422-b8ad8439c3e2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jun 28 08:31:45.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4628" for this suite. 06/28/23 08:31:45.281
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":220,"skipped":3637,"failed":0}
------------------------------
â€¢ [4.137 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:31:41.151
    Jun 28 08:31:41.151: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename emptydir 06/28/23 08:31:41.152
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:31:41.166
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:31:41.171
    [It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:136
    STEP: Creating a pod to test emptydir 0666 on tmpfs 06/28/23 08:31:41.178
    Jun 28 08:31:41.188: INFO: Waiting up to 5m0s for pod "pod-1b669b9f-70eb-44d2-b422-b8ad8439c3e2" in namespace "emptydir-4628" to be "Succeeded or Failed"
    Jun 28 08:31:41.193: INFO: Pod "pod-1b669b9f-70eb-44d2-b422-b8ad8439c3e2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.23805ms
    Jun 28 08:31:43.201: INFO: Pod "pod-1b669b9f-70eb-44d2-b422-b8ad8439c3e2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012567409s
    Jun 28 08:31:45.200: INFO: Pod "pod-1b669b9f-70eb-44d2-b422-b8ad8439c3e2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011572037s
    STEP: Saw pod success 06/28/23 08:31:45.2
    Jun 28 08:31:45.200: INFO: Pod "pod-1b669b9f-70eb-44d2-b422-b8ad8439c3e2" satisfied condition "Succeeded or Failed"
    Jun 28 08:31:45.205: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-zxlfv pod pod-1b669b9f-70eb-44d2-b422-b8ad8439c3e2 container test-container: <nil>
    STEP: delete the pod 06/28/23 08:31:45.255
    Jun 28 08:31:45.268: INFO: Waiting for pod pod-1b669b9f-70eb-44d2-b422-b8ad8439c3e2 to disappear
    Jun 28 08:31:45.272: INFO: Pod pod-1b669b9f-70eb-44d2-b422-b8ad8439c3e2 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jun 28 08:31:45.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-4628" for this suite. 06/28/23 08:31:45.281
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] PodTemplates
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:31:45.288
Jun 28 08:31:45.288: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename podtemplate 06/28/23 08:31:45.29
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:31:45.303
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:31:45.31
[It] should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
STEP: Create a pod template 06/28/23 08:31:45.317
STEP: Replace a pod template 06/28/23 08:31:45.325
Jun 28 08:31:45.339: INFO: Found updated podtemplate annotation: "true"

[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
Jun 28 08:31:45.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-3785" for this suite. 06/28/23 08:31:45.347
{"msg":"PASSED [sig-node] PodTemplates should replace a pod template [Conformance]","completed":221,"skipped":3643,"failed":0}
------------------------------
â€¢ [0.070 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:31:45.288
    Jun 28 08:31:45.288: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename podtemplate 06/28/23 08:31:45.29
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:31:45.303
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:31:45.31
    [It] should replace a pod template [Conformance]
      test/e2e/common/node/podtemplates.go:176
    STEP: Create a pod template 06/28/23 08:31:45.317
    STEP: Replace a pod template 06/28/23 08:31:45.325
    Jun 28 08:31:45.339: INFO: Found updated podtemplate annotation: "true"

    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    Jun 28 08:31:45.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-3785" for this suite. 06/28/23 08:31:45.347
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:31:45.359
Jun 28 08:31:45.359: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename job 06/28/23 08:31:45.36
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:31:45.375
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:31:45.381
[It] should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464
STEP: Creating a job 06/28/23 08:31:45.386
STEP: Ensure pods equal to paralellism count is attached to the job 06/28/23 08:31:45.393
STEP: patching /status 06/28/23 08:31:47.4
STEP: updating /status 06/28/23 08:31:47.409
STEP: get /status 06/28/23 08:31:47.442
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Jun 28 08:31:47.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-4613" for this suite. 06/28/23 08:31:47.456
{"msg":"PASSED [sig-apps] Job should apply changes to a job status [Conformance]","completed":222,"skipped":3660,"failed":0}
------------------------------
â€¢ [2.104 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:31:45.359
    Jun 28 08:31:45.359: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename job 06/28/23 08:31:45.36
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:31:45.375
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:31:45.381
    [It] should apply changes to a job status [Conformance]
      test/e2e/apps/job.go:464
    STEP: Creating a job 06/28/23 08:31:45.386
    STEP: Ensure pods equal to paralellism count is attached to the job 06/28/23 08:31:45.393
    STEP: patching /status 06/28/23 08:31:47.4
    STEP: updating /status 06/28/23 08:31:47.409
    STEP: get /status 06/28/23 08:31:47.442
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Jun 28 08:31:47.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-4613" for this suite. 06/28/23 08:31:47.456
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:31:47.463
Jun 28 08:31:47.463: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename downward-api 06/28/23 08:31:47.464
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:31:47.477
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:31:47.481
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220
STEP: Creating a pod to test downward API volume plugin 06/28/23 08:31:47.486
Jun 28 08:31:47.495: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5c2fe1e9-1944-4fb0-b87c-112143d18c52" in namespace "downward-api-9500" to be "Succeeded or Failed"
Jun 28 08:31:47.499: INFO: Pod "downwardapi-volume-5c2fe1e9-1944-4fb0-b87c-112143d18c52": Phase="Pending", Reason="", readiness=false. Elapsed: 4.364234ms
Jun 28 08:31:49.506: INFO: Pod "downwardapi-volume-5c2fe1e9-1944-4fb0-b87c-112143d18c52": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011127515s
Jun 28 08:31:51.505: INFO: Pod "downwardapi-volume-5c2fe1e9-1944-4fb0-b87c-112143d18c52": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010301296s
STEP: Saw pod success 06/28/23 08:31:51.505
Jun 28 08:31:51.505: INFO: Pod "downwardapi-volume-5c2fe1e9-1944-4fb0-b87c-112143d18c52" satisfied condition "Succeeded or Failed"
Jun 28 08:31:51.510: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-zxlfv pod downwardapi-volume-5c2fe1e9-1944-4fb0-b87c-112143d18c52 container client-container: <nil>
STEP: delete the pod 06/28/23 08:31:51.522
Jun 28 08:31:51.536: INFO: Waiting for pod downwardapi-volume-5c2fe1e9-1944-4fb0-b87c-112143d18c52 to disappear
Jun 28 08:31:51.541: INFO: Pod downwardapi-volume-5c2fe1e9-1944-4fb0-b87c-112143d18c52 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jun 28 08:31:51.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9500" for this suite. 06/28/23 08:31:51.55
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","completed":223,"skipped":3675,"failed":0}
------------------------------
â€¢ [4.097 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:31:47.463
    Jun 28 08:31:47.463: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename downward-api 06/28/23 08:31:47.464
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:31:47.477
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:31:47.481
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:220
    STEP: Creating a pod to test downward API volume plugin 06/28/23 08:31:47.486
    Jun 28 08:31:47.495: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5c2fe1e9-1944-4fb0-b87c-112143d18c52" in namespace "downward-api-9500" to be "Succeeded or Failed"
    Jun 28 08:31:47.499: INFO: Pod "downwardapi-volume-5c2fe1e9-1944-4fb0-b87c-112143d18c52": Phase="Pending", Reason="", readiness=false. Elapsed: 4.364234ms
    Jun 28 08:31:49.506: INFO: Pod "downwardapi-volume-5c2fe1e9-1944-4fb0-b87c-112143d18c52": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011127515s
    Jun 28 08:31:51.505: INFO: Pod "downwardapi-volume-5c2fe1e9-1944-4fb0-b87c-112143d18c52": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010301296s
    STEP: Saw pod success 06/28/23 08:31:51.505
    Jun 28 08:31:51.505: INFO: Pod "downwardapi-volume-5c2fe1e9-1944-4fb0-b87c-112143d18c52" satisfied condition "Succeeded or Failed"
    Jun 28 08:31:51.510: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-zxlfv pod downwardapi-volume-5c2fe1e9-1944-4fb0-b87c-112143d18c52 container client-container: <nil>
    STEP: delete the pod 06/28/23 08:31:51.522
    Jun 28 08:31:51.536: INFO: Waiting for pod downwardapi-volume-5c2fe1e9-1944-4fb0-b87c-112143d18c52 to disappear
    Jun 28 08:31:51.541: INFO: Pod downwardapi-volume-5c2fe1e9-1944-4fb0-b87c-112143d18c52 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jun 28 08:31:51.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-9500" for this suite. 06/28/23 08:31:51.55
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:31:51.561
Jun 28 08:31:51.561: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename services 06/28/23 08:31:51.562
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:31:51.579
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:31:51.585
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443
STEP: creating a service externalname-service with the type=ExternalName in namespace services-9984 06/28/23 08:31:51.591
STEP: changing the ExternalName service to type=NodePort 06/28/23 08:31:51.599
STEP: creating replication controller externalname-service in namespace services-9984 06/28/23 08:31:51.623
I0628 08:31:51.630988      18 runners.go:193] Created replication controller with name: externalname-service, namespace: services-9984, replica count: 2
I0628 08:31:54.683072      18 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 28 08:31:54.683: INFO: Creating new exec pod
Jun 28 08:31:54.690: INFO: Waiting up to 5m0s for pod "execpodlp5hw" in namespace "services-9984" to be "running"
Jun 28 08:31:54.695: INFO: Pod "execpodlp5hw": Phase="Pending", Reason="", readiness=false. Elapsed: 4.866307ms
Jun 28 08:31:56.700: INFO: Pod "execpodlp5hw": Phase="Running", Reason="", readiness=true. Elapsed: 2.00992286s
Jun 28 08:31:56.700: INFO: Pod "execpodlp5hw" satisfied condition "running"
Jun 28 08:31:57.710: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-9984 exec execpodlp5hw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jun 28 08:31:58.177: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jun 28 08:31:58.177: INFO: stdout: "externalname-service-jztff"
Jun 28 08:31:58.177: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-9984 exec execpodlp5hw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.20.3.187 80'
Jun 28 08:31:58.608: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.20.3.187 80\nConnection to 172.20.3.187 80 port [tcp/http] succeeded!\n"
Jun 28 08:31:58.608: INFO: stdout: ""
Jun 28 08:31:59.609: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-9984 exec execpodlp5hw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.20.3.187 80'
Jun 28 08:32:00.047: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.20.3.187 80\nConnection to 172.20.3.187 80 port [tcp/http] succeeded!\n"
Jun 28 08:32:00.047: INFO: stdout: "externalname-service-jztff"
Jun 28 08:32:00.047: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-9984 exec execpodlp5hw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.11.5 31130'
Jun 28 08:32:00.490: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.11.5 31130\nConnection to 192.168.11.5 31130 port [tcp/*] succeeded!\n"
Jun 28 08:32:00.490: INFO: stdout: ""
Jun 28 08:32:01.490: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-9984 exec execpodlp5hw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.11.5 31130'
Jun 28 08:32:01.983: INFO: stderr: "+ nc -v -t -w 2 192.168.11.5 31130\n+ echo hostName\nConnection to 192.168.11.5 31130 port [tcp/*] succeeded!\n"
Jun 28 08:32:01.983: INFO: stdout: ""
Jun 28 08:32:02.490: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-9984 exec execpodlp5hw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.11.5 31130'
Jun 28 08:32:02.922: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.11.5 31130\nConnection to 192.168.11.5 31130 port [tcp/*] succeeded!\n"
Jun 28 08:32:02.922: INFO: stdout: ""
Jun 28 08:32:03.491: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-9984 exec execpodlp5hw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.11.5 31130'
Jun 28 08:32:03.917: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.11.5 31130\nConnection to 192.168.11.5 31130 port [tcp/*] succeeded!\n"
Jun 28 08:32:03.918: INFO: stdout: ""
Jun 28 08:32:04.491: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-9984 exec execpodlp5hw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.11.5 31130'
Jun 28 08:32:04.956: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.11.5 31130\nConnection to 192.168.11.5 31130 port [tcp/*] succeeded!\n"
Jun 28 08:32:04.956: INFO: stdout: ""
Jun 28 08:32:05.490: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-9984 exec execpodlp5hw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.11.5 31130'
Jun 28 08:32:05.926: INFO: stderr: "+ nc -v -t -w 2 192.168.11.5 31130\n+ echo hostName\nConnection to 192.168.11.5 31130 port [tcp/*] succeeded!\n"
Jun 28 08:32:05.926: INFO: stdout: ""
Jun 28 08:32:06.490: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-9984 exec execpodlp5hw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.11.5 31130'
Jun 28 08:32:06.914: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.11.5 31130\nConnection to 192.168.11.5 31130 port [tcp/*] succeeded!\n"
Jun 28 08:32:06.914: INFO: stdout: ""
Jun 28 08:32:07.490: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-9984 exec execpodlp5hw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.11.5 31130'
Jun 28 08:32:07.890: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.11.5 31130\nConnection to 192.168.11.5 31130 port [tcp/*] succeeded!\n"
Jun 28 08:32:07.890: INFO: stdout: "externalname-service-k5ctm"
Jun 28 08:32:07.890: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-9984 exec execpodlp5hw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.11.3 31130'
Jun 28 08:32:08.292: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.11.3 31130\nConnection to 192.168.11.3 31130 port [tcp/*] succeeded!\n"
Jun 28 08:32:08.292: INFO: stdout: "externalname-service-k5ctm"
Jun 28 08:32:08.292: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jun 28 08:32:08.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9984" for this suite. 06/28/23 08:32:08.324
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","completed":224,"skipped":3715,"failed":0}
------------------------------
â€¢ [SLOW TEST] [16.774 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:31:51.561
    Jun 28 08:31:51.561: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename services 06/28/23 08:31:51.562
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:31:51.579
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:31:51.585
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ExternalName to NodePort [Conformance]
      test/e2e/network/service.go:1443
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-9984 06/28/23 08:31:51.591
    STEP: changing the ExternalName service to type=NodePort 06/28/23 08:31:51.599
    STEP: creating replication controller externalname-service in namespace services-9984 06/28/23 08:31:51.623
    I0628 08:31:51.630988      18 runners.go:193] Created replication controller with name: externalname-service, namespace: services-9984, replica count: 2
    I0628 08:31:54.683072      18 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jun 28 08:31:54.683: INFO: Creating new exec pod
    Jun 28 08:31:54.690: INFO: Waiting up to 5m0s for pod "execpodlp5hw" in namespace "services-9984" to be "running"
    Jun 28 08:31:54.695: INFO: Pod "execpodlp5hw": Phase="Pending", Reason="", readiness=false. Elapsed: 4.866307ms
    Jun 28 08:31:56.700: INFO: Pod "execpodlp5hw": Phase="Running", Reason="", readiness=true. Elapsed: 2.00992286s
    Jun 28 08:31:56.700: INFO: Pod "execpodlp5hw" satisfied condition "running"
    Jun 28 08:31:57.710: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-9984 exec execpodlp5hw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Jun 28 08:31:58.177: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Jun 28 08:31:58.177: INFO: stdout: "externalname-service-jztff"
    Jun 28 08:31:58.177: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-9984 exec execpodlp5hw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.20.3.187 80'
    Jun 28 08:31:58.608: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.20.3.187 80\nConnection to 172.20.3.187 80 port [tcp/http] succeeded!\n"
    Jun 28 08:31:58.608: INFO: stdout: ""
    Jun 28 08:31:59.609: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-9984 exec execpodlp5hw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.20.3.187 80'
    Jun 28 08:32:00.047: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.20.3.187 80\nConnection to 172.20.3.187 80 port [tcp/http] succeeded!\n"
    Jun 28 08:32:00.047: INFO: stdout: "externalname-service-jztff"
    Jun 28 08:32:00.047: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-9984 exec execpodlp5hw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.11.5 31130'
    Jun 28 08:32:00.490: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.11.5 31130\nConnection to 192.168.11.5 31130 port [tcp/*] succeeded!\n"
    Jun 28 08:32:00.490: INFO: stdout: ""
    Jun 28 08:32:01.490: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-9984 exec execpodlp5hw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.11.5 31130'
    Jun 28 08:32:01.983: INFO: stderr: "+ nc -v -t -w 2 192.168.11.5 31130\n+ echo hostName\nConnection to 192.168.11.5 31130 port [tcp/*] succeeded!\n"
    Jun 28 08:32:01.983: INFO: stdout: ""
    Jun 28 08:32:02.490: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-9984 exec execpodlp5hw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.11.5 31130'
    Jun 28 08:32:02.922: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.11.5 31130\nConnection to 192.168.11.5 31130 port [tcp/*] succeeded!\n"
    Jun 28 08:32:02.922: INFO: stdout: ""
    Jun 28 08:32:03.491: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-9984 exec execpodlp5hw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.11.5 31130'
    Jun 28 08:32:03.917: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.11.5 31130\nConnection to 192.168.11.5 31130 port [tcp/*] succeeded!\n"
    Jun 28 08:32:03.918: INFO: stdout: ""
    Jun 28 08:32:04.491: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-9984 exec execpodlp5hw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.11.5 31130'
    Jun 28 08:32:04.956: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.11.5 31130\nConnection to 192.168.11.5 31130 port [tcp/*] succeeded!\n"
    Jun 28 08:32:04.956: INFO: stdout: ""
    Jun 28 08:32:05.490: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-9984 exec execpodlp5hw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.11.5 31130'
    Jun 28 08:32:05.926: INFO: stderr: "+ nc -v -t -w 2 192.168.11.5 31130\n+ echo hostName\nConnection to 192.168.11.5 31130 port [tcp/*] succeeded!\n"
    Jun 28 08:32:05.926: INFO: stdout: ""
    Jun 28 08:32:06.490: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-9984 exec execpodlp5hw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.11.5 31130'
    Jun 28 08:32:06.914: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.11.5 31130\nConnection to 192.168.11.5 31130 port [tcp/*] succeeded!\n"
    Jun 28 08:32:06.914: INFO: stdout: ""
    Jun 28 08:32:07.490: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-9984 exec execpodlp5hw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.11.5 31130'
    Jun 28 08:32:07.890: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.11.5 31130\nConnection to 192.168.11.5 31130 port [tcp/*] succeeded!\n"
    Jun 28 08:32:07.890: INFO: stdout: "externalname-service-k5ctm"
    Jun 28 08:32:07.890: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=services-9984 exec execpodlp5hw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.11.3 31130'
    Jun 28 08:32:08.292: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.11.3 31130\nConnection to 192.168.11.3 31130 port [tcp/*] succeeded!\n"
    Jun 28 08:32:08.292: INFO: stdout: "externalname-service-k5ctm"
    Jun 28 08:32:08.292: INFO: Cleaning up the ExternalName to NodePort test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jun 28 08:32:08.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-9984" for this suite. 06/28/23 08:32:08.324
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:32:08.336
Jun 28 08:32:08.336: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename downward-api 06/28/23 08:32:08.337
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:32:08.356
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:32:08.361
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52
STEP: Creating a pod to test downward API volume plugin 06/28/23 08:32:08.366
Jun 28 08:32:08.376: INFO: Waiting up to 5m0s for pod "downwardapi-volume-70ad958a-6899-4ae9-8f50-e88014b56f97" in namespace "downward-api-2046" to be "Succeeded or Failed"
Jun 28 08:32:08.380: INFO: Pod "downwardapi-volume-70ad958a-6899-4ae9-8f50-e88014b56f97": Phase="Pending", Reason="", readiness=false. Elapsed: 3.716126ms
Jun 28 08:32:10.386: INFO: Pod "downwardapi-volume-70ad958a-6899-4ae9-8f50-e88014b56f97": Phase="Running", Reason="", readiness=false. Elapsed: 2.009514338s
Jun 28 08:32:12.385: INFO: Pod "downwardapi-volume-70ad958a-6899-4ae9-8f50-e88014b56f97": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009348025s
STEP: Saw pod success 06/28/23 08:32:12.385
Jun 28 08:32:12.386: INFO: Pod "downwardapi-volume-70ad958a-6899-4ae9-8f50-e88014b56f97" satisfied condition "Succeeded or Failed"
Jun 28 08:32:12.390: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod downwardapi-volume-70ad958a-6899-4ae9-8f50-e88014b56f97 container client-container: <nil>
STEP: delete the pod 06/28/23 08:32:12.439
Jun 28 08:32:12.450: INFO: Waiting for pod downwardapi-volume-70ad958a-6899-4ae9-8f50-e88014b56f97 to disappear
Jun 28 08:32:12.454: INFO: Pod downwardapi-volume-70ad958a-6899-4ae9-8f50-e88014b56f97 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jun 28 08:32:12.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2046" for this suite. 06/28/23 08:32:12.461
{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","completed":225,"skipped":3726,"failed":0}
------------------------------
â€¢ [4.132 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:32:08.336
    Jun 28 08:32:08.336: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename downward-api 06/28/23 08:32:08.337
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:32:08.356
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:32:08.361
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:52
    STEP: Creating a pod to test downward API volume plugin 06/28/23 08:32:08.366
    Jun 28 08:32:08.376: INFO: Waiting up to 5m0s for pod "downwardapi-volume-70ad958a-6899-4ae9-8f50-e88014b56f97" in namespace "downward-api-2046" to be "Succeeded or Failed"
    Jun 28 08:32:08.380: INFO: Pod "downwardapi-volume-70ad958a-6899-4ae9-8f50-e88014b56f97": Phase="Pending", Reason="", readiness=false. Elapsed: 3.716126ms
    Jun 28 08:32:10.386: INFO: Pod "downwardapi-volume-70ad958a-6899-4ae9-8f50-e88014b56f97": Phase="Running", Reason="", readiness=false. Elapsed: 2.009514338s
    Jun 28 08:32:12.385: INFO: Pod "downwardapi-volume-70ad958a-6899-4ae9-8f50-e88014b56f97": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009348025s
    STEP: Saw pod success 06/28/23 08:32:12.385
    Jun 28 08:32:12.386: INFO: Pod "downwardapi-volume-70ad958a-6899-4ae9-8f50-e88014b56f97" satisfied condition "Succeeded or Failed"
    Jun 28 08:32:12.390: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod downwardapi-volume-70ad958a-6899-4ae9-8f50-e88014b56f97 container client-container: <nil>
    STEP: delete the pod 06/28/23 08:32:12.439
    Jun 28 08:32:12.450: INFO: Waiting for pod downwardapi-volume-70ad958a-6899-4ae9-8f50-e88014b56f97 to disappear
    Jun 28 08:32:12.454: INFO: Pod downwardapi-volume-70ad958a-6899-4ae9-8f50-e88014b56f97 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jun 28 08:32:12.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-2046" for this suite. 06/28/23 08:32:12.461
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:32:12.472
Jun 28 08:32:12.472: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename deployment 06/28/23 08:32:12.473
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:32:12.486
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:32:12.49
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
Jun 28 08:32:12.506: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Jun 28 08:32:17.511: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 06/28/23 08:32:17.511
Jun 28 08:32:17.511: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 06/28/23 08:32:17.523
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jun 28 08:32:17.537: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-1448  120afed1-2fa7-42ab-8dec-452385cc1508 73791 1 2023-06-28 08:32:17 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-06-28 08:32:17 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0032f3178 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Jun 28 08:32:17.542: INFO: New ReplicaSet "test-cleanup-deployment-69cb9c5497" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-69cb9c5497  deployment-1448  0ac877e1-f9d3-450a-a0e1-f6dbb20aaf8a 73795 1 2023-06-28 08:32:17 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 120afed1-2fa7-42ab-8dec-452385cc1508 0xc002721887 0xc002721888}] [] [{kube-controller-manager Update apps/v1 2023-06-28 08:32:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"120afed1-2fa7-42ab-8dec-452385cc1508\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 69cb9c5497,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002721918 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun 28 08:32:17.542: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Jun 28 08:32:17.542: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-1448  ea71c72a-62e0-4bdd-8e39-04b3fc21c401 73793 1 2023-06-28 08:32:12 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 120afed1-2fa7-42ab-8dec-452385cc1508 0xc002721757 0xc002721758}] [] [{e2e.test Update apps/v1 2023-06-28 08:32:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-28 08:32:13 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-06-28 08:32:17 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"120afed1-2fa7-42ab-8dec-452385cc1508\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc002721818 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jun 28 08:32:17.548: INFO: Pod "test-cleanup-controller-vlxzj" is available:
&Pod{ObjectMeta:{test-cleanup-controller-vlxzj test-cleanup-controller- deployment-1448  ce834ebd-3416-4c71-9b63-38bc10b42d32 73762 0 2023-06-28 08:32:12 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/containerID:61073650f249575fcb9724ee33c35d7fece63f11fdd5390fd875f9c0b6c7f1ba cni.projectcalico.org/podIP:172.21.30.107/32 cni.projectcalico.org/podIPs:172.21.30.107/32] [{apps/v1 ReplicaSet test-cleanup-controller ea71c72a-62e0-4bdd-8e39-04b3fc21c401 0xc002721d97 0xc002721d98}] [] [{kube-controller-manager Update v1 2023-06-28 08:32:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ea71c72a-62e0-4bdd-8e39-04b3fc21c401\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-06-28 08:32:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-06-28 08:32:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.30.107\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jgxnw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jgxnw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-rhel-749f7d55c8xdd8b6-zxlfv,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:32:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:32:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:32:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:32:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.11.5,PodIP:172.21.30.107,StartTime:2023-06-28 08:32:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-28 08:32:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://79dd763f7e55174ee76d593c56f58eb3ef72c6d5da786a6af872cb2124693cfa,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.21.30.107,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 28 08:32:17.548: INFO: Pod "test-cleanup-deployment-69cb9c5497-4ktwj" is not available:
&Pod{ObjectMeta:{test-cleanup-deployment-69cb9c5497-4ktwj test-cleanup-deployment-69cb9c5497- deployment-1448  02230bff-f4c0-4bc0-b55e-980e089a6061 73797 0 2023-06-28 08:32:17 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[] [{apps/v1 ReplicaSet test-cleanup-deployment-69cb9c5497 0ac877e1-f9d3-450a-a0e1-f6dbb20aaf8a 0xc002721fa7 0xc002721fa8}] [] [{kube-controller-manager Update v1 2023-06-28 08:32:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0ac877e1-f9d3-450a-a0e1-f6dbb20aaf8a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lcsjq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lcsjq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jun 28 08:32:17.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1448" for this suite. 06/28/23 08:32:17.556
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","completed":226,"skipped":3736,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.091 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:32:12.472
    Jun 28 08:32:12.472: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename deployment 06/28/23 08:32:12.473
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:32:12.486
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:32:12.49
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should delete old replica sets [Conformance]
      test/e2e/apps/deployment.go:122
    Jun 28 08:32:12.506: INFO: Pod name cleanup-pod: Found 0 pods out of 1
    Jun 28 08:32:17.511: INFO: Pod name cleanup-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 06/28/23 08:32:17.511
    Jun 28 08:32:17.511: INFO: Creating deployment test-cleanup-deployment
    STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 06/28/23 08:32:17.523
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jun 28 08:32:17.537: INFO: Deployment "test-cleanup-deployment":
    &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-1448  120afed1-2fa7-42ab-8dec-452385cc1508 73791 1 2023-06-28 08:32:17 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-06-28 08:32:17 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0032f3178 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

    Jun 28 08:32:17.542: INFO: New ReplicaSet "test-cleanup-deployment-69cb9c5497" of Deployment "test-cleanup-deployment":
    &ReplicaSet{ObjectMeta:{test-cleanup-deployment-69cb9c5497  deployment-1448  0ac877e1-f9d3-450a-a0e1-f6dbb20aaf8a 73795 1 2023-06-28 08:32:17 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 120afed1-2fa7-42ab-8dec-452385cc1508 0xc002721887 0xc002721888}] [] [{kube-controller-manager Update apps/v1 2023-06-28 08:32:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"120afed1-2fa7-42ab-8dec-452385cc1508\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 69cb9c5497,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002721918 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jun 28 08:32:17.542: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
    Jun 28 08:32:17.542: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-1448  ea71c72a-62e0-4bdd-8e39-04b3fc21c401 73793 1 2023-06-28 08:32:12 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 120afed1-2fa7-42ab-8dec-452385cc1508 0xc002721757 0xc002721758}] [] [{e2e.test Update apps/v1 2023-06-28 08:32:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-28 08:32:13 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-06-28 08:32:17 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"120afed1-2fa7-42ab-8dec-452385cc1508\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc002721818 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jun 28 08:32:17.548: INFO: Pod "test-cleanup-controller-vlxzj" is available:
    &Pod{ObjectMeta:{test-cleanup-controller-vlxzj test-cleanup-controller- deployment-1448  ce834ebd-3416-4c71-9b63-38bc10b42d32 73762 0 2023-06-28 08:32:12 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/containerID:61073650f249575fcb9724ee33c35d7fece63f11fdd5390fd875f9c0b6c7f1ba cni.projectcalico.org/podIP:172.21.30.107/32 cni.projectcalico.org/podIPs:172.21.30.107/32] [{apps/v1 ReplicaSet test-cleanup-controller ea71c72a-62e0-4bdd-8e39-04b3fc21c401 0xc002721d97 0xc002721d98}] [] [{kube-controller-manager Update v1 2023-06-28 08:32:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ea71c72a-62e0-4bdd-8e39-04b3fc21c401\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-06-28 08:32:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-06-28 08:32:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.30.107\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jgxnw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jgxnw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-rhel-749f7d55c8xdd8b6-zxlfv,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:32:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:32:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:32:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:32:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.11.5,PodIP:172.21.30.107,StartTime:2023-06-28 08:32:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-28 08:32:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://79dd763f7e55174ee76d593c56f58eb3ef72c6d5da786a6af872cb2124693cfa,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.21.30.107,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 28 08:32:17.548: INFO: Pod "test-cleanup-deployment-69cb9c5497-4ktwj" is not available:
    &Pod{ObjectMeta:{test-cleanup-deployment-69cb9c5497-4ktwj test-cleanup-deployment-69cb9c5497- deployment-1448  02230bff-f4c0-4bc0-b55e-980e089a6061 73797 0 2023-06-28 08:32:17 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[] [{apps/v1 ReplicaSet test-cleanup-deployment-69cb9c5497 0ac877e1-f9d3-450a-a0e1-f6dbb20aaf8a 0xc002721fa7 0xc002721fa8}] [] [{kube-controller-manager Update v1 2023-06-28 08:32:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0ac877e1-f9d3-450a-a0e1-f6dbb20aaf8a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lcsjq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lcsjq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jun 28 08:32:17.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-1448" for this suite. 06/28/23 08:32:17.556
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:32:17.563
Jun 28 08:32:17.563: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename webhook 06/28/23 08:32:17.564
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:32:17.579
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:32:17.583
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 06/28/23 08:32:17.6
STEP: Create role binding to let webhook read extension-apiserver-authentication 06/28/23 08:32:18.139
STEP: Deploying the webhook pod 06/28/23 08:32:18.148
STEP: Wait for the deployment to be ready 06/28/23 08:32:18.161
Jun 28 08:32:18.170: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 06/28/23 08:32:20.384
STEP: Verifying the service has paired with the endpoint 06/28/23 08:32:20.47
Jun 28 08:32:21.471: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290
Jun 28 08:32:21.508: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3609-crds.webhook.example.com via the AdmissionRegistration API 06/28/23 08:32:21.621
STEP: Creating a custom resource that should be mutated by the webhook 06/28/23 08:32:22.196
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jun 28 08:32:24.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8460" for this suite. 06/28/23 08:32:24.903
STEP: Destroying namespace "webhook-8460-markers" for this suite. 06/28/23 08:32:24.91
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","completed":227,"skipped":3737,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.391 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:32:17.563
    Jun 28 08:32:17.563: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename webhook 06/28/23 08:32:17.564
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:32:17.579
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:32:17.583
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 06/28/23 08:32:17.6
    STEP: Create role binding to let webhook read extension-apiserver-authentication 06/28/23 08:32:18.139
    STEP: Deploying the webhook pod 06/28/23 08:32:18.148
    STEP: Wait for the deployment to be ready 06/28/23 08:32:18.161
    Jun 28 08:32:18.170: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 06/28/23 08:32:20.384
    STEP: Verifying the service has paired with the endpoint 06/28/23 08:32:20.47
    Jun 28 08:32:21.471: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource [Conformance]
      test/e2e/apimachinery/webhook.go:290
    Jun 28 08:32:21.508: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3609-crds.webhook.example.com via the AdmissionRegistration API 06/28/23 08:32:21.621
    STEP: Creating a custom resource that should be mutated by the webhook 06/28/23 08:32:22.196
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jun 28 08:32:24.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-8460" for this suite. 06/28/23 08:32:24.903
    STEP: Destroying namespace "webhook-8460-markers" for this suite. 06/28/23 08:32:24.91
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:32:24.954
Jun 28 08:32:24.954: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename replicaset 06/28/23 08:32:24.955
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:32:24.982
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:32:24.989
[It] Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 06/28/23 08:32:24.996
Jun 28 08:32:25.013: INFO: Pod name sample-pod: Found 0 pods out of 1
Jun 28 08:32:30.018: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 06/28/23 08:32:30.018
STEP: getting scale subresource 06/28/23 08:32:30.018
STEP: updating a scale subresource 06/28/23 08:32:30.023
STEP: verifying the replicaset Spec.Replicas was modified 06/28/23 08:32:30.029
STEP: Patch a scale subresource 06/28/23 08:32:30.034
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Jun 28 08:32:30.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-208" for this suite. 06/28/23 08:32:30.059
{"msg":"PASSED [sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]","completed":228,"skipped":3757,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.111 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:32:24.954
    Jun 28 08:32:24.954: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename replicaset 06/28/23 08:32:24.955
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:32:24.982
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:32:24.989
    [It] Replicaset should have a working scale subresource [Conformance]
      test/e2e/apps/replica_set.go:143
    STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 06/28/23 08:32:24.996
    Jun 28 08:32:25.013: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jun 28 08:32:30.018: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 06/28/23 08:32:30.018
    STEP: getting scale subresource 06/28/23 08:32:30.018
    STEP: updating a scale subresource 06/28/23 08:32:30.023
    STEP: verifying the replicaset Spec.Replicas was modified 06/28/23 08:32:30.029
    STEP: Patch a scale subresource 06/28/23 08:32:30.034
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Jun 28 08:32:30.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-208" for this suite. 06/28/23 08:32:30.059
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:32:30.067
Jun 28 08:32:30.067: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename disruption 06/28/23 08:32:30.068
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:32:30.08
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:32:30.084
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346
STEP: Creating a pdb that targets all three pods in a test replica set 06/28/23 08:32:30.089
STEP: Waiting for the pdb to be processed 06/28/23 08:32:30.093
STEP: First trying to evict a pod which shouldn't be evictable 06/28/23 08:32:32.11
STEP: Waiting for all pods to be running 06/28/23 08:32:32.11
Jun 28 08:32:32.114: INFO: pods: 0 < 3
STEP: locating a running pod 06/28/23 08:32:34.121
STEP: Updating the pdb to allow a pod to be evicted 06/28/23 08:32:34.133
STEP: Waiting for the pdb to be processed 06/28/23 08:32:34.142
STEP: Trying to evict the same pod we tried earlier which should now be evictable 06/28/23 08:32:36.151
STEP: Waiting for all pods to be running 06/28/23 08:32:36.151
STEP: Waiting for the pdb to observed all healthy pods 06/28/23 08:32:36.157
STEP: Patching the pdb to disallow a pod to be evicted 06/28/23 08:32:36.175
STEP: Waiting for the pdb to be processed 06/28/23 08:32:36.193
STEP: Waiting for all pods to be running 06/28/23 08:32:36.2
Jun 28 08:32:36.206: INFO: running pods: 2 < 3
STEP: locating a running pod 06/28/23 08:32:38.213
STEP: Deleting the pdb to allow a pod to be evicted 06/28/23 08:32:38.226
STEP: Waiting for the pdb to be deleted 06/28/23 08:32:38.233
STEP: Trying to evict the same pod we tried earlier which should now be evictable 06/28/23 08:32:38.237
STEP: Waiting for all pods to be running 06/28/23 08:32:38.237
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Jun 28 08:32:38.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-1169" for this suite. 06/28/23 08:32:38.263
{"msg":"PASSED [sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]","completed":229,"skipped":3797,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.201 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:32:30.067
    Jun 28 08:32:30.067: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename disruption 06/28/23 08:32:30.068
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:32:30.08
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:32:30.084
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should block an eviction until the PDB is updated to allow it [Conformance]
      test/e2e/apps/disruption.go:346
    STEP: Creating a pdb that targets all three pods in a test replica set 06/28/23 08:32:30.089
    STEP: Waiting for the pdb to be processed 06/28/23 08:32:30.093
    STEP: First trying to evict a pod which shouldn't be evictable 06/28/23 08:32:32.11
    STEP: Waiting for all pods to be running 06/28/23 08:32:32.11
    Jun 28 08:32:32.114: INFO: pods: 0 < 3
    STEP: locating a running pod 06/28/23 08:32:34.121
    STEP: Updating the pdb to allow a pod to be evicted 06/28/23 08:32:34.133
    STEP: Waiting for the pdb to be processed 06/28/23 08:32:34.142
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 06/28/23 08:32:36.151
    STEP: Waiting for all pods to be running 06/28/23 08:32:36.151
    STEP: Waiting for the pdb to observed all healthy pods 06/28/23 08:32:36.157
    STEP: Patching the pdb to disallow a pod to be evicted 06/28/23 08:32:36.175
    STEP: Waiting for the pdb to be processed 06/28/23 08:32:36.193
    STEP: Waiting for all pods to be running 06/28/23 08:32:36.2
    Jun 28 08:32:36.206: INFO: running pods: 2 < 3
    STEP: locating a running pod 06/28/23 08:32:38.213
    STEP: Deleting the pdb to allow a pod to be evicted 06/28/23 08:32:38.226
    STEP: Waiting for the pdb to be deleted 06/28/23 08:32:38.233
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 06/28/23 08:32:38.237
    STEP: Waiting for all pods to be running 06/28/23 08:32:38.237
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Jun 28 08:32:38.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-1169" for this suite. 06/28/23 08:32:38.263
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Pods
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:32:38.268
Jun 28 08:32:38.268: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename pods 06/28/23 08:32:38.269
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:32:38.28
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:32:38.288
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203
STEP: creating pod 06/28/23 08:32:38.296
Jun 28 08:32:38.304: INFO: Waiting up to 5m0s for pod "pod-hostip-d01d987e-d587-466a-89d1-8686657b160f" in namespace "pods-416" to be "running and ready"
Jun 28 08:32:38.308: INFO: Pod "pod-hostip-d01d987e-d587-466a-89d1-8686657b160f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.239245ms
Jun 28 08:32:38.308: INFO: The phase of Pod pod-hostip-d01d987e-d587-466a-89d1-8686657b160f is Pending, waiting for it to be Running (with Ready = true)
Jun 28 08:32:40.314: INFO: Pod "pod-hostip-d01d987e-d587-466a-89d1-8686657b160f": Phase="Running", Reason="", readiness=true. Elapsed: 2.009934254s
Jun 28 08:32:40.314: INFO: The phase of Pod pod-hostip-d01d987e-d587-466a-89d1-8686657b160f is Running (Ready = true)
Jun 28 08:32:40.314: INFO: Pod "pod-hostip-d01d987e-d587-466a-89d1-8686657b160f" satisfied condition "running and ready"
Jun 28 08:32:40.322: INFO: Pod pod-hostip-d01d987e-d587-466a-89d1-8686657b160f has hostIP: 192.168.11.3
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jun 28 08:32:40.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-416" for this suite. 06/28/23 08:32:40.329
{"msg":"PASSED [sig-node] Pods should get a host IP [NodeConformance] [Conformance]","completed":230,"skipped":3798,"failed":0}
------------------------------
â€¢ [2.067 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:32:38.268
    Jun 28 08:32:38.268: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename pods 06/28/23 08:32:38.269
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:32:38.28
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:32:38.288
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should get a host IP [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:203
    STEP: creating pod 06/28/23 08:32:38.296
    Jun 28 08:32:38.304: INFO: Waiting up to 5m0s for pod "pod-hostip-d01d987e-d587-466a-89d1-8686657b160f" in namespace "pods-416" to be "running and ready"
    Jun 28 08:32:38.308: INFO: Pod "pod-hostip-d01d987e-d587-466a-89d1-8686657b160f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.239245ms
    Jun 28 08:32:38.308: INFO: The phase of Pod pod-hostip-d01d987e-d587-466a-89d1-8686657b160f is Pending, waiting for it to be Running (with Ready = true)
    Jun 28 08:32:40.314: INFO: Pod "pod-hostip-d01d987e-d587-466a-89d1-8686657b160f": Phase="Running", Reason="", readiness=true. Elapsed: 2.009934254s
    Jun 28 08:32:40.314: INFO: The phase of Pod pod-hostip-d01d987e-d587-466a-89d1-8686657b160f is Running (Ready = true)
    Jun 28 08:32:40.314: INFO: Pod "pod-hostip-d01d987e-d587-466a-89d1-8686657b160f" satisfied condition "running and ready"
    Jun 28 08:32:40.322: INFO: Pod pod-hostip-d01d987e-d587-466a-89d1-8686657b160f has hostIP: 192.168.11.3
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jun 28 08:32:40.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-416" for this suite. 06/28/23 08:32:40.329
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:337
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:32:40.336
Jun 28 08:32:40.336: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename kubectl 06/28/23 08:32:40.337
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:32:40.348
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:32:40.352
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:324
[It] should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:337
STEP: creating a replication controller 06/28/23 08:32:40.357
Jun 28 08:32:40.357: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-1877 create -f -'
Jun 28 08:32:41.060: INFO: stderr: ""
Jun 28 08:32:41.060: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 06/28/23 08:32:41.06
Jun 28 08:32:41.060: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-1877 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun 28 08:32:41.134: INFO: stderr: ""
Jun 28 08:32:41.134: INFO: stdout: "update-demo-nautilus-7scnp update-demo-nautilus-qfpr7 "
Jun 28 08:32:41.134: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-1877 get pods update-demo-nautilus-7scnp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 28 08:32:41.205: INFO: stderr: ""
Jun 28 08:32:41.205: INFO: stdout: ""
Jun 28 08:32:41.205: INFO: update-demo-nautilus-7scnp is created but not running
Jun 28 08:32:46.208: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-1877 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun 28 08:32:46.274: INFO: stderr: ""
Jun 28 08:32:46.274: INFO: stdout: "update-demo-nautilus-7scnp update-demo-nautilus-qfpr7 "
Jun 28 08:32:46.274: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-1877 get pods update-demo-nautilus-7scnp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 28 08:32:46.346: INFO: stderr: ""
Jun 28 08:32:46.346: INFO: stdout: "true"
Jun 28 08:32:46.346: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-1877 get pods update-demo-nautilus-7scnp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun 28 08:32:46.414: INFO: stderr: ""
Jun 28 08:32:46.414: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jun 28 08:32:46.414: INFO: validating pod update-demo-nautilus-7scnp
Jun 28 08:32:46.509: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 28 08:32:46.509: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 28 08:32:46.509: INFO: update-demo-nautilus-7scnp is verified up and running
Jun 28 08:32:46.509: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-1877 get pods update-demo-nautilus-qfpr7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 28 08:32:46.580: INFO: stderr: ""
Jun 28 08:32:46.580: INFO: stdout: "true"
Jun 28 08:32:46.580: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-1877 get pods update-demo-nautilus-qfpr7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun 28 08:32:46.646: INFO: stderr: ""
Jun 28 08:32:46.647: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jun 28 08:32:46.647: INFO: validating pod update-demo-nautilus-qfpr7
Jun 28 08:32:46.740: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 28 08:32:46.740: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 28 08:32:46.740: INFO: update-demo-nautilus-qfpr7 is verified up and running
STEP: using delete to clean up resources 06/28/23 08:32:46.74
Jun 28 08:32:46.740: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-1877 delete --grace-period=0 --force -f -'
Jun 28 08:32:46.806: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 28 08:32:46.806: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jun 28 08:32:46.806: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-1877 get rc,svc -l name=update-demo --no-headers'
Jun 28 08:32:46.926: INFO: stderr: "No resources found in kubectl-1877 namespace.\n"
Jun 28 08:32:46.926: INFO: stdout: ""
Jun 28 08:32:46.926: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-1877 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun 28 08:32:47.025: INFO: stderr: ""
Jun 28 08:32:47.025: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jun 28 08:32:47.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1877" for this suite. 06/28/23 08:32:47.033
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","completed":231,"skipped":3815,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.704 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:322
    should create and stop a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:337

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:32:40.336
    Jun 28 08:32:40.336: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename kubectl 06/28/23 08:32:40.337
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:32:40.348
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:32:40.352
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:324
    [It] should create and stop a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:337
    STEP: creating a replication controller 06/28/23 08:32:40.357
    Jun 28 08:32:40.357: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-1877 create -f -'
    Jun 28 08:32:41.060: INFO: stderr: ""
    Jun 28 08:32:41.060: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 06/28/23 08:32:41.06
    Jun 28 08:32:41.060: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-1877 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jun 28 08:32:41.134: INFO: stderr: ""
    Jun 28 08:32:41.134: INFO: stdout: "update-demo-nautilus-7scnp update-demo-nautilus-qfpr7 "
    Jun 28 08:32:41.134: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-1877 get pods update-demo-nautilus-7scnp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jun 28 08:32:41.205: INFO: stderr: ""
    Jun 28 08:32:41.205: INFO: stdout: ""
    Jun 28 08:32:41.205: INFO: update-demo-nautilus-7scnp is created but not running
    Jun 28 08:32:46.208: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-1877 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jun 28 08:32:46.274: INFO: stderr: ""
    Jun 28 08:32:46.274: INFO: stdout: "update-demo-nautilus-7scnp update-demo-nautilus-qfpr7 "
    Jun 28 08:32:46.274: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-1877 get pods update-demo-nautilus-7scnp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jun 28 08:32:46.346: INFO: stderr: ""
    Jun 28 08:32:46.346: INFO: stdout: "true"
    Jun 28 08:32:46.346: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-1877 get pods update-demo-nautilus-7scnp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jun 28 08:32:46.414: INFO: stderr: ""
    Jun 28 08:32:46.414: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jun 28 08:32:46.414: INFO: validating pod update-demo-nautilus-7scnp
    Jun 28 08:32:46.509: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jun 28 08:32:46.509: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jun 28 08:32:46.509: INFO: update-demo-nautilus-7scnp is verified up and running
    Jun 28 08:32:46.509: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-1877 get pods update-demo-nautilus-qfpr7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jun 28 08:32:46.580: INFO: stderr: ""
    Jun 28 08:32:46.580: INFO: stdout: "true"
    Jun 28 08:32:46.580: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-1877 get pods update-demo-nautilus-qfpr7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jun 28 08:32:46.646: INFO: stderr: ""
    Jun 28 08:32:46.647: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jun 28 08:32:46.647: INFO: validating pod update-demo-nautilus-qfpr7
    Jun 28 08:32:46.740: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jun 28 08:32:46.740: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jun 28 08:32:46.740: INFO: update-demo-nautilus-qfpr7 is verified up and running
    STEP: using delete to clean up resources 06/28/23 08:32:46.74
    Jun 28 08:32:46.740: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-1877 delete --grace-period=0 --force -f -'
    Jun 28 08:32:46.806: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jun 28 08:32:46.806: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Jun 28 08:32:46.806: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-1877 get rc,svc -l name=update-demo --no-headers'
    Jun 28 08:32:46.926: INFO: stderr: "No resources found in kubectl-1877 namespace.\n"
    Jun 28 08:32:46.926: INFO: stdout: ""
    Jun 28 08:32:46.926: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-1877 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Jun 28 08:32:47.025: INFO: stderr: ""
    Jun 28 08:32:47.025: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jun 28 08:32:47.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-1877" for this suite. 06/28/23 08:32:47.033
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:32:47.053
Jun 28 08:32:47.053: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename projected 06/28/23 08:32:47.054
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:32:47.066
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:32:47.07
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66
STEP: Creating projection with secret that has name projected-secret-test-e43e88c2-fac0-482c-832c-f3237093d017 06/28/23 08:32:47.073
STEP: Creating a pod to test consume secrets 06/28/23 08:32:47.078
Jun 28 08:32:47.085: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-752fc960-a137-42bf-b724-a79051d36622" in namespace "projected-6756" to be "Succeeded or Failed"
Jun 28 08:32:47.090: INFO: Pod "pod-projected-secrets-752fc960-a137-42bf-b724-a79051d36622": Phase="Pending", Reason="", readiness=false. Elapsed: 4.237702ms
Jun 28 08:32:49.095: INFO: Pod "pod-projected-secrets-752fc960-a137-42bf-b724-a79051d36622": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009342878s
Jun 28 08:32:51.094: INFO: Pod "pod-projected-secrets-752fc960-a137-42bf-b724-a79051d36622": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009014317s
STEP: Saw pod success 06/28/23 08:32:51.094
Jun 28 08:32:51.095: INFO: Pod "pod-projected-secrets-752fc960-a137-42bf-b724-a79051d36622" satisfied condition "Succeeded or Failed"
Jun 28 08:32:51.099: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-zxlfv pod pod-projected-secrets-752fc960-a137-42bf-b724-a79051d36622 container projected-secret-volume-test: <nil>
STEP: delete the pod 06/28/23 08:32:51.147
Jun 28 08:32:51.159: INFO: Waiting for pod pod-projected-secrets-752fc960-a137-42bf-b724-a79051d36622 to disappear
Jun 28 08:32:51.163: INFO: Pod pod-projected-secrets-752fc960-a137-42bf-b724-a79051d36622 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Jun 28 08:32:51.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6756" for this suite. 06/28/23 08:32:51.17
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","completed":232,"skipped":3865,"failed":0}
------------------------------
â€¢ [4.122 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:32:47.053
    Jun 28 08:32:47.053: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename projected 06/28/23 08:32:47.054
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:32:47.066
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:32:47.07
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:66
    STEP: Creating projection with secret that has name projected-secret-test-e43e88c2-fac0-482c-832c-f3237093d017 06/28/23 08:32:47.073
    STEP: Creating a pod to test consume secrets 06/28/23 08:32:47.078
    Jun 28 08:32:47.085: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-752fc960-a137-42bf-b724-a79051d36622" in namespace "projected-6756" to be "Succeeded or Failed"
    Jun 28 08:32:47.090: INFO: Pod "pod-projected-secrets-752fc960-a137-42bf-b724-a79051d36622": Phase="Pending", Reason="", readiness=false. Elapsed: 4.237702ms
    Jun 28 08:32:49.095: INFO: Pod "pod-projected-secrets-752fc960-a137-42bf-b724-a79051d36622": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009342878s
    Jun 28 08:32:51.094: INFO: Pod "pod-projected-secrets-752fc960-a137-42bf-b724-a79051d36622": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009014317s
    STEP: Saw pod success 06/28/23 08:32:51.094
    Jun 28 08:32:51.095: INFO: Pod "pod-projected-secrets-752fc960-a137-42bf-b724-a79051d36622" satisfied condition "Succeeded or Failed"
    Jun 28 08:32:51.099: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-zxlfv pod pod-projected-secrets-752fc960-a137-42bf-b724-a79051d36622 container projected-secret-volume-test: <nil>
    STEP: delete the pod 06/28/23 08:32:51.147
    Jun 28 08:32:51.159: INFO: Waiting for pod pod-projected-secrets-752fc960-a137-42bf-b724-a79051d36622 to disappear
    Jun 28 08:32:51.163: INFO: Pod pod-projected-secrets-752fc960-a137-42bf-b724-a79051d36622 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Jun 28 08:32:51.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6756" for this suite. 06/28/23 08:32:51.17
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:32:51.177
Jun 28 08:32:51.177: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename var-expansion 06/28/23 08:32:51.177
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:32:51.189
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:32:51.193
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185
Jun 28 08:32:51.205: INFO: Waiting up to 2m0s for pod "var-expansion-bcd62ca6-df19-4039-8aa1-5b83e28bb455" in namespace "var-expansion-932" to be "container 0 failed with reason CreateContainerConfigError"
Jun 28 08:32:51.209: INFO: Pod "var-expansion-bcd62ca6-df19-4039-8aa1-5b83e28bb455": Phase="Pending", Reason="", readiness=false. Elapsed: 3.638049ms
Jun 28 08:32:53.214: INFO: Pod "var-expansion-bcd62ca6-df19-4039-8aa1-5b83e28bb455": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009143391s
Jun 28 08:32:53.214: INFO: Pod "var-expansion-bcd62ca6-df19-4039-8aa1-5b83e28bb455" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Jun 28 08:32:53.214: INFO: Deleting pod "var-expansion-bcd62ca6-df19-4039-8aa1-5b83e28bb455" in namespace "var-expansion-932"
Jun 28 08:32:53.227: INFO: Wait up to 5m0s for pod "var-expansion-bcd62ca6-df19-4039-8aa1-5b83e28bb455" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jun 28 08:32:57.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-932" for this suite. 06/28/23 08:32:57.243
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]","completed":233,"skipped":3905,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.072 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:32:51.177
    Jun 28 08:32:51.177: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename var-expansion 06/28/23 08:32:51.177
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:32:51.189
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:32:51.193
    [It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
      test/e2e/common/node/expansion.go:185
    Jun 28 08:32:51.205: INFO: Waiting up to 2m0s for pod "var-expansion-bcd62ca6-df19-4039-8aa1-5b83e28bb455" in namespace "var-expansion-932" to be "container 0 failed with reason CreateContainerConfigError"
    Jun 28 08:32:51.209: INFO: Pod "var-expansion-bcd62ca6-df19-4039-8aa1-5b83e28bb455": Phase="Pending", Reason="", readiness=false. Elapsed: 3.638049ms
    Jun 28 08:32:53.214: INFO: Pod "var-expansion-bcd62ca6-df19-4039-8aa1-5b83e28bb455": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009143391s
    Jun 28 08:32:53.214: INFO: Pod "var-expansion-bcd62ca6-df19-4039-8aa1-5b83e28bb455" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Jun 28 08:32:53.214: INFO: Deleting pod "var-expansion-bcd62ca6-df19-4039-8aa1-5b83e28bb455" in namespace "var-expansion-932"
    Jun 28 08:32:53.227: INFO: Wait up to 5m0s for pod "var-expansion-bcd62ca6-df19-4039-8aa1-5b83e28bb455" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jun 28 08:32:57.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-932" for this suite. 06/28/23 08:32:57.243
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:32:57.249
Jun 28 08:32:57.249: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename webhook 06/28/23 08:32:57.25
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:32:57.263
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:32:57.267
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 06/28/23 08:32:57.284
STEP: Create role binding to let webhook read extension-apiserver-authentication 06/28/23 08:32:57.455
STEP: Deploying the webhook pod 06/28/23 08:32:57.464
STEP: Wait for the deployment to be ready 06/28/23 08:32:57.476
Jun 28 08:32:57.485: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 06/28/23 08:32:59.502
STEP: Verifying the service has paired with the endpoint 06/28/23 08:32:59.512
Jun 28 08:33:00.512: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263
STEP: Registering the mutating pod webhook via the AdmissionRegistration API 06/28/23 08:33:00.518
STEP: create a pod that should be updated by the webhook 06/28/23 08:33:00.626
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jun 28 08:33:00.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6283" for this suite. 06/28/23 08:33:00.742
STEP: Destroying namespace "webhook-6283-markers" for this suite. 06/28/23 08:33:00.748
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","completed":234,"skipped":3921,"failed":0}
------------------------------
â€¢ [3.540 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:32:57.249
    Jun 28 08:32:57.249: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename webhook 06/28/23 08:32:57.25
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:32:57.263
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:32:57.267
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 06/28/23 08:32:57.284
    STEP: Create role binding to let webhook read extension-apiserver-authentication 06/28/23 08:32:57.455
    STEP: Deploying the webhook pod 06/28/23 08:32:57.464
    STEP: Wait for the deployment to be ready 06/28/23 08:32:57.476
    Jun 28 08:32:57.485: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 06/28/23 08:32:59.502
    STEP: Verifying the service has paired with the endpoint 06/28/23 08:32:59.512
    Jun 28 08:33:00.512: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate pod and apply defaults after mutation [Conformance]
      test/e2e/apimachinery/webhook.go:263
    STEP: Registering the mutating pod webhook via the AdmissionRegistration API 06/28/23 08:33:00.518
    STEP: create a pod that should be updated by the webhook 06/28/23 08:33:00.626
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jun 28 08:33:00.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-6283" for this suite. 06/28/23 08:33:00.742
    STEP: Destroying namespace "webhook-6283-markers" for this suite. 06/28/23 08:33:00.748
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] EndpointSlice
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:33:00.79
Jun 28 08:33:00.790: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename endpointslice 06/28/23 08:33:00.791
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:33:00.803
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:33:00.808
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204
STEP: referencing a single matching pod 06/28/23 08:33:05.891
STEP: referencing matching pods with named port 06/28/23 08:33:10.914
STEP: creating empty Endpoints and EndpointSlices for no matching Pods 06/28/23 08:33:15.922
STEP: recreating EndpointSlices after they've been deleted 06/28/23 08:33:20.936
Jun 28 08:33:20.958: INFO: EndpointSlice for Service endpointslice-926/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Jun 28 08:33:30.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-926" for this suite. 06/28/23 08:33:30.993
{"msg":"PASSED [sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]","completed":235,"skipped":3925,"failed":0}
------------------------------
â€¢ [SLOW TEST] [30.211 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:33:00.79
    Jun 28 08:33:00.790: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename endpointslice 06/28/23 08:33:00.791
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:33:00.803
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:33:00.808
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
      test/e2e/network/endpointslice.go:204
    STEP: referencing a single matching pod 06/28/23 08:33:05.891
    STEP: referencing matching pods with named port 06/28/23 08:33:10.914
    STEP: creating empty Endpoints and EndpointSlices for no matching Pods 06/28/23 08:33:15.922
    STEP: recreating EndpointSlices after they've been deleted 06/28/23 08:33:20.936
    Jun 28 08:33:20.958: INFO: EndpointSlice for Service endpointslice-926/example-named-port not found
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Jun 28 08:33:30.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-926" for this suite. 06/28/23 08:33:30.993
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:33:31.013
Jun 28 08:33:31.013: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename custom-resource-definition 06/28/23 08:33:31.013
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:33:31.038
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:33:31.044
[It] listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
Jun 28 08:33:31.050: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jun 28 08:33:37.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-8598" for this suite. 06/28/23 08:33:37.349
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","completed":236,"skipped":4096,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.345 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    listing custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:85

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:33:31.013
    Jun 28 08:33:31.013: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename custom-resource-definition 06/28/23 08:33:31.013
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:33:31.038
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:33:31.044
    [It] listing custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:85
    Jun 28 08:33:31.050: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jun 28 08:33:37.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-8598" for this suite. 06/28/23 08:33:37.349
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:33:37.358
Jun 28 08:33:37.358: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename downward-api 06/28/23 08:33:37.359
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:33:37.371
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:33:37.375
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129
STEP: Creating the pod 06/28/23 08:33:37.38
Jun 28 08:33:37.388: INFO: Waiting up to 5m0s for pod "labelsupdate2c8f2248-47bd-4050-9b07-abe9f029c251" in namespace "downward-api-6128" to be "running and ready"
Jun 28 08:33:37.393: INFO: Pod "labelsupdate2c8f2248-47bd-4050-9b07-abe9f029c251": Phase="Pending", Reason="", readiness=false. Elapsed: 4.830041ms
Jun 28 08:33:37.393: INFO: The phase of Pod labelsupdate2c8f2248-47bd-4050-9b07-abe9f029c251 is Pending, waiting for it to be Running (with Ready = true)
Jun 28 08:33:39.399: INFO: Pod "labelsupdate2c8f2248-47bd-4050-9b07-abe9f029c251": Phase="Running", Reason="", readiness=true. Elapsed: 2.011116785s
Jun 28 08:33:39.399: INFO: The phase of Pod labelsupdate2c8f2248-47bd-4050-9b07-abe9f029c251 is Running (Ready = true)
Jun 28 08:33:39.399: INFO: Pod "labelsupdate2c8f2248-47bd-4050-9b07-abe9f029c251" satisfied condition "running and ready"
Jun 28 08:33:39.928: INFO: Successfully updated pod "labelsupdate2c8f2248-47bd-4050-9b07-abe9f029c251"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jun 28 08:33:44.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6128" for this suite. 06/28/23 08:33:44.111
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","completed":237,"skipped":4106,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.760 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:33:37.358
    Jun 28 08:33:37.358: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename downward-api 06/28/23 08:33:37.359
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:33:37.371
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:33:37.375
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:129
    STEP: Creating the pod 06/28/23 08:33:37.38
    Jun 28 08:33:37.388: INFO: Waiting up to 5m0s for pod "labelsupdate2c8f2248-47bd-4050-9b07-abe9f029c251" in namespace "downward-api-6128" to be "running and ready"
    Jun 28 08:33:37.393: INFO: Pod "labelsupdate2c8f2248-47bd-4050-9b07-abe9f029c251": Phase="Pending", Reason="", readiness=false. Elapsed: 4.830041ms
    Jun 28 08:33:37.393: INFO: The phase of Pod labelsupdate2c8f2248-47bd-4050-9b07-abe9f029c251 is Pending, waiting for it to be Running (with Ready = true)
    Jun 28 08:33:39.399: INFO: Pod "labelsupdate2c8f2248-47bd-4050-9b07-abe9f029c251": Phase="Running", Reason="", readiness=true. Elapsed: 2.011116785s
    Jun 28 08:33:39.399: INFO: The phase of Pod labelsupdate2c8f2248-47bd-4050-9b07-abe9f029c251 is Running (Ready = true)
    Jun 28 08:33:39.399: INFO: Pod "labelsupdate2c8f2248-47bd-4050-9b07-abe9f029c251" satisfied condition "running and ready"
    Jun 28 08:33:39.928: INFO: Successfully updated pod "labelsupdate2c8f2248-47bd-4050-9b07-abe9f029c251"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jun 28 08:33:44.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-6128" for this suite. 06/28/23 08:33:44.111
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application
  should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:392
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:33:44.122
Jun 28 08:33:44.122: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename kubectl 06/28/23 08:33:44.123
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:33:44.134
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:33:44.138
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:392
STEP: creating all guestbook components 06/28/23 08:33:44.142
Jun 28 08:33:44.142: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Jun 28 08:33:44.142: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-3377 create -f -'
Jun 28 08:33:44.867: INFO: stderr: ""
Jun 28 08:33:44.867: INFO: stdout: "service/agnhost-replica created\n"
Jun 28 08:33:44.867: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Jun 28 08:33:44.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-3377 create -f -'
Jun 28 08:33:45.077: INFO: stderr: ""
Jun 28 08:33:45.077: INFO: stdout: "service/agnhost-primary created\n"
Jun 28 08:33:45.078: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Jun 28 08:33:45.078: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-3377 create -f -'
Jun 28 08:33:45.274: INFO: stderr: ""
Jun 28 08:33:45.274: INFO: stdout: "service/frontend created\n"
Jun 28 08:33:45.275: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Jun 28 08:33:45.275: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-3377 create -f -'
Jun 28 08:33:45.472: INFO: stderr: ""
Jun 28 08:33:45.472: INFO: stdout: "deployment.apps/frontend created\n"
Jun 28 08:33:45.472: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jun 28 08:33:45.472: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-3377 create -f -'
Jun 28 08:33:45.660: INFO: stderr: ""
Jun 28 08:33:45.660: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Jun 28 08:33:45.660: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jun 28 08:33:45.660: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-3377 create -f -'
Jun 28 08:33:45.877: INFO: stderr: ""
Jun 28 08:33:45.877: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app 06/28/23 08:33:45.877
Jun 28 08:33:45.878: INFO: Waiting for all frontend pods to be Running.
Jun 28 08:33:50.930: INFO: Waiting for frontend to serve content.
Jun 28 08:33:51.023: INFO: Trying to add a new entry to the guestbook.
Jun 28 08:33:51.150: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources 06/28/23 08:33:51.196
Jun 28 08:33:51.196: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-3377 delete --grace-period=0 --force -f -'
Jun 28 08:33:51.279: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 28 08:33:51.279: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources 06/28/23 08:33:51.279
Jun 28 08:33:51.279: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-3377 delete --grace-period=0 --force -f -'
Jun 28 08:33:51.371: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 28 08:33:51.371: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 06/28/23 08:33:51.371
Jun 28 08:33:51.371: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-3377 delete --grace-period=0 --force -f -'
Jun 28 08:33:51.460: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 28 08:33:51.460: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources 06/28/23 08:33:51.46
Jun 28 08:33:51.460: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-3377 delete --grace-period=0 --force -f -'
Jun 28 08:33:51.536: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 28 08:33:51.536: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources 06/28/23 08:33:51.536
Jun 28 08:33:51.536: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-3377 delete --grace-period=0 --force -f -'
Jun 28 08:33:51.625: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 28 08:33:51.625: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 06/28/23 08:33:51.625
Jun 28 08:33:51.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-3377 delete --grace-period=0 --force -f -'
Jun 28 08:33:51.760: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 28 08:33:51.760: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jun 28 08:33:51.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3377" for this suite. 06/28/23 08:33:51.774
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","completed":238,"skipped":4137,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.659 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Guestbook application
  test/e2e/kubectl/kubectl.go:367
    should create and stop a working application  [Conformance]
    test/e2e/kubectl/kubectl.go:392

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:33:44.122
    Jun 28 08:33:44.122: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename kubectl 06/28/23 08:33:44.123
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:33:44.134
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:33:44.138
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should create and stop a working application  [Conformance]
      test/e2e/kubectl/kubectl.go:392
    STEP: creating all guestbook components 06/28/23 08:33:44.142
    Jun 28 08:33:44.142: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-replica
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      ports:
      - port: 6379
      selector:
        app: agnhost
        role: replica
        tier: backend

    Jun 28 08:33:44.142: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-3377 create -f -'
    Jun 28 08:33:44.867: INFO: stderr: ""
    Jun 28 08:33:44.867: INFO: stdout: "service/agnhost-replica created\n"
    Jun 28 08:33:44.867: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-primary
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      ports:
      - port: 6379
        targetPort: 6379
      selector:
        app: agnhost
        role: primary
        tier: backend

    Jun 28 08:33:44.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-3377 create -f -'
    Jun 28 08:33:45.077: INFO: stderr: ""
    Jun 28 08:33:45.077: INFO: stdout: "service/agnhost-primary created\n"
    Jun 28 08:33:45.078: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: frontend
      labels:
        app: guestbook
        tier: frontend
    spec:
      # if your cluster supports it, uncomment the following to automatically create
      # an external load-balanced IP for the frontend service.
      # type: LoadBalancer
      ports:
      - port: 80
      selector:
        app: guestbook
        tier: frontend

    Jun 28 08:33:45.078: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-3377 create -f -'
    Jun 28 08:33:45.274: INFO: stderr: ""
    Jun 28 08:33:45.274: INFO: stdout: "service/frontend created\n"
    Jun 28 08:33:45.275: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: frontend
    spec:
      replicas: 3
      selector:
        matchLabels:
          app: guestbook
          tier: frontend
      template:
        metadata:
          labels:
            app: guestbook
            tier: frontend
        spec:
          containers:
          - name: guestbook-frontend
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--backend-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 80

    Jun 28 08:33:45.275: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-3377 create -f -'
    Jun 28 08:33:45.472: INFO: stderr: ""
    Jun 28 08:33:45.472: INFO: stdout: "deployment.apps/frontend created\n"
    Jun 28 08:33:45.472: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-primary
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: agnhost
          role: primary
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: primary
            tier: backend
        spec:
          containers:
          - name: primary
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Jun 28 08:33:45.472: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-3377 create -f -'
    Jun 28 08:33:45.660: INFO: stderr: ""
    Jun 28 08:33:45.660: INFO: stdout: "deployment.apps/agnhost-primary created\n"
    Jun 28 08:33:45.660: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-replica
    spec:
      replicas: 2
      selector:
        matchLabels:
          app: agnhost
          role: replica
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: replica
            tier: backend
        spec:
          containers:
          - name: replica
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Jun 28 08:33:45.660: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-3377 create -f -'
    Jun 28 08:33:45.877: INFO: stderr: ""
    Jun 28 08:33:45.877: INFO: stdout: "deployment.apps/agnhost-replica created\n"
    STEP: validating guestbook app 06/28/23 08:33:45.877
    Jun 28 08:33:45.878: INFO: Waiting for all frontend pods to be Running.
    Jun 28 08:33:50.930: INFO: Waiting for frontend to serve content.
    Jun 28 08:33:51.023: INFO: Trying to add a new entry to the guestbook.
    Jun 28 08:33:51.150: INFO: Verifying that added entry can be retrieved.
    STEP: using delete to clean up resources 06/28/23 08:33:51.196
    Jun 28 08:33:51.196: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-3377 delete --grace-period=0 --force -f -'
    Jun 28 08:33:51.279: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jun 28 08:33:51.279: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
    STEP: using delete to clean up resources 06/28/23 08:33:51.279
    Jun 28 08:33:51.279: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-3377 delete --grace-period=0 --force -f -'
    Jun 28 08:33:51.371: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jun 28 08:33:51.371: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 06/28/23 08:33:51.371
    Jun 28 08:33:51.371: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-3377 delete --grace-period=0 --force -f -'
    Jun 28 08:33:51.460: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jun 28 08:33:51.460: INFO: stdout: "service \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 06/28/23 08:33:51.46
    Jun 28 08:33:51.460: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-3377 delete --grace-period=0 --force -f -'
    Jun 28 08:33:51.536: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jun 28 08:33:51.536: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 06/28/23 08:33:51.536
    Jun 28 08:33:51.536: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-3377 delete --grace-period=0 --force -f -'
    Jun 28 08:33:51.625: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jun 28 08:33:51.625: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 06/28/23 08:33:51.625
    Jun 28 08:33:51.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-3377 delete --grace-period=0 --force -f -'
    Jun 28 08:33:51.760: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jun 28 08:33:51.760: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jun 28 08:33:51.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-3377" for this suite. 06/28/23 08:33:51.774
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod
  should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:33:51.782
Jun 28 08:33:51.782: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename kubelet-test 06/28/23 08:33:51.783
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:33:51.808
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:33:51.814
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
Jun 28 08:33:51.830: INFO: Waiting up to 5m0s for pod "busybox-scheduling-67e29039-f8bb-4a28-9b80-d093975fcafe" in namespace "kubelet-test-5525" to be "running and ready"
Jun 28 08:33:51.835: INFO: Pod "busybox-scheduling-67e29039-f8bb-4a28-9b80-d093975fcafe": Phase="Pending", Reason="", readiness=false. Elapsed: 5.353259ms
Jun 28 08:33:51.835: INFO: The phase of Pod busybox-scheduling-67e29039-f8bb-4a28-9b80-d093975fcafe is Pending, waiting for it to be Running (with Ready = true)
Jun 28 08:33:53.839: INFO: Pod "busybox-scheduling-67e29039-f8bb-4a28-9b80-d093975fcafe": Phase="Running", Reason="", readiness=true. Elapsed: 2.009847051s
Jun 28 08:33:53.840: INFO: The phase of Pod busybox-scheduling-67e29039-f8bb-4a28-9b80-d093975fcafe is Running (Ready = true)
Jun 28 08:33:53.840: INFO: Pod "busybox-scheduling-67e29039-f8bb-4a28-9b80-d093975fcafe" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Jun 28 08:33:53.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5525" for this suite. 06/28/23 08:33:53.86
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","completed":239,"skipped":4153,"failed":0}
------------------------------
â€¢ [2.084 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command in a pod
  test/e2e/common/node/kubelet.go:44
    should print the output to logs [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:33:51.782
    Jun 28 08:33:51.782: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename kubelet-test 06/28/23 08:33:51.783
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:33:51.808
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:33:51.814
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should print the output to logs [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:52
    Jun 28 08:33:51.830: INFO: Waiting up to 5m0s for pod "busybox-scheduling-67e29039-f8bb-4a28-9b80-d093975fcafe" in namespace "kubelet-test-5525" to be "running and ready"
    Jun 28 08:33:51.835: INFO: Pod "busybox-scheduling-67e29039-f8bb-4a28-9b80-d093975fcafe": Phase="Pending", Reason="", readiness=false. Elapsed: 5.353259ms
    Jun 28 08:33:51.835: INFO: The phase of Pod busybox-scheduling-67e29039-f8bb-4a28-9b80-d093975fcafe is Pending, waiting for it to be Running (with Ready = true)
    Jun 28 08:33:53.839: INFO: Pod "busybox-scheduling-67e29039-f8bb-4a28-9b80-d093975fcafe": Phase="Running", Reason="", readiness=true. Elapsed: 2.009847051s
    Jun 28 08:33:53.840: INFO: The phase of Pod busybox-scheduling-67e29039-f8bb-4a28-9b80-d093975fcafe is Running (Ready = true)
    Jun 28 08:33:53.840: INFO: Pod "busybox-scheduling-67e29039-f8bb-4a28-9b80-d093975fcafe" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Jun 28 08:33:53.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-5525" for this suite. 06/28/23 08:33:53.86
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:33:53.867
Jun 28 08:33:53.867: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename pods 06/28/23 08:33:53.868
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:33:53.883
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:33:53.894
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895
STEP: creating a Pod with a static label 06/28/23 08:33:53.914
STEP: watching for Pod to be ready 06/28/23 08:33:53.923
Jun 28 08:33:53.925: INFO: observed Pod pod-test in namespace pods-5414 in phase Pending with labels: map[test-pod-static:true] & conditions []
Jun 28 08:33:53.927: INFO: observed Pod pod-test in namespace pods-5414 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-28 08:33:53 +0000 UTC  }]
Jun 28 08:33:53.940: INFO: observed Pod pod-test in namespace pods-5414 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-28 08:33:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-28 08:33:53 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-28 08:33:53 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-28 08:33:53 +0000 UTC  }]
Jun 28 08:33:54.426: INFO: observed Pod pod-test in namespace pods-5414 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-28 08:33:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-28 08:33:53 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-28 08:33:53 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-28 08:33:53 +0000 UTC  }]
Jun 28 08:33:55.454: INFO: Found Pod pod-test in namespace pods-5414 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-28 08:33:53 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-06-28 08:33:55 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-06-28 08:33:55 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-28 08:33:53 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data 06/28/23 08:33:55.459
STEP: getting the Pod and ensuring that it's patched 06/28/23 08:33:55.469
STEP: replacing the Pod's status Ready condition to False 06/28/23 08:33:55.473
STEP: check the Pod again to ensure its Ready conditions are False 06/28/23 08:33:55.485
STEP: deleting the Pod via a Collection with a LabelSelector 06/28/23 08:33:55.485
STEP: watching for the Pod to be deleted 06/28/23 08:33:55.495
Jun 28 08:33:55.498: INFO: observed event type MODIFIED
Jun 28 08:33:56.101: INFO: observed event type MODIFIED
Jun 28 08:33:57.682: INFO: observed event type MODIFIED
Jun 28 08:33:58.461: INFO: observed event type MODIFIED
Jun 28 08:33:58.468: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jun 28 08:33:58.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5414" for this suite. 06/28/23 08:33:58.483
{"msg":"PASSED [sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","completed":240,"skipped":4173,"failed":0}
------------------------------
â€¢ [4.624 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:33:53.867
    Jun 28 08:33:53.867: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename pods 06/28/23 08:33:53.868
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:33:53.883
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:33:53.894
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should run through the lifecycle of Pods and PodStatus [Conformance]
      test/e2e/common/node/pods.go:895
    STEP: creating a Pod with a static label 06/28/23 08:33:53.914
    STEP: watching for Pod to be ready 06/28/23 08:33:53.923
    Jun 28 08:33:53.925: INFO: observed Pod pod-test in namespace pods-5414 in phase Pending with labels: map[test-pod-static:true] & conditions []
    Jun 28 08:33:53.927: INFO: observed Pod pod-test in namespace pods-5414 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-28 08:33:53 +0000 UTC  }]
    Jun 28 08:33:53.940: INFO: observed Pod pod-test in namespace pods-5414 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-28 08:33:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-28 08:33:53 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-28 08:33:53 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-28 08:33:53 +0000 UTC  }]
    Jun 28 08:33:54.426: INFO: observed Pod pod-test in namespace pods-5414 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-28 08:33:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-28 08:33:53 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-28 08:33:53 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-28 08:33:53 +0000 UTC  }]
    Jun 28 08:33:55.454: INFO: Found Pod pod-test in namespace pods-5414 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-28 08:33:53 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-06-28 08:33:55 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-06-28 08:33:55 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-28 08:33:53 +0000 UTC  }]
    STEP: patching the Pod with a new Label and updated data 06/28/23 08:33:55.459
    STEP: getting the Pod and ensuring that it's patched 06/28/23 08:33:55.469
    STEP: replacing the Pod's status Ready condition to False 06/28/23 08:33:55.473
    STEP: check the Pod again to ensure its Ready conditions are False 06/28/23 08:33:55.485
    STEP: deleting the Pod via a Collection with a LabelSelector 06/28/23 08:33:55.485
    STEP: watching for the Pod to be deleted 06/28/23 08:33:55.495
    Jun 28 08:33:55.498: INFO: observed event type MODIFIED
    Jun 28 08:33:56.101: INFO: observed event type MODIFIED
    Jun 28 08:33:57.682: INFO: observed event type MODIFIED
    Jun 28 08:33:58.461: INFO: observed event type MODIFIED
    Jun 28 08:33:58.468: INFO: observed event type MODIFIED
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jun 28 08:33:58.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-5414" for this suite. 06/28/23 08:33:58.483
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] ControllerRevision [Serial]
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:33:58.491
Jun 28 08:33:58.491: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename controllerrevisions 06/28/23 08:33:58.492
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:33:58.507
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:33:58.511
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:93
[It] should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
STEP: Creating DaemonSet "e2e-q67kd-daemon-set" 06/28/23 08:33:58.542
STEP: Check that daemon pods launch on every node of the cluster. 06/28/23 08:33:58.548
Jun 28 08:33:58.560: INFO: Number of nodes with available pods controlled by daemonset e2e-q67kd-daemon-set: 0
Jun 28 08:33:58.560: INFO: Node ske-rhel-749f7d55c8xdd8b6-ct4cp is running 0 daemon pod, expected 1
Jun 28 08:33:59.573: INFO: Number of nodes with available pods controlled by daemonset e2e-q67kd-daemon-set: 1
Jun 28 08:33:59.574: INFO: Node ske-rhel-749f7d55c8xdd8b6-srshq is running 0 daemon pod, expected 1
Jun 28 08:34:00.575: INFO: Number of nodes with available pods controlled by daemonset e2e-q67kd-daemon-set: 3
Jun 28 08:34:00.575: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-q67kd-daemon-set
STEP: Confirm DaemonSet "e2e-q67kd-daemon-set" successfully created with "daemonset-name=e2e-q67kd-daemon-set" label 06/28/23 08:34:00.579
STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-q67kd-daemon-set" 06/28/23 08:34:00.591
Jun 28 08:34:00.600: INFO: Located ControllerRevision: "e2e-q67kd-daemon-set-56cb777cb4"
STEP: Patching ControllerRevision "e2e-q67kd-daemon-set-56cb777cb4" 06/28/23 08:34:00.605
Jun 28 08:34:00.615: INFO: e2e-q67kd-daemon-set-56cb777cb4 has been patched
STEP: Create a new ControllerRevision 06/28/23 08:34:00.615
Jun 28 08:34:00.623: INFO: Created ControllerRevision: e2e-q67kd-daemon-set-5f4b766c46
STEP: Confirm that there are two ControllerRevisions 06/28/23 08:34:00.623
Jun 28 08:34:00.623: INFO: Requesting list of ControllerRevisions to confirm quantity
Jun 28 08:34:00.632: INFO: Found 2 ControllerRevisions
STEP: Deleting ControllerRevision "e2e-q67kd-daemon-set-56cb777cb4" 06/28/23 08:34:00.632
STEP: Confirm that there is only one ControllerRevision 06/28/23 08:34:00.639
Jun 28 08:34:00.639: INFO: Requesting list of ControllerRevisions to confirm quantity
Jun 28 08:34:00.644: INFO: Found 1 ControllerRevisions
STEP: Updating ControllerRevision "e2e-q67kd-daemon-set-5f4b766c46" 06/28/23 08:34:00.648
Jun 28 08:34:00.661: INFO: e2e-q67kd-daemon-set-5f4b766c46 has been updated
STEP: Generate another ControllerRevision by patching the Daemonset 06/28/23 08:34:00.661
W0628 08:34:00.670486      18 warnings.go:70] unknown field "updateStrategy"
STEP: Confirm that there are two ControllerRevisions 06/28/23 08:34:00.67
Jun 28 08:34:00.670: INFO: Requesting list of ControllerRevisions to confirm quantity
Jun 28 08:34:01.675: INFO: Requesting list of ControllerRevisions to confirm quantity
Jun 28 08:34:01.680: INFO: Found 2 ControllerRevisions
STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-q67kd-daemon-set-5f4b766c46=updated" 06/28/23 08:34:01.68
STEP: Confirm that there is only one ControllerRevision 06/28/23 08:34:01.691
Jun 28 08:34:01.691: INFO: Requesting list of ControllerRevisions to confirm quantity
Jun 28 08:34:01.697: INFO: Found 1 ControllerRevisions
Jun 28 08:34:01.702: INFO: ControllerRevision "e2e-q67kd-daemon-set-6b5d566667" has revision 3
[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:58
STEP: Deleting DaemonSet "e2e-q67kd-daemon-set" 06/28/23 08:34:01.707
STEP: deleting DaemonSet.extensions e2e-q67kd-daemon-set in namespace controllerrevisions-9638, will wait for the garbage collector to delete the pods 06/28/23 08:34:01.707
Jun 28 08:34:01.774: INFO: Deleting DaemonSet.extensions e2e-q67kd-daemon-set took: 9.556144ms
Jun 28 08:34:01.875: INFO: Terminating DaemonSet.extensions e2e-q67kd-daemon-set pods took: 100.628408ms
Jun 28 08:34:03.380: INFO: Number of nodes with available pods controlled by daemonset e2e-q67kd-daemon-set: 0
Jun 28 08:34:03.380: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-q67kd-daemon-set
Jun 28 08:34:03.386: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"75324"},"items":null}

Jun 28 08:34:03.390: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"75324"},"items":null}

[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/framework.go:187
Jun 28 08:34:03.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "controllerrevisions-9638" for this suite. 06/28/23 08:34:03.42
{"msg":"PASSED [sig-apps] ControllerRevision [Serial] should manage the lifecycle of a ControllerRevision [Conformance]","completed":241,"skipped":4179,"failed":0}
------------------------------
â€¢ [4.935 seconds]
[sig-apps] ControllerRevision [Serial]
test/e2e/apps/framework.go:23
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:33:58.491
    Jun 28 08:33:58.491: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename controllerrevisions 06/28/23 08:33:58.492
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:33:58.507
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:33:58.511
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:93
    [It] should manage the lifecycle of a ControllerRevision [Conformance]
      test/e2e/apps/controller_revision.go:124
    STEP: Creating DaemonSet "e2e-q67kd-daemon-set" 06/28/23 08:33:58.542
    STEP: Check that daemon pods launch on every node of the cluster. 06/28/23 08:33:58.548
    Jun 28 08:33:58.560: INFO: Number of nodes with available pods controlled by daemonset e2e-q67kd-daemon-set: 0
    Jun 28 08:33:58.560: INFO: Node ske-rhel-749f7d55c8xdd8b6-ct4cp is running 0 daemon pod, expected 1
    Jun 28 08:33:59.573: INFO: Number of nodes with available pods controlled by daemonset e2e-q67kd-daemon-set: 1
    Jun 28 08:33:59.574: INFO: Node ske-rhel-749f7d55c8xdd8b6-srshq is running 0 daemon pod, expected 1
    Jun 28 08:34:00.575: INFO: Number of nodes with available pods controlled by daemonset e2e-q67kd-daemon-set: 3
    Jun 28 08:34:00.575: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-q67kd-daemon-set
    STEP: Confirm DaemonSet "e2e-q67kd-daemon-set" successfully created with "daemonset-name=e2e-q67kd-daemon-set" label 06/28/23 08:34:00.579
    STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-q67kd-daemon-set" 06/28/23 08:34:00.591
    Jun 28 08:34:00.600: INFO: Located ControllerRevision: "e2e-q67kd-daemon-set-56cb777cb4"
    STEP: Patching ControllerRevision "e2e-q67kd-daemon-set-56cb777cb4" 06/28/23 08:34:00.605
    Jun 28 08:34:00.615: INFO: e2e-q67kd-daemon-set-56cb777cb4 has been patched
    STEP: Create a new ControllerRevision 06/28/23 08:34:00.615
    Jun 28 08:34:00.623: INFO: Created ControllerRevision: e2e-q67kd-daemon-set-5f4b766c46
    STEP: Confirm that there are two ControllerRevisions 06/28/23 08:34:00.623
    Jun 28 08:34:00.623: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jun 28 08:34:00.632: INFO: Found 2 ControllerRevisions
    STEP: Deleting ControllerRevision "e2e-q67kd-daemon-set-56cb777cb4" 06/28/23 08:34:00.632
    STEP: Confirm that there is only one ControllerRevision 06/28/23 08:34:00.639
    Jun 28 08:34:00.639: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jun 28 08:34:00.644: INFO: Found 1 ControllerRevisions
    STEP: Updating ControllerRevision "e2e-q67kd-daemon-set-5f4b766c46" 06/28/23 08:34:00.648
    Jun 28 08:34:00.661: INFO: e2e-q67kd-daemon-set-5f4b766c46 has been updated
    STEP: Generate another ControllerRevision by patching the Daemonset 06/28/23 08:34:00.661
    W0628 08:34:00.670486      18 warnings.go:70] unknown field "updateStrategy"
    STEP: Confirm that there are two ControllerRevisions 06/28/23 08:34:00.67
    Jun 28 08:34:00.670: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jun 28 08:34:01.675: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jun 28 08:34:01.680: INFO: Found 2 ControllerRevisions
    STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-q67kd-daemon-set-5f4b766c46=updated" 06/28/23 08:34:01.68
    STEP: Confirm that there is only one ControllerRevision 06/28/23 08:34:01.691
    Jun 28 08:34:01.691: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jun 28 08:34:01.697: INFO: Found 1 ControllerRevisions
    Jun 28 08:34:01.702: INFO: ControllerRevision "e2e-q67kd-daemon-set-6b5d566667" has revision 3
    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:58
    STEP: Deleting DaemonSet "e2e-q67kd-daemon-set" 06/28/23 08:34:01.707
    STEP: deleting DaemonSet.extensions e2e-q67kd-daemon-set in namespace controllerrevisions-9638, will wait for the garbage collector to delete the pods 06/28/23 08:34:01.707
    Jun 28 08:34:01.774: INFO: Deleting DaemonSet.extensions e2e-q67kd-daemon-set took: 9.556144ms
    Jun 28 08:34:01.875: INFO: Terminating DaemonSet.extensions e2e-q67kd-daemon-set pods took: 100.628408ms
    Jun 28 08:34:03.380: INFO: Number of nodes with available pods controlled by daemonset e2e-q67kd-daemon-set: 0
    Jun 28 08:34:03.380: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-q67kd-daemon-set
    Jun 28 08:34:03.386: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"75324"},"items":null}

    Jun 28 08:34:03.390: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"75324"},"items":null}

    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/framework.go:187
    Jun 28 08:34:03.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "controllerrevisions-9638" for this suite. 06/28/23 08:34:03.42
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:34:03.43
Jun 28 08:34:03.430: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename limitrange 06/28/23 08:34:03.431
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:34:03.445
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:34:03.448
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57
STEP: Creating a LimitRange 06/28/23 08:34:03.453
STEP: Setting up watch 06/28/23 08:34:03.453
STEP: Submitting a LimitRange 06/28/23 08:34:03.557
STEP: Verifying LimitRange creation was observed 06/28/23 08:34:03.563
STEP: Fetching the LimitRange to ensure it has proper values 06/28/23 08:34:03.563
Jun 28 08:34:03.567: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jun 28 08:34:03.567: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements 06/28/23 08:34:03.567
STEP: Ensuring Pod has resource requirements applied from LimitRange 06/28/23 08:34:03.573
Jun 28 08:34:03.579: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jun 28 08:34:03.579: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements 06/28/23 08:34:03.579
STEP: Ensuring Pod has merged resource requirements applied from LimitRange 06/28/23 08:34:03.586
Jun 28 08:34:03.591: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Jun 28 08:34:03.591: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources 06/28/23 08:34:03.591
STEP: Failing to create a Pod with more than max resources 06/28/23 08:34:03.594
STEP: Updating a LimitRange 06/28/23 08:34:03.597
STEP: Verifying LimitRange updating is effective 06/28/23 08:34:03.604
STEP: Creating a Pod with less than former min resources 06/28/23 08:34:05.61
STEP: Failing to create a Pod with more than max resources 06/28/23 08:34:05.617
STEP: Deleting a LimitRange 06/28/23 08:34:05.621
STEP: Verifying the LimitRange was deleted 06/28/23 08:34:05.627
Jun 28 08:34:10.633: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources 06/28/23 08:34:10.633
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/framework.go:187
Jun 28 08:34:10.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-7385" for this suite. 06/28/23 08:34:10.658
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","completed":242,"skipped":4242,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.236 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:34:03.43
    Jun 28 08:34:03.430: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename limitrange 06/28/23 08:34:03.431
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:34:03.445
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:34:03.448
    [It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
      test/e2e/scheduling/limit_range.go:57
    STEP: Creating a LimitRange 06/28/23 08:34:03.453
    STEP: Setting up watch 06/28/23 08:34:03.453
    STEP: Submitting a LimitRange 06/28/23 08:34:03.557
    STEP: Verifying LimitRange creation was observed 06/28/23 08:34:03.563
    STEP: Fetching the LimitRange to ensure it has proper values 06/28/23 08:34:03.563
    Jun 28 08:34:03.567: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Jun 28 08:34:03.567: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with no resource requirements 06/28/23 08:34:03.567
    STEP: Ensuring Pod has resource requirements applied from LimitRange 06/28/23 08:34:03.573
    Jun 28 08:34:03.579: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Jun 28 08:34:03.579: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with partial resource requirements 06/28/23 08:34:03.579
    STEP: Ensuring Pod has merged resource requirements applied from LimitRange 06/28/23 08:34:03.586
    Jun 28 08:34:03.591: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
    Jun 28 08:34:03.591: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Failing to create a Pod with less than min resources 06/28/23 08:34:03.591
    STEP: Failing to create a Pod with more than max resources 06/28/23 08:34:03.594
    STEP: Updating a LimitRange 06/28/23 08:34:03.597
    STEP: Verifying LimitRange updating is effective 06/28/23 08:34:03.604
    STEP: Creating a Pod with less than former min resources 06/28/23 08:34:05.61
    STEP: Failing to create a Pod with more than max resources 06/28/23 08:34:05.617
    STEP: Deleting a LimitRange 06/28/23 08:34:05.621
    STEP: Verifying the LimitRange was deleted 06/28/23 08:34:05.627
    Jun 28 08:34:10.633: INFO: limitRange is already deleted
    STEP: Creating a Pod with more than former max resources 06/28/23 08:34:10.633
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/framework.go:187
    Jun 28 08:34:10.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "limitrange-7385" for this suite. 06/28/23 08:34:10.658
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:34:10.668
Jun 28 08:34:10.668: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename deployment 06/28/23 08:34:10.669
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:34:10.681
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:34:10.685
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
Jun 28 08:34:10.700: INFO: Pod name rollover-pod: Found 0 pods out of 1
Jun 28 08:34:15.706: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 06/28/23 08:34:15.706
Jun 28 08:34:15.706: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Jun 28 08:34:17.712: INFO: Creating deployment "test-rollover-deployment"
Jun 28 08:34:17.723: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Jun 28 08:34:19.733: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Jun 28 08:34:19.743: INFO: Ensure that both replica sets have 1 created replica
Jun 28 08:34:19.751: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Jun 28 08:34:19.761: INFO: Updating deployment test-rollover-deployment
Jun 28 08:34:19.761: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Jun 28 08:34:21.771: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Jun 28 08:34:21.781: INFO: Make sure deployment "test-rollover-deployment" is complete
Jun 28 08:34:21.789: INFO: all replica sets need to contain the pod-template-hash label
Jun 28 08:34:21.789: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.June, 28, 8, 34, 17, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 28, 8, 34, 17, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 28, 8, 34, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 28, 8, 34, 17, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 28 08:34:23.799: INFO: all replica sets need to contain the pod-template-hash label
Jun 28 08:34:23.799: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.June, 28, 8, 34, 17, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 28, 8, 34, 17, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 28, 8, 34, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 28, 8, 34, 17, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 28 08:34:25.799: INFO: all replica sets need to contain the pod-template-hash label
Jun 28 08:34:25.800: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.June, 28, 8, 34, 17, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 28, 8, 34, 17, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 28, 8, 34, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 28, 8, 34, 17, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 28 08:34:27.801: INFO: all replica sets need to contain the pod-template-hash label
Jun 28 08:34:27.801: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.June, 28, 8, 34, 17, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 28, 8, 34, 17, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 28, 8, 34, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 28, 8, 34, 17, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 28 08:34:29.800: INFO: all replica sets need to contain the pod-template-hash label
Jun 28 08:34:29.800: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.June, 28, 8, 34, 17, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 28, 8, 34, 17, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 28, 8, 34, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 28, 8, 34, 17, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 28 08:34:31.806: INFO: 
Jun 28 08:34:31.806: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jun 28 08:34:31.819: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-2759  3edd5e19-84c8-47e7-8584-36432cc1ede1 75612 2 2023-06-28 08:34:17 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-06-28 08:34:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-28 08:34:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00552f5a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-06-28 08:34:17 +0000 UTC,LastTransitionTime:2023-06-28 08:34:17 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6d45fd857b" has successfully progressed.,LastUpdateTime:2023-06-28 08:34:31 +0000 UTC,LastTransitionTime:2023-06-28 08:34:17 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jun 28 08:34:31.823: INFO: New ReplicaSet "test-rollover-deployment-6d45fd857b" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-6d45fd857b  deployment-2759  483e8dd2-83e8-42a9-90bf-aa9183950288 75602 2 2023-06-28 08:34:19 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 3edd5e19-84c8-47e7-8584-36432cc1ede1 0xc0043053d7 0xc0043053d8}] [] [{kube-controller-manager Update apps/v1 2023-06-28 08:34:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3edd5e19-84c8-47e7-8584-36432cc1ede1\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-28 08:34:31 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6d45fd857b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004305488 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jun 28 08:34:31.823: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Jun 28 08:34:31.824: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-2759  2c133b9d-fbb4-4f3e-8705-7de5b8546b9a 75611 2 2023-06-28 08:34:10 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 3edd5e19-84c8-47e7-8584-36432cc1ede1 0xc004305187 0xc004305188}] [] [{e2e.test Update apps/v1 2023-06-28 08:34:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-28 08:34:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3edd5e19-84c8-47e7-8584-36432cc1ede1\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-06-28 08:34:31 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004305248 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun 28 08:34:31.824: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-59b9df946d  deployment-2759  23e9e6de-9ddf-477f-acd4-d61c3d325f12 75539 2 2023-06-28 08:34:17 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 3edd5e19-84c8-47e7-8584-36432cc1ede1 0xc0043052b7 0xc0043052b8}] [] [{kube-controller-manager Update apps/v1 2023-06-28 08:34:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3edd5e19-84c8-47e7-8584-36432cc1ede1\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-28 08:34:19 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 59b9df946d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004305368 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun 28 08:34:31.828: INFO: Pod "test-rollover-deployment-6d45fd857b-42tfp" is available:
&Pod{ObjectMeta:{test-rollover-deployment-6d45fd857b-42tfp test-rollover-deployment-6d45fd857b- deployment-2759  8744a4d3-a661-41c7-8598-57a19b8b8f25 75558 0 2023-06-28 08:34:19 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[cni.projectcalico.org/containerID:f58f51e242a5faf1d22bb702a89c2e08fd305ef4e5ed54a41c87660f1bf92aab cni.projectcalico.org/podIP:172.21.122.10/32 cni.projectcalico.org/podIPs:172.21.122.10/32] [{apps/v1 ReplicaSet test-rollover-deployment-6d45fd857b 483e8dd2-83e8-42a9-90bf-aa9183950288 0xc004305a07 0xc004305a08}] [] [{kube-controller-manager Update v1 2023-06-28 08:34:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"483e8dd2-83e8-42a9-90bf-aa9183950288\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-06-28 08:34:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-06-28 08:34:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.122.10\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jrjk9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jrjk9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-rhel-749f7d55c8xdd8b6-ct4cp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:34:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:34:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:34:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:34:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.11.3,PodIP:172.21.122.10,StartTime:2023-06-28 08:34:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-28 08:34:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://22db0b967e6168963b9ba40a688ece028688612b6479471ad80bd1f54554a01c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.21.122.10,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jun 28 08:34:31.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2759" for this suite. 06/28/23 08:34:31.835
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","completed":243,"skipped":4267,"failed":0}
------------------------------
â€¢ [SLOW TEST] [21.173 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:34:10.668
    Jun 28 08:34:10.668: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename deployment 06/28/23 08:34:10.669
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:34:10.681
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:34:10.685
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support rollover [Conformance]
      test/e2e/apps/deployment.go:132
    Jun 28 08:34:10.700: INFO: Pod name rollover-pod: Found 0 pods out of 1
    Jun 28 08:34:15.706: INFO: Pod name rollover-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 06/28/23 08:34:15.706
    Jun 28 08:34:15.706: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
    Jun 28 08:34:17.712: INFO: Creating deployment "test-rollover-deployment"
    Jun 28 08:34:17.723: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
    Jun 28 08:34:19.733: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
    Jun 28 08:34:19.743: INFO: Ensure that both replica sets have 1 created replica
    Jun 28 08:34:19.751: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
    Jun 28 08:34:19.761: INFO: Updating deployment test-rollover-deployment
    Jun 28 08:34:19.761: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
    Jun 28 08:34:21.771: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
    Jun 28 08:34:21.781: INFO: Make sure deployment "test-rollover-deployment" is complete
    Jun 28 08:34:21.789: INFO: all replica sets need to contain the pod-template-hash label
    Jun 28 08:34:21.789: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.June, 28, 8, 34, 17, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 28, 8, 34, 17, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 28, 8, 34, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 28, 8, 34, 17, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun 28 08:34:23.799: INFO: all replica sets need to contain the pod-template-hash label
    Jun 28 08:34:23.799: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.June, 28, 8, 34, 17, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 28, 8, 34, 17, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 28, 8, 34, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 28, 8, 34, 17, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun 28 08:34:25.799: INFO: all replica sets need to contain the pod-template-hash label
    Jun 28 08:34:25.800: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.June, 28, 8, 34, 17, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 28, 8, 34, 17, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 28, 8, 34, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 28, 8, 34, 17, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun 28 08:34:27.801: INFO: all replica sets need to contain the pod-template-hash label
    Jun 28 08:34:27.801: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.June, 28, 8, 34, 17, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 28, 8, 34, 17, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 28, 8, 34, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 28, 8, 34, 17, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun 28 08:34:29.800: INFO: all replica sets need to contain the pod-template-hash label
    Jun 28 08:34:29.800: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.June, 28, 8, 34, 17, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 28, 8, 34, 17, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 28, 8, 34, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 28, 8, 34, 17, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun 28 08:34:31.806: INFO: 
    Jun 28 08:34:31.806: INFO: Ensure that both old replica sets have no replicas
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jun 28 08:34:31.819: INFO: Deployment "test-rollover-deployment":
    &Deployment{ObjectMeta:{test-rollover-deployment  deployment-2759  3edd5e19-84c8-47e7-8584-36432cc1ede1 75612 2 2023-06-28 08:34:17 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-06-28 08:34:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-28 08:34:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00552f5a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-06-28 08:34:17 +0000 UTC,LastTransitionTime:2023-06-28 08:34:17 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6d45fd857b" has successfully progressed.,LastUpdateTime:2023-06-28 08:34:31 +0000 UTC,LastTransitionTime:2023-06-28 08:34:17 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jun 28 08:34:31.823: INFO: New ReplicaSet "test-rollover-deployment-6d45fd857b" of Deployment "test-rollover-deployment":
    &ReplicaSet{ObjectMeta:{test-rollover-deployment-6d45fd857b  deployment-2759  483e8dd2-83e8-42a9-90bf-aa9183950288 75602 2 2023-06-28 08:34:19 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 3edd5e19-84c8-47e7-8584-36432cc1ede1 0xc0043053d7 0xc0043053d8}] [] [{kube-controller-manager Update apps/v1 2023-06-28 08:34:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3edd5e19-84c8-47e7-8584-36432cc1ede1\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-28 08:34:31 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6d45fd857b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004305488 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jun 28 08:34:31.823: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
    Jun 28 08:34:31.824: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-2759  2c133b9d-fbb4-4f3e-8705-7de5b8546b9a 75611 2 2023-06-28 08:34:10 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 3edd5e19-84c8-47e7-8584-36432cc1ede1 0xc004305187 0xc004305188}] [] [{e2e.test Update apps/v1 2023-06-28 08:34:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-28 08:34:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3edd5e19-84c8-47e7-8584-36432cc1ede1\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-06-28 08:34:31 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004305248 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jun 28 08:34:31.824: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-59b9df946d  deployment-2759  23e9e6de-9ddf-477f-acd4-d61c3d325f12 75539 2 2023-06-28 08:34:17 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 3edd5e19-84c8-47e7-8584-36432cc1ede1 0xc0043052b7 0xc0043052b8}] [] [{kube-controller-manager Update apps/v1 2023-06-28 08:34:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3edd5e19-84c8-47e7-8584-36432cc1ede1\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-28 08:34:19 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 59b9df946d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004305368 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jun 28 08:34:31.828: INFO: Pod "test-rollover-deployment-6d45fd857b-42tfp" is available:
    &Pod{ObjectMeta:{test-rollover-deployment-6d45fd857b-42tfp test-rollover-deployment-6d45fd857b- deployment-2759  8744a4d3-a661-41c7-8598-57a19b8b8f25 75558 0 2023-06-28 08:34:19 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[cni.projectcalico.org/containerID:f58f51e242a5faf1d22bb702a89c2e08fd305ef4e5ed54a41c87660f1bf92aab cni.projectcalico.org/podIP:172.21.122.10/32 cni.projectcalico.org/podIPs:172.21.122.10/32] [{apps/v1 ReplicaSet test-rollover-deployment-6d45fd857b 483e8dd2-83e8-42a9-90bf-aa9183950288 0xc004305a07 0xc004305a08}] [] [{kube-controller-manager Update v1 2023-06-28 08:34:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"483e8dd2-83e8-42a9-90bf-aa9183950288\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-06-28 08:34:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-06-28 08:34:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.122.10\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jrjk9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jrjk9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-rhel-749f7d55c8xdd8b6-ct4cp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:34:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:34:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:34:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:34:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.11.3,PodIP:172.21.122.10,StartTime:2023-06-28 08:34:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-28 08:34:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://22db0b967e6168963b9ba40a688ece028688612b6479471ad80bd1f54554a01c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.21.122.10,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jun 28 08:34:31.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-2759" for this suite. 06/28/23 08:34:31.835
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:34:31.842
Jun 28 08:34:31.842: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename watch 06/28/23 08:34:31.843
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:34:31.854
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:34:31.858
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
STEP: creating a watch on configmaps with label A 06/28/23 08:34:31.864
STEP: creating a watch on configmaps with label B 06/28/23 08:34:31.866
STEP: creating a watch on configmaps with label A or B 06/28/23 08:34:31.868
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 06/28/23 08:34:31.87
Jun 28 08:34:31.877: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7320  4ae5ce0c-cf21-43e1-83df-aa011a92e994 75624 0 2023-06-28 08:34:31 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-28 08:34:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 28 08:34:31.877: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7320  4ae5ce0c-cf21-43e1-83df-aa011a92e994 75624 0 2023-06-28 08:34:31 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-28 08:34:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification 06/28/23 08:34:31.877
Jun 28 08:34:31.888: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7320  4ae5ce0c-cf21-43e1-83df-aa011a92e994 75625 0 2023-06-28 08:34:31 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-28 08:34:31 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 28 08:34:31.888: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7320  4ae5ce0c-cf21-43e1-83df-aa011a92e994 75625 0 2023-06-28 08:34:31 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-28 08:34:31 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification 06/28/23 08:34:31.888
Jun 28 08:34:31.898: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7320  4ae5ce0c-cf21-43e1-83df-aa011a92e994 75626 0 2023-06-28 08:34:31 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-28 08:34:31 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 28 08:34:31.898: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7320  4ae5ce0c-cf21-43e1-83df-aa011a92e994 75626 0 2023-06-28 08:34:31 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-28 08:34:31 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification 06/28/23 08:34:31.898
Jun 28 08:34:31.905: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7320  4ae5ce0c-cf21-43e1-83df-aa011a92e994 75627 0 2023-06-28 08:34:31 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-28 08:34:31 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 28 08:34:31.905: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7320  4ae5ce0c-cf21-43e1-83df-aa011a92e994 75627 0 2023-06-28 08:34:31 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-28 08:34:31 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 06/28/23 08:34:31.905
Jun 28 08:34:31.911: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7320  d1393ee7-e73b-4505-b270-0bba54fbf112 75628 0 2023-06-28 08:34:31 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-06-28 08:34:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 28 08:34:31.911: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7320  d1393ee7-e73b-4505-b270-0bba54fbf112 75628 0 2023-06-28 08:34:31 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-06-28 08:34:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification 06/28/23 08:34:41.911
Jun 28 08:34:41.920: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7320  d1393ee7-e73b-4505-b270-0bba54fbf112 75702 0 2023-06-28 08:34:31 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-06-28 08:34:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 28 08:34:41.920: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7320  d1393ee7-e73b-4505-b270-0bba54fbf112 75702 0 2023-06-28 08:34:31 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-06-28 08:34:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Jun 28 08:34:51.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7320" for this suite. 06/28/23 08:34:51.935
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","completed":244,"skipped":4281,"failed":0}
------------------------------
â€¢ [SLOW TEST] [20.104 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:34:31.842
    Jun 28 08:34:31.842: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename watch 06/28/23 08:34:31.843
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:34:31.854
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:34:31.858
    [It] should observe add, update, and delete watch notifications on configmaps [Conformance]
      test/e2e/apimachinery/watch.go:60
    STEP: creating a watch on configmaps with label A 06/28/23 08:34:31.864
    STEP: creating a watch on configmaps with label B 06/28/23 08:34:31.866
    STEP: creating a watch on configmaps with label A or B 06/28/23 08:34:31.868
    STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 06/28/23 08:34:31.87
    Jun 28 08:34:31.877: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7320  4ae5ce0c-cf21-43e1-83df-aa011a92e994 75624 0 2023-06-28 08:34:31 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-28 08:34:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jun 28 08:34:31.877: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7320  4ae5ce0c-cf21-43e1-83df-aa011a92e994 75624 0 2023-06-28 08:34:31 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-28 08:34:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A and ensuring the correct watchers observe the notification 06/28/23 08:34:31.877
    Jun 28 08:34:31.888: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7320  4ae5ce0c-cf21-43e1-83df-aa011a92e994 75625 0 2023-06-28 08:34:31 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-28 08:34:31 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jun 28 08:34:31.888: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7320  4ae5ce0c-cf21-43e1-83df-aa011a92e994 75625 0 2023-06-28 08:34:31 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-28 08:34:31 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A again and ensuring the correct watchers observe the notification 06/28/23 08:34:31.888
    Jun 28 08:34:31.898: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7320  4ae5ce0c-cf21-43e1-83df-aa011a92e994 75626 0 2023-06-28 08:34:31 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-28 08:34:31 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jun 28 08:34:31.898: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7320  4ae5ce0c-cf21-43e1-83df-aa011a92e994 75626 0 2023-06-28 08:34:31 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-28 08:34:31 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap A and ensuring the correct watchers observe the notification 06/28/23 08:34:31.898
    Jun 28 08:34:31.905: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7320  4ae5ce0c-cf21-43e1-83df-aa011a92e994 75627 0 2023-06-28 08:34:31 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-28 08:34:31 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jun 28 08:34:31.905: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7320  4ae5ce0c-cf21-43e1-83df-aa011a92e994 75627 0 2023-06-28 08:34:31 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-28 08:34:31 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 06/28/23 08:34:31.905
    Jun 28 08:34:31.911: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7320  d1393ee7-e73b-4505-b270-0bba54fbf112 75628 0 2023-06-28 08:34:31 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-06-28 08:34:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jun 28 08:34:31.911: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7320  d1393ee7-e73b-4505-b270-0bba54fbf112 75628 0 2023-06-28 08:34:31 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-06-28 08:34:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap B and ensuring the correct watchers observe the notification 06/28/23 08:34:41.911
    Jun 28 08:34:41.920: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7320  d1393ee7-e73b-4505-b270-0bba54fbf112 75702 0 2023-06-28 08:34:31 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-06-28 08:34:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jun 28 08:34:41.920: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7320  d1393ee7-e73b-4505-b270-0bba54fbf112 75702 0 2023-06-28 08:34:31 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-06-28 08:34:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Jun 28 08:34:51.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-7320" for this suite. 06/28/23 08:34:51.935
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:34:51.947
Jun 28 08:34:51.947: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename crd-webhook 06/28/23 08:34:51.948
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:34:51.962
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:34:51.967
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 06/28/23 08:34:51.975
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 06/28/23 08:34:52.393
STEP: Deploying the custom resource conversion webhook pod 06/28/23 08:34:52.405
STEP: Wait for the deployment to be ready 06/28/23 08:34:52.427
Jun 28 08:34:52.437: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 06/28/23 08:34:54.454
STEP: Verifying the service has paired with the endpoint 06/28/23 08:34:54.467
Jun 28 08:34:55.467: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
Jun 28 08:34:55.474: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Creating a v1 custom resource 06/28/23 08:34:58.308
STEP: v2 custom resource should be converted 06/28/23 08:34:58.316
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jun 28 08:34:58.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-2641" for this suite. 06/28/23 08:34:58.852
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","completed":245,"skipped":4290,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.956 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:34:51.947
    Jun 28 08:34:51.947: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename crd-webhook 06/28/23 08:34:51.948
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:34:51.962
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:34:51.967
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 06/28/23 08:34:51.975
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 06/28/23 08:34:52.393
    STEP: Deploying the custom resource conversion webhook pod 06/28/23 08:34:52.405
    STEP: Wait for the deployment to be ready 06/28/23 08:34:52.427
    Jun 28 08:34:52.437: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 06/28/23 08:34:54.454
    STEP: Verifying the service has paired with the endpoint 06/28/23 08:34:54.467
    Jun 28 08:34:55.467: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert from CR v1 to CR v2 [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:149
    Jun 28 08:34:55.474: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Creating a v1 custom resource 06/28/23 08:34:58.308
    STEP: v2 custom resource should be converted 06/28/23 08:34:58.316
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jun 28 08:34:58.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-webhook-2641" for this suite. 06/28/23 08:34:58.852
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:34:58.904
Jun 28 08:34:58.904: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename secrets 06/28/23 08:34:58.905
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:34:58.926
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:34:58.933
[It] should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139
STEP: Creating projection with secret that has name secret-emptykey-test-118cc616-58c6-4d80-a769-307fa7dd9b75 06/28/23 08:34:58.939
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Jun 28 08:34:58.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1531" for this suite. 06/28/23 08:34:58.952
{"msg":"PASSED [sig-node] Secrets should fail to create secret due to empty secret key [Conformance]","completed":246,"skipped":4327,"failed":0}
------------------------------
â€¢ [0.057 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:34:58.904
    Jun 28 08:34:58.904: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename secrets 06/28/23 08:34:58.905
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:34:58.926
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:34:58.933
    [It] should fail to create secret due to empty secret key [Conformance]
      test/e2e/common/node/secrets.go:139
    STEP: Creating projection with secret that has name secret-emptykey-test-118cc616-58c6-4d80-a769-307fa7dd9b75 06/28/23 08:34:58.939
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Jun 28 08:34:58.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-1531" for this suite. 06/28/23 08:34:58.952
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:34:58.962
Jun 28 08:34:58.963: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename replicaset 06/28/23 08:34:58.963
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:34:58.983
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:34:58.99
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
STEP: Given a Pod with a 'name' label pod-adoption-release is created 06/28/23 08:34:59.005
Jun 28 08:34:59.019: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-7165" to be "running and ready"
Jun 28 08:34:59.029: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 10.595416ms
Jun 28 08:34:59.029: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Jun 28 08:35:01.036: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.017556023s
Jun 28 08:35:01.036: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
Jun 28 08:35:01.036: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
STEP: When a replicaset with a matching selector is created 06/28/23 08:35:01.041
STEP: Then the orphan pod is adopted 06/28/23 08:35:01.049
STEP: When the matched label of one of its pods change 06/28/23 08:35:02.061
Jun 28 08:35:02.066: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released 06/28/23 08:35:02.084
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Jun 28 08:35:03.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-7165" for this suite. 06/28/23 08:35:03.105
{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","completed":247,"skipped":4374,"failed":0}
------------------------------
â€¢ [4.150 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:34:58.962
    Jun 28 08:34:58.963: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename replicaset 06/28/23 08:34:58.963
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:34:58.983
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:34:58.99
    [It] should adopt matching pods on creation and release no longer matching pods [Conformance]
      test/e2e/apps/replica_set.go:131
    STEP: Given a Pod with a 'name' label pod-adoption-release is created 06/28/23 08:34:59.005
    Jun 28 08:34:59.019: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-7165" to be "running and ready"
    Jun 28 08:34:59.029: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 10.595416ms
    Jun 28 08:34:59.029: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Jun 28 08:35:01.036: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.017556023s
    Jun 28 08:35:01.036: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
    Jun 28 08:35:01.036: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
    STEP: When a replicaset with a matching selector is created 06/28/23 08:35:01.041
    STEP: Then the orphan pod is adopted 06/28/23 08:35:01.049
    STEP: When the matched label of one of its pods change 06/28/23 08:35:02.061
    Jun 28 08:35:02.066: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
    STEP: Then the pod is released 06/28/23 08:35:02.084
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Jun 28 08:35:03.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-7165" for this suite. 06/28/23 08:35:03.105
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:35:03.114
Jun 28 08:35:03.114: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename cronjob 06/28/23 08:35:03.115
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:35:03.129
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:35:03.133
[It] should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
STEP: Creating a ReplaceConcurrent cronjob 06/28/23 08:35:03.138
STEP: Ensuring a job is scheduled 06/28/23 08:35:03.145
STEP: Ensuring exactly one is scheduled 06/28/23 08:36:01.151
STEP: Ensuring exactly one running job exists by listing jobs explicitly 06/28/23 08:36:01.158
STEP: Ensuring the job is replaced with a new one 06/28/23 08:36:01.163
STEP: Removing cronjob 06/28/23 08:37:01.169
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Jun 28 08:37:01.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-4592" for this suite. 06/28/23 08:37:01.185
{"msg":"PASSED [sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]","completed":248,"skipped":4431,"failed":0}
------------------------------
â€¢ [SLOW TEST] [118.078 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:35:03.114
    Jun 28 08:35:03.114: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename cronjob 06/28/23 08:35:03.115
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:35:03.129
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:35:03.133
    [It] should replace jobs when ReplaceConcurrent [Conformance]
      test/e2e/apps/cronjob.go:160
    STEP: Creating a ReplaceConcurrent cronjob 06/28/23 08:35:03.138
    STEP: Ensuring a job is scheduled 06/28/23 08:35:03.145
    STEP: Ensuring exactly one is scheduled 06/28/23 08:36:01.151
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 06/28/23 08:36:01.158
    STEP: Ensuring the job is replaced with a new one 06/28/23 08:36:01.163
    STEP: Removing cronjob 06/28/23 08:37:01.169
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Jun 28 08:37:01.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-4592" for this suite. 06/28/23 08:37:01.185
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:37:01.193
Jun 28 08:37:01.193: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename daemonsets 06/28/23 08:37:01.194
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:37:01.21
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:37:01.215
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293
STEP: Creating a simple DaemonSet "daemon-set" 06/28/23 08:37:01.258
STEP: Check that daemon pods launch on every node of the cluster. 06/28/23 08:37:01.267
Jun 28 08:37:01.282: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 28 08:37:01.282: INFO: Node ske-rhel-749f7d55c8xdd8b6-ct4cp is running 0 daemon pod, expected 1
Jun 28 08:37:02.301: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 28 08:37:02.301: INFO: Node ske-rhel-749f7d55c8xdd8b6-ct4cp is running 0 daemon pod, expected 1
Jun 28 08:37:03.296: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jun 28 08:37:03.296: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 06/28/23 08:37:03.302
Jun 28 08:37:03.329: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jun 28 08:37:03.329: INFO: Node ske-rhel-749f7d55c8xdd8b6-ct4cp is running 0 daemon pod, expected 1
Jun 28 08:37:04.345: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jun 28 08:37:04.345: INFO: Node ske-rhel-749f7d55c8xdd8b6-ct4cp is running 0 daemon pod, expected 1
Jun 28 08:37:05.344: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jun 28 08:37:05.344: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted. 06/28/23 08:37:05.344
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 06/28/23 08:37:05.353
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8050, will wait for the garbage collector to delete the pods 06/28/23 08:37:05.353
Jun 28 08:37:05.420: INFO: Deleting DaemonSet.extensions daemon-set took: 10.606808ms
Jun 28 08:37:05.520: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.652124ms
Jun 28 08:37:08.025: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 28 08:37:08.025: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jun 28 08:37:08.030: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"76517"},"items":null}

Jun 28 08:37:08.033: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"76517"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Jun 28 08:37:08.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8050" for this suite. 06/28/23 08:37:08.065
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","completed":249,"skipped":4445,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.879 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:37:01.193
    Jun 28 08:37:01.193: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename daemonsets 06/28/23 08:37:01.194
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:37:01.21
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:37:01.215
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should retry creating failed daemon pods [Conformance]
      test/e2e/apps/daemon_set.go:293
    STEP: Creating a simple DaemonSet "daemon-set" 06/28/23 08:37:01.258
    STEP: Check that daemon pods launch on every node of the cluster. 06/28/23 08:37:01.267
    Jun 28 08:37:01.282: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun 28 08:37:01.282: INFO: Node ske-rhel-749f7d55c8xdd8b6-ct4cp is running 0 daemon pod, expected 1
    Jun 28 08:37:02.301: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun 28 08:37:02.301: INFO: Node ske-rhel-749f7d55c8xdd8b6-ct4cp is running 0 daemon pod, expected 1
    Jun 28 08:37:03.296: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jun 28 08:37:03.296: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 06/28/23 08:37:03.302
    Jun 28 08:37:03.329: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jun 28 08:37:03.329: INFO: Node ske-rhel-749f7d55c8xdd8b6-ct4cp is running 0 daemon pod, expected 1
    Jun 28 08:37:04.345: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jun 28 08:37:04.345: INFO: Node ske-rhel-749f7d55c8xdd8b6-ct4cp is running 0 daemon pod, expected 1
    Jun 28 08:37:05.344: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jun 28 08:37:05.344: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Wait for the failed daemon pod to be completely deleted. 06/28/23 08:37:05.344
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 06/28/23 08:37:05.353
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8050, will wait for the garbage collector to delete the pods 06/28/23 08:37:05.353
    Jun 28 08:37:05.420: INFO: Deleting DaemonSet.extensions daemon-set took: 10.606808ms
    Jun 28 08:37:05.520: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.652124ms
    Jun 28 08:37:08.025: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun 28 08:37:08.025: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jun 28 08:37:08.030: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"76517"},"items":null}

    Jun 28 08:37:08.033: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"76517"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Jun 28 08:37:08.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-8050" for this suite. 06/28/23 08:37:08.065
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:37:08.074
Jun 28 08:37:08.074: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename projected 06/28/23 08:37:08.074
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:37:08.088
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:37:08.092
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45
STEP: Creating projection with secret that has name projected-secret-test-d446f1a5-f0d4-4b0b-a22a-57d439c22a94 06/28/23 08:37:08.097
STEP: Creating a pod to test consume secrets 06/28/23 08:37:08.103
Jun 28 08:37:08.113: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-5a466e66-72a1-477f-9fa4-27eed2bab6c4" in namespace "projected-3503" to be "Succeeded or Failed"
Jun 28 08:37:08.119: INFO: Pod "pod-projected-secrets-5a466e66-72a1-477f-9fa4-27eed2bab6c4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.393168ms
Jun 28 08:37:10.124: INFO: Pod "pod-projected-secrets-5a466e66-72a1-477f-9fa4-27eed2bab6c4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010931923s
Jun 28 08:37:12.125: INFO: Pod "pod-projected-secrets-5a466e66-72a1-477f-9fa4-27eed2bab6c4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011433895s
STEP: Saw pod success 06/28/23 08:37:12.125
Jun 28 08:37:12.125: INFO: Pod "pod-projected-secrets-5a466e66-72a1-477f-9fa4-27eed2bab6c4" satisfied condition "Succeeded or Failed"
Jun 28 08:37:12.129: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-zxlfv pod pod-projected-secrets-5a466e66-72a1-477f-9fa4-27eed2bab6c4 container projected-secret-volume-test: <nil>
STEP: delete the pod 06/28/23 08:37:12.202
Jun 28 08:37:12.215: INFO: Waiting for pod pod-projected-secrets-5a466e66-72a1-477f-9fa4-27eed2bab6c4 to disappear
Jun 28 08:37:12.220: INFO: Pod pod-projected-secrets-5a466e66-72a1-477f-9fa4-27eed2bab6c4 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Jun 28 08:37:12.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3503" for this suite. 06/28/23 08:37:12.228
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","completed":250,"skipped":4481,"failed":0}
------------------------------
â€¢ [4.160 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:37:08.074
    Jun 28 08:37:08.074: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename projected 06/28/23 08:37:08.074
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:37:08.088
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:37:08.092
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:45
    STEP: Creating projection with secret that has name projected-secret-test-d446f1a5-f0d4-4b0b-a22a-57d439c22a94 06/28/23 08:37:08.097
    STEP: Creating a pod to test consume secrets 06/28/23 08:37:08.103
    Jun 28 08:37:08.113: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-5a466e66-72a1-477f-9fa4-27eed2bab6c4" in namespace "projected-3503" to be "Succeeded or Failed"
    Jun 28 08:37:08.119: INFO: Pod "pod-projected-secrets-5a466e66-72a1-477f-9fa4-27eed2bab6c4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.393168ms
    Jun 28 08:37:10.124: INFO: Pod "pod-projected-secrets-5a466e66-72a1-477f-9fa4-27eed2bab6c4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010931923s
    Jun 28 08:37:12.125: INFO: Pod "pod-projected-secrets-5a466e66-72a1-477f-9fa4-27eed2bab6c4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011433895s
    STEP: Saw pod success 06/28/23 08:37:12.125
    Jun 28 08:37:12.125: INFO: Pod "pod-projected-secrets-5a466e66-72a1-477f-9fa4-27eed2bab6c4" satisfied condition "Succeeded or Failed"
    Jun 28 08:37:12.129: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-zxlfv pod pod-projected-secrets-5a466e66-72a1-477f-9fa4-27eed2bab6c4 container projected-secret-volume-test: <nil>
    STEP: delete the pod 06/28/23 08:37:12.202
    Jun 28 08:37:12.215: INFO: Waiting for pod pod-projected-secrets-5a466e66-72a1-477f-9fa4-27eed2bab6c4 to disappear
    Jun 28 08:37:12.220: INFO: Pod pod-projected-secrets-5a466e66-72a1-477f-9fa4-27eed2bab6c4 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Jun 28 08:37:12.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3503" for this suite. 06/28/23 08:37:12.228
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:37:12.235
Jun 28 08:37:12.235: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename endpointslice 06/28/23 08:37:12.235
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:37:12.247
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:37:12.251
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Jun 28 08:37:12.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-6702" for this suite. 06/28/23 08:37:12.312
{"msg":"PASSED [sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]","completed":251,"skipped":4496,"failed":0}
------------------------------
â€¢ [0.086 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:37:12.235
    Jun 28 08:37:12.235: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename endpointslice 06/28/23 08:37:12.235
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:37:12.247
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:37:12.251
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
      test/e2e/network/endpointslice.go:101
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Jun 28 08:37:12.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-6702" for this suite. 06/28/23 08:37:12.312
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:37:12.322
Jun 28 08:37:12.322: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename projected 06/28/23 08:37:12.323
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:37:12.337
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:37:12.341
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234
STEP: Creating a pod to test downward API volume plugin 06/28/23 08:37:12.346
Jun 28 08:37:12.356: INFO: Waiting up to 5m0s for pod "downwardapi-volume-db30e1a1-4a13-4290-ac65-1852a55255cd" in namespace "projected-9328" to be "Succeeded or Failed"
Jun 28 08:37:12.360: INFO: Pod "downwardapi-volume-db30e1a1-4a13-4290-ac65-1852a55255cd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.141213ms
Jun 28 08:37:14.367: INFO: Pod "downwardapi-volume-db30e1a1-4a13-4290-ac65-1852a55255cd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011154437s
Jun 28 08:37:16.366: INFO: Pod "downwardapi-volume-db30e1a1-4a13-4290-ac65-1852a55255cd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010733354s
STEP: Saw pod success 06/28/23 08:37:16.366
Jun 28 08:37:16.367: INFO: Pod "downwardapi-volume-db30e1a1-4a13-4290-ac65-1852a55255cd" satisfied condition "Succeeded or Failed"
Jun 28 08:37:16.372: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-zxlfv pod downwardapi-volume-db30e1a1-4a13-4290-ac65-1852a55255cd container client-container: <nil>
STEP: delete the pod 06/28/23 08:37:16.383
Jun 28 08:37:16.394: INFO: Waiting for pod downwardapi-volume-db30e1a1-4a13-4290-ac65-1852a55255cd to disappear
Jun 28 08:37:16.400: INFO: Pod downwardapi-volume-db30e1a1-4a13-4290-ac65-1852a55255cd no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jun 28 08:37:16.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9328" for this suite. 06/28/23 08:37:16.409
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","completed":252,"skipped":4518,"failed":0}
------------------------------
â€¢ [4.094 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:37:12.322
    Jun 28 08:37:12.322: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename projected 06/28/23 08:37:12.323
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:37:12.337
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:37:12.341
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:234
    STEP: Creating a pod to test downward API volume plugin 06/28/23 08:37:12.346
    Jun 28 08:37:12.356: INFO: Waiting up to 5m0s for pod "downwardapi-volume-db30e1a1-4a13-4290-ac65-1852a55255cd" in namespace "projected-9328" to be "Succeeded or Failed"
    Jun 28 08:37:12.360: INFO: Pod "downwardapi-volume-db30e1a1-4a13-4290-ac65-1852a55255cd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.141213ms
    Jun 28 08:37:14.367: INFO: Pod "downwardapi-volume-db30e1a1-4a13-4290-ac65-1852a55255cd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011154437s
    Jun 28 08:37:16.366: INFO: Pod "downwardapi-volume-db30e1a1-4a13-4290-ac65-1852a55255cd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010733354s
    STEP: Saw pod success 06/28/23 08:37:16.366
    Jun 28 08:37:16.367: INFO: Pod "downwardapi-volume-db30e1a1-4a13-4290-ac65-1852a55255cd" satisfied condition "Succeeded or Failed"
    Jun 28 08:37:16.372: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-zxlfv pod downwardapi-volume-db30e1a1-4a13-4290-ac65-1852a55255cd container client-container: <nil>
    STEP: delete the pod 06/28/23 08:37:16.383
    Jun 28 08:37:16.394: INFO: Waiting for pod downwardapi-volume-db30e1a1-4a13-4290-ac65-1852a55255cd to disappear
    Jun 28 08:37:16.400: INFO: Pod downwardapi-volume-db30e1a1-4a13-4290-ac65-1852a55255cd no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jun 28 08:37:16.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-9328" for this suite. 06/28/23 08:37:16.409
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] Garbage collector
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:37:16.416
Jun 28 08:37:16.416: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename gc 06/28/23 08:37:16.417
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:37:16.434
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:37:16.44
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
STEP: create the rc1 06/28/23 08:37:16.455
STEP: create the rc2 06/28/23 08:37:16.463
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 06/28/23 08:37:21.475
STEP: delete the rc simpletest-rc-to-be-deleted 06/28/23 08:37:22.068
STEP: wait for the rc to be deleted 06/28/23 08:37:22.076
Jun 28 08:37:27.096: INFO: 70 pods remaining
Jun 28 08:37:27.096: INFO: 70 pods has nil DeletionTimestamp
Jun 28 08:37:27.096: INFO: 
STEP: Gathering metrics 06/28/23 08:37:32.095
W0628 08:37:32.108418      18 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jun 28 08:37:32.108: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Jun 28 08:37:32.108: INFO: Deleting pod "simpletest-rc-to-be-deleted-2lmc8" in namespace "gc-8167"
Jun 28 08:37:32.118: INFO: Deleting pod "simpletest-rc-to-be-deleted-46ssn" in namespace "gc-8167"
Jun 28 08:37:32.133: INFO: Deleting pod "simpletest-rc-to-be-deleted-4mtfv" in namespace "gc-8167"
Jun 28 08:37:32.142: INFO: Deleting pod "simpletest-rc-to-be-deleted-4xhbp" in namespace "gc-8167"
Jun 28 08:37:32.154: INFO: Deleting pod "simpletest-rc-to-be-deleted-5hkcv" in namespace "gc-8167"
Jun 28 08:37:32.166: INFO: Deleting pod "simpletest-rc-to-be-deleted-5mw6d" in namespace "gc-8167"
Jun 28 08:37:32.182: INFO: Deleting pod "simpletest-rc-to-be-deleted-5r9cc" in namespace "gc-8167"
Jun 28 08:37:32.200: INFO: Deleting pod "simpletest-rc-to-be-deleted-5trbs" in namespace "gc-8167"
Jun 28 08:37:32.217: INFO: Deleting pod "simpletest-rc-to-be-deleted-5z64v" in namespace "gc-8167"
Jun 28 08:37:32.236: INFO: Deleting pod "simpletest-rc-to-be-deleted-6r9sg" in namespace "gc-8167"
Jun 28 08:37:32.249: INFO: Deleting pod "simpletest-rc-to-be-deleted-6slgf" in namespace "gc-8167"
Jun 28 08:37:32.263: INFO: Deleting pod "simpletest-rc-to-be-deleted-79cr2" in namespace "gc-8167"
Jun 28 08:37:32.274: INFO: Deleting pod "simpletest-rc-to-be-deleted-7npwl" in namespace "gc-8167"
Jun 28 08:37:32.287: INFO: Deleting pod "simpletest-rc-to-be-deleted-7srmd" in namespace "gc-8167"
Jun 28 08:37:32.302: INFO: Deleting pod "simpletest-rc-to-be-deleted-7v92s" in namespace "gc-8167"
Jun 28 08:37:32.317: INFO: Deleting pod "simpletest-rc-to-be-deleted-86kht" in namespace "gc-8167"
Jun 28 08:37:32.344: INFO: Deleting pod "simpletest-rc-to-be-deleted-8f64m" in namespace "gc-8167"
Jun 28 08:37:32.362: INFO: Deleting pod "simpletest-rc-to-be-deleted-8h2ld" in namespace "gc-8167"
Jun 28 08:37:32.377: INFO: Deleting pod "simpletest-rc-to-be-deleted-8kdg2" in namespace "gc-8167"
Jun 28 08:37:32.390: INFO: Deleting pod "simpletest-rc-to-be-deleted-94255" in namespace "gc-8167"
Jun 28 08:37:32.407: INFO: Deleting pod "simpletest-rc-to-be-deleted-97f2z" in namespace "gc-8167"
Jun 28 08:37:32.417: INFO: Deleting pod "simpletest-rc-to-be-deleted-99cz2" in namespace "gc-8167"
Jun 28 08:37:32.428: INFO: Deleting pod "simpletest-rc-to-be-deleted-9qtbc" in namespace "gc-8167"
Jun 28 08:37:32.443: INFO: Deleting pod "simpletest-rc-to-be-deleted-9x9rg" in namespace "gc-8167"
Jun 28 08:37:32.456: INFO: Deleting pod "simpletest-rc-to-be-deleted-b5brw" in namespace "gc-8167"
Jun 28 08:37:32.472: INFO: Deleting pod "simpletest-rc-to-be-deleted-br6pc" in namespace "gc-8167"
Jun 28 08:37:32.481: INFO: Deleting pod "simpletest-rc-to-be-deleted-bvnft" in namespace "gc-8167"
Jun 28 08:37:32.491: INFO: Deleting pod "simpletest-rc-to-be-deleted-c2wp9" in namespace "gc-8167"
Jun 28 08:37:32.507: INFO: Deleting pod "simpletest-rc-to-be-deleted-cg8gt" in namespace "gc-8167"
Jun 28 08:37:32.523: INFO: Deleting pod "simpletest-rc-to-be-deleted-clf2v" in namespace "gc-8167"
Jun 28 08:37:32.535: INFO: Deleting pod "simpletest-rc-to-be-deleted-clwbd" in namespace "gc-8167"
Jun 28 08:37:32.551: INFO: Deleting pod "simpletest-rc-to-be-deleted-cmz6p" in namespace "gc-8167"
Jun 28 08:37:32.564: INFO: Deleting pod "simpletest-rc-to-be-deleted-cnprz" in namespace "gc-8167"
Jun 28 08:37:32.577: INFO: Deleting pod "simpletest-rc-to-be-deleted-d5lgh" in namespace "gc-8167"
Jun 28 08:37:32.592: INFO: Deleting pod "simpletest-rc-to-be-deleted-djf54" in namespace "gc-8167"
Jun 28 08:37:32.606: INFO: Deleting pod "simpletest-rc-to-be-deleted-dl5xp" in namespace "gc-8167"
Jun 28 08:37:32.622: INFO: Deleting pod "simpletest-rc-to-be-deleted-fblp9" in namespace "gc-8167"
Jun 28 08:37:32.633: INFO: Deleting pod "simpletest-rc-to-be-deleted-fc7db" in namespace "gc-8167"
Jun 28 08:37:32.646: INFO: Deleting pod "simpletest-rc-to-be-deleted-fg9gn" in namespace "gc-8167"
Jun 28 08:37:32.661: INFO: Deleting pod "simpletest-rc-to-be-deleted-fm4df" in namespace "gc-8167"
Jun 28 08:37:32.675: INFO: Deleting pod "simpletest-rc-to-be-deleted-fm8l5" in namespace "gc-8167"
Jun 28 08:37:32.687: INFO: Deleting pod "simpletest-rc-to-be-deleted-frw4r" in namespace "gc-8167"
Jun 28 08:37:32.702: INFO: Deleting pod "simpletest-rc-to-be-deleted-gjjrq" in namespace "gc-8167"
Jun 28 08:37:32.719: INFO: Deleting pod "simpletest-rc-to-be-deleted-gwvkz" in namespace "gc-8167"
Jun 28 08:37:32.737: INFO: Deleting pod "simpletest-rc-to-be-deleted-h4l77" in namespace "gc-8167"
Jun 28 08:37:32.748: INFO: Deleting pod "simpletest-rc-to-be-deleted-h866j" in namespace "gc-8167"
Jun 28 08:37:32.761: INFO: Deleting pod "simpletest-rc-to-be-deleted-hn2fn" in namespace "gc-8167"
Jun 28 08:37:32.773: INFO: Deleting pod "simpletest-rc-to-be-deleted-hpgzr" in namespace "gc-8167"
Jun 28 08:37:32.785: INFO: Deleting pod "simpletest-rc-to-be-deleted-jw7kw" in namespace "gc-8167"
Jun 28 08:37:32.804: INFO: Deleting pod "simpletest-rc-to-be-deleted-jz7gz" in namespace "gc-8167"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Jun 28 08:37:32.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8167" for this suite. 06/28/23 08:37:32.864
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","completed":253,"skipped":4521,"failed":0}
------------------------------
â€¢ [SLOW TEST] [16.465 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:37:16.416
    Jun 28 08:37:16.416: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename gc 06/28/23 08:37:16.417
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:37:16.434
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:37:16.44
    [It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
      test/e2e/apimachinery/garbage_collector.go:735
    STEP: create the rc1 06/28/23 08:37:16.455
    STEP: create the rc2 06/28/23 08:37:16.463
    STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 06/28/23 08:37:21.475
    STEP: delete the rc simpletest-rc-to-be-deleted 06/28/23 08:37:22.068
    STEP: wait for the rc to be deleted 06/28/23 08:37:22.076
    Jun 28 08:37:27.096: INFO: 70 pods remaining
    Jun 28 08:37:27.096: INFO: 70 pods has nil DeletionTimestamp
    Jun 28 08:37:27.096: INFO: 
    STEP: Gathering metrics 06/28/23 08:37:32.095
    W0628 08:37:32.108418      18 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Jun 28 08:37:32.108: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Jun 28 08:37:32.108: INFO: Deleting pod "simpletest-rc-to-be-deleted-2lmc8" in namespace "gc-8167"
    Jun 28 08:37:32.118: INFO: Deleting pod "simpletest-rc-to-be-deleted-46ssn" in namespace "gc-8167"
    Jun 28 08:37:32.133: INFO: Deleting pod "simpletest-rc-to-be-deleted-4mtfv" in namespace "gc-8167"
    Jun 28 08:37:32.142: INFO: Deleting pod "simpletest-rc-to-be-deleted-4xhbp" in namespace "gc-8167"
    Jun 28 08:37:32.154: INFO: Deleting pod "simpletest-rc-to-be-deleted-5hkcv" in namespace "gc-8167"
    Jun 28 08:37:32.166: INFO: Deleting pod "simpletest-rc-to-be-deleted-5mw6d" in namespace "gc-8167"
    Jun 28 08:37:32.182: INFO: Deleting pod "simpletest-rc-to-be-deleted-5r9cc" in namespace "gc-8167"
    Jun 28 08:37:32.200: INFO: Deleting pod "simpletest-rc-to-be-deleted-5trbs" in namespace "gc-8167"
    Jun 28 08:37:32.217: INFO: Deleting pod "simpletest-rc-to-be-deleted-5z64v" in namespace "gc-8167"
    Jun 28 08:37:32.236: INFO: Deleting pod "simpletest-rc-to-be-deleted-6r9sg" in namespace "gc-8167"
    Jun 28 08:37:32.249: INFO: Deleting pod "simpletest-rc-to-be-deleted-6slgf" in namespace "gc-8167"
    Jun 28 08:37:32.263: INFO: Deleting pod "simpletest-rc-to-be-deleted-79cr2" in namespace "gc-8167"
    Jun 28 08:37:32.274: INFO: Deleting pod "simpletest-rc-to-be-deleted-7npwl" in namespace "gc-8167"
    Jun 28 08:37:32.287: INFO: Deleting pod "simpletest-rc-to-be-deleted-7srmd" in namespace "gc-8167"
    Jun 28 08:37:32.302: INFO: Deleting pod "simpletest-rc-to-be-deleted-7v92s" in namespace "gc-8167"
    Jun 28 08:37:32.317: INFO: Deleting pod "simpletest-rc-to-be-deleted-86kht" in namespace "gc-8167"
    Jun 28 08:37:32.344: INFO: Deleting pod "simpletest-rc-to-be-deleted-8f64m" in namespace "gc-8167"
    Jun 28 08:37:32.362: INFO: Deleting pod "simpletest-rc-to-be-deleted-8h2ld" in namespace "gc-8167"
    Jun 28 08:37:32.377: INFO: Deleting pod "simpletest-rc-to-be-deleted-8kdg2" in namespace "gc-8167"
    Jun 28 08:37:32.390: INFO: Deleting pod "simpletest-rc-to-be-deleted-94255" in namespace "gc-8167"
    Jun 28 08:37:32.407: INFO: Deleting pod "simpletest-rc-to-be-deleted-97f2z" in namespace "gc-8167"
    Jun 28 08:37:32.417: INFO: Deleting pod "simpletest-rc-to-be-deleted-99cz2" in namespace "gc-8167"
    Jun 28 08:37:32.428: INFO: Deleting pod "simpletest-rc-to-be-deleted-9qtbc" in namespace "gc-8167"
    Jun 28 08:37:32.443: INFO: Deleting pod "simpletest-rc-to-be-deleted-9x9rg" in namespace "gc-8167"
    Jun 28 08:37:32.456: INFO: Deleting pod "simpletest-rc-to-be-deleted-b5brw" in namespace "gc-8167"
    Jun 28 08:37:32.472: INFO: Deleting pod "simpletest-rc-to-be-deleted-br6pc" in namespace "gc-8167"
    Jun 28 08:37:32.481: INFO: Deleting pod "simpletest-rc-to-be-deleted-bvnft" in namespace "gc-8167"
    Jun 28 08:37:32.491: INFO: Deleting pod "simpletest-rc-to-be-deleted-c2wp9" in namespace "gc-8167"
    Jun 28 08:37:32.507: INFO: Deleting pod "simpletest-rc-to-be-deleted-cg8gt" in namespace "gc-8167"
    Jun 28 08:37:32.523: INFO: Deleting pod "simpletest-rc-to-be-deleted-clf2v" in namespace "gc-8167"
    Jun 28 08:37:32.535: INFO: Deleting pod "simpletest-rc-to-be-deleted-clwbd" in namespace "gc-8167"
    Jun 28 08:37:32.551: INFO: Deleting pod "simpletest-rc-to-be-deleted-cmz6p" in namespace "gc-8167"
    Jun 28 08:37:32.564: INFO: Deleting pod "simpletest-rc-to-be-deleted-cnprz" in namespace "gc-8167"
    Jun 28 08:37:32.577: INFO: Deleting pod "simpletest-rc-to-be-deleted-d5lgh" in namespace "gc-8167"
    Jun 28 08:37:32.592: INFO: Deleting pod "simpletest-rc-to-be-deleted-djf54" in namespace "gc-8167"
    Jun 28 08:37:32.606: INFO: Deleting pod "simpletest-rc-to-be-deleted-dl5xp" in namespace "gc-8167"
    Jun 28 08:37:32.622: INFO: Deleting pod "simpletest-rc-to-be-deleted-fblp9" in namespace "gc-8167"
    Jun 28 08:37:32.633: INFO: Deleting pod "simpletest-rc-to-be-deleted-fc7db" in namespace "gc-8167"
    Jun 28 08:37:32.646: INFO: Deleting pod "simpletest-rc-to-be-deleted-fg9gn" in namespace "gc-8167"
    Jun 28 08:37:32.661: INFO: Deleting pod "simpletest-rc-to-be-deleted-fm4df" in namespace "gc-8167"
    Jun 28 08:37:32.675: INFO: Deleting pod "simpletest-rc-to-be-deleted-fm8l5" in namespace "gc-8167"
    Jun 28 08:37:32.687: INFO: Deleting pod "simpletest-rc-to-be-deleted-frw4r" in namespace "gc-8167"
    Jun 28 08:37:32.702: INFO: Deleting pod "simpletest-rc-to-be-deleted-gjjrq" in namespace "gc-8167"
    Jun 28 08:37:32.719: INFO: Deleting pod "simpletest-rc-to-be-deleted-gwvkz" in namespace "gc-8167"
    Jun 28 08:37:32.737: INFO: Deleting pod "simpletest-rc-to-be-deleted-h4l77" in namespace "gc-8167"
    Jun 28 08:37:32.748: INFO: Deleting pod "simpletest-rc-to-be-deleted-h866j" in namespace "gc-8167"
    Jun 28 08:37:32.761: INFO: Deleting pod "simpletest-rc-to-be-deleted-hn2fn" in namespace "gc-8167"
    Jun 28 08:37:32.773: INFO: Deleting pod "simpletest-rc-to-be-deleted-hpgzr" in namespace "gc-8167"
    Jun 28 08:37:32.785: INFO: Deleting pod "simpletest-rc-to-be-deleted-jw7kw" in namespace "gc-8167"
    Jun 28 08:37:32.804: INFO: Deleting pod "simpletest-rc-to-be-deleted-jz7gz" in namespace "gc-8167"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Jun 28 08:37:32.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-8167" for this suite. 06/28/23 08:37:32.864
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:37:32.882
Jun 28 08:37:32.882: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename replication-controller 06/28/23 08:37:32.882
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:37:32.901
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:37:32.907
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100
STEP: Given a ReplicationController is created 06/28/23 08:37:32.913
STEP: When the matched label of one of its pods change 06/28/23 08:37:32.919
Jun 28 08:37:32.926: INFO: Pod name pod-release: Found 0 pods out of 1
Jun 28 08:37:37.932: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released 06/28/23 08:37:37.947
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Jun 28 08:37:38.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8489" for this suite. 06/28/23 08:37:38.97
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","completed":254,"skipped":4533,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.096 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:37:32.882
    Jun 28 08:37:32.882: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename replication-controller 06/28/23 08:37:32.882
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:37:32.901
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:37:32.907
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should release no longer matching pods [Conformance]
      test/e2e/apps/rc.go:100
    STEP: Given a ReplicationController is created 06/28/23 08:37:32.913
    STEP: When the matched label of one of its pods change 06/28/23 08:37:32.919
    Jun 28 08:37:32.926: INFO: Pod name pod-release: Found 0 pods out of 1
    Jun 28 08:37:37.932: INFO: Pod name pod-release: Found 1 pods out of 1
    STEP: Then the pod is released 06/28/23 08:37:37.947
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Jun 28 08:37:38.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-8489" for this suite. 06/28/23 08:37:38.97
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:37:38.978
Jun 28 08:37:38.978: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename projected 06/28/23 08:37:38.979
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:37:38.994
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:37:39
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260
STEP: Creating a pod to test downward API volume plugin 06/28/23 08:37:39.007
Jun 28 08:37:39.016: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fb6da135-771c-4065-92e1-1c05ca83673a" in namespace "projected-8901" to be "Succeeded or Failed"
Jun 28 08:37:39.021: INFO: Pod "downwardapi-volume-fb6da135-771c-4065-92e1-1c05ca83673a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.146968ms
Jun 28 08:37:41.027: INFO: Pod "downwardapi-volume-fb6da135-771c-4065-92e1-1c05ca83673a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010759758s
Jun 28 08:37:43.028: INFO: Pod "downwardapi-volume-fb6da135-771c-4065-92e1-1c05ca83673a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01225808s
Jun 28 08:37:45.028: INFO: Pod "downwardapi-volume-fb6da135-771c-4065-92e1-1c05ca83673a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.01185699s
STEP: Saw pod success 06/28/23 08:37:45.028
Jun 28 08:37:45.028: INFO: Pod "downwardapi-volume-fb6da135-771c-4065-92e1-1c05ca83673a" satisfied condition "Succeeded or Failed"
Jun 28 08:37:45.032: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod downwardapi-volume-fb6da135-771c-4065-92e1-1c05ca83673a container client-container: <nil>
STEP: delete the pod 06/28/23 08:37:45.083
Jun 28 08:37:45.094: INFO: Waiting for pod downwardapi-volume-fb6da135-771c-4065-92e1-1c05ca83673a to disappear
Jun 28 08:37:45.098: INFO: Pod downwardapi-volume-fb6da135-771c-4065-92e1-1c05ca83673a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jun 28 08:37:45.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8901" for this suite. 06/28/23 08:37:45.108
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","completed":255,"skipped":4535,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.137 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:37:38.978
    Jun 28 08:37:38.978: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename projected 06/28/23 08:37:38.979
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:37:38.994
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:37:39
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:260
    STEP: Creating a pod to test downward API volume plugin 06/28/23 08:37:39.007
    Jun 28 08:37:39.016: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fb6da135-771c-4065-92e1-1c05ca83673a" in namespace "projected-8901" to be "Succeeded or Failed"
    Jun 28 08:37:39.021: INFO: Pod "downwardapi-volume-fb6da135-771c-4065-92e1-1c05ca83673a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.146968ms
    Jun 28 08:37:41.027: INFO: Pod "downwardapi-volume-fb6da135-771c-4065-92e1-1c05ca83673a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010759758s
    Jun 28 08:37:43.028: INFO: Pod "downwardapi-volume-fb6da135-771c-4065-92e1-1c05ca83673a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01225808s
    Jun 28 08:37:45.028: INFO: Pod "downwardapi-volume-fb6da135-771c-4065-92e1-1c05ca83673a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.01185699s
    STEP: Saw pod success 06/28/23 08:37:45.028
    Jun 28 08:37:45.028: INFO: Pod "downwardapi-volume-fb6da135-771c-4065-92e1-1c05ca83673a" satisfied condition "Succeeded or Failed"
    Jun 28 08:37:45.032: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod downwardapi-volume-fb6da135-771c-4065-92e1-1c05ca83673a container client-container: <nil>
    STEP: delete the pod 06/28/23 08:37:45.083
    Jun 28 08:37:45.094: INFO: Waiting for pod downwardapi-volume-fb6da135-771c-4065-92e1-1c05ca83673a to disappear
    Jun 28 08:37:45.098: INFO: Pod downwardapi-volume-fb6da135-771c-4065-92e1-1c05ca83673a no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jun 28 08:37:45.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-8901" for this suite. 06/28/23 08:37:45.108
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43
[BeforeEach] [sig-storage] Projected combined
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:37:45.116
Jun 28 08:37:45.116: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename projected 06/28/23 08:37:45.117
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:37:45.136
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:37:45.14
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43
STEP: Creating configMap with name configmap-projected-all-test-volume-fd978a8f-c641-48f5-b59f-dd2404ba12ef 06/28/23 08:37:45.145
STEP: Creating secret with name secret-projected-all-test-volume-d1e3054b-f0aa-4f2f-878a-2554cd870316 06/28/23 08:37:45.151
STEP: Creating a pod to test Check all projections for projected volume plugin 06/28/23 08:37:45.157
Jun 28 08:37:45.166: INFO: Waiting up to 5m0s for pod "projected-volume-f41f72d5-c0fd-4664-98b7-ec7aaa8afa71" in namespace "projected-18" to be "Succeeded or Failed"
Jun 28 08:37:45.171: INFO: Pod "projected-volume-f41f72d5-c0fd-4664-98b7-ec7aaa8afa71": Phase="Pending", Reason="", readiness=false. Elapsed: 4.61366ms
Jun 28 08:37:47.177: INFO: Pod "projected-volume-f41f72d5-c0fd-4664-98b7-ec7aaa8afa71": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010259663s
Jun 28 08:37:49.177: INFO: Pod "projected-volume-f41f72d5-c0fd-4664-98b7-ec7aaa8afa71": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010507334s
STEP: Saw pod success 06/28/23 08:37:49.177
Jun 28 08:37:49.177: INFO: Pod "projected-volume-f41f72d5-c0fd-4664-98b7-ec7aaa8afa71" satisfied condition "Succeeded or Failed"
Jun 28 08:37:49.182: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod projected-volume-f41f72d5-c0fd-4664-98b7-ec7aaa8afa71 container projected-all-volume-test: <nil>
STEP: delete the pod 06/28/23 08:37:49.232
Jun 28 08:37:49.245: INFO: Waiting for pod projected-volume-f41f72d5-c0fd-4664-98b7-ec7aaa8afa71 to disappear
Jun 28 08:37:49.249: INFO: Pod projected-volume-f41f72d5-c0fd-4664-98b7-ec7aaa8afa71 no longer exists
[AfterEach] [sig-storage] Projected combined
  test/e2e/framework/framework.go:187
Jun 28 08:37:49.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-18" for this suite. 06/28/23 08:37:49.259
{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","completed":256,"skipped":4553,"failed":0}
------------------------------
â€¢ [4.151 seconds]
[sig-storage] Projected combined
test/e2e/common/storage/framework.go:23
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected combined
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:37:45.116
    Jun 28 08:37:45.116: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename projected 06/28/23 08:37:45.117
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:37:45.136
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:37:45.14
    [It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
      test/e2e/common/storage/projected_combined.go:43
    STEP: Creating configMap with name configmap-projected-all-test-volume-fd978a8f-c641-48f5-b59f-dd2404ba12ef 06/28/23 08:37:45.145
    STEP: Creating secret with name secret-projected-all-test-volume-d1e3054b-f0aa-4f2f-878a-2554cd870316 06/28/23 08:37:45.151
    STEP: Creating a pod to test Check all projections for projected volume plugin 06/28/23 08:37:45.157
    Jun 28 08:37:45.166: INFO: Waiting up to 5m0s for pod "projected-volume-f41f72d5-c0fd-4664-98b7-ec7aaa8afa71" in namespace "projected-18" to be "Succeeded or Failed"
    Jun 28 08:37:45.171: INFO: Pod "projected-volume-f41f72d5-c0fd-4664-98b7-ec7aaa8afa71": Phase="Pending", Reason="", readiness=false. Elapsed: 4.61366ms
    Jun 28 08:37:47.177: INFO: Pod "projected-volume-f41f72d5-c0fd-4664-98b7-ec7aaa8afa71": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010259663s
    Jun 28 08:37:49.177: INFO: Pod "projected-volume-f41f72d5-c0fd-4664-98b7-ec7aaa8afa71": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010507334s
    STEP: Saw pod success 06/28/23 08:37:49.177
    Jun 28 08:37:49.177: INFO: Pod "projected-volume-f41f72d5-c0fd-4664-98b7-ec7aaa8afa71" satisfied condition "Succeeded or Failed"
    Jun 28 08:37:49.182: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod projected-volume-f41f72d5-c0fd-4664-98b7-ec7aaa8afa71 container projected-all-volume-test: <nil>
    STEP: delete the pod 06/28/23 08:37:49.232
    Jun 28 08:37:49.245: INFO: Waiting for pod projected-volume-f41f72d5-c0fd-4664-98b7-ec7aaa8afa71 to disappear
    Jun 28 08:37:49.249: INFO: Pod projected-volume-f41f72d5-c0fd-4664-98b7-ec7aaa8afa71 no longer exists
    [AfterEach] [sig-storage] Projected combined
      test/e2e/framework/framework.go:187
    Jun 28 08:37:49.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-18" for this suite. 06/28/23 08:37:49.259
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:37:49.269
Jun 28 08:37:49.270: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename var-expansion 06/28/23 08:37:49.271
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:37:49.289
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:37:49.294
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224
STEP: creating the pod with failed condition 06/28/23 08:37:49.299
Jun 28 08:37:49.314: INFO: Waiting up to 2m0s for pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736" in namespace "var-expansion-6348" to be "running"
Jun 28 08:37:49.319: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 5.270139ms
Jun 28 08:37:51.326: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011744131s
Jun 28 08:37:53.325: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011380988s
Jun 28 08:37:55.326: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 6.011473706s
Jun 28 08:37:57.327: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 8.012652232s
Jun 28 08:37:59.325: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 10.01131098s
Jun 28 08:38:01.327: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 12.013155398s
Jun 28 08:38:03.325: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 14.011218696s
Jun 28 08:38:05.325: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 16.011264531s
Jun 28 08:38:07.327: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 18.012607734s
Jun 28 08:38:09.325: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 20.011018769s
Jun 28 08:38:11.326: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 22.012073645s
Jun 28 08:38:13.325: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 24.011090877s
Jun 28 08:38:15.326: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 26.012308659s
Jun 28 08:38:17.325: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 28.01104074s
Jun 28 08:38:19.326: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 30.011644585s
Jun 28 08:38:21.325: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 32.010944816s
Jun 28 08:38:23.325: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 34.011372149s
Jun 28 08:38:25.325: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 36.010767569s
Jun 28 08:38:27.326: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 38.011536624s
Jun 28 08:38:29.325: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 40.011270527s
Jun 28 08:38:31.326: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 42.011486728s
Jun 28 08:38:33.325: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 44.01123393s
Jun 28 08:38:35.326: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 46.012039412s
Jun 28 08:38:37.326: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 48.011539023s
Jun 28 08:38:39.326: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 50.011447608s
Jun 28 08:38:41.327: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 52.012439502s
Jun 28 08:38:43.326: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 54.011622078s
Jun 28 08:38:45.326: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 56.011618223s
Jun 28 08:38:47.326: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 58.012399108s
Jun 28 08:38:49.325: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.011281789s
Jun 28 08:38:51.326: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.011955961s
Jun 28 08:38:53.324: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.010215136s
Jun 28 08:38:55.325: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.01076863s
Jun 28 08:38:57.325: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.011317942s
Jun 28 08:38:59.325: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.011343287s
Jun 28 08:39:01.327: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.013187038s
Jun 28 08:39:03.326: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.012280472s
Jun 28 08:39:05.329: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.015269174s
Jun 28 08:39:07.327: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.012687783s
Jun 28 08:39:09.326: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.011617811s
Jun 28 08:39:11.331: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.016948609s
Jun 28 08:39:13.326: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.01185748s
Jun 28 08:39:15.325: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.010665048s
Jun 28 08:39:17.325: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.010564652s
Jun 28 08:39:19.325: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.010670174s
Jun 28 08:39:21.326: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.011848762s
Jun 28 08:39:23.325: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.010919901s
Jun 28 08:39:25.325: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.010456878s
Jun 28 08:39:27.324: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.010336263s
Jun 28 08:39:29.325: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.010811918s
Jun 28 08:39:31.325: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.010442434s
Jun 28 08:39:33.325: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.010776335s
Jun 28 08:39:35.326: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.01169907s
Jun 28 08:39:37.326: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.011538774s
Jun 28 08:39:39.325: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.011272902s
Jun 28 08:39:41.327: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.012695664s
Jun 28 08:39:43.326: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.0116973s
Jun 28 08:39:45.326: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.011428605s
Jun 28 08:39:47.324: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.010197439s
Jun 28 08:39:49.326: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.01189481s
Jun 28 08:39:49.330: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.016227387s
STEP: updating the pod 06/28/23 08:39:49.33
Jun 28 08:39:49.843: INFO: Successfully updated pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736"
STEP: waiting for pod running 06/28/23 08:39:49.843
Jun 28 08:39:49.843: INFO: Waiting up to 2m0s for pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736" in namespace "var-expansion-6348" to be "running"
Jun 28 08:39:49.848: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 5.041461ms
Jun 28 08:39:51.853: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Running", Reason="", readiness=true. Elapsed: 2.010514616s
Jun 28 08:39:51.853: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736" satisfied condition "running"
STEP: deleting the pod gracefully 06/28/23 08:39:51.853
Jun 28 08:39:51.853: INFO: Deleting pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736" in namespace "var-expansion-6348"
Jun 28 08:39:51.861: INFO: Wait up to 5m0s for pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jun 28 08:40:23.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6348" for this suite. 06/28/23 08:40:23.878
{"msg":"PASSED [sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]","completed":257,"skipped":4598,"failed":0}
------------------------------
â€¢ [SLOW TEST] [154.615 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:37:49.269
    Jun 28 08:37:49.270: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename var-expansion 06/28/23 08:37:49.271
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:37:49.289
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:37:49.294
    [It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:224
    STEP: creating the pod with failed condition 06/28/23 08:37:49.299
    Jun 28 08:37:49.314: INFO: Waiting up to 2m0s for pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736" in namespace "var-expansion-6348" to be "running"
    Jun 28 08:37:49.319: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 5.270139ms
    Jun 28 08:37:51.326: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011744131s
    Jun 28 08:37:53.325: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011380988s
    Jun 28 08:37:55.326: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 6.011473706s
    Jun 28 08:37:57.327: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 8.012652232s
    Jun 28 08:37:59.325: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 10.01131098s
    Jun 28 08:38:01.327: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 12.013155398s
    Jun 28 08:38:03.325: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 14.011218696s
    Jun 28 08:38:05.325: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 16.011264531s
    Jun 28 08:38:07.327: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 18.012607734s
    Jun 28 08:38:09.325: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 20.011018769s
    Jun 28 08:38:11.326: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 22.012073645s
    Jun 28 08:38:13.325: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 24.011090877s
    Jun 28 08:38:15.326: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 26.012308659s
    Jun 28 08:38:17.325: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 28.01104074s
    Jun 28 08:38:19.326: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 30.011644585s
    Jun 28 08:38:21.325: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 32.010944816s
    Jun 28 08:38:23.325: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 34.011372149s
    Jun 28 08:38:25.325: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 36.010767569s
    Jun 28 08:38:27.326: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 38.011536624s
    Jun 28 08:38:29.325: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 40.011270527s
    Jun 28 08:38:31.326: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 42.011486728s
    Jun 28 08:38:33.325: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 44.01123393s
    Jun 28 08:38:35.326: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 46.012039412s
    Jun 28 08:38:37.326: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 48.011539023s
    Jun 28 08:38:39.326: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 50.011447608s
    Jun 28 08:38:41.327: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 52.012439502s
    Jun 28 08:38:43.326: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 54.011622078s
    Jun 28 08:38:45.326: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 56.011618223s
    Jun 28 08:38:47.326: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 58.012399108s
    Jun 28 08:38:49.325: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.011281789s
    Jun 28 08:38:51.326: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.011955961s
    Jun 28 08:38:53.324: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.010215136s
    Jun 28 08:38:55.325: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.01076863s
    Jun 28 08:38:57.325: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.011317942s
    Jun 28 08:38:59.325: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.011343287s
    Jun 28 08:39:01.327: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.013187038s
    Jun 28 08:39:03.326: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.012280472s
    Jun 28 08:39:05.329: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.015269174s
    Jun 28 08:39:07.327: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.012687783s
    Jun 28 08:39:09.326: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.011617811s
    Jun 28 08:39:11.331: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.016948609s
    Jun 28 08:39:13.326: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.01185748s
    Jun 28 08:39:15.325: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.010665048s
    Jun 28 08:39:17.325: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.010564652s
    Jun 28 08:39:19.325: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.010670174s
    Jun 28 08:39:21.326: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.011848762s
    Jun 28 08:39:23.325: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.010919901s
    Jun 28 08:39:25.325: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.010456878s
    Jun 28 08:39:27.324: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.010336263s
    Jun 28 08:39:29.325: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.010811918s
    Jun 28 08:39:31.325: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.010442434s
    Jun 28 08:39:33.325: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.010776335s
    Jun 28 08:39:35.326: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.01169907s
    Jun 28 08:39:37.326: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.011538774s
    Jun 28 08:39:39.325: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.011272902s
    Jun 28 08:39:41.327: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.012695664s
    Jun 28 08:39:43.326: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.0116973s
    Jun 28 08:39:45.326: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.011428605s
    Jun 28 08:39:47.324: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.010197439s
    Jun 28 08:39:49.326: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.01189481s
    Jun 28 08:39:49.330: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.016227387s
    STEP: updating the pod 06/28/23 08:39:49.33
    Jun 28 08:39:49.843: INFO: Successfully updated pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736"
    STEP: waiting for pod running 06/28/23 08:39:49.843
    Jun 28 08:39:49.843: INFO: Waiting up to 2m0s for pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736" in namespace "var-expansion-6348" to be "running"
    Jun 28 08:39:49.848: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Pending", Reason="", readiness=false. Elapsed: 5.041461ms
    Jun 28 08:39:51.853: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736": Phase="Running", Reason="", readiness=true. Elapsed: 2.010514616s
    Jun 28 08:39:51.853: INFO: Pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736" satisfied condition "running"
    STEP: deleting the pod gracefully 06/28/23 08:39:51.853
    Jun 28 08:39:51.853: INFO: Deleting pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736" in namespace "var-expansion-6348"
    Jun 28 08:39:51.861: INFO: Wait up to 5m0s for pod "var-expansion-effe52fa-c52b-445e-941e-626c0489d736" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jun 28 08:40:23.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-6348" for this suite. 06/28/23 08:40:23.878
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:40:23.885
Jun 28 08:40:23.885: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename projected 06/28/23 08:40:23.886
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:40:23.897
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:40:23.903
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56
STEP: Creating configMap with name projected-configmap-test-volume-7f67b3f5-de68-4c1d-96a0-b30c7119c3d4 06/28/23 08:40:23.912
STEP: Creating a pod to test consume configMaps 06/28/23 08:40:23.918
Jun 28 08:40:23.928: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3f450790-05f7-4519-9130-1a4e6e2b62a9" in namespace "projected-2275" to be "Succeeded or Failed"
Jun 28 08:40:23.933: INFO: Pod "pod-projected-configmaps-3f450790-05f7-4519-9130-1a4e6e2b62a9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.24243ms
Jun 28 08:40:25.939: INFO: Pod "pod-projected-configmaps-3f450790-05f7-4519-9130-1a4e6e2b62a9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010567017s
Jun 28 08:40:27.938: INFO: Pod "pod-projected-configmaps-3f450790-05f7-4519-9130-1a4e6e2b62a9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009606975s
STEP: Saw pod success 06/28/23 08:40:27.938
Jun 28 08:40:27.938: INFO: Pod "pod-projected-configmaps-3f450790-05f7-4519-9130-1a4e6e2b62a9" satisfied condition "Succeeded or Failed"
Jun 28 08:40:27.943: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod pod-projected-configmaps-3f450790-05f7-4519-9130-1a4e6e2b62a9 container agnhost-container: <nil>
STEP: delete the pod 06/28/23 08:40:28.031
Jun 28 08:40:28.045: INFO: Waiting for pod pod-projected-configmaps-3f450790-05f7-4519-9130-1a4e6e2b62a9 to disappear
Jun 28 08:40:28.049: INFO: Pod pod-projected-configmaps-3f450790-05f7-4519-9130-1a4e6e2b62a9 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jun 28 08:40:28.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2275" for this suite. 06/28/23 08:40:28.057
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":258,"skipped":4613,"failed":0}
------------------------------
â€¢ [4.178 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:40:23.885
    Jun 28 08:40:23.885: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename projected 06/28/23 08:40:23.886
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:40:23.897
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:40:23.903
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:56
    STEP: Creating configMap with name projected-configmap-test-volume-7f67b3f5-de68-4c1d-96a0-b30c7119c3d4 06/28/23 08:40:23.912
    STEP: Creating a pod to test consume configMaps 06/28/23 08:40:23.918
    Jun 28 08:40:23.928: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3f450790-05f7-4519-9130-1a4e6e2b62a9" in namespace "projected-2275" to be "Succeeded or Failed"
    Jun 28 08:40:23.933: INFO: Pod "pod-projected-configmaps-3f450790-05f7-4519-9130-1a4e6e2b62a9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.24243ms
    Jun 28 08:40:25.939: INFO: Pod "pod-projected-configmaps-3f450790-05f7-4519-9130-1a4e6e2b62a9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010567017s
    Jun 28 08:40:27.938: INFO: Pod "pod-projected-configmaps-3f450790-05f7-4519-9130-1a4e6e2b62a9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009606975s
    STEP: Saw pod success 06/28/23 08:40:27.938
    Jun 28 08:40:27.938: INFO: Pod "pod-projected-configmaps-3f450790-05f7-4519-9130-1a4e6e2b62a9" satisfied condition "Succeeded or Failed"
    Jun 28 08:40:27.943: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod pod-projected-configmaps-3f450790-05f7-4519-9130-1a4e6e2b62a9 container agnhost-container: <nil>
    STEP: delete the pod 06/28/23 08:40:28.031
    Jun 28 08:40:28.045: INFO: Waiting for pod pod-projected-configmaps-3f450790-05f7-4519-9130-1a4e6e2b62a9 to disappear
    Jun 28 08:40:28.049: INFO: Pod pod-projected-configmaps-3f450790-05f7-4519-9130-1a4e6e2b62a9 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jun 28 08:40:28.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-2275" for this suite. 06/28/23 08:40:28.057
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:40:28.064
Jun 28 08:40:28.064: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename replication-controller 06/28/23 08:40:28.064
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:40:28.077
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:40:28.081
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66
STEP: Creating replication controller my-hostname-basic-1c6f6e71-bb5e-48d4-b494-964ca16779f3 06/28/23 08:40:28.085
Jun 28 08:40:28.094: INFO: Pod name my-hostname-basic-1c6f6e71-bb5e-48d4-b494-964ca16779f3: Found 0 pods out of 1
Jun 28 08:40:33.101: INFO: Pod name my-hostname-basic-1c6f6e71-bb5e-48d4-b494-964ca16779f3: Found 1 pods out of 1
Jun 28 08:40:33.101: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-1c6f6e71-bb5e-48d4-b494-964ca16779f3" are running
Jun 28 08:40:33.101: INFO: Waiting up to 5m0s for pod "my-hostname-basic-1c6f6e71-bb5e-48d4-b494-964ca16779f3-kb452" in namespace "replication-controller-4264" to be "running"
Jun 28 08:40:33.106: INFO: Pod "my-hostname-basic-1c6f6e71-bb5e-48d4-b494-964ca16779f3-kb452": Phase="Running", Reason="", readiness=true. Elapsed: 4.892285ms
Jun 28 08:40:33.106: INFO: Pod "my-hostname-basic-1c6f6e71-bb5e-48d4-b494-964ca16779f3-kb452" satisfied condition "running"
Jun 28 08:40:33.106: INFO: Pod "my-hostname-basic-1c6f6e71-bb5e-48d4-b494-964ca16779f3-kb452" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-28 08:40:28 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-28 08:40:29 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-28 08:40:29 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-28 08:40:28 +0000 UTC Reason: Message:}])
Jun 28 08:40:33.106: INFO: Trying to dial the pod
Jun 28 08:40:38.208: INFO: Controller my-hostname-basic-1c6f6e71-bb5e-48d4-b494-964ca16779f3: Got expected result from replica 1 [my-hostname-basic-1c6f6e71-bb5e-48d4-b494-964ca16779f3-kb452]: "my-hostname-basic-1c6f6e71-bb5e-48d4-b494-964ca16779f3-kb452", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Jun 28 08:40:38.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4264" for this suite. 06/28/23 08:40:38.216
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","completed":259,"skipped":4629,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.159 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:40:28.064
    Jun 28 08:40:28.064: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename replication-controller 06/28/23 08:40:28.064
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:40:28.077
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:40:28.081
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/rc.go:66
    STEP: Creating replication controller my-hostname-basic-1c6f6e71-bb5e-48d4-b494-964ca16779f3 06/28/23 08:40:28.085
    Jun 28 08:40:28.094: INFO: Pod name my-hostname-basic-1c6f6e71-bb5e-48d4-b494-964ca16779f3: Found 0 pods out of 1
    Jun 28 08:40:33.101: INFO: Pod name my-hostname-basic-1c6f6e71-bb5e-48d4-b494-964ca16779f3: Found 1 pods out of 1
    Jun 28 08:40:33.101: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-1c6f6e71-bb5e-48d4-b494-964ca16779f3" are running
    Jun 28 08:40:33.101: INFO: Waiting up to 5m0s for pod "my-hostname-basic-1c6f6e71-bb5e-48d4-b494-964ca16779f3-kb452" in namespace "replication-controller-4264" to be "running"
    Jun 28 08:40:33.106: INFO: Pod "my-hostname-basic-1c6f6e71-bb5e-48d4-b494-964ca16779f3-kb452": Phase="Running", Reason="", readiness=true. Elapsed: 4.892285ms
    Jun 28 08:40:33.106: INFO: Pod "my-hostname-basic-1c6f6e71-bb5e-48d4-b494-964ca16779f3-kb452" satisfied condition "running"
    Jun 28 08:40:33.106: INFO: Pod "my-hostname-basic-1c6f6e71-bb5e-48d4-b494-964ca16779f3-kb452" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-28 08:40:28 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-28 08:40:29 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-28 08:40:29 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-28 08:40:28 +0000 UTC Reason: Message:}])
    Jun 28 08:40:33.106: INFO: Trying to dial the pod
    Jun 28 08:40:38.208: INFO: Controller my-hostname-basic-1c6f6e71-bb5e-48d4-b494-964ca16779f3: Got expected result from replica 1 [my-hostname-basic-1c6f6e71-bb5e-48d4-b494-964ca16779f3-kb452]: "my-hostname-basic-1c6f6e71-bb5e-48d4-b494-964ca16779f3-kb452", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Jun 28 08:40:38.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-4264" for this suite. 06/28/23 08:40:38.216
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch
  should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1650
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:40:38.224
Jun 28 08:40:38.224: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename kubectl 06/28/23 08:40:38.225
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:40:38.238
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:40:38.243
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1650
STEP: creating Agnhost RC 06/28/23 08:40:38.247
Jun 28 08:40:38.247: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-9344 create -f -'
Jun 28 08:40:38.966: INFO: stderr: ""
Jun 28 08:40:38.966: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 06/28/23 08:40:38.966
Jun 28 08:40:39.972: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 28 08:40:39.973: INFO: Found 0 / 1
Jun 28 08:40:40.971: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 28 08:40:40.971: INFO: Found 1 / 1
Jun 28 08:40:40.971: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods 06/28/23 08:40:40.971
Jun 28 08:40:40.975: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 28 08:40:40.975: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jun 28 08:40:40.975: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-9344 patch pod agnhost-primary-xbklp -p {"metadata":{"annotations":{"x":"y"}}}'
Jun 28 08:40:41.052: INFO: stderr: ""
Jun 28 08:40:41.052: INFO: stdout: "pod/agnhost-primary-xbklp patched\n"
STEP: checking annotations 06/28/23 08:40:41.052
Jun 28 08:40:41.056: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 28 08:40:41.056: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jun 28 08:40:41.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9344" for this suite. 06/28/23 08:40:41.063
{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","completed":260,"skipped":4648,"failed":0}
------------------------------
â€¢ [2.845 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl patch
  test/e2e/kubectl/kubectl.go:1644
    should add annotations for pods in rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:40:38.224
    Jun 28 08:40:38.224: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename kubectl 06/28/23 08:40:38.225
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:40:38.238
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:40:38.243
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should add annotations for pods in rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1650
    STEP: creating Agnhost RC 06/28/23 08:40:38.247
    Jun 28 08:40:38.247: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-9344 create -f -'
    Jun 28 08:40:38.966: INFO: stderr: ""
    Jun 28 08:40:38.966: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 06/28/23 08:40:38.966
    Jun 28 08:40:39.972: INFO: Selector matched 1 pods for map[app:agnhost]
    Jun 28 08:40:39.973: INFO: Found 0 / 1
    Jun 28 08:40:40.971: INFO: Selector matched 1 pods for map[app:agnhost]
    Jun 28 08:40:40.971: INFO: Found 1 / 1
    Jun 28 08:40:40.971: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    STEP: patching all pods 06/28/23 08:40:40.971
    Jun 28 08:40:40.975: INFO: Selector matched 1 pods for map[app:agnhost]
    Jun 28 08:40:40.975: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Jun 28 08:40:40.975: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-9344 patch pod agnhost-primary-xbklp -p {"metadata":{"annotations":{"x":"y"}}}'
    Jun 28 08:40:41.052: INFO: stderr: ""
    Jun 28 08:40:41.052: INFO: stdout: "pod/agnhost-primary-xbklp patched\n"
    STEP: checking annotations 06/28/23 08:40:41.052
    Jun 28 08:40:41.056: INFO: Selector matched 1 pods for map[app:agnhost]
    Jun 28 08:40:41.056: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jun 28 08:40:41.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-9344" for this suite. 06/28/23 08:40:41.063
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:130
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:40:41.072
Jun 28 08:40:41.072: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename container-lifecycle-hook 06/28/23 08:40:41.072
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:40:41.084
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:40:41.088
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 06/28/23 08:40:41.099
Jun 28 08:40:41.107: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-1113" to be "running and ready"
Jun 28 08:40:41.111: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 3.976876ms
Jun 28 08:40:41.111: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jun 28 08:40:43.117: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.010031205s
Jun 28 08:40:43.117: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jun 28 08:40:43.117: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:130
STEP: create the pod with lifecycle hook 06/28/23 08:40:43.122
Jun 28 08:40:43.127: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-1113" to be "running and ready"
Jun 28 08:40:43.132: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 5.027877ms
Jun 28 08:40:43.132: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jun 28 08:40:45.138: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.010862081s
Jun 28 08:40:45.138: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
Jun 28 08:40:45.138: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
STEP: check poststart hook 06/28/23 08:40:45.142
STEP: delete the pod with lifecycle hook 06/28/23 08:40:45.151
Jun 28 08:40:45.159: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun 28 08:40:45.164: INFO: Pod pod-with-poststart-http-hook still exists
Jun 28 08:40:47.164: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun 28 08:40:47.169: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Jun 28 08:40:47.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-1113" for this suite. 06/28/23 08:40:47.178
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","completed":261,"skipped":4717,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.113 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:40:41.072
    Jun 28 08:40:41.072: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename container-lifecycle-hook 06/28/23 08:40:41.072
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:40:41.084
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:40:41.088
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 06/28/23 08:40:41.099
    Jun 28 08:40:41.107: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-1113" to be "running and ready"
    Jun 28 08:40:41.111: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 3.976876ms
    Jun 28 08:40:41.111: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jun 28 08:40:43.117: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.010031205s
    Jun 28 08:40:43.117: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jun 28 08:40:43.117: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:130
    STEP: create the pod with lifecycle hook 06/28/23 08:40:43.122
    Jun 28 08:40:43.127: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-1113" to be "running and ready"
    Jun 28 08:40:43.132: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 5.027877ms
    Jun 28 08:40:43.132: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Jun 28 08:40:45.138: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.010862081s
    Jun 28 08:40:45.138: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
    Jun 28 08:40:45.138: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
    STEP: check poststart hook 06/28/23 08:40:45.142
    STEP: delete the pod with lifecycle hook 06/28/23 08:40:45.151
    Jun 28 08:40:45.159: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Jun 28 08:40:45.164: INFO: Pod pod-with-poststart-http-hook still exists
    Jun 28 08:40:47.164: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Jun 28 08:40:47.169: INFO: Pod pod-with-poststart-http-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Jun 28 08:40:47.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-1113" for this suite. 06/28/23 08:40:47.178
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] DNS
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:40:47.185
Jun 28 08:40:47.185: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename dns 06/28/23 08:40:47.186
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:40:47.2
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:40:47.204
[It] should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 06/28/23 08:40:47.209
Jun 28 08:40:47.216: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-9214  1168ec66-d271-4c05-a241-d9130e1136f4 79753 0 2023-06-28 08:40:47 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-06-28 08:40:47 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-j95xq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-j95xq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 28 08:40:47.217: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-9214" to be "running and ready"
Jun 28 08:40:47.220: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 3.572376ms
Jun 28 08:40:47.220: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Jun 28 08:40:49.226: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.009177273s
Jun 28 08:40:49.226: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
Jun 28 08:40:49.226: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
STEP: Verifying customized DNS suffix list is configured on pod... 06/28/23 08:40:49.226
Jun 28 08:40:49.226: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-9214 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 28 08:40:49.226: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
Jun 28 08:40:49.227: INFO: ExecWithOptions: Clientset creation
Jun 28 08:40:49.227: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/dns-9214/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
STEP: Verifying customized DNS server is configured on pod... 06/28/23 08:40:49.633
Jun 28 08:40:49.633: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-9214 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 28 08:40:49.633: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
Jun 28 08:40:49.634: INFO: ExecWithOptions: Clientset creation
Jun 28 08:40:49.634: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/dns-9214/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jun 28 08:40:50.027: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jun 28 08:40:50.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9214" for this suite. 06/28/23 08:40:50.046
{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","completed":262,"skipped":4721,"failed":0}
------------------------------
â€¢ [2.868 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:40:47.185
    Jun 28 08:40:47.185: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename dns 06/28/23 08:40:47.186
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:40:47.2
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:40:47.204
    [It] should support configurable pod DNS nameservers [Conformance]
      test/e2e/network/dns.go:411
    STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 06/28/23 08:40:47.209
    Jun 28 08:40:47.216: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-9214  1168ec66-d271-4c05-a241-d9130e1136f4 79753 0 2023-06-28 08:40:47 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-06-28 08:40:47 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-j95xq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-j95xq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 28 08:40:47.217: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-9214" to be "running and ready"
    Jun 28 08:40:47.220: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 3.572376ms
    Jun 28 08:40:47.220: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
    Jun 28 08:40:49.226: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.009177273s
    Jun 28 08:40:49.226: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
    Jun 28 08:40:49.226: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
    STEP: Verifying customized DNS suffix list is configured on pod... 06/28/23 08:40:49.226
    Jun 28 08:40:49.226: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-9214 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 28 08:40:49.226: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    Jun 28 08:40:49.227: INFO: ExecWithOptions: Clientset creation
    Jun 28 08:40:49.227: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/dns-9214/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    STEP: Verifying customized DNS server is configured on pod... 06/28/23 08:40:49.633
    Jun 28 08:40:49.633: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-9214 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 28 08:40:49.633: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    Jun 28 08:40:49.634: INFO: ExecWithOptions: Clientset creation
    Jun 28 08:40:49.634: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/dns-9214/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jun 28 08:40:50.027: INFO: Deleting pod test-dns-nameservers...
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jun 28 08:40:50.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-9214" for this suite. 06/28/23 08:40:50.046
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:315
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:40:50.054
Jun 28 08:40:50.054: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename statefulset 06/28/23 08:40:50.054
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:40:50.067
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:40:50.073
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-1053 06/28/23 08:40:50.078
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:315
STEP: Creating a new StatefulSet 06/28/23 08:40:50.083
Jun 28 08:40:50.093: INFO: Found 0 stateful pods, waiting for 3
Jun 28 08:41:00.100: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 28 08:41:00.100: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 28 08:41:00.100: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 06/28/23 08:41:00.114
Jun 28 08:41:00.139: INFO: Updating stateful set ss2
STEP: Creating a new revision 06/28/23 08:41:00.139
STEP: Not applying an update when the partition is greater than the number of replicas 06/28/23 08:41:10.162
STEP: Performing a canary update 06/28/23 08:41:10.162
Jun 28 08:41:10.185: INFO: Updating stateful set ss2
Jun 28 08:41:10.196: INFO: Waiting for Pod statefulset-1053/ss2-2 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
STEP: Restoring Pods to the correct revision when they are deleted 06/28/23 08:41:20.208
Jun 28 08:41:20.240: INFO: Found 2 stateful pods, waiting for 3
Jun 28 08:41:30.247: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 28 08:41:30.247: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 28 08:41:30.247: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update 06/28/23 08:41:30.256
Jun 28 08:41:30.279: INFO: Updating stateful set ss2
Jun 28 08:41:30.290: INFO: Waiting for Pod statefulset-1053/ss2-1 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
Jun 28 08:41:40.323: INFO: Updating stateful set ss2
Jun 28 08:41:40.331: INFO: Waiting for StatefulSet statefulset-1053/ss2 to complete update
Jun 28 08:41:40.331: INFO: Waiting for Pod statefulset-1053/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jun 28 08:41:50.344: INFO: Deleting all statefulset in ns statefulset-1053
Jun 28 08:41:50.348: INFO: Scaling statefulset ss2 to 0
Jun 28 08:42:00.371: INFO: Waiting for statefulset status.replicas updated to 0
Jun 28 08:42:00.377: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jun 28 08:42:00.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1053" for this suite. 06/28/23 08:42:00.404
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","completed":263,"skipped":4738,"failed":0}
------------------------------
â€¢ [SLOW TEST] [70.357 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    test/e2e/apps/statefulset.go:315

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:40:50.054
    Jun 28 08:40:50.054: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename statefulset 06/28/23 08:40:50.054
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:40:50.067
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:40:50.073
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-1053 06/28/23 08:40:50.078
    [It] should perform canary updates and phased rolling updates of template modifications [Conformance]
      test/e2e/apps/statefulset.go:315
    STEP: Creating a new StatefulSet 06/28/23 08:40:50.083
    Jun 28 08:40:50.093: INFO: Found 0 stateful pods, waiting for 3
    Jun 28 08:41:00.100: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Jun 28 08:41:00.100: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Jun 28 08:41:00.100: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 06/28/23 08:41:00.114
    Jun 28 08:41:00.139: INFO: Updating stateful set ss2
    STEP: Creating a new revision 06/28/23 08:41:00.139
    STEP: Not applying an update when the partition is greater than the number of replicas 06/28/23 08:41:10.162
    STEP: Performing a canary update 06/28/23 08:41:10.162
    Jun 28 08:41:10.185: INFO: Updating stateful set ss2
    Jun 28 08:41:10.196: INFO: Waiting for Pod statefulset-1053/ss2-2 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    STEP: Restoring Pods to the correct revision when they are deleted 06/28/23 08:41:20.208
    Jun 28 08:41:20.240: INFO: Found 2 stateful pods, waiting for 3
    Jun 28 08:41:30.247: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Jun 28 08:41:30.247: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Jun 28 08:41:30.247: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Performing a phased rolling update 06/28/23 08:41:30.256
    Jun 28 08:41:30.279: INFO: Updating stateful set ss2
    Jun 28 08:41:30.290: INFO: Waiting for Pod statefulset-1053/ss2-1 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    Jun 28 08:41:40.323: INFO: Updating stateful set ss2
    Jun 28 08:41:40.331: INFO: Waiting for StatefulSet statefulset-1053/ss2 to complete update
    Jun 28 08:41:40.331: INFO: Waiting for Pod statefulset-1053/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jun 28 08:41:50.344: INFO: Deleting all statefulset in ns statefulset-1053
    Jun 28 08:41:50.348: INFO: Scaling statefulset ss2 to 0
    Jun 28 08:42:00.371: INFO: Waiting for statefulset status.replicas updated to 0
    Jun 28 08:42:00.377: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jun 28 08:42:00.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-1053" for this suite. 06/28/23 08:42:00.404
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:42:00.413
Jun 28 08:42:00.413: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename container-probe 06/28/23 08:42:00.414
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:42:00.428
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:42:00.434
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211
STEP: Creating pod test-webserver-236c0a79-6b22-4c98-b76c-51b1f027956a in namespace container-probe-4816 06/28/23 08:42:00.438
Jun 28 08:42:00.447: INFO: Waiting up to 5m0s for pod "test-webserver-236c0a79-6b22-4c98-b76c-51b1f027956a" in namespace "container-probe-4816" to be "not pending"
Jun 28 08:42:00.451: INFO: Pod "test-webserver-236c0a79-6b22-4c98-b76c-51b1f027956a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.735599ms
Jun 28 08:42:02.458: INFO: Pod "test-webserver-236c0a79-6b22-4c98-b76c-51b1f027956a": Phase="Running", Reason="", readiness=true. Elapsed: 2.011268437s
Jun 28 08:42:02.458: INFO: Pod "test-webserver-236c0a79-6b22-4c98-b76c-51b1f027956a" satisfied condition "not pending"
Jun 28 08:42:02.458: INFO: Started pod test-webserver-236c0a79-6b22-4c98-b76c-51b1f027956a in namespace container-probe-4816
STEP: checking the pod's current state and verifying that restartCount is present 06/28/23 08:42:02.458
Jun 28 08:42:02.462: INFO: Initial restart count of pod test-webserver-236c0a79-6b22-4c98-b76c-51b1f027956a is 0
STEP: deleting the pod 06/28/23 08:46:03.16
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jun 28 08:46:03.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4816" for this suite. 06/28/23 08:46:03.185
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","completed":264,"skipped":4777,"failed":0}
------------------------------
â€¢ [SLOW TEST] [242.782 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:42:00.413
    Jun 28 08:42:00.413: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename container-probe 06/28/23 08:42:00.414
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:42:00.428
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:42:00.434
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:211
    STEP: Creating pod test-webserver-236c0a79-6b22-4c98-b76c-51b1f027956a in namespace container-probe-4816 06/28/23 08:42:00.438
    Jun 28 08:42:00.447: INFO: Waiting up to 5m0s for pod "test-webserver-236c0a79-6b22-4c98-b76c-51b1f027956a" in namespace "container-probe-4816" to be "not pending"
    Jun 28 08:42:00.451: INFO: Pod "test-webserver-236c0a79-6b22-4c98-b76c-51b1f027956a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.735599ms
    Jun 28 08:42:02.458: INFO: Pod "test-webserver-236c0a79-6b22-4c98-b76c-51b1f027956a": Phase="Running", Reason="", readiness=true. Elapsed: 2.011268437s
    Jun 28 08:42:02.458: INFO: Pod "test-webserver-236c0a79-6b22-4c98-b76c-51b1f027956a" satisfied condition "not pending"
    Jun 28 08:42:02.458: INFO: Started pod test-webserver-236c0a79-6b22-4c98-b76c-51b1f027956a in namespace container-probe-4816
    STEP: checking the pod's current state and verifying that restartCount is present 06/28/23 08:42:02.458
    Jun 28 08:42:02.462: INFO: Initial restart count of pod test-webserver-236c0a79-6b22-4c98-b76c-51b1f027956a is 0
    STEP: deleting the pod 06/28/23 08:46:03.16
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jun 28 08:46:03.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-4816" for this suite. 06/28/23 08:46:03.185
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:46:03.195
Jun 28 08:46:03.195: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename daemonsets 06/28/23 08:46:03.196
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:46:03.214
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:46:03.22
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431
Jun 28 08:46:03.270: INFO: Create a RollingUpdate DaemonSet
Jun 28 08:46:03.277: INFO: Check that daemon pods launch on every node of the cluster
Jun 28 08:46:03.291: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 28 08:46:03.291: INFO: Node ske-rhel-749f7d55c8xdd8b6-ct4cp is running 0 daemon pod, expected 1
Jun 28 08:46:04.308: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jun 28 08:46:04.308: INFO: Node ske-rhel-749f7d55c8xdd8b6-srshq is running 0 daemon pod, expected 1
Jun 28 08:46:05.310: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jun 28 08:46:05.310: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
Jun 28 08:46:05.310: INFO: Update the DaemonSet to trigger a rollout
Jun 28 08:46:05.324: INFO: Updating DaemonSet daemon-set
Jun 28 08:46:08.351: INFO: Roll back the DaemonSet before rollout is complete
Jun 28 08:46:08.366: INFO: Updating DaemonSet daemon-set
Jun 28 08:46:08.366: INFO: Make sure DaemonSet rollback is complete
Jun 28 08:46:08.372: INFO: Wrong image for pod: daemon-set-ncbnm. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-2, got: foo:non-existent.
Jun 28 08:46:08.372: INFO: Pod daemon-set-ncbnm is not available
Jun 28 08:46:12.394: INFO: Pod daemon-set-bkvft is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 06/28/23 08:46:12.514
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4166, will wait for the garbage collector to delete the pods 06/28/23 08:46:12.514
Jun 28 08:46:12.690: INFO: Deleting DaemonSet.extensions daemon-set took: 47.192447ms
Jun 28 08:46:12.891: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.707764ms
Jun 28 08:46:15.796: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 28 08:46:15.796: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jun 28 08:46:15.802: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"81430"},"items":null}

Jun 28 08:46:15.807: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"81430"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Jun 28 08:46:15.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4166" for this suite. 06/28/23 08:46:15.848
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","completed":265,"skipped":4788,"failed":0}
------------------------------
â€¢ [SLOW TEST] [12.663 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:46:03.195
    Jun 28 08:46:03.195: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename daemonsets 06/28/23 08:46:03.196
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:46:03.214
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:46:03.22
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should rollback without unnecessary restarts [Conformance]
      test/e2e/apps/daemon_set.go:431
    Jun 28 08:46:03.270: INFO: Create a RollingUpdate DaemonSet
    Jun 28 08:46:03.277: INFO: Check that daemon pods launch on every node of the cluster
    Jun 28 08:46:03.291: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun 28 08:46:03.291: INFO: Node ske-rhel-749f7d55c8xdd8b6-ct4cp is running 0 daemon pod, expected 1
    Jun 28 08:46:04.308: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jun 28 08:46:04.308: INFO: Node ske-rhel-749f7d55c8xdd8b6-srshq is running 0 daemon pod, expected 1
    Jun 28 08:46:05.310: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jun 28 08:46:05.310: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    Jun 28 08:46:05.310: INFO: Update the DaemonSet to trigger a rollout
    Jun 28 08:46:05.324: INFO: Updating DaemonSet daemon-set
    Jun 28 08:46:08.351: INFO: Roll back the DaemonSet before rollout is complete
    Jun 28 08:46:08.366: INFO: Updating DaemonSet daemon-set
    Jun 28 08:46:08.366: INFO: Make sure DaemonSet rollback is complete
    Jun 28 08:46:08.372: INFO: Wrong image for pod: daemon-set-ncbnm. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-2, got: foo:non-existent.
    Jun 28 08:46:08.372: INFO: Pod daemon-set-ncbnm is not available
    Jun 28 08:46:12.394: INFO: Pod daemon-set-bkvft is not available
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 06/28/23 08:46:12.514
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4166, will wait for the garbage collector to delete the pods 06/28/23 08:46:12.514
    Jun 28 08:46:12.690: INFO: Deleting DaemonSet.extensions daemon-set took: 47.192447ms
    Jun 28 08:46:12.891: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.707764ms
    Jun 28 08:46:15.796: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun 28 08:46:15.796: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jun 28 08:46:15.802: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"81430"},"items":null}

    Jun 28 08:46:15.807: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"81430"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Jun 28 08:46:15.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-4166" for this suite. 06/28/23 08:46:15.848
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:46:15.859
Jun 28 08:46:15.859: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename secrets 06/28/23 08:46:15.86
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:46:15.878
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:46:15.883
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88
STEP: Creating secret with name secret-test-map-e70aab89-6950-482f-a8e7-41c1bb410c3d 06/28/23 08:46:15.888
STEP: Creating a pod to test consume secrets 06/28/23 08:46:15.897
Jun 28 08:46:15.908: INFO: Waiting up to 5m0s for pod "pod-secrets-fb6da07a-81f1-47ce-bc81-592cb473d410" in namespace "secrets-6889" to be "Succeeded or Failed"
Jun 28 08:46:15.913: INFO: Pod "pod-secrets-fb6da07a-81f1-47ce-bc81-592cb473d410": Phase="Pending", Reason="", readiness=false. Elapsed: 4.66024ms
Jun 28 08:46:17.918: INFO: Pod "pod-secrets-fb6da07a-81f1-47ce-bc81-592cb473d410": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010219948s
Jun 28 08:46:19.919: INFO: Pod "pod-secrets-fb6da07a-81f1-47ce-bc81-592cb473d410": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010628669s
STEP: Saw pod success 06/28/23 08:46:19.919
Jun 28 08:46:19.919: INFO: Pod "pod-secrets-fb6da07a-81f1-47ce-bc81-592cb473d410" satisfied condition "Succeeded or Failed"
Jun 28 08:46:19.924: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod pod-secrets-fb6da07a-81f1-47ce-bc81-592cb473d410 container secret-volume-test: <nil>
STEP: delete the pod 06/28/23 08:46:19.974
Jun 28 08:46:20.009: INFO: Waiting for pod pod-secrets-fb6da07a-81f1-47ce-bc81-592cb473d410 to disappear
Jun 28 08:46:20.035: INFO: Pod pod-secrets-fb6da07a-81f1-47ce-bc81-592cb473d410 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jun 28 08:46:20.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6889" for this suite. 06/28/23 08:46:20.1
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":266,"skipped":4788,"failed":0}
------------------------------
â€¢ [4.266 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:46:15.859
    Jun 28 08:46:15.859: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename secrets 06/28/23 08:46:15.86
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:46:15.878
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:46:15.883
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:88
    STEP: Creating secret with name secret-test-map-e70aab89-6950-482f-a8e7-41c1bb410c3d 06/28/23 08:46:15.888
    STEP: Creating a pod to test consume secrets 06/28/23 08:46:15.897
    Jun 28 08:46:15.908: INFO: Waiting up to 5m0s for pod "pod-secrets-fb6da07a-81f1-47ce-bc81-592cb473d410" in namespace "secrets-6889" to be "Succeeded or Failed"
    Jun 28 08:46:15.913: INFO: Pod "pod-secrets-fb6da07a-81f1-47ce-bc81-592cb473d410": Phase="Pending", Reason="", readiness=false. Elapsed: 4.66024ms
    Jun 28 08:46:17.918: INFO: Pod "pod-secrets-fb6da07a-81f1-47ce-bc81-592cb473d410": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010219948s
    Jun 28 08:46:19.919: INFO: Pod "pod-secrets-fb6da07a-81f1-47ce-bc81-592cb473d410": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010628669s
    STEP: Saw pod success 06/28/23 08:46:19.919
    Jun 28 08:46:19.919: INFO: Pod "pod-secrets-fb6da07a-81f1-47ce-bc81-592cb473d410" satisfied condition "Succeeded or Failed"
    Jun 28 08:46:19.924: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod pod-secrets-fb6da07a-81f1-47ce-bc81-592cb473d410 container secret-volume-test: <nil>
    STEP: delete the pod 06/28/23 08:46:19.974
    Jun 28 08:46:20.009: INFO: Waiting for pod pod-secrets-fb6da07a-81f1-47ce-bc81-592cb473d410 to disappear
    Jun 28 08:46:20.035: INFO: Pod pod-secrets-fb6da07a-81f1-47ce-bc81-592cb473d410 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jun 28 08:46:20.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-6889" for this suite. 06/28/23 08:46:20.1
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:350
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:46:20.125
Jun 28 08:46:20.125: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename kubectl 06/28/23 08:46:20.125
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:46:20.233
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:46:20.242
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:324
[It] should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:350
STEP: creating a replication controller 06/28/23 08:46:20.261
Jun 28 08:46:20.262: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-9958 create -f -'
Jun 28 08:46:20.628: INFO: stderr: ""
Jun 28 08:46:20.628: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 06/28/23 08:46:20.628
Jun 28 08:46:20.628: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-9958 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun 28 08:46:20.752: INFO: stderr: ""
Jun 28 08:46:20.752: INFO: stdout: "update-demo-nautilus-d76js update-demo-nautilus-h64bc "
Jun 28 08:46:20.752: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-9958 get pods update-demo-nautilus-d76js -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 28 08:46:20.843: INFO: stderr: ""
Jun 28 08:46:20.843: INFO: stdout: ""
Jun 28 08:46:20.843: INFO: update-demo-nautilus-d76js is created but not running
Jun 28 08:46:25.843: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-9958 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun 28 08:46:26.062: INFO: stderr: ""
Jun 28 08:46:26.062: INFO: stdout: "update-demo-nautilus-d76js update-demo-nautilus-h64bc "
Jun 28 08:46:26.062: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-9958 get pods update-demo-nautilus-d76js -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 28 08:46:26.156: INFO: stderr: ""
Jun 28 08:46:26.156: INFO: stdout: "true"
Jun 28 08:46:26.156: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-9958 get pods update-demo-nautilus-d76js -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun 28 08:46:26.222: INFO: stderr: ""
Jun 28 08:46:26.222: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jun 28 08:46:26.222: INFO: validating pod update-demo-nautilus-d76js
Jun 28 08:46:26.319: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 28 08:46:26.319: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 28 08:46:26.319: INFO: update-demo-nautilus-d76js is verified up and running
Jun 28 08:46:26.319: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-9958 get pods update-demo-nautilus-h64bc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 28 08:46:26.418: INFO: stderr: ""
Jun 28 08:46:26.418: INFO: stdout: "true"
Jun 28 08:46:26.419: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-9958 get pods update-demo-nautilus-h64bc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun 28 08:46:26.487: INFO: stderr: ""
Jun 28 08:46:26.487: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jun 28 08:46:26.487: INFO: validating pod update-demo-nautilus-h64bc
Jun 28 08:46:26.580: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 28 08:46:26.580: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 28 08:46:26.580: INFO: update-demo-nautilus-h64bc is verified up and running
STEP: scaling down the replication controller 06/28/23 08:46:26.58
Jun 28 08:46:26.581: INFO: scanned /root for discovery docs: <nil>
Jun 28 08:46:26.581: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-9958 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Jun 28 08:46:27.678: INFO: stderr: ""
Jun 28 08:46:27.678: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 06/28/23 08:46:27.678
Jun 28 08:46:27.678: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-9958 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun 28 08:46:27.749: INFO: stderr: ""
Jun 28 08:46:27.749: INFO: stdout: "update-demo-nautilus-d76js update-demo-nautilus-h64bc "
STEP: Replicas for name=update-demo: expected=1 actual=2 06/28/23 08:46:27.749
Jun 28 08:46:32.750: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-9958 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun 28 08:46:32.817: INFO: stderr: ""
Jun 28 08:46:32.817: INFO: stdout: "update-demo-nautilus-d76js "
Jun 28 08:46:32.817: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-9958 get pods update-demo-nautilus-d76js -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 28 08:46:32.885: INFO: stderr: ""
Jun 28 08:46:32.885: INFO: stdout: "true"
Jun 28 08:46:32.885: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-9958 get pods update-demo-nautilus-d76js -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun 28 08:46:32.951: INFO: stderr: ""
Jun 28 08:46:32.951: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jun 28 08:46:32.951: INFO: validating pod update-demo-nautilus-d76js
Jun 28 08:46:32.959: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 28 08:46:32.959: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 28 08:46:32.959: INFO: update-demo-nautilus-d76js is verified up and running
STEP: scaling up the replication controller 06/28/23 08:46:32.959
Jun 28 08:46:32.960: INFO: scanned /root for discovery docs: <nil>
Jun 28 08:46:32.960: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-9958 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Jun 28 08:46:34.051: INFO: stderr: ""
Jun 28 08:46:34.051: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 06/28/23 08:46:34.051
Jun 28 08:46:34.051: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-9958 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun 28 08:46:34.120: INFO: stderr: ""
Jun 28 08:46:34.120: INFO: stdout: "update-demo-nautilus-d76js update-demo-nautilus-nvg2c "
Jun 28 08:46:34.120: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-9958 get pods update-demo-nautilus-d76js -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 28 08:46:34.207: INFO: stderr: ""
Jun 28 08:46:34.207: INFO: stdout: "true"
Jun 28 08:46:34.207: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-9958 get pods update-demo-nautilus-d76js -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun 28 08:46:34.274: INFO: stderr: ""
Jun 28 08:46:34.274: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jun 28 08:46:34.274: INFO: validating pod update-demo-nautilus-d76js
Jun 28 08:46:34.283: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 28 08:46:34.283: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 28 08:46:34.283: INFO: update-demo-nautilus-d76js is verified up and running
Jun 28 08:46:34.283: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-9958 get pods update-demo-nautilus-nvg2c -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 28 08:46:34.469: INFO: stderr: ""
Jun 28 08:46:34.469: INFO: stdout: ""
Jun 28 08:46:34.469: INFO: update-demo-nautilus-nvg2c is created but not running
Jun 28 08:46:39.471: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-9958 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun 28 08:46:39.540: INFO: stderr: ""
Jun 28 08:46:39.540: INFO: stdout: "update-demo-nautilus-d76js update-demo-nautilus-nvg2c "
Jun 28 08:46:39.540: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-9958 get pods update-demo-nautilus-d76js -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 28 08:46:39.607: INFO: stderr: ""
Jun 28 08:46:39.607: INFO: stdout: "true"
Jun 28 08:46:39.607: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-9958 get pods update-demo-nautilus-d76js -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun 28 08:46:39.676: INFO: stderr: ""
Jun 28 08:46:39.676: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jun 28 08:46:39.676: INFO: validating pod update-demo-nautilus-d76js
Jun 28 08:46:39.685: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 28 08:46:39.685: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 28 08:46:39.685: INFO: update-demo-nautilus-d76js is verified up and running
Jun 28 08:46:39.685: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-9958 get pods update-demo-nautilus-nvg2c -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 28 08:46:39.875: INFO: stderr: ""
Jun 28 08:46:39.875: INFO: stdout: "true"
Jun 28 08:46:39.875: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-9958 get pods update-demo-nautilus-nvg2c -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun 28 08:46:39.970: INFO: stderr: ""
Jun 28 08:46:39.970: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jun 28 08:46:39.970: INFO: validating pod update-demo-nautilus-nvg2c
Jun 28 08:46:40.143: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 28 08:46:40.143: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 28 08:46:40.143: INFO: update-demo-nautilus-nvg2c is verified up and running
STEP: using delete to clean up resources 06/28/23 08:46:40.143
Jun 28 08:46:40.143: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-9958 delete --grace-period=0 --force -f -'
Jun 28 08:46:40.386: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 28 08:46:40.386: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jun 28 08:46:40.386: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-9958 get rc,svc -l name=update-demo --no-headers'
Jun 28 08:46:40.548: INFO: stderr: "No resources found in kubectl-9958 namespace.\n"
Jun 28 08:46:40.548: INFO: stdout: ""
Jun 28 08:46:40.548: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-9958 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun 28 08:46:40.737: INFO: stderr: ""
Jun 28 08:46:40.737: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jun 28 08:46:40.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9958" for this suite. 06/28/23 08:46:40.754
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","completed":267,"skipped":4793,"failed":0}
------------------------------
â€¢ [SLOW TEST] [20.636 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:322
    should scale a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:350

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:46:20.125
    Jun 28 08:46:20.125: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename kubectl 06/28/23 08:46:20.125
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:46:20.233
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:46:20.242
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:324
    [It] should scale a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:350
    STEP: creating a replication controller 06/28/23 08:46:20.261
    Jun 28 08:46:20.262: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-9958 create -f -'
    Jun 28 08:46:20.628: INFO: stderr: ""
    Jun 28 08:46:20.628: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 06/28/23 08:46:20.628
    Jun 28 08:46:20.628: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-9958 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jun 28 08:46:20.752: INFO: stderr: ""
    Jun 28 08:46:20.752: INFO: stdout: "update-demo-nautilus-d76js update-demo-nautilus-h64bc "
    Jun 28 08:46:20.752: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-9958 get pods update-demo-nautilus-d76js -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jun 28 08:46:20.843: INFO: stderr: ""
    Jun 28 08:46:20.843: INFO: stdout: ""
    Jun 28 08:46:20.843: INFO: update-demo-nautilus-d76js is created but not running
    Jun 28 08:46:25.843: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-9958 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jun 28 08:46:26.062: INFO: stderr: ""
    Jun 28 08:46:26.062: INFO: stdout: "update-demo-nautilus-d76js update-demo-nautilus-h64bc "
    Jun 28 08:46:26.062: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-9958 get pods update-demo-nautilus-d76js -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jun 28 08:46:26.156: INFO: stderr: ""
    Jun 28 08:46:26.156: INFO: stdout: "true"
    Jun 28 08:46:26.156: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-9958 get pods update-demo-nautilus-d76js -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jun 28 08:46:26.222: INFO: stderr: ""
    Jun 28 08:46:26.222: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jun 28 08:46:26.222: INFO: validating pod update-demo-nautilus-d76js
    Jun 28 08:46:26.319: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jun 28 08:46:26.319: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jun 28 08:46:26.319: INFO: update-demo-nautilus-d76js is verified up and running
    Jun 28 08:46:26.319: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-9958 get pods update-demo-nautilus-h64bc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jun 28 08:46:26.418: INFO: stderr: ""
    Jun 28 08:46:26.418: INFO: stdout: "true"
    Jun 28 08:46:26.419: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-9958 get pods update-demo-nautilus-h64bc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jun 28 08:46:26.487: INFO: stderr: ""
    Jun 28 08:46:26.487: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jun 28 08:46:26.487: INFO: validating pod update-demo-nautilus-h64bc
    Jun 28 08:46:26.580: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jun 28 08:46:26.580: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jun 28 08:46:26.580: INFO: update-demo-nautilus-h64bc is verified up and running
    STEP: scaling down the replication controller 06/28/23 08:46:26.58
    Jun 28 08:46:26.581: INFO: scanned /root for discovery docs: <nil>
    Jun 28 08:46:26.581: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-9958 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
    Jun 28 08:46:27.678: INFO: stderr: ""
    Jun 28 08:46:27.678: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 06/28/23 08:46:27.678
    Jun 28 08:46:27.678: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-9958 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jun 28 08:46:27.749: INFO: stderr: ""
    Jun 28 08:46:27.749: INFO: stdout: "update-demo-nautilus-d76js update-demo-nautilus-h64bc "
    STEP: Replicas for name=update-demo: expected=1 actual=2 06/28/23 08:46:27.749
    Jun 28 08:46:32.750: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-9958 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jun 28 08:46:32.817: INFO: stderr: ""
    Jun 28 08:46:32.817: INFO: stdout: "update-demo-nautilus-d76js "
    Jun 28 08:46:32.817: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-9958 get pods update-demo-nautilus-d76js -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jun 28 08:46:32.885: INFO: stderr: ""
    Jun 28 08:46:32.885: INFO: stdout: "true"
    Jun 28 08:46:32.885: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-9958 get pods update-demo-nautilus-d76js -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jun 28 08:46:32.951: INFO: stderr: ""
    Jun 28 08:46:32.951: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jun 28 08:46:32.951: INFO: validating pod update-demo-nautilus-d76js
    Jun 28 08:46:32.959: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jun 28 08:46:32.959: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jun 28 08:46:32.959: INFO: update-demo-nautilus-d76js is verified up and running
    STEP: scaling up the replication controller 06/28/23 08:46:32.959
    Jun 28 08:46:32.960: INFO: scanned /root for discovery docs: <nil>
    Jun 28 08:46:32.960: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-9958 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
    Jun 28 08:46:34.051: INFO: stderr: ""
    Jun 28 08:46:34.051: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 06/28/23 08:46:34.051
    Jun 28 08:46:34.051: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-9958 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jun 28 08:46:34.120: INFO: stderr: ""
    Jun 28 08:46:34.120: INFO: stdout: "update-demo-nautilus-d76js update-demo-nautilus-nvg2c "
    Jun 28 08:46:34.120: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-9958 get pods update-demo-nautilus-d76js -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jun 28 08:46:34.207: INFO: stderr: ""
    Jun 28 08:46:34.207: INFO: stdout: "true"
    Jun 28 08:46:34.207: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-9958 get pods update-demo-nautilus-d76js -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jun 28 08:46:34.274: INFO: stderr: ""
    Jun 28 08:46:34.274: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jun 28 08:46:34.274: INFO: validating pod update-demo-nautilus-d76js
    Jun 28 08:46:34.283: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jun 28 08:46:34.283: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jun 28 08:46:34.283: INFO: update-demo-nautilus-d76js is verified up and running
    Jun 28 08:46:34.283: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-9958 get pods update-demo-nautilus-nvg2c -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jun 28 08:46:34.469: INFO: stderr: ""
    Jun 28 08:46:34.469: INFO: stdout: ""
    Jun 28 08:46:34.469: INFO: update-demo-nautilus-nvg2c is created but not running
    Jun 28 08:46:39.471: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-9958 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jun 28 08:46:39.540: INFO: stderr: ""
    Jun 28 08:46:39.540: INFO: stdout: "update-demo-nautilus-d76js update-demo-nautilus-nvg2c "
    Jun 28 08:46:39.540: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-9958 get pods update-demo-nautilus-d76js -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jun 28 08:46:39.607: INFO: stderr: ""
    Jun 28 08:46:39.607: INFO: stdout: "true"
    Jun 28 08:46:39.607: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-9958 get pods update-demo-nautilus-d76js -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jun 28 08:46:39.676: INFO: stderr: ""
    Jun 28 08:46:39.676: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jun 28 08:46:39.676: INFO: validating pod update-demo-nautilus-d76js
    Jun 28 08:46:39.685: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jun 28 08:46:39.685: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jun 28 08:46:39.685: INFO: update-demo-nautilus-d76js is verified up and running
    Jun 28 08:46:39.685: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-9958 get pods update-demo-nautilus-nvg2c -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jun 28 08:46:39.875: INFO: stderr: ""
    Jun 28 08:46:39.875: INFO: stdout: "true"
    Jun 28 08:46:39.875: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-9958 get pods update-demo-nautilus-nvg2c -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jun 28 08:46:39.970: INFO: stderr: ""
    Jun 28 08:46:39.970: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jun 28 08:46:39.970: INFO: validating pod update-demo-nautilus-nvg2c
    Jun 28 08:46:40.143: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jun 28 08:46:40.143: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jun 28 08:46:40.143: INFO: update-demo-nautilus-nvg2c is verified up and running
    STEP: using delete to clean up resources 06/28/23 08:46:40.143
    Jun 28 08:46:40.143: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-9958 delete --grace-period=0 --force -f -'
    Jun 28 08:46:40.386: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jun 28 08:46:40.386: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Jun 28 08:46:40.386: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-9958 get rc,svc -l name=update-demo --no-headers'
    Jun 28 08:46:40.548: INFO: stderr: "No resources found in kubectl-9958 namespace.\n"
    Jun 28 08:46:40.548: INFO: stdout: ""
    Jun 28 08:46:40.548: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-9958 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Jun 28 08:46:40.737: INFO: stderr: ""
    Jun 28 08:46:40.737: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jun 28 08:46:40.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-9958" for this suite. 06/28/23 08:46:40.754
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:46:40.761
Jun 28 08:46:40.762: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename configmap 06/28/23 08:46:40.762
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:46:40.781
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:46:40.79
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88
STEP: Creating configMap with name configmap-test-volume-map-9e3bd6ed-84a7-4983-a881-dd0341db9c8f 06/28/23 08:46:40.795
STEP: Creating a pod to test consume configMaps 06/28/23 08:46:40.804
Jun 28 08:46:40.813: INFO: Waiting up to 5m0s for pod "pod-configmaps-21d1bef8-5168-4317-aef1-69c627823d0a" in namespace "configmap-6487" to be "Succeeded or Failed"
Jun 28 08:46:40.817: INFO: Pod "pod-configmaps-21d1bef8-5168-4317-aef1-69c627823d0a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.191014ms
Jun 28 08:46:42.844: INFO: Pod "pod-configmaps-21d1bef8-5168-4317-aef1-69c627823d0a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031634358s
Jun 28 08:46:44.823: INFO: Pod "pod-configmaps-21d1bef8-5168-4317-aef1-69c627823d0a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010055988s
STEP: Saw pod success 06/28/23 08:46:44.823
Jun 28 08:46:44.823: INFO: Pod "pod-configmaps-21d1bef8-5168-4317-aef1-69c627823d0a" satisfied condition "Succeeded or Failed"
Jun 28 08:46:44.828: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod pod-configmaps-21d1bef8-5168-4317-aef1-69c627823d0a container agnhost-container: <nil>
STEP: delete the pod 06/28/23 08:46:44.841
Jun 28 08:46:44.855: INFO: Waiting for pod pod-configmaps-21d1bef8-5168-4317-aef1-69c627823d0a to disappear
Jun 28 08:46:44.859: INFO: Pod pod-configmaps-21d1bef8-5168-4317-aef1-69c627823d0a no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jun 28 08:46:44.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6487" for this suite. 06/28/23 08:46:44.873
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":268,"skipped":4817,"failed":0}
------------------------------
â€¢ [4.120 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:46:40.761
    Jun 28 08:46:40.762: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename configmap 06/28/23 08:46:40.762
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:46:40.781
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:46:40.79
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:88
    STEP: Creating configMap with name configmap-test-volume-map-9e3bd6ed-84a7-4983-a881-dd0341db9c8f 06/28/23 08:46:40.795
    STEP: Creating a pod to test consume configMaps 06/28/23 08:46:40.804
    Jun 28 08:46:40.813: INFO: Waiting up to 5m0s for pod "pod-configmaps-21d1bef8-5168-4317-aef1-69c627823d0a" in namespace "configmap-6487" to be "Succeeded or Failed"
    Jun 28 08:46:40.817: INFO: Pod "pod-configmaps-21d1bef8-5168-4317-aef1-69c627823d0a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.191014ms
    Jun 28 08:46:42.844: INFO: Pod "pod-configmaps-21d1bef8-5168-4317-aef1-69c627823d0a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031634358s
    Jun 28 08:46:44.823: INFO: Pod "pod-configmaps-21d1bef8-5168-4317-aef1-69c627823d0a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010055988s
    STEP: Saw pod success 06/28/23 08:46:44.823
    Jun 28 08:46:44.823: INFO: Pod "pod-configmaps-21d1bef8-5168-4317-aef1-69c627823d0a" satisfied condition "Succeeded or Failed"
    Jun 28 08:46:44.828: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod pod-configmaps-21d1bef8-5168-4317-aef1-69c627823d0a container agnhost-container: <nil>
    STEP: delete the pod 06/28/23 08:46:44.841
    Jun 28 08:46:44.855: INFO: Waiting for pod pod-configmaps-21d1bef8-5168-4317-aef1-69c627823d0a to disappear
    Jun 28 08:46:44.859: INFO: Pod pod-configmaps-21d1bef8-5168-4317-aef1-69c627823d0a no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jun 28 08:46:44.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-6487" for this suite. 06/28/23 08:46:44.873
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:46:44.882
Jun 28 08:46:44.882: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename replicaset 06/28/23 08:46:44.883
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:46:44.9
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:46:44.905
[It] should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
STEP: Create a Replicaset 06/28/23 08:46:44.915
STEP: Verify that the required pods have come up. 06/28/23 08:46:44.922
Jun 28 08:46:44.926: INFO: Pod name sample-pod: Found 0 pods out of 1
Jun 28 08:46:49.934: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 06/28/23 08:46:49.934
STEP: Getting /status 06/28/23 08:46:49.934
Jun 28 08:46:49.940: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status 06/28/23 08:46:49.94
Jun 28 08:46:49.953: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated 06/28/23 08:46:49.953
Jun 28 08:46:49.956: INFO: Observed &ReplicaSet event: ADDED
Jun 28 08:46:49.956: INFO: Observed &ReplicaSet event: MODIFIED
Jun 28 08:46:49.956: INFO: Observed &ReplicaSet event: MODIFIED
Jun 28 08:46:49.956: INFO: Observed &ReplicaSet event: MODIFIED
Jun 28 08:46:49.956: INFO: Found replicaset test-rs in namespace replicaset-7385 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jun 28 08:46:49.956: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status 06/28/23 08:46:49.956
Jun 28 08:46:49.956: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jun 28 08:46:49.965: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched 06/28/23 08:46:49.965
Jun 28 08:46:49.968: INFO: Observed &ReplicaSet event: ADDED
Jun 28 08:46:49.968: INFO: Observed &ReplicaSet event: MODIFIED
Jun 28 08:46:49.968: INFO: Observed &ReplicaSet event: MODIFIED
Jun 28 08:46:49.969: INFO: Observed &ReplicaSet event: MODIFIED
Jun 28 08:46:49.969: INFO: Observed replicaset test-rs in namespace replicaset-7385 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jun 28 08:46:49.969: INFO: Observed &ReplicaSet event: MODIFIED
Jun 28 08:46:49.969: INFO: Found replicaset test-rs in namespace replicaset-7385 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Jun 28 08:46:49.969: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Jun 28 08:46:49.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-7385" for this suite. 06/28/23 08:46:49.979
{"msg":"PASSED [sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]","completed":269,"skipped":4825,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.105 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:46:44.882
    Jun 28 08:46:44.882: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename replicaset 06/28/23 08:46:44.883
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:46:44.9
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:46:44.905
    [It] should validate Replicaset Status endpoints [Conformance]
      test/e2e/apps/replica_set.go:176
    STEP: Create a Replicaset 06/28/23 08:46:44.915
    STEP: Verify that the required pods have come up. 06/28/23 08:46:44.922
    Jun 28 08:46:44.926: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jun 28 08:46:49.934: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 06/28/23 08:46:49.934
    STEP: Getting /status 06/28/23 08:46:49.934
    Jun 28 08:46:49.940: INFO: Replicaset test-rs has Conditions: []
    STEP: updating the Replicaset Status 06/28/23 08:46:49.94
    Jun 28 08:46:49.953: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the ReplicaSet status to be updated 06/28/23 08:46:49.953
    Jun 28 08:46:49.956: INFO: Observed &ReplicaSet event: ADDED
    Jun 28 08:46:49.956: INFO: Observed &ReplicaSet event: MODIFIED
    Jun 28 08:46:49.956: INFO: Observed &ReplicaSet event: MODIFIED
    Jun 28 08:46:49.956: INFO: Observed &ReplicaSet event: MODIFIED
    Jun 28 08:46:49.956: INFO: Found replicaset test-rs in namespace replicaset-7385 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jun 28 08:46:49.956: INFO: Replicaset test-rs has an updated status
    STEP: patching the Replicaset Status 06/28/23 08:46:49.956
    Jun 28 08:46:49.956: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Jun 28 08:46:49.965: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Replicaset status to be patched 06/28/23 08:46:49.965
    Jun 28 08:46:49.968: INFO: Observed &ReplicaSet event: ADDED
    Jun 28 08:46:49.968: INFO: Observed &ReplicaSet event: MODIFIED
    Jun 28 08:46:49.968: INFO: Observed &ReplicaSet event: MODIFIED
    Jun 28 08:46:49.969: INFO: Observed &ReplicaSet event: MODIFIED
    Jun 28 08:46:49.969: INFO: Observed replicaset test-rs in namespace replicaset-7385 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jun 28 08:46:49.969: INFO: Observed &ReplicaSet event: MODIFIED
    Jun 28 08:46:49.969: INFO: Found replicaset test-rs in namespace replicaset-7385 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
    Jun 28 08:46:49.969: INFO: Replicaset test-rs has a patched status
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Jun 28 08:46:49.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-7385" for this suite. 06/28/23 08:46:49.979
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:46:49.989
Jun 28 08:46:49.989: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename configmap 06/28/23 08:46:49.989
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:46:50.003
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:46:50.007
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92
STEP: Creating configMap configmap-2052/configmap-test-f09a90fd-1fb4-4932-95e8-0f9be36d811c 06/28/23 08:46:50.013
STEP: Creating a pod to test consume configMaps 06/28/23 08:46:50.019
Jun 28 08:46:50.028: INFO: Waiting up to 5m0s for pod "pod-configmaps-db586b59-7944-492d-b239-a546aa67b263" in namespace "configmap-2052" to be "Succeeded or Failed"
Jun 28 08:46:50.034: INFO: Pod "pod-configmaps-db586b59-7944-492d-b239-a546aa67b263": Phase="Pending", Reason="", readiness=false. Elapsed: 5.220386ms
Jun 28 08:46:52.039: INFO: Pod "pod-configmaps-db586b59-7944-492d-b239-a546aa67b263": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010689216s
Jun 28 08:46:54.039: INFO: Pod "pod-configmaps-db586b59-7944-492d-b239-a546aa67b263": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010388764s
STEP: Saw pod success 06/28/23 08:46:54.039
Jun 28 08:46:54.039: INFO: Pod "pod-configmaps-db586b59-7944-492d-b239-a546aa67b263" satisfied condition "Succeeded or Failed"
Jun 28 08:46:54.043: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod pod-configmaps-db586b59-7944-492d-b239-a546aa67b263 container env-test: <nil>
STEP: delete the pod 06/28/23 08:46:54.071
Jun 28 08:46:54.084: INFO: Waiting for pod pod-configmaps-db586b59-7944-492d-b239-a546aa67b263 to disappear
Jun 28 08:46:54.088: INFO: Pod pod-configmaps-db586b59-7944-492d-b239-a546aa67b263 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Jun 28 08:46:54.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2052" for this suite. 06/28/23 08:46:54.097
{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","completed":270,"skipped":4864,"failed":0}
------------------------------
â€¢ [4.114 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:46:49.989
    Jun 28 08:46:49.989: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename configmap 06/28/23 08:46:49.989
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:46:50.003
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:46:50.007
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:92
    STEP: Creating configMap configmap-2052/configmap-test-f09a90fd-1fb4-4932-95e8-0f9be36d811c 06/28/23 08:46:50.013
    STEP: Creating a pod to test consume configMaps 06/28/23 08:46:50.019
    Jun 28 08:46:50.028: INFO: Waiting up to 5m0s for pod "pod-configmaps-db586b59-7944-492d-b239-a546aa67b263" in namespace "configmap-2052" to be "Succeeded or Failed"
    Jun 28 08:46:50.034: INFO: Pod "pod-configmaps-db586b59-7944-492d-b239-a546aa67b263": Phase="Pending", Reason="", readiness=false. Elapsed: 5.220386ms
    Jun 28 08:46:52.039: INFO: Pod "pod-configmaps-db586b59-7944-492d-b239-a546aa67b263": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010689216s
    Jun 28 08:46:54.039: INFO: Pod "pod-configmaps-db586b59-7944-492d-b239-a546aa67b263": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010388764s
    STEP: Saw pod success 06/28/23 08:46:54.039
    Jun 28 08:46:54.039: INFO: Pod "pod-configmaps-db586b59-7944-492d-b239-a546aa67b263" satisfied condition "Succeeded or Failed"
    Jun 28 08:46:54.043: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod pod-configmaps-db586b59-7944-492d-b239-a546aa67b263 container env-test: <nil>
    STEP: delete the pod 06/28/23 08:46:54.071
    Jun 28 08:46:54.084: INFO: Waiting for pod pod-configmaps-db586b59-7944-492d-b239-a546aa67b263 to disappear
    Jun 28 08:46:54.088: INFO: Pod pod-configmaps-db586b59-7944-492d-b239-a546aa67b263 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Jun 28 08:46:54.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-2052" for this suite. 06/28/23 08:46:54.097
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:46:54.103
Jun 28 08:46:54.103: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename subpath 06/28/23 08:46:54.104
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:46:54.118
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:46:54.123
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 06/28/23 08:46:54.128
[It] should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
STEP: Creating pod pod-subpath-test-configmap-8lf9 06/28/23 08:46:54.138
STEP: Creating a pod to test atomic-volume-subpath 06/28/23 08:46:54.138
Jun 28 08:46:54.147: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-8lf9" in namespace "subpath-2111" to be "Succeeded or Failed"
Jun 28 08:46:54.151: INFO: Pod "pod-subpath-test-configmap-8lf9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.668589ms
Jun 28 08:46:56.156: INFO: Pod "pod-subpath-test-configmap-8lf9": Phase="Running", Reason="", readiness=true. Elapsed: 2.008871696s
Jun 28 08:46:58.158: INFO: Pod "pod-subpath-test-configmap-8lf9": Phase="Running", Reason="", readiness=true. Elapsed: 4.010497572s
Jun 28 08:47:00.157: INFO: Pod "pod-subpath-test-configmap-8lf9": Phase="Running", Reason="", readiness=true. Elapsed: 6.009692446s
Jun 28 08:47:02.156: INFO: Pod "pod-subpath-test-configmap-8lf9": Phase="Running", Reason="", readiness=true. Elapsed: 8.009234427s
Jun 28 08:47:04.157: INFO: Pod "pod-subpath-test-configmap-8lf9": Phase="Running", Reason="", readiness=true. Elapsed: 10.009842613s
Jun 28 08:47:06.159: INFO: Pod "pod-subpath-test-configmap-8lf9": Phase="Running", Reason="", readiness=true. Elapsed: 12.011485448s
Jun 28 08:47:08.157: INFO: Pod "pod-subpath-test-configmap-8lf9": Phase="Running", Reason="", readiness=true. Elapsed: 14.009751553s
Jun 28 08:47:10.158: INFO: Pod "pod-subpath-test-configmap-8lf9": Phase="Running", Reason="", readiness=true. Elapsed: 16.010352132s
Jun 28 08:47:12.157: INFO: Pod "pod-subpath-test-configmap-8lf9": Phase="Running", Reason="", readiness=true. Elapsed: 18.009929819s
Jun 28 08:47:14.158: INFO: Pod "pod-subpath-test-configmap-8lf9": Phase="Running", Reason="", readiness=true. Elapsed: 20.0106544s
Jun 28 08:47:16.158: INFO: Pod "pod-subpath-test-configmap-8lf9": Phase="Running", Reason="", readiness=false. Elapsed: 22.011066286s
Jun 28 08:47:18.157: INFO: Pod "pod-subpath-test-configmap-8lf9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.009715314s
STEP: Saw pod success 06/28/23 08:47:18.157
Jun 28 08:47:18.157: INFO: Pod "pod-subpath-test-configmap-8lf9" satisfied condition "Succeeded or Failed"
Jun 28 08:47:18.162: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod pod-subpath-test-configmap-8lf9 container test-container-subpath-configmap-8lf9: <nil>
STEP: delete the pod 06/28/23 08:47:18.173
Jun 28 08:47:18.185: INFO: Waiting for pod pod-subpath-test-configmap-8lf9 to disappear
Jun 28 08:47:18.189: INFO: Pod pod-subpath-test-configmap-8lf9 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-8lf9 06/28/23 08:47:18.189
Jun 28 08:47:18.189: INFO: Deleting pod "pod-subpath-test-configmap-8lf9" in namespace "subpath-2111"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Jun 28 08:47:18.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2111" for this suite. 06/28/23 08:47:18.202
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [Conformance]","completed":271,"skipped":4870,"failed":0}
------------------------------
â€¢ [SLOW TEST] [24.105 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod [Conformance]
    test/e2e/storage/subpath.go:70

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:46:54.103
    Jun 28 08:46:54.103: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename subpath 06/28/23 08:46:54.104
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:46:54.118
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:46:54.123
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 06/28/23 08:46:54.128
    [It] should support subpaths with configmap pod [Conformance]
      test/e2e/storage/subpath.go:70
    STEP: Creating pod pod-subpath-test-configmap-8lf9 06/28/23 08:46:54.138
    STEP: Creating a pod to test atomic-volume-subpath 06/28/23 08:46:54.138
    Jun 28 08:46:54.147: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-8lf9" in namespace "subpath-2111" to be "Succeeded or Failed"
    Jun 28 08:46:54.151: INFO: Pod "pod-subpath-test-configmap-8lf9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.668589ms
    Jun 28 08:46:56.156: INFO: Pod "pod-subpath-test-configmap-8lf9": Phase="Running", Reason="", readiness=true. Elapsed: 2.008871696s
    Jun 28 08:46:58.158: INFO: Pod "pod-subpath-test-configmap-8lf9": Phase="Running", Reason="", readiness=true. Elapsed: 4.010497572s
    Jun 28 08:47:00.157: INFO: Pod "pod-subpath-test-configmap-8lf9": Phase="Running", Reason="", readiness=true. Elapsed: 6.009692446s
    Jun 28 08:47:02.156: INFO: Pod "pod-subpath-test-configmap-8lf9": Phase="Running", Reason="", readiness=true. Elapsed: 8.009234427s
    Jun 28 08:47:04.157: INFO: Pod "pod-subpath-test-configmap-8lf9": Phase="Running", Reason="", readiness=true. Elapsed: 10.009842613s
    Jun 28 08:47:06.159: INFO: Pod "pod-subpath-test-configmap-8lf9": Phase="Running", Reason="", readiness=true. Elapsed: 12.011485448s
    Jun 28 08:47:08.157: INFO: Pod "pod-subpath-test-configmap-8lf9": Phase="Running", Reason="", readiness=true. Elapsed: 14.009751553s
    Jun 28 08:47:10.158: INFO: Pod "pod-subpath-test-configmap-8lf9": Phase="Running", Reason="", readiness=true. Elapsed: 16.010352132s
    Jun 28 08:47:12.157: INFO: Pod "pod-subpath-test-configmap-8lf9": Phase="Running", Reason="", readiness=true. Elapsed: 18.009929819s
    Jun 28 08:47:14.158: INFO: Pod "pod-subpath-test-configmap-8lf9": Phase="Running", Reason="", readiness=true. Elapsed: 20.0106544s
    Jun 28 08:47:16.158: INFO: Pod "pod-subpath-test-configmap-8lf9": Phase="Running", Reason="", readiness=false. Elapsed: 22.011066286s
    Jun 28 08:47:18.157: INFO: Pod "pod-subpath-test-configmap-8lf9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.009715314s
    STEP: Saw pod success 06/28/23 08:47:18.157
    Jun 28 08:47:18.157: INFO: Pod "pod-subpath-test-configmap-8lf9" satisfied condition "Succeeded or Failed"
    Jun 28 08:47:18.162: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod pod-subpath-test-configmap-8lf9 container test-container-subpath-configmap-8lf9: <nil>
    STEP: delete the pod 06/28/23 08:47:18.173
    Jun 28 08:47:18.185: INFO: Waiting for pod pod-subpath-test-configmap-8lf9 to disappear
    Jun 28 08:47:18.189: INFO: Pod pod-subpath-test-configmap-8lf9 no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-8lf9 06/28/23 08:47:18.189
    Jun 28 08:47:18.189: INFO: Deleting pod "pod-subpath-test-configmap-8lf9" in namespace "subpath-2111"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Jun 28 08:47:18.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-2111" for this suite. 06/28/23 08:47:18.202
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:47:18.209
Jun 28 08:47:18.209: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename projected 06/28/23 08:47:18.21
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:47:18.223
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:47:18.228
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206
STEP: Creating a pod to test downward API volume plugin 06/28/23 08:47:18.232
Jun 28 08:47:18.241: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bc1f1a37-a3be-499b-86ed-43f4bf2ffff3" in namespace "projected-5735" to be "Succeeded or Failed"
Jun 28 08:47:18.245: INFO: Pod "downwardapi-volume-bc1f1a37-a3be-499b-86ed-43f4bf2ffff3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.256307ms
Jun 28 08:47:20.251: INFO: Pod "downwardapi-volume-bc1f1a37-a3be-499b-86ed-43f4bf2ffff3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010444453s
Jun 28 08:47:22.250: INFO: Pod "downwardapi-volume-bc1f1a37-a3be-499b-86ed-43f4bf2ffff3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009364995s
STEP: Saw pod success 06/28/23 08:47:22.25
Jun 28 08:47:22.250: INFO: Pod "downwardapi-volume-bc1f1a37-a3be-499b-86ed-43f4bf2ffff3" satisfied condition "Succeeded or Failed"
Jun 28 08:47:22.255: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod downwardapi-volume-bc1f1a37-a3be-499b-86ed-43f4bf2ffff3 container client-container: <nil>
STEP: delete the pod 06/28/23 08:47:22.268
Jun 28 08:47:22.282: INFO: Waiting for pod downwardapi-volume-bc1f1a37-a3be-499b-86ed-43f4bf2ffff3 to disappear
Jun 28 08:47:22.286: INFO: Pod downwardapi-volume-bc1f1a37-a3be-499b-86ed-43f4bf2ffff3 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jun 28 08:47:22.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5735" for this suite. 06/28/23 08:47:22.294
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","completed":272,"skipped":4877,"failed":0}
------------------------------
â€¢ [4.091 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:47:18.209
    Jun 28 08:47:18.209: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename projected 06/28/23 08:47:18.21
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:47:18.223
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:47:18.228
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:206
    STEP: Creating a pod to test downward API volume plugin 06/28/23 08:47:18.232
    Jun 28 08:47:18.241: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bc1f1a37-a3be-499b-86ed-43f4bf2ffff3" in namespace "projected-5735" to be "Succeeded or Failed"
    Jun 28 08:47:18.245: INFO: Pod "downwardapi-volume-bc1f1a37-a3be-499b-86ed-43f4bf2ffff3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.256307ms
    Jun 28 08:47:20.251: INFO: Pod "downwardapi-volume-bc1f1a37-a3be-499b-86ed-43f4bf2ffff3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010444453s
    Jun 28 08:47:22.250: INFO: Pod "downwardapi-volume-bc1f1a37-a3be-499b-86ed-43f4bf2ffff3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009364995s
    STEP: Saw pod success 06/28/23 08:47:22.25
    Jun 28 08:47:22.250: INFO: Pod "downwardapi-volume-bc1f1a37-a3be-499b-86ed-43f4bf2ffff3" satisfied condition "Succeeded or Failed"
    Jun 28 08:47:22.255: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod downwardapi-volume-bc1f1a37-a3be-499b-86ed-43f4bf2ffff3 container client-container: <nil>
    STEP: delete the pod 06/28/23 08:47:22.268
    Jun 28 08:47:22.282: INFO: Waiting for pod downwardapi-volume-bc1f1a37-a3be-499b-86ed-43f4bf2ffff3 to disappear
    Jun 28 08:47:22.286: INFO: Pod downwardapi-volume-bc1f1a37-a3be-499b-86ed-43f4bf2ffff3 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jun 28 08:47:22.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5735" for this suite. 06/28/23 08:47:22.294
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:47:22.301
Jun 28 08:47:22.301: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename webhook 06/28/23 08:47:22.302
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:47:22.319
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:47:22.323
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 06/28/23 08:47:22.343
STEP: Create role binding to let webhook read extension-apiserver-authentication 06/28/23 08:47:22.607
STEP: Deploying the webhook pod 06/28/23 08:47:22.616
STEP: Wait for the deployment to be ready 06/28/23 08:47:22.629
Jun 28 08:47:22.637: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 06/28/23 08:47:24.652
STEP: Verifying the service has paired with the endpoint 06/28/23 08:47:24.667
Jun 28 08:47:25.667: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322
Jun 28 08:47:25.673: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-5857-crds.webhook.example.com via the AdmissionRegistration API 06/28/23 08:47:26.188
STEP: Creating a custom resource while v1 is storage version 06/28/23 08:47:26.294
STEP: Patching Custom Resource Definition to set v2 as storage 06/28/23 08:47:28.488
STEP: Patching the custom resource while v2 is storage version 06/28/23 08:47:28.506
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jun 28 08:47:29.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6286" for this suite. 06/28/23 08:47:29.095
STEP: Destroying namespace "webhook-6286-markers" for this suite. 06/28/23 08:47:29.102
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","completed":273,"skipped":4884,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.843 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:47:22.301
    Jun 28 08:47:22.301: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename webhook 06/28/23 08:47:22.302
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:47:22.319
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:47:22.323
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 06/28/23 08:47:22.343
    STEP: Create role binding to let webhook read extension-apiserver-authentication 06/28/23 08:47:22.607
    STEP: Deploying the webhook pod 06/28/23 08:47:22.616
    STEP: Wait for the deployment to be ready 06/28/23 08:47:22.629
    Jun 28 08:47:22.637: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 06/28/23 08:47:24.652
    STEP: Verifying the service has paired with the endpoint 06/28/23 08:47:24.667
    Jun 28 08:47:25.667: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with different stored version [Conformance]
      test/e2e/apimachinery/webhook.go:322
    Jun 28 08:47:25.673: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-5857-crds.webhook.example.com via the AdmissionRegistration API 06/28/23 08:47:26.188
    STEP: Creating a custom resource while v1 is storage version 06/28/23 08:47:26.294
    STEP: Patching Custom Resource Definition to set v2 as storage 06/28/23 08:47:28.488
    STEP: Patching the custom resource while v2 is storage version 06/28/23 08:47:28.506
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jun 28 08:47:29.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-6286" for this suite. 06/28/23 08:47:29.095
    STEP: Destroying namespace "webhook-6286-markers" for this suite. 06/28/23 08:47:29.102
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Services
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3620
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:47:29.144
Jun 28 08:47:29.144: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename services 06/28/23 08:47:29.145
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:47:29.158
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:47:29.164
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should delete a collection of services [Conformance]
  test/e2e/network/service.go:3620
STEP: creating a collection of services 06/28/23 08:47:29.169
Jun 28 08:47:29.169: INFO: Creating e2e-svc-a-gr6l9
Jun 28 08:47:29.179: INFO: Creating e2e-svc-b-p7drp
Jun 28 08:47:29.191: INFO: Creating e2e-svc-c-gs8lm
STEP: deleting service collection 06/28/23 08:47:29.204
Jun 28 08:47:29.230: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jun 28 08:47:29.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1464" for this suite. 06/28/23 08:47:29.239
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should delete a collection of services [Conformance]","completed":274,"skipped":4884,"failed":0}
------------------------------
â€¢ [0.103 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3620

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:47:29.144
    Jun 28 08:47:29.144: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename services 06/28/23 08:47:29.145
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:47:29.158
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:47:29.164
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should delete a collection of services [Conformance]
      test/e2e/network/service.go:3620
    STEP: creating a collection of services 06/28/23 08:47:29.169
    Jun 28 08:47:29.169: INFO: Creating e2e-svc-a-gr6l9
    Jun 28 08:47:29.179: INFO: Creating e2e-svc-b-p7drp
    Jun 28 08:47:29.191: INFO: Creating e2e-svc-c-gs8lm
    STEP: deleting service collection 06/28/23 08:47:29.204
    Jun 28 08:47:29.230: INFO: Collection of services has been deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jun 28 08:47:29.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-1464" for this suite. 06/28/23 08:47:29.239
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:47:29.247
Jun 28 08:47:29.247: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename resourcequota 06/28/23 08:47:29.248
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:47:29.262
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:47:29.267
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680
STEP: Creating a ResourceQuota with terminating scope 06/28/23 08:47:29.272
STEP: Ensuring ResourceQuota status is calculated 06/28/23 08:47:29.278
STEP: Creating a ResourceQuota with not terminating scope 06/28/23 08:47:31.284
STEP: Ensuring ResourceQuota status is calculated 06/28/23 08:47:31.291
STEP: Creating a long running pod 06/28/23 08:47:33.296
STEP: Ensuring resource quota with not terminating scope captures the pod usage 06/28/23 08:47:33.31
STEP: Ensuring resource quota with terminating scope ignored the pod usage 06/28/23 08:47:35.316
STEP: Deleting the pod 06/28/23 08:47:37.323
STEP: Ensuring resource quota status released the pod usage 06/28/23 08:47:37.334
STEP: Creating a terminating pod 06/28/23 08:47:39.34
STEP: Ensuring resource quota with terminating scope captures the pod usage 06/28/23 08:47:39.351
STEP: Ensuring resource quota with not terminating scope ignored the pod usage 06/28/23 08:47:41.357
STEP: Deleting the pod 06/28/23 08:47:43.362
STEP: Ensuring resource quota status released the pod usage 06/28/23 08:47:43.374
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jun 28 08:47:45.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4976" for this suite. 06/28/23 08:47:45.389
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","completed":275,"skipped":4897,"failed":0}
------------------------------
â€¢ [SLOW TEST] [16.149 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:47:29.247
    Jun 28 08:47:29.247: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename resourcequota 06/28/23 08:47:29.248
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:47:29.262
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:47:29.267
    [It] should verify ResourceQuota with terminating scopes. [Conformance]
      test/e2e/apimachinery/resource_quota.go:680
    STEP: Creating a ResourceQuota with terminating scope 06/28/23 08:47:29.272
    STEP: Ensuring ResourceQuota status is calculated 06/28/23 08:47:29.278
    STEP: Creating a ResourceQuota with not terminating scope 06/28/23 08:47:31.284
    STEP: Ensuring ResourceQuota status is calculated 06/28/23 08:47:31.291
    STEP: Creating a long running pod 06/28/23 08:47:33.296
    STEP: Ensuring resource quota with not terminating scope captures the pod usage 06/28/23 08:47:33.31
    STEP: Ensuring resource quota with terminating scope ignored the pod usage 06/28/23 08:47:35.316
    STEP: Deleting the pod 06/28/23 08:47:37.323
    STEP: Ensuring resource quota status released the pod usage 06/28/23 08:47:37.334
    STEP: Creating a terminating pod 06/28/23 08:47:39.34
    STEP: Ensuring resource quota with terminating scope captures the pod usage 06/28/23 08:47:39.351
    STEP: Ensuring resource quota with not terminating scope ignored the pod usage 06/28/23 08:47:41.357
    STEP: Deleting the pod 06/28/23 08:47:43.362
    STEP: Ensuring resource quota status released the pod usage 06/28/23 08:47:43.374
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jun 28 08:47:45.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-4976" for this suite. 06/28/23 08:47:45.389
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:47:45.398
Jun 28 08:47:45.398: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename gc 06/28/23 08:47:45.399
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:47:45.412
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:47:45.417
[It] should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
STEP: create the deployment 06/28/23 08:47:45.421
STEP: Wait for the Deployment to create new ReplicaSet 06/28/23 08:47:45.428
STEP: delete the deployment 06/28/23 08:47:45.434
STEP: wait for all rs to be garbage collected 06/28/23 08:47:45.445
STEP: expected 0 rs, got 1 rs 06/28/23 08:47:45.456
STEP: expected 0 pods, got 2 pods 06/28/23 08:47:45.463
STEP: Gathering metrics 06/28/23 08:47:45.977
W0628 08:47:45.989728      18 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jun 28 08:47:45.989: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Jun 28 08:47:45.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7269" for this suite. 06/28/23 08:47:45.998
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","completed":276,"skipped":4965,"failed":0}
------------------------------
â€¢ [0.608 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:47:45.398
    Jun 28 08:47:45.398: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename gc 06/28/23 08:47:45.399
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:47:45.412
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:47:45.417
    [It] should delete RS created by deployment when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:491
    STEP: create the deployment 06/28/23 08:47:45.421
    STEP: Wait for the Deployment to create new ReplicaSet 06/28/23 08:47:45.428
    STEP: delete the deployment 06/28/23 08:47:45.434
    STEP: wait for all rs to be garbage collected 06/28/23 08:47:45.445
    STEP: expected 0 rs, got 1 rs 06/28/23 08:47:45.456
    STEP: expected 0 pods, got 2 pods 06/28/23 08:47:45.463
    STEP: Gathering metrics 06/28/23 08:47:45.977
    W0628 08:47:45.989728      18 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Jun 28 08:47:45.989: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Jun 28 08:47:45.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-7269" for this suite. 06/28/23 08:47:45.998
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:485
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:47:46.007
Jun 28 08:47:46.007: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename security-context-test 06/28/23 08:47:46.008
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:47:46.023
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:47:46.028
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:485
Jun 28 08:47:46.045: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-fe3aa331-842e-410f-9daf-ba9ee9f78b15" in namespace "security-context-test-3211" to be "Succeeded or Failed"
Jun 28 08:47:46.049: INFO: Pod "busybox-readonly-false-fe3aa331-842e-410f-9daf-ba9ee9f78b15": Phase="Pending", Reason="", readiness=false. Elapsed: 4.095406ms
Jun 28 08:47:48.054: INFO: Pod "busybox-readonly-false-fe3aa331-842e-410f-9daf-ba9ee9f78b15": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009576249s
Jun 28 08:47:50.055: INFO: Pod "busybox-readonly-false-fe3aa331-842e-410f-9daf-ba9ee9f78b15": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010403663s
Jun 28 08:47:50.055: INFO: Pod "busybox-readonly-false-fe3aa331-842e-410f-9daf-ba9ee9f78b15" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Jun 28 08:47:50.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-3211" for this suite. 06/28/23 08:47:50.064
{"msg":"PASSED [sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","completed":277,"skipped":4968,"failed":0}
------------------------------
â€¢ [4.066 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with readOnlyRootFilesystem
  test/e2e/common/node/security_context.go:429
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:485

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:47:46.007
    Jun 28 08:47:46.007: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename security-context-test 06/28/23 08:47:46.008
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:47:46.023
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:47:46.028
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:485
    Jun 28 08:47:46.045: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-fe3aa331-842e-410f-9daf-ba9ee9f78b15" in namespace "security-context-test-3211" to be "Succeeded or Failed"
    Jun 28 08:47:46.049: INFO: Pod "busybox-readonly-false-fe3aa331-842e-410f-9daf-ba9ee9f78b15": Phase="Pending", Reason="", readiness=false. Elapsed: 4.095406ms
    Jun 28 08:47:48.054: INFO: Pod "busybox-readonly-false-fe3aa331-842e-410f-9daf-ba9ee9f78b15": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009576249s
    Jun 28 08:47:50.055: INFO: Pod "busybox-readonly-false-fe3aa331-842e-410f-9daf-ba9ee9f78b15": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010403663s
    Jun 28 08:47:50.055: INFO: Pod "busybox-readonly-false-fe3aa331-842e-410f-9daf-ba9ee9f78b15" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Jun 28 08:47:50.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-3211" for this suite. 06/28/23 08:47:50.064
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:247
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:47:50.075
Jun 28 08:47:50.075: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename container-runtime 06/28/23 08:47:50.076
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:47:50.092
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:47:50.097
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:247
STEP: create the container 06/28/23 08:47:50.102
STEP: wait for the container to reach Succeeded 06/28/23 08:47:50.112
STEP: get the container status 06/28/23 08:47:54.141
STEP: the container should be terminated 06/28/23 08:47:54.146
STEP: the termination message should be set 06/28/23 08:47:54.146
Jun 28 08:47:54.146: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container 06/28/23 08:47:54.146
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Jun 28 08:47:54.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6108" for this suite. 06/28/23 08:47:54.17
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":278,"skipped":5012,"failed":0}
------------------------------
â€¢ [4.102 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:247

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:47:50.075
    Jun 28 08:47:50.075: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename container-runtime 06/28/23 08:47:50.076
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:47:50.092
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:47:50.097
    [It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:247
    STEP: create the container 06/28/23 08:47:50.102
    STEP: wait for the container to reach Succeeded 06/28/23 08:47:50.112
    STEP: get the container status 06/28/23 08:47:54.141
    STEP: the container should be terminated 06/28/23 08:47:54.146
    STEP: the termination message should be set 06/28/23 08:47:54.146
    Jun 28 08:47:54.146: INFO: Expected: &{OK} to match Container's Termination Message: OK --
    STEP: delete the container 06/28/23 08:47:54.146
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Jun 28 08:47:54.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-6108" for this suite. 06/28/23 08:47:54.17
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:47:54.179
Jun 28 08:47:54.179: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename emptydir 06/28/23 08:47:54.179
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:47:54.193
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:47:54.197
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196
STEP: Creating a pod to test emptydir 0644 on node default medium 06/28/23 08:47:54.201
Jun 28 08:47:54.210: INFO: Waiting up to 5m0s for pod "pod-a66f901c-bdfa-4405-9bbf-48d0ee563881" in namespace "emptydir-3636" to be "Succeeded or Failed"
Jun 28 08:47:54.214: INFO: Pod "pod-a66f901c-bdfa-4405-9bbf-48d0ee563881": Phase="Pending", Reason="", readiness=false. Elapsed: 4.188474ms
Jun 28 08:47:56.225: INFO: Pod "pod-a66f901c-bdfa-4405-9bbf-48d0ee563881": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015689247s
Jun 28 08:47:58.225: INFO: Pod "pod-a66f901c-bdfa-4405-9bbf-48d0ee563881": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015689875s
STEP: Saw pod success 06/28/23 08:47:58.225
Jun 28 08:47:58.225: INFO: Pod "pod-a66f901c-bdfa-4405-9bbf-48d0ee563881" satisfied condition "Succeeded or Failed"
Jun 28 08:47:58.230: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod pod-a66f901c-bdfa-4405-9bbf-48d0ee563881 container test-container: <nil>
STEP: delete the pod 06/28/23 08:47:58.279
Jun 28 08:47:58.290: INFO: Waiting for pod pod-a66f901c-bdfa-4405-9bbf-48d0ee563881 to disappear
Jun 28 08:47:58.295: INFO: Pod pod-a66f901c-bdfa-4405-9bbf-48d0ee563881 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jun 28 08:47:58.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3636" for this suite. 06/28/23 08:47:58.303
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":279,"skipped":5076,"failed":0}
------------------------------
â€¢ [4.132 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:47:54.179
    Jun 28 08:47:54.179: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename emptydir 06/28/23 08:47:54.179
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:47:54.193
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:47:54.197
    [It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:196
    STEP: Creating a pod to test emptydir 0644 on node default medium 06/28/23 08:47:54.201
    Jun 28 08:47:54.210: INFO: Waiting up to 5m0s for pod "pod-a66f901c-bdfa-4405-9bbf-48d0ee563881" in namespace "emptydir-3636" to be "Succeeded or Failed"
    Jun 28 08:47:54.214: INFO: Pod "pod-a66f901c-bdfa-4405-9bbf-48d0ee563881": Phase="Pending", Reason="", readiness=false. Elapsed: 4.188474ms
    Jun 28 08:47:56.225: INFO: Pod "pod-a66f901c-bdfa-4405-9bbf-48d0ee563881": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015689247s
    Jun 28 08:47:58.225: INFO: Pod "pod-a66f901c-bdfa-4405-9bbf-48d0ee563881": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015689875s
    STEP: Saw pod success 06/28/23 08:47:58.225
    Jun 28 08:47:58.225: INFO: Pod "pod-a66f901c-bdfa-4405-9bbf-48d0ee563881" satisfied condition "Succeeded or Failed"
    Jun 28 08:47:58.230: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod pod-a66f901c-bdfa-4405-9bbf-48d0ee563881 container test-container: <nil>
    STEP: delete the pod 06/28/23 08:47:58.279
    Jun 28 08:47:58.290: INFO: Waiting for pod pod-a66f901c-bdfa-4405-9bbf-48d0ee563881 to disappear
    Jun 28 08:47:58.295: INFO: Pod pod-a66f901c-bdfa-4405-9bbf-48d0ee563881 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jun 28 08:47:58.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-3636" for this suite. 06/28/23 08:47:58.303
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:47:58.312
Jun 28 08:47:58.312: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename cronjob 06/28/23 08:47:58.313
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:47:58.329
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:47:58.333
[It] should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
STEP: Creating a cronjob 06/28/23 08:47:58.338
STEP: creating 06/28/23 08:47:58.338
STEP: getting 06/28/23 08:47:58.344
STEP: listing 06/28/23 08:47:58.348
STEP: watching 06/28/23 08:47:58.352
Jun 28 08:47:58.352: INFO: starting watch
STEP: cluster-wide listing 06/28/23 08:47:58.354
STEP: cluster-wide watching 06/28/23 08:47:58.358
Jun 28 08:47:58.358: INFO: starting watch
STEP: patching 06/28/23 08:47:58.361
STEP: updating 06/28/23 08:47:58.367
Jun 28 08:47:58.378: INFO: waiting for watch events with expected annotations
Jun 28 08:47:58.378: INFO: saw patched and updated annotations
STEP: patching /status 06/28/23 08:47:58.378
STEP: updating /status 06/28/23 08:47:58.385
STEP: get /status 06/28/23 08:47:58.394
STEP: deleting 06/28/23 08:47:58.399
STEP: deleting a collection 06/28/23 08:47:58.415
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Jun 28 08:47:58.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-9070" for this suite. 06/28/23 08:47:58.435
{"msg":"PASSED [sig-apps] CronJob should support CronJob API operations [Conformance]","completed":280,"skipped":5118,"failed":0}
------------------------------
â€¢ [0.130 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:47:58.312
    Jun 28 08:47:58.312: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename cronjob 06/28/23 08:47:58.313
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:47:58.329
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:47:58.333
    [It] should support CronJob API operations [Conformance]
      test/e2e/apps/cronjob.go:319
    STEP: Creating a cronjob 06/28/23 08:47:58.338
    STEP: creating 06/28/23 08:47:58.338
    STEP: getting 06/28/23 08:47:58.344
    STEP: listing 06/28/23 08:47:58.348
    STEP: watching 06/28/23 08:47:58.352
    Jun 28 08:47:58.352: INFO: starting watch
    STEP: cluster-wide listing 06/28/23 08:47:58.354
    STEP: cluster-wide watching 06/28/23 08:47:58.358
    Jun 28 08:47:58.358: INFO: starting watch
    STEP: patching 06/28/23 08:47:58.361
    STEP: updating 06/28/23 08:47:58.367
    Jun 28 08:47:58.378: INFO: waiting for watch events with expected annotations
    Jun 28 08:47:58.378: INFO: saw patched and updated annotations
    STEP: patching /status 06/28/23 08:47:58.378
    STEP: updating /status 06/28/23 08:47:58.385
    STEP: get /status 06/28/23 08:47:58.394
    STEP: deleting 06/28/23 08:47:58.399
    STEP: deleting a collection 06/28/23 08:47:58.415
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Jun 28 08:47:58.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-9070" for this suite. 06/28/23 08:47:58.435
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:47:58.442
Jun 28 08:47:58.442: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename runtimeclass 06/28/23 08:47:58.442
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:47:58.455
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:47:58.46
[It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Jun 28 08:47:58.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-66" for this suite. 06/28/23 08:47:58.479
{"msg":"PASSED [sig-node] RuntimeClass should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]","completed":281,"skipped":5119,"failed":0}
------------------------------
â€¢ [0.045 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:47:58.442
    Jun 28 08:47:58.442: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename runtimeclass 06/28/23 08:47:58.442
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:47:58.455
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:47:58.46
    [It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:55
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Jun 28 08:47:58.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-66" for this suite. 06/28/23 08:47:58.479
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:47:58.487
Jun 28 08:47:58.491: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename container-probe 06/28/23 08:47:58.492
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:47:58.505
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:47:58.51
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180
STEP: Creating pod liveness-591729c2-b4c8-4901-8dc1-18076703a19b in namespace container-probe-841 06/28/23 08:47:58.515
Jun 28 08:47:58.526: INFO: Waiting up to 5m0s for pod "liveness-591729c2-b4c8-4901-8dc1-18076703a19b" in namespace "container-probe-841" to be "not pending"
Jun 28 08:47:58.531: INFO: Pod "liveness-591729c2-b4c8-4901-8dc1-18076703a19b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.978532ms
Jun 28 08:48:00.539: INFO: Pod "liveness-591729c2-b4c8-4901-8dc1-18076703a19b": Phase="Running", Reason="", readiness=true. Elapsed: 2.012265334s
Jun 28 08:48:00.539: INFO: Pod "liveness-591729c2-b4c8-4901-8dc1-18076703a19b" satisfied condition "not pending"
Jun 28 08:48:00.539: INFO: Started pod liveness-591729c2-b4c8-4901-8dc1-18076703a19b in namespace container-probe-841
STEP: checking the pod's current state and verifying that restartCount is present 06/28/23 08:48:00.539
Jun 28 08:48:00.544: INFO: Initial restart count of pod liveness-591729c2-b4c8-4901-8dc1-18076703a19b is 0
STEP: deleting the pod 06/28/23 08:52:01.285
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jun 28 08:52:01.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-841" for this suite. 06/28/23 08:52:01.316
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","completed":282,"skipped":5148,"failed":0}
------------------------------
â€¢ [SLOW TEST] [242.839 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:47:58.487
    Jun 28 08:47:58.491: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename container-probe 06/28/23 08:47:58.492
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:47:58.505
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:47:58.51
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:180
    STEP: Creating pod liveness-591729c2-b4c8-4901-8dc1-18076703a19b in namespace container-probe-841 06/28/23 08:47:58.515
    Jun 28 08:47:58.526: INFO: Waiting up to 5m0s for pod "liveness-591729c2-b4c8-4901-8dc1-18076703a19b" in namespace "container-probe-841" to be "not pending"
    Jun 28 08:47:58.531: INFO: Pod "liveness-591729c2-b4c8-4901-8dc1-18076703a19b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.978532ms
    Jun 28 08:48:00.539: INFO: Pod "liveness-591729c2-b4c8-4901-8dc1-18076703a19b": Phase="Running", Reason="", readiness=true. Elapsed: 2.012265334s
    Jun 28 08:48:00.539: INFO: Pod "liveness-591729c2-b4c8-4901-8dc1-18076703a19b" satisfied condition "not pending"
    Jun 28 08:48:00.539: INFO: Started pod liveness-591729c2-b4c8-4901-8dc1-18076703a19b in namespace container-probe-841
    STEP: checking the pod's current state and verifying that restartCount is present 06/28/23 08:48:00.539
    Jun 28 08:48:00.544: INFO: Initial restart count of pod liveness-591729c2-b4c8-4901-8dc1-18076703a19b is 0
    STEP: deleting the pod 06/28/23 08:52:01.285
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jun 28 08:52:01.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-841" for this suite. 06/28/23 08:52:01.316
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:52:01.327
Jun 28 08:52:01.327: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename projected 06/28/23 08:52:01.328
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:52:01.349
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:52:01.357
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374
STEP: Creating configMap with name projected-configmap-test-volume-b67ebefd-603c-4091-9705-21106c3b2272 06/28/23 08:52:01.363
STEP: Creating a pod to test consume configMaps 06/28/23 08:52:01.372
Jun 28 08:52:01.384: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a5578074-dab4-45a4-adc2-4da0cb6e94bc" in namespace "projected-4470" to be "Succeeded or Failed"
Jun 28 08:52:01.389: INFO: Pod "pod-projected-configmaps-a5578074-dab4-45a4-adc2-4da0cb6e94bc": Phase="Pending", Reason="", readiness=false. Elapsed: 5.347814ms
Jun 28 08:52:03.396: INFO: Pod "pod-projected-configmaps-a5578074-dab4-45a4-adc2-4da0cb6e94bc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0120498s
Jun 28 08:52:05.396: INFO: Pod "pod-projected-configmaps-a5578074-dab4-45a4-adc2-4da0cb6e94bc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011912892s
STEP: Saw pod success 06/28/23 08:52:05.396
Jun 28 08:52:05.396: INFO: Pod "pod-projected-configmaps-a5578074-dab4-45a4-adc2-4da0cb6e94bc" satisfied condition "Succeeded or Failed"
Jun 28 08:52:05.400: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod pod-projected-configmaps-a5578074-dab4-45a4-adc2-4da0cb6e94bc container projected-configmap-volume-test: <nil>
STEP: delete the pod 06/28/23 08:52:05.45
Jun 28 08:52:05.462: INFO: Waiting for pod pod-projected-configmaps-a5578074-dab4-45a4-adc2-4da0cb6e94bc to disappear
Jun 28 08:52:05.466: INFO: Pod pod-projected-configmaps-a5578074-dab4-45a4-adc2-4da0cb6e94bc no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jun 28 08:52:05.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4470" for this suite. 06/28/23 08:52:05.474
{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","completed":283,"skipped":5153,"failed":0}
------------------------------
â€¢ [4.154 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:52:01.327
    Jun 28 08:52:01.327: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename projected 06/28/23 08:52:01.328
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:52:01.349
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:52:01.357
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:374
    STEP: Creating configMap with name projected-configmap-test-volume-b67ebefd-603c-4091-9705-21106c3b2272 06/28/23 08:52:01.363
    STEP: Creating a pod to test consume configMaps 06/28/23 08:52:01.372
    Jun 28 08:52:01.384: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a5578074-dab4-45a4-adc2-4da0cb6e94bc" in namespace "projected-4470" to be "Succeeded or Failed"
    Jun 28 08:52:01.389: INFO: Pod "pod-projected-configmaps-a5578074-dab4-45a4-adc2-4da0cb6e94bc": Phase="Pending", Reason="", readiness=false. Elapsed: 5.347814ms
    Jun 28 08:52:03.396: INFO: Pod "pod-projected-configmaps-a5578074-dab4-45a4-adc2-4da0cb6e94bc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0120498s
    Jun 28 08:52:05.396: INFO: Pod "pod-projected-configmaps-a5578074-dab4-45a4-adc2-4da0cb6e94bc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011912892s
    STEP: Saw pod success 06/28/23 08:52:05.396
    Jun 28 08:52:05.396: INFO: Pod "pod-projected-configmaps-a5578074-dab4-45a4-adc2-4da0cb6e94bc" satisfied condition "Succeeded or Failed"
    Jun 28 08:52:05.400: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod pod-projected-configmaps-a5578074-dab4-45a4-adc2-4da0cb6e94bc container projected-configmap-volume-test: <nil>
    STEP: delete the pod 06/28/23 08:52:05.45
    Jun 28 08:52:05.462: INFO: Waiting for pod pod-projected-configmaps-a5578074-dab4-45a4-adc2-4da0cb6e94bc to disappear
    Jun 28 08:52:05.466: INFO: Pod pod-projected-configmaps-a5578074-dab4-45a4-adc2-4da0cb6e94bc no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jun 28 08:52:05.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4470" for this suite. 06/28/23 08:52:05.474
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:52:05.482
Jun 28 08:52:05.482: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename projected 06/28/23 08:52:05.483
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:52:05.496
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:52:05.501
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161
STEP: Creating the pod 06/28/23 08:52:05.505
Jun 28 08:52:05.516: INFO: Waiting up to 5m0s for pod "annotationupdate8cd750c8-772a-4e58-bc75-c0534dc12df8" in namespace "projected-5619" to be "running and ready"
Jun 28 08:52:05.520: INFO: Pod "annotationupdate8cd750c8-772a-4e58-bc75-c0534dc12df8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.093135ms
Jun 28 08:52:05.520: INFO: The phase of Pod annotationupdate8cd750c8-772a-4e58-bc75-c0534dc12df8 is Pending, waiting for it to be Running (with Ready = true)
Jun 28 08:52:07.527: INFO: Pod "annotationupdate8cd750c8-772a-4e58-bc75-c0534dc12df8": Phase="Running", Reason="", readiness=true. Elapsed: 2.01156666s
Jun 28 08:52:07.527: INFO: The phase of Pod annotationupdate8cd750c8-772a-4e58-bc75-c0534dc12df8 is Running (Ready = true)
Jun 28 08:52:07.527: INFO: Pod "annotationupdate8cd750c8-772a-4e58-bc75-c0534dc12df8" satisfied condition "running and ready"
Jun 28 08:52:08.058: INFO: Successfully updated pod "annotationupdate8cd750c8-772a-4e58-bc75-c0534dc12df8"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jun 28 08:52:10.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5619" for this suite. 06/28/23 08:52:10.089
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","completed":284,"skipped":5160,"failed":0}
------------------------------
â€¢ [4.615 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:52:05.482
    Jun 28 08:52:05.482: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename projected 06/28/23 08:52:05.483
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:52:05.496
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:52:05.501
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:161
    STEP: Creating the pod 06/28/23 08:52:05.505
    Jun 28 08:52:05.516: INFO: Waiting up to 5m0s for pod "annotationupdate8cd750c8-772a-4e58-bc75-c0534dc12df8" in namespace "projected-5619" to be "running and ready"
    Jun 28 08:52:05.520: INFO: Pod "annotationupdate8cd750c8-772a-4e58-bc75-c0534dc12df8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.093135ms
    Jun 28 08:52:05.520: INFO: The phase of Pod annotationupdate8cd750c8-772a-4e58-bc75-c0534dc12df8 is Pending, waiting for it to be Running (with Ready = true)
    Jun 28 08:52:07.527: INFO: Pod "annotationupdate8cd750c8-772a-4e58-bc75-c0534dc12df8": Phase="Running", Reason="", readiness=true. Elapsed: 2.01156666s
    Jun 28 08:52:07.527: INFO: The phase of Pod annotationupdate8cd750c8-772a-4e58-bc75-c0534dc12df8 is Running (Ready = true)
    Jun 28 08:52:07.527: INFO: Pod "annotationupdate8cd750c8-772a-4e58-bc75-c0534dc12df8" satisfied condition "running and ready"
    Jun 28 08:52:08.058: INFO: Successfully updated pod "annotationupdate8cd750c8-772a-4e58-bc75-c0534dc12df8"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jun 28 08:52:10.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5619" for this suite. 06/28/23 08:52:10.089
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:52:10.098
Jun 28 08:52:10.098: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename endpointslice 06/28/23 08:52:10.099
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:52:10.114
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:52:10.12
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352
STEP: getting /apis 06/28/23 08:52:10.126
STEP: getting /apis/discovery.k8s.io 06/28/23 08:52:10.131
STEP: getting /apis/discovery.k8s.iov1 06/28/23 08:52:10.133
STEP: creating 06/28/23 08:52:10.136
STEP: getting 06/28/23 08:52:10.163
STEP: listing 06/28/23 08:52:10.168
STEP: watching 06/28/23 08:52:10.173
Jun 28 08:52:10.173: INFO: starting watch
STEP: cluster-wide listing 06/28/23 08:52:10.176
STEP: cluster-wide watching 06/28/23 08:52:10.182
Jun 28 08:52:10.182: INFO: starting watch
STEP: patching 06/28/23 08:52:10.184
STEP: updating 06/28/23 08:52:10.192
Jun 28 08:52:10.203: INFO: waiting for watch events with expected annotations
Jun 28 08:52:10.203: INFO: saw patched and updated annotations
STEP: deleting 06/28/23 08:52:10.203
STEP: deleting a collection 06/28/23 08:52:10.22
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Jun 28 08:52:10.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-3112" for this suite. 06/28/23 08:52:10.249
{"msg":"PASSED [sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]","completed":285,"skipped":5176,"failed":0}
------------------------------
â€¢ [0.159 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:52:10.098
    Jun 28 08:52:10.098: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename endpointslice 06/28/23 08:52:10.099
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:52:10.114
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:52:10.12
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should support creating EndpointSlice API operations [Conformance]
      test/e2e/network/endpointslice.go:352
    STEP: getting /apis 06/28/23 08:52:10.126
    STEP: getting /apis/discovery.k8s.io 06/28/23 08:52:10.131
    STEP: getting /apis/discovery.k8s.iov1 06/28/23 08:52:10.133
    STEP: creating 06/28/23 08:52:10.136
    STEP: getting 06/28/23 08:52:10.163
    STEP: listing 06/28/23 08:52:10.168
    STEP: watching 06/28/23 08:52:10.173
    Jun 28 08:52:10.173: INFO: starting watch
    STEP: cluster-wide listing 06/28/23 08:52:10.176
    STEP: cluster-wide watching 06/28/23 08:52:10.182
    Jun 28 08:52:10.182: INFO: starting watch
    STEP: patching 06/28/23 08:52:10.184
    STEP: updating 06/28/23 08:52:10.192
    Jun 28 08:52:10.203: INFO: waiting for watch events with expected annotations
    Jun 28 08:52:10.203: INFO: saw patched and updated annotations
    STEP: deleting 06/28/23 08:52:10.203
    STEP: deleting a collection 06/28/23 08:52:10.22
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Jun 28 08:52:10.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-3112" for this suite. 06/28/23 08:52:10.249
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:52:10.257
Jun 28 08:52:10.257: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename pod-network-test 06/28/23 08:52:10.258
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:52:10.274
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:52:10.279
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
STEP: Performing setup for networking test in namespace pod-network-test-3271 06/28/23 08:52:10.285
STEP: creating a selector 06/28/23 08:52:10.285
STEP: Creating the service pods in kubernetes 06/28/23 08:52:10.285
Jun 28 08:52:10.286: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jun 28 08:52:10.321: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-3271" to be "running and ready"
Jun 28 08:52:10.326: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.984531ms
Jun 28 08:52:10.326: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun 28 08:52:12.332: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.010867765s
Jun 28 08:52:12.332: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 28 08:52:14.332: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.011180679s
Jun 28 08:52:14.332: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 28 08:52:16.333: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.011620516s
Jun 28 08:52:16.333: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 28 08:52:18.333: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.011264515s
Jun 28 08:52:18.333: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 28 08:52:20.333: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.011522074s
Jun 28 08:52:20.333: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 28 08:52:22.332: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.010769171s
Jun 28 08:52:22.332: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 28 08:52:24.333: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.011519324s
Jun 28 08:52:24.333: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 28 08:52:26.332: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.010387806s
Jun 28 08:52:26.332: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 28 08:52:28.333: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.011301391s
Jun 28 08:52:28.333: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 28 08:52:30.332: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.01110472s
Jun 28 08:52:30.332: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 28 08:52:32.333: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.011659169s
Jun 28 08:52:32.333: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jun 28 08:52:32.333: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jun 28 08:52:32.337: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-3271" to be "running and ready"
Jun 28 08:52:32.344: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 6.024087ms
Jun 28 08:52:32.344: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jun 28 08:52:32.344: INFO: Pod "netserver-1" satisfied condition "running and ready"
Jun 28 08:52:32.351: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-3271" to be "running and ready"
Jun 28 08:52:32.355: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 4.46507ms
Jun 28 08:52:32.355: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Jun 28 08:52:32.355: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 06/28/23 08:52:32.36
Jun 28 08:52:32.366: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-3271" to be "running"
Jun 28 08:52:32.373: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.235676ms
Jun 28 08:52:34.379: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.012973552s
Jun 28 08:52:34.379: INFO: Pod "test-container-pod" satisfied condition "running"
Jun 28 08:52:34.384: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Jun 28 08:52:34.384: INFO: Breadth first check of 172.21.122.63 on host 192.168.11.3...
Jun 28 08:52:34.389: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.21.122.59:9080/dial?request=hostname&protocol=http&host=172.21.122.63&port=8083&tries=1'] Namespace:pod-network-test-3271 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 28 08:52:34.389: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
Jun 28 08:52:34.389: INFO: ExecWithOptions: Clientset creation
Jun 28 08:52:34.389: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/pod-network-test-3271/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.21.122.59%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.21.122.63%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jun 28 08:52:34.802: INFO: Waiting for responses: map[]
Jun 28 08:52:34.802: INFO: reached 172.21.122.63 after 0/1 tries
Jun 28 08:52:34.802: INFO: Breadth first check of 172.21.122.91 on host 192.168.11.4...
Jun 28 08:52:34.807: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.21.122.59:9080/dial?request=hostname&protocol=http&host=172.21.122.91&port=8083&tries=1'] Namespace:pod-network-test-3271 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 28 08:52:34.807: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
Jun 28 08:52:34.808: INFO: ExecWithOptions: Clientset creation
Jun 28 08:52:34.808: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/pod-network-test-3271/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.21.122.59%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.21.122.91%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jun 28 08:52:35.218: INFO: Waiting for responses: map[]
Jun 28 08:52:35.218: INFO: reached 172.21.122.91 after 0/1 tries
Jun 28 08:52:35.218: INFO: Breadth first check of 172.21.30.114 on host 192.168.11.5...
Jun 28 08:52:35.224: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.21.122.59:9080/dial?request=hostname&protocol=http&host=172.21.30.114&port=8083&tries=1'] Namespace:pod-network-test-3271 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 28 08:52:35.224: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
Jun 28 08:52:35.224: INFO: ExecWithOptions: Clientset creation
Jun 28 08:52:35.224: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/pod-network-test-3271/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.21.122.59%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.21.30.114%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jun 28 08:52:35.637: INFO: Waiting for responses: map[]
Jun 28 08:52:35.637: INFO: reached 172.21.30.114 after 0/1 tries
Jun 28 08:52:35.637: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Jun 28 08:52:35.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-3271" for this suite. 06/28/23 08:52:35.645
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","completed":286,"skipped":5178,"failed":0}
------------------------------
â€¢ [SLOW TEST] [25.396 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:52:10.257
    Jun 28 08:52:10.257: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename pod-network-test 06/28/23 08:52:10.258
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:52:10.274
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:52:10.279
    [It] should function for intra-pod communication: http [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:82
    STEP: Performing setup for networking test in namespace pod-network-test-3271 06/28/23 08:52:10.285
    STEP: creating a selector 06/28/23 08:52:10.285
    STEP: Creating the service pods in kubernetes 06/28/23 08:52:10.285
    Jun 28 08:52:10.286: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jun 28 08:52:10.321: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-3271" to be "running and ready"
    Jun 28 08:52:10.326: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.984531ms
    Jun 28 08:52:10.326: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jun 28 08:52:12.332: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.010867765s
    Jun 28 08:52:12.332: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 28 08:52:14.332: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.011180679s
    Jun 28 08:52:14.332: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 28 08:52:16.333: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.011620516s
    Jun 28 08:52:16.333: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 28 08:52:18.333: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.011264515s
    Jun 28 08:52:18.333: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 28 08:52:20.333: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.011522074s
    Jun 28 08:52:20.333: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 28 08:52:22.332: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.010769171s
    Jun 28 08:52:22.332: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 28 08:52:24.333: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.011519324s
    Jun 28 08:52:24.333: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 28 08:52:26.332: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.010387806s
    Jun 28 08:52:26.332: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 28 08:52:28.333: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.011301391s
    Jun 28 08:52:28.333: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 28 08:52:30.332: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.01110472s
    Jun 28 08:52:30.332: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 28 08:52:32.333: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.011659169s
    Jun 28 08:52:32.333: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jun 28 08:52:32.333: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jun 28 08:52:32.337: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-3271" to be "running and ready"
    Jun 28 08:52:32.344: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 6.024087ms
    Jun 28 08:52:32.344: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jun 28 08:52:32.344: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Jun 28 08:52:32.351: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-3271" to be "running and ready"
    Jun 28 08:52:32.355: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 4.46507ms
    Jun 28 08:52:32.355: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Jun 28 08:52:32.355: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 06/28/23 08:52:32.36
    Jun 28 08:52:32.366: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-3271" to be "running"
    Jun 28 08:52:32.373: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.235676ms
    Jun 28 08:52:34.379: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.012973552s
    Jun 28 08:52:34.379: INFO: Pod "test-container-pod" satisfied condition "running"
    Jun 28 08:52:34.384: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Jun 28 08:52:34.384: INFO: Breadth first check of 172.21.122.63 on host 192.168.11.3...
    Jun 28 08:52:34.389: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.21.122.59:9080/dial?request=hostname&protocol=http&host=172.21.122.63&port=8083&tries=1'] Namespace:pod-network-test-3271 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 28 08:52:34.389: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    Jun 28 08:52:34.389: INFO: ExecWithOptions: Clientset creation
    Jun 28 08:52:34.389: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/pod-network-test-3271/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.21.122.59%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.21.122.63%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jun 28 08:52:34.802: INFO: Waiting for responses: map[]
    Jun 28 08:52:34.802: INFO: reached 172.21.122.63 after 0/1 tries
    Jun 28 08:52:34.802: INFO: Breadth first check of 172.21.122.91 on host 192.168.11.4...
    Jun 28 08:52:34.807: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.21.122.59:9080/dial?request=hostname&protocol=http&host=172.21.122.91&port=8083&tries=1'] Namespace:pod-network-test-3271 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 28 08:52:34.807: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    Jun 28 08:52:34.808: INFO: ExecWithOptions: Clientset creation
    Jun 28 08:52:34.808: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/pod-network-test-3271/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.21.122.59%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.21.122.91%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jun 28 08:52:35.218: INFO: Waiting for responses: map[]
    Jun 28 08:52:35.218: INFO: reached 172.21.122.91 after 0/1 tries
    Jun 28 08:52:35.218: INFO: Breadth first check of 172.21.30.114 on host 192.168.11.5...
    Jun 28 08:52:35.224: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.21.122.59:9080/dial?request=hostname&protocol=http&host=172.21.30.114&port=8083&tries=1'] Namespace:pod-network-test-3271 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 28 08:52:35.224: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    Jun 28 08:52:35.224: INFO: ExecWithOptions: Clientset creation
    Jun 28 08:52:35.224: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/pod-network-test-3271/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.21.122.59%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.21.30.114%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jun 28 08:52:35.637: INFO: Waiting for responses: map[]
    Jun 28 08:52:35.637: INFO: reached 172.21.30.114 after 0/1 tries
    Jun 28 08:52:35.637: INFO: Going to retry 0 out of 3 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Jun 28 08:52:35.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-3271" for this suite. 06/28/23 08:52:35.645
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:52:35.657
Jun 28 08:52:35.657: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename container-probe 06/28/23 08:52:35.658
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:52:35.671
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:52:35.676
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68
Jun 28 08:52:35.691: INFO: Waiting up to 5m0s for pod "test-webserver-3631a8ee-895c-4389-8350-f7f491c14067" in namespace "container-probe-8183" to be "running and ready"
Jun 28 08:52:35.695: INFO: Pod "test-webserver-3631a8ee-895c-4389-8350-f7f491c14067": Phase="Pending", Reason="", readiness=false. Elapsed: 3.834103ms
Jun 28 08:52:35.695: INFO: The phase of Pod test-webserver-3631a8ee-895c-4389-8350-f7f491c14067 is Pending, waiting for it to be Running (with Ready = true)
Jun 28 08:52:37.700: INFO: Pod "test-webserver-3631a8ee-895c-4389-8350-f7f491c14067": Phase="Running", Reason="", readiness=false. Elapsed: 2.00896596s
Jun 28 08:52:37.700: INFO: The phase of Pod test-webserver-3631a8ee-895c-4389-8350-f7f491c14067 is Running (Ready = false)
Jun 28 08:52:39.701: INFO: Pod "test-webserver-3631a8ee-895c-4389-8350-f7f491c14067": Phase="Running", Reason="", readiness=false. Elapsed: 4.010177169s
Jun 28 08:52:39.701: INFO: The phase of Pod test-webserver-3631a8ee-895c-4389-8350-f7f491c14067 is Running (Ready = false)
Jun 28 08:52:41.700: INFO: Pod "test-webserver-3631a8ee-895c-4389-8350-f7f491c14067": Phase="Running", Reason="", readiness=false. Elapsed: 6.009606431s
Jun 28 08:52:41.700: INFO: The phase of Pod test-webserver-3631a8ee-895c-4389-8350-f7f491c14067 is Running (Ready = false)
Jun 28 08:52:43.700: INFO: Pod "test-webserver-3631a8ee-895c-4389-8350-f7f491c14067": Phase="Running", Reason="", readiness=false. Elapsed: 8.009699419s
Jun 28 08:52:43.701: INFO: The phase of Pod test-webserver-3631a8ee-895c-4389-8350-f7f491c14067 is Running (Ready = false)
Jun 28 08:52:45.700: INFO: Pod "test-webserver-3631a8ee-895c-4389-8350-f7f491c14067": Phase="Running", Reason="", readiness=false. Elapsed: 10.009490295s
Jun 28 08:52:45.700: INFO: The phase of Pod test-webserver-3631a8ee-895c-4389-8350-f7f491c14067 is Running (Ready = false)
Jun 28 08:52:47.700: INFO: Pod "test-webserver-3631a8ee-895c-4389-8350-f7f491c14067": Phase="Running", Reason="", readiness=false. Elapsed: 12.00951428s
Jun 28 08:52:47.700: INFO: The phase of Pod test-webserver-3631a8ee-895c-4389-8350-f7f491c14067 is Running (Ready = false)
Jun 28 08:52:49.703: INFO: Pod "test-webserver-3631a8ee-895c-4389-8350-f7f491c14067": Phase="Running", Reason="", readiness=false. Elapsed: 14.011849795s
Jun 28 08:52:49.703: INFO: The phase of Pod test-webserver-3631a8ee-895c-4389-8350-f7f491c14067 is Running (Ready = false)
Jun 28 08:52:51.700: INFO: Pod "test-webserver-3631a8ee-895c-4389-8350-f7f491c14067": Phase="Running", Reason="", readiness=false. Elapsed: 16.009181066s
Jun 28 08:52:51.700: INFO: The phase of Pod test-webserver-3631a8ee-895c-4389-8350-f7f491c14067 is Running (Ready = false)
Jun 28 08:52:53.701: INFO: Pod "test-webserver-3631a8ee-895c-4389-8350-f7f491c14067": Phase="Running", Reason="", readiness=false. Elapsed: 18.010104285s
Jun 28 08:52:53.701: INFO: The phase of Pod test-webserver-3631a8ee-895c-4389-8350-f7f491c14067 is Running (Ready = false)
Jun 28 08:52:55.701: INFO: Pod "test-webserver-3631a8ee-895c-4389-8350-f7f491c14067": Phase="Running", Reason="", readiness=false. Elapsed: 20.010554711s
Jun 28 08:52:55.701: INFO: The phase of Pod test-webserver-3631a8ee-895c-4389-8350-f7f491c14067 is Running (Ready = false)
Jun 28 08:52:57.701: INFO: Pod "test-webserver-3631a8ee-895c-4389-8350-f7f491c14067": Phase="Running", Reason="", readiness=true. Elapsed: 22.009766679s
Jun 28 08:52:57.701: INFO: The phase of Pod test-webserver-3631a8ee-895c-4389-8350-f7f491c14067 is Running (Ready = true)
Jun 28 08:52:57.701: INFO: Pod "test-webserver-3631a8ee-895c-4389-8350-f7f491c14067" satisfied condition "running and ready"
Jun 28 08:52:57.705: INFO: Container started at 2023-06-28 08:52:36 +0000 UTC, pod became ready at 2023-06-28 08:52:56 +0000 UTC
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jun 28 08:52:57.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8183" for this suite. 06/28/23 08:52:57.714
{"msg":"PASSED [sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","completed":287,"skipped":5208,"failed":0}
------------------------------
â€¢ [SLOW TEST] [22.064 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:52:35.657
    Jun 28 08:52:35.657: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename container-probe 06/28/23 08:52:35.658
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:52:35.671
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:52:35.676
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:68
    Jun 28 08:52:35.691: INFO: Waiting up to 5m0s for pod "test-webserver-3631a8ee-895c-4389-8350-f7f491c14067" in namespace "container-probe-8183" to be "running and ready"
    Jun 28 08:52:35.695: INFO: Pod "test-webserver-3631a8ee-895c-4389-8350-f7f491c14067": Phase="Pending", Reason="", readiness=false. Elapsed: 3.834103ms
    Jun 28 08:52:35.695: INFO: The phase of Pod test-webserver-3631a8ee-895c-4389-8350-f7f491c14067 is Pending, waiting for it to be Running (with Ready = true)
    Jun 28 08:52:37.700: INFO: Pod "test-webserver-3631a8ee-895c-4389-8350-f7f491c14067": Phase="Running", Reason="", readiness=false. Elapsed: 2.00896596s
    Jun 28 08:52:37.700: INFO: The phase of Pod test-webserver-3631a8ee-895c-4389-8350-f7f491c14067 is Running (Ready = false)
    Jun 28 08:52:39.701: INFO: Pod "test-webserver-3631a8ee-895c-4389-8350-f7f491c14067": Phase="Running", Reason="", readiness=false. Elapsed: 4.010177169s
    Jun 28 08:52:39.701: INFO: The phase of Pod test-webserver-3631a8ee-895c-4389-8350-f7f491c14067 is Running (Ready = false)
    Jun 28 08:52:41.700: INFO: Pod "test-webserver-3631a8ee-895c-4389-8350-f7f491c14067": Phase="Running", Reason="", readiness=false. Elapsed: 6.009606431s
    Jun 28 08:52:41.700: INFO: The phase of Pod test-webserver-3631a8ee-895c-4389-8350-f7f491c14067 is Running (Ready = false)
    Jun 28 08:52:43.700: INFO: Pod "test-webserver-3631a8ee-895c-4389-8350-f7f491c14067": Phase="Running", Reason="", readiness=false. Elapsed: 8.009699419s
    Jun 28 08:52:43.701: INFO: The phase of Pod test-webserver-3631a8ee-895c-4389-8350-f7f491c14067 is Running (Ready = false)
    Jun 28 08:52:45.700: INFO: Pod "test-webserver-3631a8ee-895c-4389-8350-f7f491c14067": Phase="Running", Reason="", readiness=false. Elapsed: 10.009490295s
    Jun 28 08:52:45.700: INFO: The phase of Pod test-webserver-3631a8ee-895c-4389-8350-f7f491c14067 is Running (Ready = false)
    Jun 28 08:52:47.700: INFO: Pod "test-webserver-3631a8ee-895c-4389-8350-f7f491c14067": Phase="Running", Reason="", readiness=false. Elapsed: 12.00951428s
    Jun 28 08:52:47.700: INFO: The phase of Pod test-webserver-3631a8ee-895c-4389-8350-f7f491c14067 is Running (Ready = false)
    Jun 28 08:52:49.703: INFO: Pod "test-webserver-3631a8ee-895c-4389-8350-f7f491c14067": Phase="Running", Reason="", readiness=false. Elapsed: 14.011849795s
    Jun 28 08:52:49.703: INFO: The phase of Pod test-webserver-3631a8ee-895c-4389-8350-f7f491c14067 is Running (Ready = false)
    Jun 28 08:52:51.700: INFO: Pod "test-webserver-3631a8ee-895c-4389-8350-f7f491c14067": Phase="Running", Reason="", readiness=false. Elapsed: 16.009181066s
    Jun 28 08:52:51.700: INFO: The phase of Pod test-webserver-3631a8ee-895c-4389-8350-f7f491c14067 is Running (Ready = false)
    Jun 28 08:52:53.701: INFO: Pod "test-webserver-3631a8ee-895c-4389-8350-f7f491c14067": Phase="Running", Reason="", readiness=false. Elapsed: 18.010104285s
    Jun 28 08:52:53.701: INFO: The phase of Pod test-webserver-3631a8ee-895c-4389-8350-f7f491c14067 is Running (Ready = false)
    Jun 28 08:52:55.701: INFO: Pod "test-webserver-3631a8ee-895c-4389-8350-f7f491c14067": Phase="Running", Reason="", readiness=false. Elapsed: 20.010554711s
    Jun 28 08:52:55.701: INFO: The phase of Pod test-webserver-3631a8ee-895c-4389-8350-f7f491c14067 is Running (Ready = false)
    Jun 28 08:52:57.701: INFO: Pod "test-webserver-3631a8ee-895c-4389-8350-f7f491c14067": Phase="Running", Reason="", readiness=true. Elapsed: 22.009766679s
    Jun 28 08:52:57.701: INFO: The phase of Pod test-webserver-3631a8ee-895c-4389-8350-f7f491c14067 is Running (Ready = true)
    Jun 28 08:52:57.701: INFO: Pod "test-webserver-3631a8ee-895c-4389-8350-f7f491c14067" satisfied condition "running and ready"
    Jun 28 08:52:57.705: INFO: Container started at 2023-06-28 08:52:36 +0000 UTC, pod became ready at 2023-06-28 08:52:56 +0000 UTC
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jun 28 08:52:57.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-8183" for this suite. 06/28/23 08:52:57.714
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:52:57.721
Jun 28 08:52:57.721: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename downward-api 06/28/23 08:52:57.723
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:52:57.737
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:52:57.742
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161
STEP: Creating the pod 06/28/23 08:52:57.746
Jun 28 08:52:57.756: INFO: Waiting up to 5m0s for pod "annotationupdateeb332ef4-ac02-47cc-b612-394081b9e7e5" in namespace "downward-api-253" to be "running and ready"
Jun 28 08:52:57.761: INFO: Pod "annotationupdateeb332ef4-ac02-47cc-b612-394081b9e7e5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.262584ms
Jun 28 08:52:57.761: INFO: The phase of Pod annotationupdateeb332ef4-ac02-47cc-b612-394081b9e7e5 is Pending, waiting for it to be Running (with Ready = true)
Jun 28 08:52:59.767: INFO: Pod "annotationupdateeb332ef4-ac02-47cc-b612-394081b9e7e5": Phase="Running", Reason="", readiness=true. Elapsed: 2.011022864s
Jun 28 08:52:59.767: INFO: The phase of Pod annotationupdateeb332ef4-ac02-47cc-b612-394081b9e7e5 is Running (Ready = true)
Jun 28 08:52:59.767: INFO: Pod "annotationupdateeb332ef4-ac02-47cc-b612-394081b9e7e5" satisfied condition "running and ready"
Jun 28 08:53:00.338: INFO: Successfully updated pod "annotationupdateeb332ef4-ac02-47cc-b612-394081b9e7e5"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jun 28 08:53:04.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-253" for this suite. 06/28/23 08:53:04.384
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","completed":288,"skipped":5211,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.670 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:52:57.721
    Jun 28 08:52:57.721: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename downward-api 06/28/23 08:52:57.723
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:52:57.737
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:52:57.742
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:161
    STEP: Creating the pod 06/28/23 08:52:57.746
    Jun 28 08:52:57.756: INFO: Waiting up to 5m0s for pod "annotationupdateeb332ef4-ac02-47cc-b612-394081b9e7e5" in namespace "downward-api-253" to be "running and ready"
    Jun 28 08:52:57.761: INFO: Pod "annotationupdateeb332ef4-ac02-47cc-b612-394081b9e7e5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.262584ms
    Jun 28 08:52:57.761: INFO: The phase of Pod annotationupdateeb332ef4-ac02-47cc-b612-394081b9e7e5 is Pending, waiting for it to be Running (with Ready = true)
    Jun 28 08:52:59.767: INFO: Pod "annotationupdateeb332ef4-ac02-47cc-b612-394081b9e7e5": Phase="Running", Reason="", readiness=true. Elapsed: 2.011022864s
    Jun 28 08:52:59.767: INFO: The phase of Pod annotationupdateeb332ef4-ac02-47cc-b612-394081b9e7e5 is Running (Ready = true)
    Jun 28 08:52:59.767: INFO: Pod "annotationupdateeb332ef4-ac02-47cc-b612-394081b9e7e5" satisfied condition "running and ready"
    Jun 28 08:53:00.338: INFO: Successfully updated pod "annotationupdateeb332ef4-ac02-47cc-b612-394081b9e7e5"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jun 28 08:53:04.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-253" for this suite. 06/28/23 08:53:04.384
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] server version
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
[BeforeEach] [sig-api-machinery] server version
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:53:04.392
Jun 28 08:53:04.392: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename server-version 06/28/23 08:53:04.393
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:53:04.41
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:53:04.415
[It] should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
STEP: Request ServerVersion 06/28/23 08:53:04.421
STEP: Confirm major version 06/28/23 08:53:04.423
Jun 28 08:53:04.423: INFO: Major version: 1
STEP: Confirm minor version 06/28/23 08:53:04.423
Jun 28 08:53:04.424: INFO: cleanMinorVersion: 25
Jun 28 08:53:04.424: INFO: Minor version: 25+
[AfterEach] [sig-api-machinery] server version
  test/e2e/framework/framework.go:187
Jun 28 08:53:04.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-2675" for this suite. 06/28/23 08:53:04.433
{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","completed":289,"skipped":5214,"failed":0}
------------------------------
â€¢ [0.048 seconds]
[sig-api-machinery] server version
test/e2e/apimachinery/framework.go:23
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] server version
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:53:04.392
    Jun 28 08:53:04.392: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename server-version 06/28/23 08:53:04.393
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:53:04.41
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:53:04.415
    [It] should find the server version [Conformance]
      test/e2e/apimachinery/server_version.go:39
    STEP: Request ServerVersion 06/28/23 08:53:04.421
    STEP: Confirm major version 06/28/23 08:53:04.423
    Jun 28 08:53:04.423: INFO: Major version: 1
    STEP: Confirm minor version 06/28/23 08:53:04.423
    Jun 28 08:53:04.424: INFO: cleanMinorVersion: 25
    Jun 28 08:53:04.424: INFO: Minor version: 25+
    [AfterEach] [sig-api-machinery] server version
      test/e2e/framework/framework.go:187
    Jun 28 08:53:04.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "server-version-2675" for this suite. 06/28/23 08:53:04.433
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:53:04.441
Jun 28 08:53:04.441: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename kubelet-test 06/28/23 08:53:04.442
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:53:04.458
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:53:04.464
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
Jun 28 08:53:04.479: INFO: Waiting up to 5m0s for pod "busybox-readonly-fs7bd5d488-9fe2-4a9f-b17d-9a65deb2d035" in namespace "kubelet-test-5914" to be "running and ready"
Jun 28 08:53:04.484: INFO: Pod "busybox-readonly-fs7bd5d488-9fe2-4a9f-b17d-9a65deb2d035": Phase="Pending", Reason="", readiness=false. Elapsed: 4.691946ms
Jun 28 08:53:04.484: INFO: The phase of Pod busybox-readonly-fs7bd5d488-9fe2-4a9f-b17d-9a65deb2d035 is Pending, waiting for it to be Running (with Ready = true)
Jun 28 08:53:06.489: INFO: Pod "busybox-readonly-fs7bd5d488-9fe2-4a9f-b17d-9a65deb2d035": Phase="Running", Reason="", readiness=true. Elapsed: 2.010279367s
Jun 28 08:53:06.489: INFO: The phase of Pod busybox-readonly-fs7bd5d488-9fe2-4a9f-b17d-9a65deb2d035 is Running (Ready = true)
Jun 28 08:53:06.489: INFO: Pod "busybox-readonly-fs7bd5d488-9fe2-4a9f-b17d-9a65deb2d035" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Jun 28 08:53:06.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5914" for this suite. 06/28/23 08:53:06.52
{"msg":"PASSED [sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","completed":290,"skipped":5232,"failed":0}
------------------------------
â€¢ [2.086 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a read only busybox container
  test/e2e/common/node/kubelet.go:175
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:53:04.441
    Jun 28 08:53:04.441: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename kubelet-test 06/28/23 08:53:04.442
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:53:04.458
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:53:04.464
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:184
    Jun 28 08:53:04.479: INFO: Waiting up to 5m0s for pod "busybox-readonly-fs7bd5d488-9fe2-4a9f-b17d-9a65deb2d035" in namespace "kubelet-test-5914" to be "running and ready"
    Jun 28 08:53:04.484: INFO: Pod "busybox-readonly-fs7bd5d488-9fe2-4a9f-b17d-9a65deb2d035": Phase="Pending", Reason="", readiness=false. Elapsed: 4.691946ms
    Jun 28 08:53:04.484: INFO: The phase of Pod busybox-readonly-fs7bd5d488-9fe2-4a9f-b17d-9a65deb2d035 is Pending, waiting for it to be Running (with Ready = true)
    Jun 28 08:53:06.489: INFO: Pod "busybox-readonly-fs7bd5d488-9fe2-4a9f-b17d-9a65deb2d035": Phase="Running", Reason="", readiness=true. Elapsed: 2.010279367s
    Jun 28 08:53:06.489: INFO: The phase of Pod busybox-readonly-fs7bd5d488-9fe2-4a9f-b17d-9a65deb2d035 is Running (Ready = true)
    Jun 28 08:53:06.489: INFO: Pod "busybox-readonly-fs7bd5d488-9fe2-4a9f-b17d-9a65deb2d035" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Jun 28 08:53:06.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-5914" for this suite. 06/28/23 08:53:06.52
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:53:06.527
Jun 28 08:53:06.527: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename resourcequota 06/28/23 08:53:06.528
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:53:06.542
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:53:06.547
[It] should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793
STEP: Creating a ResourceQuota with best effort scope 06/28/23 08:53:06.552
STEP: Ensuring ResourceQuota status is calculated 06/28/23 08:53:06.558
STEP: Creating a ResourceQuota with not best effort scope 06/28/23 08:53:08.563
STEP: Ensuring ResourceQuota status is calculated 06/28/23 08:53:08.569
STEP: Creating a best-effort pod 06/28/23 08:53:10.578
STEP: Ensuring resource quota with best effort scope captures the pod usage 06/28/23 08:53:10.598
STEP: Ensuring resource quota with not best effort ignored the pod usage 06/28/23 08:53:12.603
STEP: Deleting the pod 06/28/23 08:53:14.609
STEP: Ensuring resource quota status released the pod usage 06/28/23 08:53:14.62
STEP: Creating a not best-effort pod 06/28/23 08:53:16.627
STEP: Ensuring resource quota with not best effort scope captures the pod usage 06/28/23 08:53:16.638
STEP: Ensuring resource quota with best effort scope ignored the pod usage 06/28/23 08:53:18.643
STEP: Deleting the pod 06/28/23 08:53:20.648
STEP: Ensuring resource quota status released the pod usage 06/28/23 08:53:20.659
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jun 28 08:53:22.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7557" for this suite. 06/28/23 08:53:22.671
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","completed":291,"skipped":5234,"failed":0}
------------------------------
â€¢ [SLOW TEST] [16.150 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:53:06.527
    Jun 28 08:53:06.527: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename resourcequota 06/28/23 08:53:06.528
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:53:06.542
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:53:06.547
    [It] should verify ResourceQuota with best effort scope. [Conformance]
      test/e2e/apimachinery/resource_quota.go:793
    STEP: Creating a ResourceQuota with best effort scope 06/28/23 08:53:06.552
    STEP: Ensuring ResourceQuota status is calculated 06/28/23 08:53:06.558
    STEP: Creating a ResourceQuota with not best effort scope 06/28/23 08:53:08.563
    STEP: Ensuring ResourceQuota status is calculated 06/28/23 08:53:08.569
    STEP: Creating a best-effort pod 06/28/23 08:53:10.578
    STEP: Ensuring resource quota with best effort scope captures the pod usage 06/28/23 08:53:10.598
    STEP: Ensuring resource quota with not best effort ignored the pod usage 06/28/23 08:53:12.603
    STEP: Deleting the pod 06/28/23 08:53:14.609
    STEP: Ensuring resource quota status released the pod usage 06/28/23 08:53:14.62
    STEP: Creating a not best-effort pod 06/28/23 08:53:16.627
    STEP: Ensuring resource quota with not best effort scope captures the pod usage 06/28/23 08:53:16.638
    STEP: Ensuring resource quota with best effort scope ignored the pod usage 06/28/23 08:53:18.643
    STEP: Deleting the pod 06/28/23 08:53:20.648
    STEP: Ensuring resource quota status released the pod usage 06/28/23 08:53:20.659
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jun 28 08:53:22.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-7557" for this suite. 06/28/23 08:53:22.671
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:53:22.678
Jun 28 08:53:22.679: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename containers 06/28/23 08:53:22.679
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:53:22.693
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:53:22.697
[It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72
STEP: Creating a pod to test override command 06/28/23 08:53:22.701
Jun 28 08:53:22.709: INFO: Waiting up to 5m0s for pod "client-containers-feeca310-227d-4128-b5d6-7456599e80f7" in namespace "containers-8528" to be "Succeeded or Failed"
Jun 28 08:53:22.713: INFO: Pod "client-containers-feeca310-227d-4128-b5d6-7456599e80f7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.953271ms
Jun 28 08:53:24.719: INFO: Pod "client-containers-feeca310-227d-4128-b5d6-7456599e80f7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009803321s
Jun 28 08:53:26.718: INFO: Pod "client-containers-feeca310-227d-4128-b5d6-7456599e80f7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008940647s
STEP: Saw pod success 06/28/23 08:53:26.718
Jun 28 08:53:26.718: INFO: Pod "client-containers-feeca310-227d-4128-b5d6-7456599e80f7" satisfied condition "Succeeded or Failed"
Jun 28 08:53:26.722: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-zxlfv pod client-containers-feeca310-227d-4128-b5d6-7456599e80f7 container agnhost-container: <nil>
STEP: delete the pod 06/28/23 08:53:26.771
Jun 28 08:53:26.784: INFO: Waiting for pod client-containers-feeca310-227d-4128-b5d6-7456599e80f7 to disappear
Jun 28 08:53:26.789: INFO: Pod client-containers-feeca310-227d-4128-b5d6-7456599e80f7 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Jun 28 08:53:26.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-8528" for this suite. 06/28/23 08:53:26.796
{"msg":"PASSED [sig-node] Containers should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]","completed":292,"skipped":5261,"failed":0}
------------------------------
â€¢ [4.123 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:53:22.678
    Jun 28 08:53:22.679: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename containers 06/28/23 08:53:22.679
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:53:22.693
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:53:22.697
    [It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:72
    STEP: Creating a pod to test override command 06/28/23 08:53:22.701
    Jun 28 08:53:22.709: INFO: Waiting up to 5m0s for pod "client-containers-feeca310-227d-4128-b5d6-7456599e80f7" in namespace "containers-8528" to be "Succeeded or Failed"
    Jun 28 08:53:22.713: INFO: Pod "client-containers-feeca310-227d-4128-b5d6-7456599e80f7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.953271ms
    Jun 28 08:53:24.719: INFO: Pod "client-containers-feeca310-227d-4128-b5d6-7456599e80f7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009803321s
    Jun 28 08:53:26.718: INFO: Pod "client-containers-feeca310-227d-4128-b5d6-7456599e80f7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008940647s
    STEP: Saw pod success 06/28/23 08:53:26.718
    Jun 28 08:53:26.718: INFO: Pod "client-containers-feeca310-227d-4128-b5d6-7456599e80f7" satisfied condition "Succeeded or Failed"
    Jun 28 08:53:26.722: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-zxlfv pod client-containers-feeca310-227d-4128-b5d6-7456599e80f7 container agnhost-container: <nil>
    STEP: delete the pod 06/28/23 08:53:26.771
    Jun 28 08:53:26.784: INFO: Waiting for pod client-containers-feeca310-227d-4128-b5d6-7456599e80f7 to disappear
    Jun 28 08:53:26.789: INFO: Pod client-containers-feeca310-227d-4128-b5d6-7456599e80f7 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Jun 28 08:53:26.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-8528" for this suite. 06/28/23 08:53:26.796
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:53:26.807
Jun 28 08:53:26.807: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename crd-publish-openapi 06/28/23 08:53:26.808
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:53:26.821
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:53:26.825
[It] works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152
Jun 28 08:53:26.829: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 06/28/23 08:53:28.953
Jun 28 08:53:28.953: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=crd-publish-openapi-9650 --namespace=crd-publish-openapi-9650 create -f -'
Jun 28 08:53:29.605: INFO: stderr: ""
Jun 28 08:53:29.605: INFO: stdout: "e2e-test-crd-publish-openapi-3414-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jun 28 08:53:29.605: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=crd-publish-openapi-9650 --namespace=crd-publish-openapi-9650 delete e2e-test-crd-publish-openapi-3414-crds test-cr'
Jun 28 08:53:29.682: INFO: stderr: ""
Jun 28 08:53:29.682: INFO: stdout: "e2e-test-crd-publish-openapi-3414-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Jun 28 08:53:29.682: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=crd-publish-openapi-9650 --namespace=crd-publish-openapi-9650 apply -f -'
Jun 28 08:53:29.884: INFO: stderr: ""
Jun 28 08:53:29.884: INFO: stdout: "e2e-test-crd-publish-openapi-3414-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jun 28 08:53:29.884: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=crd-publish-openapi-9650 --namespace=crd-publish-openapi-9650 delete e2e-test-crd-publish-openapi-3414-crds test-cr'
Jun 28 08:53:29.956: INFO: stderr: ""
Jun 28 08:53:29.956: INFO: stdout: "e2e-test-crd-publish-openapi-3414-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema 06/28/23 08:53:29.956
Jun 28 08:53:29.956: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=crd-publish-openapi-9650 explain e2e-test-crd-publish-openapi-3414-crds'
Jun 28 08:53:30.152: INFO: stderr: ""
Jun 28 08:53:30.152: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-3414-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jun 28 08:53:32.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9650" for this suite. 06/28/23 08:53:32.303
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","completed":293,"skipped":5312,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.503 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:53:26.807
    Jun 28 08:53:26.807: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename crd-publish-openapi 06/28/23 08:53:26.808
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:53:26.821
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:53:26.825
    [It] works for CRD without validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:152
    Jun 28 08:53:26.829: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 06/28/23 08:53:28.953
    Jun 28 08:53:28.953: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=crd-publish-openapi-9650 --namespace=crd-publish-openapi-9650 create -f -'
    Jun 28 08:53:29.605: INFO: stderr: ""
    Jun 28 08:53:29.605: INFO: stdout: "e2e-test-crd-publish-openapi-3414-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Jun 28 08:53:29.605: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=crd-publish-openapi-9650 --namespace=crd-publish-openapi-9650 delete e2e-test-crd-publish-openapi-3414-crds test-cr'
    Jun 28 08:53:29.682: INFO: stderr: ""
    Jun 28 08:53:29.682: INFO: stdout: "e2e-test-crd-publish-openapi-3414-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    Jun 28 08:53:29.682: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=crd-publish-openapi-9650 --namespace=crd-publish-openapi-9650 apply -f -'
    Jun 28 08:53:29.884: INFO: stderr: ""
    Jun 28 08:53:29.884: INFO: stdout: "e2e-test-crd-publish-openapi-3414-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Jun 28 08:53:29.884: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=crd-publish-openapi-9650 --namespace=crd-publish-openapi-9650 delete e2e-test-crd-publish-openapi-3414-crds test-cr'
    Jun 28 08:53:29.956: INFO: stderr: ""
    Jun 28 08:53:29.956: INFO: stdout: "e2e-test-crd-publish-openapi-3414-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR without validation schema 06/28/23 08:53:29.956
    Jun 28 08:53:29.956: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=crd-publish-openapi-9650 explain e2e-test-crd-publish-openapi-3414-crds'
    Jun 28 08:53:30.152: INFO: stderr: ""
    Jun 28 08:53:30.152: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-3414-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jun 28 08:53:32.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-9650" for this suite. 06/28/23 08:53:32.303
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions
  should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:822
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:53:32.311
Jun 28 08:53:32.311: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename kubectl 06/28/23 08:53:32.311
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:53:32.323
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:53:32.327
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:822
STEP: validating api versions 06/28/23 08:53:32.332
Jun 28 08:53:32.332: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-3201 api-versions'
Jun 28 08:53:32.398: INFO: stderr: ""
Jun 28 08:53:32.398: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nautoscaling/v2beta2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta2\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jun 28 08:53:32.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3201" for this suite. 06/28/23 08:53:32.406
{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","completed":294,"skipped":5337,"failed":0}
------------------------------
â€¢ [0.102 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  test/e2e/kubectl/kubectl.go:816
    should check if v1 is in available api versions  [Conformance]
    test/e2e/kubectl/kubectl.go:822

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:53:32.311
    Jun 28 08:53:32.311: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename kubectl 06/28/23 08:53:32.311
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:53:32.323
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:53:32.327
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if v1 is in available api versions  [Conformance]
      test/e2e/kubectl/kubectl.go:822
    STEP: validating api versions 06/28/23 08:53:32.332
    Jun 28 08:53:32.332: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-3201 api-versions'
    Jun 28 08:53:32.398: INFO: stderr: ""
    Jun 28 08:53:32.398: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nautoscaling/v2beta2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta2\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jun 28 08:53:32.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-3201" for this suite. 06/28/23 08:53:32.406
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:53:32.416
Jun 28 08:53:32.416: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename projected 06/28/23 08:53:32.417
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:53:32.429
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:53:32.433
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248
STEP: Creating a pod to test downward API volume plugin 06/28/23 08:53:32.438
Jun 28 08:53:32.447: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f5e63436-0d37-454a-b5ef-8cf466c61bf6" in namespace "projected-5998" to be "Succeeded or Failed"
Jun 28 08:53:32.452: INFO: Pod "downwardapi-volume-f5e63436-0d37-454a-b5ef-8cf466c61bf6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.154052ms
Jun 28 08:53:34.458: INFO: Pod "downwardapi-volume-f5e63436-0d37-454a-b5ef-8cf466c61bf6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010496222s
Jun 28 08:53:36.458: INFO: Pod "downwardapi-volume-f5e63436-0d37-454a-b5ef-8cf466c61bf6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010651376s
STEP: Saw pod success 06/28/23 08:53:36.458
Jun 28 08:53:36.458: INFO: Pod "downwardapi-volume-f5e63436-0d37-454a-b5ef-8cf466c61bf6" satisfied condition "Succeeded or Failed"
Jun 28 08:53:36.463: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod downwardapi-volume-f5e63436-0d37-454a-b5ef-8cf466c61bf6 container client-container: <nil>
STEP: delete the pod 06/28/23 08:53:36.479
Jun 28 08:53:36.491: INFO: Waiting for pod downwardapi-volume-f5e63436-0d37-454a-b5ef-8cf466c61bf6 to disappear
Jun 28 08:53:36.495: INFO: Pod downwardapi-volume-f5e63436-0d37-454a-b5ef-8cf466c61bf6 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jun 28 08:53:36.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5998" for this suite. 06/28/23 08:53:36.502
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","completed":295,"skipped":5378,"failed":0}
------------------------------
â€¢ [4.094 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:53:32.416
    Jun 28 08:53:32.416: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename projected 06/28/23 08:53:32.417
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:53:32.429
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:53:32.433
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:248
    STEP: Creating a pod to test downward API volume plugin 06/28/23 08:53:32.438
    Jun 28 08:53:32.447: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f5e63436-0d37-454a-b5ef-8cf466c61bf6" in namespace "projected-5998" to be "Succeeded or Failed"
    Jun 28 08:53:32.452: INFO: Pod "downwardapi-volume-f5e63436-0d37-454a-b5ef-8cf466c61bf6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.154052ms
    Jun 28 08:53:34.458: INFO: Pod "downwardapi-volume-f5e63436-0d37-454a-b5ef-8cf466c61bf6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010496222s
    Jun 28 08:53:36.458: INFO: Pod "downwardapi-volume-f5e63436-0d37-454a-b5ef-8cf466c61bf6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010651376s
    STEP: Saw pod success 06/28/23 08:53:36.458
    Jun 28 08:53:36.458: INFO: Pod "downwardapi-volume-f5e63436-0d37-454a-b5ef-8cf466c61bf6" satisfied condition "Succeeded or Failed"
    Jun 28 08:53:36.463: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod downwardapi-volume-f5e63436-0d37-454a-b5ef-8cf466c61bf6 container client-container: <nil>
    STEP: delete the pod 06/28/23 08:53:36.479
    Jun 28 08:53:36.491: INFO: Waiting for pod downwardapi-volume-f5e63436-0d37-454a-b5ef-8cf466c61bf6 to disappear
    Jun 28 08:53:36.495: INFO: Pod downwardapi-volume-f5e63436-0d37-454a-b5ef-8cf466c61bf6 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jun 28 08:53:36.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5998" for this suite. 06/28/23 08:53:36.502
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:53:36.512
Jun 28 08:53:36.512: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename subpath 06/28/23 08:53:36.513
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:53:36.526
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:53:36.53
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 06/28/23 08:53:36.534
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
STEP: Creating pod pod-subpath-test-configmap-qxh9 06/28/23 08:53:36.546
STEP: Creating a pod to test atomic-volume-subpath 06/28/23 08:53:36.546
Jun 28 08:53:36.557: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-qxh9" in namespace "subpath-2241" to be "Succeeded or Failed"
Jun 28 08:53:36.561: INFO: Pod "pod-subpath-test-configmap-qxh9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.990314ms
Jun 28 08:53:38.566: INFO: Pod "pod-subpath-test-configmap-qxh9": Phase="Running", Reason="", readiness=true. Elapsed: 2.009493079s
Jun 28 08:53:40.567: INFO: Pod "pod-subpath-test-configmap-qxh9": Phase="Running", Reason="", readiness=true. Elapsed: 4.009707649s
Jun 28 08:53:42.567: INFO: Pod "pod-subpath-test-configmap-qxh9": Phase="Running", Reason="", readiness=true. Elapsed: 6.009805101s
Jun 28 08:53:44.566: INFO: Pod "pod-subpath-test-configmap-qxh9": Phase="Running", Reason="", readiness=true. Elapsed: 8.009087665s
Jun 28 08:53:46.567: INFO: Pod "pod-subpath-test-configmap-qxh9": Phase="Running", Reason="", readiness=true. Elapsed: 10.009689113s
Jun 28 08:53:48.567: INFO: Pod "pod-subpath-test-configmap-qxh9": Phase="Running", Reason="", readiness=true. Elapsed: 12.009748657s
Jun 28 08:53:50.567: INFO: Pod "pod-subpath-test-configmap-qxh9": Phase="Running", Reason="", readiness=true. Elapsed: 14.010421927s
Jun 28 08:53:52.567: INFO: Pod "pod-subpath-test-configmap-qxh9": Phase="Running", Reason="", readiness=true. Elapsed: 16.010341861s
Jun 28 08:53:54.566: INFO: Pod "pod-subpath-test-configmap-qxh9": Phase="Running", Reason="", readiness=true. Elapsed: 18.009003155s
Jun 28 08:53:56.568: INFO: Pod "pod-subpath-test-configmap-qxh9": Phase="Running", Reason="", readiness=true. Elapsed: 20.0109204s
Jun 28 08:53:58.570: INFO: Pod "pod-subpath-test-configmap-qxh9": Phase="Running", Reason="", readiness=false. Elapsed: 22.012780911s
Jun 28 08:54:00.567: INFO: Pod "pod-subpath-test-configmap-qxh9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.010309626s
STEP: Saw pod success 06/28/23 08:54:00.567
Jun 28 08:54:00.567: INFO: Pod "pod-subpath-test-configmap-qxh9" satisfied condition "Succeeded or Failed"
Jun 28 08:54:00.572: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-zxlfv pod pod-subpath-test-configmap-qxh9 container test-container-subpath-configmap-qxh9: <nil>
STEP: delete the pod 06/28/23 08:54:00.6
Jun 28 08:54:00.616: INFO: Waiting for pod pod-subpath-test-configmap-qxh9 to disappear
Jun 28 08:54:00.622: INFO: Pod pod-subpath-test-configmap-qxh9 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-qxh9 06/28/23 08:54:00.622
Jun 28 08:54:00.622: INFO: Deleting pod "pod-subpath-test-configmap-qxh9" in namespace "subpath-2241"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Jun 28 08:54:00.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2241" for this suite. 06/28/23 08:54:00.636
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [Conformance]","completed":296,"skipped":5438,"failed":0}
------------------------------
â€¢ [SLOW TEST] [24.131 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    test/e2e/storage/subpath.go:80

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:53:36.512
    Jun 28 08:53:36.512: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename subpath 06/28/23 08:53:36.513
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:53:36.526
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:53:36.53
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 06/28/23 08:53:36.534
    [It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
      test/e2e/storage/subpath.go:80
    STEP: Creating pod pod-subpath-test-configmap-qxh9 06/28/23 08:53:36.546
    STEP: Creating a pod to test atomic-volume-subpath 06/28/23 08:53:36.546
    Jun 28 08:53:36.557: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-qxh9" in namespace "subpath-2241" to be "Succeeded or Failed"
    Jun 28 08:53:36.561: INFO: Pod "pod-subpath-test-configmap-qxh9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.990314ms
    Jun 28 08:53:38.566: INFO: Pod "pod-subpath-test-configmap-qxh9": Phase="Running", Reason="", readiness=true. Elapsed: 2.009493079s
    Jun 28 08:53:40.567: INFO: Pod "pod-subpath-test-configmap-qxh9": Phase="Running", Reason="", readiness=true. Elapsed: 4.009707649s
    Jun 28 08:53:42.567: INFO: Pod "pod-subpath-test-configmap-qxh9": Phase="Running", Reason="", readiness=true. Elapsed: 6.009805101s
    Jun 28 08:53:44.566: INFO: Pod "pod-subpath-test-configmap-qxh9": Phase="Running", Reason="", readiness=true. Elapsed: 8.009087665s
    Jun 28 08:53:46.567: INFO: Pod "pod-subpath-test-configmap-qxh9": Phase="Running", Reason="", readiness=true. Elapsed: 10.009689113s
    Jun 28 08:53:48.567: INFO: Pod "pod-subpath-test-configmap-qxh9": Phase="Running", Reason="", readiness=true. Elapsed: 12.009748657s
    Jun 28 08:53:50.567: INFO: Pod "pod-subpath-test-configmap-qxh9": Phase="Running", Reason="", readiness=true. Elapsed: 14.010421927s
    Jun 28 08:53:52.567: INFO: Pod "pod-subpath-test-configmap-qxh9": Phase="Running", Reason="", readiness=true. Elapsed: 16.010341861s
    Jun 28 08:53:54.566: INFO: Pod "pod-subpath-test-configmap-qxh9": Phase="Running", Reason="", readiness=true. Elapsed: 18.009003155s
    Jun 28 08:53:56.568: INFO: Pod "pod-subpath-test-configmap-qxh9": Phase="Running", Reason="", readiness=true. Elapsed: 20.0109204s
    Jun 28 08:53:58.570: INFO: Pod "pod-subpath-test-configmap-qxh9": Phase="Running", Reason="", readiness=false. Elapsed: 22.012780911s
    Jun 28 08:54:00.567: INFO: Pod "pod-subpath-test-configmap-qxh9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.010309626s
    STEP: Saw pod success 06/28/23 08:54:00.567
    Jun 28 08:54:00.567: INFO: Pod "pod-subpath-test-configmap-qxh9" satisfied condition "Succeeded or Failed"
    Jun 28 08:54:00.572: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-zxlfv pod pod-subpath-test-configmap-qxh9 container test-container-subpath-configmap-qxh9: <nil>
    STEP: delete the pod 06/28/23 08:54:00.6
    Jun 28 08:54:00.616: INFO: Waiting for pod pod-subpath-test-configmap-qxh9 to disappear
    Jun 28 08:54:00.622: INFO: Pod pod-subpath-test-configmap-qxh9 no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-qxh9 06/28/23 08:54:00.622
    Jun 28 08:54:00.622: INFO: Deleting pod "pod-subpath-test-configmap-qxh9" in namespace "subpath-2241"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Jun 28 08:54:00.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-2241" for this suite. 06/28/23 08:54:00.636
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:527
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:54:00.647
Jun 28 08:54:00.647: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename security-context-test 06/28/23 08:54:00.657
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:54:00.671
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:54:00.676
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:527
Jun 28 08:54:00.693: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-8a6b4b2f-f624-4d44-86e8-8c547e635a4b" in namespace "security-context-test-5222" to be "Succeeded or Failed"
Jun 28 08:54:00.699: INFO: Pod "busybox-privileged-false-8a6b4b2f-f624-4d44-86e8-8c547e635a4b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.841312ms
Jun 28 08:54:02.705: INFO: Pod "busybox-privileged-false-8a6b4b2f-f624-4d44-86e8-8c547e635a4b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011858583s
Jun 28 08:54:04.705: INFO: Pod "busybox-privileged-false-8a6b4b2f-f624-4d44-86e8-8c547e635a4b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012561687s
Jun 28 08:54:04.706: INFO: Pod "busybox-privileged-false-8a6b4b2f-f624-4d44-86e8-8c547e635a4b" satisfied condition "Succeeded or Failed"
Jun 28 08:54:04.755: INFO: Got logs for pod "busybox-privileged-false-8a6b4b2f-f624-4d44-86e8-8c547e635a4b": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Jun 28 08:54:04.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-5222" for this suite. 06/28/23 08:54:04.764
{"msg":"PASSED [sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","completed":297,"skipped":5477,"failed":0}
------------------------------
â€¢ [4.123 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with privileged
  test/e2e/common/node/security_context.go:490
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:527

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:54:00.647
    Jun 28 08:54:00.647: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename security-context-test 06/28/23 08:54:00.657
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:54:00.671
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:54:00.676
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:527
    Jun 28 08:54:00.693: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-8a6b4b2f-f624-4d44-86e8-8c547e635a4b" in namespace "security-context-test-5222" to be "Succeeded or Failed"
    Jun 28 08:54:00.699: INFO: Pod "busybox-privileged-false-8a6b4b2f-f624-4d44-86e8-8c547e635a4b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.841312ms
    Jun 28 08:54:02.705: INFO: Pod "busybox-privileged-false-8a6b4b2f-f624-4d44-86e8-8c547e635a4b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011858583s
    Jun 28 08:54:04.705: INFO: Pod "busybox-privileged-false-8a6b4b2f-f624-4d44-86e8-8c547e635a4b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012561687s
    Jun 28 08:54:04.706: INFO: Pod "busybox-privileged-false-8a6b4b2f-f624-4d44-86e8-8c547e635a4b" satisfied condition "Succeeded or Failed"
    Jun 28 08:54:04.755: INFO: Got logs for pod "busybox-privileged-false-8a6b4b2f-f624-4d44-86e8-8c547e635a4b": "ip: RTNETLINK answers: Operation not permitted\n"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Jun 28 08:54:04.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-5222" for this suite. 06/28/23 08:54:04.764
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:54:04.772
Jun 28 08:54:04.772: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename emptydir 06/28/23 08:54:04.773
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:54:04.788
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:54:04.792
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146
STEP: Creating a pod to test emptydir 0777 on tmpfs 06/28/23 08:54:04.798
Jun 28 08:54:04.806: INFO: Waiting up to 5m0s for pod "pod-49db167f-987c-49c0-8b11-2ec26a7b1ddf" in namespace "emptydir-7726" to be "Succeeded or Failed"
Jun 28 08:54:04.811: INFO: Pod "pod-49db167f-987c-49c0-8b11-2ec26a7b1ddf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.852918ms
Jun 28 08:54:06.817: INFO: Pod "pod-49db167f-987c-49c0-8b11-2ec26a7b1ddf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01106112s
Jun 28 08:54:08.817: INFO: Pod "pod-49db167f-987c-49c0-8b11-2ec26a7b1ddf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010568093s
STEP: Saw pod success 06/28/23 08:54:08.817
Jun 28 08:54:08.817: INFO: Pod "pod-49db167f-987c-49c0-8b11-2ec26a7b1ddf" satisfied condition "Succeeded or Failed"
Jun 28 08:54:08.822: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod pod-49db167f-987c-49c0-8b11-2ec26a7b1ddf container test-container: <nil>
STEP: delete the pod 06/28/23 08:54:08.833
Jun 28 08:54:08.847: INFO: Waiting for pod pod-49db167f-987c-49c0-8b11-2ec26a7b1ddf to disappear
Jun 28 08:54:08.851: INFO: Pod pod-49db167f-987c-49c0-8b11-2ec26a7b1ddf no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jun 28 08:54:08.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7726" for this suite. 06/28/23 08:54:08.859
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":298,"skipped":5505,"failed":0}
------------------------------
â€¢ [4.095 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:54:04.772
    Jun 28 08:54:04.772: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename emptydir 06/28/23 08:54:04.773
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:54:04.788
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:54:04.792
    [It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:146
    STEP: Creating a pod to test emptydir 0777 on tmpfs 06/28/23 08:54:04.798
    Jun 28 08:54:04.806: INFO: Waiting up to 5m0s for pod "pod-49db167f-987c-49c0-8b11-2ec26a7b1ddf" in namespace "emptydir-7726" to be "Succeeded or Failed"
    Jun 28 08:54:04.811: INFO: Pod "pod-49db167f-987c-49c0-8b11-2ec26a7b1ddf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.852918ms
    Jun 28 08:54:06.817: INFO: Pod "pod-49db167f-987c-49c0-8b11-2ec26a7b1ddf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01106112s
    Jun 28 08:54:08.817: INFO: Pod "pod-49db167f-987c-49c0-8b11-2ec26a7b1ddf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010568093s
    STEP: Saw pod success 06/28/23 08:54:08.817
    Jun 28 08:54:08.817: INFO: Pod "pod-49db167f-987c-49c0-8b11-2ec26a7b1ddf" satisfied condition "Succeeded or Failed"
    Jun 28 08:54:08.822: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod pod-49db167f-987c-49c0-8b11-2ec26a7b1ddf container test-container: <nil>
    STEP: delete the pod 06/28/23 08:54:08.833
    Jun 28 08:54:08.847: INFO: Waiting for pod pod-49db167f-987c-49c0-8b11-2ec26a7b1ddf to disappear
    Jun 28 08:54:08.851: INFO: Pod pod-49db167f-987c-49c0-8b11-2ec26a7b1ddf no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jun 28 08:54:08.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-7726" for this suite. 06/28/23 08:54:08.859
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:54:08.868
Jun 28 08:54:08.868: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename configmap 06/28/23 08:54:08.869
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:54:08.886
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:54:08.898
[It] should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168
STEP: creating a ConfigMap 06/28/23 08:54:08.904
STEP: fetching the ConfigMap 06/28/23 08:54:08.91
STEP: patching the ConfigMap 06/28/23 08:54:08.914
STEP: listing all ConfigMaps in all namespaces with a label selector 06/28/23 08:54:08.92
STEP: deleting the ConfigMap by collection with a label selector 06/28/23 08:54:08.926
STEP: listing all ConfigMaps in test namespace 06/28/23 08:54:08.936
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Jun 28 08:54:08.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9304" for this suite. 06/28/23 08:54:08.95
{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","completed":299,"skipped":5514,"failed":0}
------------------------------
â€¢ [0.090 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:54:08.868
    Jun 28 08:54:08.868: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename configmap 06/28/23 08:54:08.869
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:54:08.886
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:54:08.898
    [It] should run through a ConfigMap lifecycle [Conformance]
      test/e2e/common/node/configmap.go:168
    STEP: creating a ConfigMap 06/28/23 08:54:08.904
    STEP: fetching the ConfigMap 06/28/23 08:54:08.91
    STEP: patching the ConfigMap 06/28/23 08:54:08.914
    STEP: listing all ConfigMaps in all namespaces with a label selector 06/28/23 08:54:08.92
    STEP: deleting the ConfigMap by collection with a label selector 06/28/23 08:54:08.926
    STEP: listing all ConfigMaps in test namespace 06/28/23 08:54:08.936
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Jun 28 08:54:08.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-9304" for this suite. 06/28/23 08:54:08.95
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] ConfigMap
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:54:08.958
Jun 28 08:54:08.958: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename configmap 06/28/23 08:54:08.959
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:54:08.987
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:54:08.994
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44
STEP: Creating configMap configmap-5281/configmap-test-282f6a7d-c436-478c-ab5d-bcf3f309e895 06/28/23 08:54:09
STEP: Creating a pod to test consume configMaps 06/28/23 08:54:09.006
Jun 28 08:54:09.016: INFO: Waiting up to 5m0s for pod "pod-configmaps-e7b0d437-e324-4337-bc4c-d3a980164753" in namespace "configmap-5281" to be "Succeeded or Failed"
Jun 28 08:54:09.022: INFO: Pod "pod-configmaps-e7b0d437-e324-4337-bc4c-d3a980164753": Phase="Pending", Reason="", readiness=false. Elapsed: 5.879961ms
Jun 28 08:54:11.027: INFO: Pod "pod-configmaps-e7b0d437-e324-4337-bc4c-d3a980164753": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011539377s
Jun 28 08:54:13.028: INFO: Pod "pod-configmaps-e7b0d437-e324-4337-bc4c-d3a980164753": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012377393s
STEP: Saw pod success 06/28/23 08:54:13.028
Jun 28 08:54:13.028: INFO: Pod "pod-configmaps-e7b0d437-e324-4337-bc4c-d3a980164753" satisfied condition "Succeeded or Failed"
Jun 28 08:54:13.033: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod pod-configmaps-e7b0d437-e324-4337-bc4c-d3a980164753 container env-test: <nil>
STEP: delete the pod 06/28/23 08:54:13.084
Jun 28 08:54:13.095: INFO: Waiting for pod pod-configmaps-e7b0d437-e324-4337-bc4c-d3a980164753 to disappear
Jun 28 08:54:13.100: INFO: Pod pod-configmaps-e7b0d437-e324-4337-bc4c-d3a980164753 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Jun 28 08:54:13.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5281" for this suite. 06/28/23 08:54:13.108
{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","completed":300,"skipped":5515,"failed":0}
------------------------------
â€¢ [4.157 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:54:08.958
    Jun 28 08:54:08.958: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename configmap 06/28/23 08:54:08.959
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:54:08.987
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:54:08.994
    [It] should be consumable via environment variable [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:44
    STEP: Creating configMap configmap-5281/configmap-test-282f6a7d-c436-478c-ab5d-bcf3f309e895 06/28/23 08:54:09
    STEP: Creating a pod to test consume configMaps 06/28/23 08:54:09.006
    Jun 28 08:54:09.016: INFO: Waiting up to 5m0s for pod "pod-configmaps-e7b0d437-e324-4337-bc4c-d3a980164753" in namespace "configmap-5281" to be "Succeeded or Failed"
    Jun 28 08:54:09.022: INFO: Pod "pod-configmaps-e7b0d437-e324-4337-bc4c-d3a980164753": Phase="Pending", Reason="", readiness=false. Elapsed: 5.879961ms
    Jun 28 08:54:11.027: INFO: Pod "pod-configmaps-e7b0d437-e324-4337-bc4c-d3a980164753": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011539377s
    Jun 28 08:54:13.028: INFO: Pod "pod-configmaps-e7b0d437-e324-4337-bc4c-d3a980164753": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012377393s
    STEP: Saw pod success 06/28/23 08:54:13.028
    Jun 28 08:54:13.028: INFO: Pod "pod-configmaps-e7b0d437-e324-4337-bc4c-d3a980164753" satisfied condition "Succeeded or Failed"
    Jun 28 08:54:13.033: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod pod-configmaps-e7b0d437-e324-4337-bc4c-d3a980164753 container env-test: <nil>
    STEP: delete the pod 06/28/23 08:54:13.084
    Jun 28 08:54:13.095: INFO: Waiting for pod pod-configmaps-e7b0d437-e324-4337-bc4c-d3a980164753 to disappear
    Jun 28 08:54:13.100: INFO: Pod pod-configmaps-e7b0d437-e324-4337-bc4c-d3a980164753 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Jun 28 08:54:13.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-5281" for this suite. 06/28/23 08:54:13.108
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] Watchers
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:54:13.115
Jun 28 08:54:13.115: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename watch 06/28/23 08:54:13.116
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:54:13.128
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:54:13.132
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
STEP: creating a watch on configmaps 06/28/23 08:54:13.137
STEP: creating a new configmap 06/28/23 08:54:13.139
STEP: modifying the configmap once 06/28/23 08:54:13.146
STEP: closing the watch once it receives two notifications 06/28/23 08:54:13.157
Jun 28 08:54:13.158: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9075  1e711ac4-e88a-4d4d-b47d-17343451d547 84258 0 2023-06-28 08:54:13 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-06-28 08:54:13 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 28 08:54:13.158: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9075  1e711ac4-e88a-4d4d-b47d-17343451d547 84259 0 2023-06-28 08:54:13 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-06-28 08:54:13 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed 06/28/23 08:54:13.158
STEP: creating a new watch on configmaps from the last resource version observed by the first watch 06/28/23 08:54:13.167
STEP: deleting the configmap 06/28/23 08:54:13.17
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 06/28/23 08:54:13.178
Jun 28 08:54:13.178: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9075  1e711ac4-e88a-4d4d-b47d-17343451d547 84260 0 2023-06-28 08:54:13 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-06-28 08:54:13 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 28 08:54:13.178: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9075  1e711ac4-e88a-4d4d-b47d-17343451d547 84261 0 2023-06-28 08:54:13 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-06-28 08:54:13 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Jun 28 08:54:13.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9075" for this suite. 06/28/23 08:54:13.188
{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","completed":301,"skipped":5515,"failed":0}
------------------------------
â€¢ [0.080 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:54:13.115
    Jun 28 08:54:13.115: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename watch 06/28/23 08:54:13.116
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:54:13.128
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:54:13.132
    [It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
      test/e2e/apimachinery/watch.go:191
    STEP: creating a watch on configmaps 06/28/23 08:54:13.137
    STEP: creating a new configmap 06/28/23 08:54:13.139
    STEP: modifying the configmap once 06/28/23 08:54:13.146
    STEP: closing the watch once it receives two notifications 06/28/23 08:54:13.157
    Jun 28 08:54:13.158: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9075  1e711ac4-e88a-4d4d-b47d-17343451d547 84258 0 2023-06-28 08:54:13 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-06-28 08:54:13 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jun 28 08:54:13.158: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9075  1e711ac4-e88a-4d4d-b47d-17343451d547 84259 0 2023-06-28 08:54:13 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-06-28 08:54:13 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time, while the watch is closed 06/28/23 08:54:13.158
    STEP: creating a new watch on configmaps from the last resource version observed by the first watch 06/28/23 08:54:13.167
    STEP: deleting the configmap 06/28/23 08:54:13.17
    STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 06/28/23 08:54:13.178
    Jun 28 08:54:13.178: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9075  1e711ac4-e88a-4d4d-b47d-17343451d547 84260 0 2023-06-28 08:54:13 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-06-28 08:54:13 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jun 28 08:54:13.178: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9075  1e711ac4-e88a-4d4d-b47d-17343451d547 84261 0 2023-06-28 08:54:13 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-06-28 08:54:13 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Jun 28 08:54:13.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-9075" for this suite. 06/28/23 08:54:13.188
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:114
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:54:13.196
Jun 28 08:54:13.196: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename container-lifecycle-hook 06/28/23 08:54:13.197
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:54:13.21
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:54:13.214
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 06/28/23 08:54:13.226
Jun 28 08:54:13.236: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-7149" to be "running and ready"
Jun 28 08:54:13.240: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 4.127261ms
Jun 28 08:54:13.240: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jun 28 08:54:15.246: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.009991768s
Jun 28 08:54:15.246: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jun 28 08:54:15.246: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:114
STEP: create the pod with lifecycle hook 06/28/23 08:54:15.25
Jun 28 08:54:15.256: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-7149" to be "running and ready"
Jun 28 08:54:15.259: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.4552ms
Jun 28 08:54:15.260: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jun 28 08:54:17.265: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.009397005s
Jun 28 08:54:17.265: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
Jun 28 08:54:17.265: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 06/28/23 08:54:17.27
Jun 28 08:54:17.277: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 28 08:54:17.281: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 28 08:54:19.282: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 28 08:54:19.287: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 28 08:54:21.281: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 28 08:54:21.286: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook 06/28/23 08:54:21.286
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Jun 28 08:54:21.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7149" for this suite. 06/28/23 08:54:21.31
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","completed":302,"skipped":5542,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.122 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:114

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:54:13.196
    Jun 28 08:54:13.196: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename container-lifecycle-hook 06/28/23 08:54:13.197
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:54:13.21
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:54:13.214
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 06/28/23 08:54:13.226
    Jun 28 08:54:13.236: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-7149" to be "running and ready"
    Jun 28 08:54:13.240: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 4.127261ms
    Jun 28 08:54:13.240: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jun 28 08:54:15.246: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.009991768s
    Jun 28 08:54:15.246: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jun 28 08:54:15.246: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:114
    STEP: create the pod with lifecycle hook 06/28/23 08:54:15.25
    Jun 28 08:54:15.256: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-7149" to be "running and ready"
    Jun 28 08:54:15.259: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.4552ms
    Jun 28 08:54:15.260: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Jun 28 08:54:17.265: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.009397005s
    Jun 28 08:54:17.265: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
    Jun 28 08:54:17.265: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 06/28/23 08:54:17.27
    Jun 28 08:54:17.277: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Jun 28 08:54:17.281: INFO: Pod pod-with-prestop-exec-hook still exists
    Jun 28 08:54:19.282: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Jun 28 08:54:19.287: INFO: Pod pod-with-prestop-exec-hook still exists
    Jun 28 08:54:21.281: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Jun 28 08:54:21.286: INFO: Pod pod-with-prestop-exec-hook no longer exists
    STEP: check prestop hook 06/28/23 08:54:21.286
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Jun 28 08:54:21.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-7149" for this suite. 06/28/23 08:54:21.31
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] HostPort
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
[BeforeEach] [sig-network] HostPort
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:54:21.318
Jun 28 08:54:21.318: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename hostport 06/28/23 08:54:21.319
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:54:21.331
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:54:21.335
[BeforeEach] [sig-network] HostPort
  test/e2e/network/hostport.go:49
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 06/28/23 08:54:21.348
Jun 28 08:54:21.356: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-1913" to be "running and ready"
Jun 28 08:54:21.360: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.931044ms
Jun 28 08:54:21.360: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jun 28 08:54:23.366: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.010113922s
Jun 28 08:54:23.366: INFO: The phase of Pod pod1 is Running (Ready = true)
Jun 28 08:54:23.366: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 192.168.11.3 on the node which pod1 resides and expect scheduled 06/28/23 08:54:23.366
Jun 28 08:54:23.372: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-1913" to be "running and ready"
Jun 28 08:54:23.376: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.854952ms
Jun 28 08:54:23.376: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jun 28 08:54:25.382: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.009489111s
Jun 28 08:54:25.382: INFO: The phase of Pod pod2 is Running (Ready = true)
Jun 28 08:54:25.382: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 192.168.11.3 but use UDP protocol on the node which pod2 resides 06/28/23 08:54:25.382
Jun 28 08:54:25.388: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-1913" to be "running and ready"
Jun 28 08:54:25.392: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.275768ms
Jun 28 08:54:25.392: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Jun 28 08:54:27.404: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.015806511s
Jun 28 08:54:27.404: INFO: The phase of Pod pod3 is Running (Ready = true)
Jun 28 08:54:27.404: INFO: Pod "pod3" satisfied condition "running and ready"
Jun 28 08:54:27.417: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-1913" to be "running and ready"
Jun 28 08:54:27.426: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 8.991294ms
Jun 28 08:54:27.426: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Jun 28 08:54:29.432: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.014355026s
Jun 28 08:54:29.432: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
Jun 28 08:54:29.432: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 06/28/23 08:54:29.436
Jun 28 08:54:29.436: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 192.168.11.3 http://127.0.0.1:54323/hostname] Namespace:hostport-1913 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 28 08:54:29.436: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
Jun 28 08:54:29.436: INFO: ExecWithOptions: Clientset creation
Jun 28 08:54:29.436: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/hostport-1913/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+192.168.11.3+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.11.3, port: 54323 06/28/23 08:54:29.861
Jun 28 08:54:29.862: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://192.168.11.3:54323/hostname] Namespace:hostport-1913 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 28 08:54:29.862: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
Jun 28 08:54:29.862: INFO: ExecWithOptions: Clientset creation
Jun 28 08:54:29.862: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/hostport-1913/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F192.168.11.3%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.11.3, port: 54323 UDP 06/28/23 08:54:30.294
Jun 28 08:54:30.294: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 192.168.11.3 54323] Namespace:hostport-1913 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 28 08:54:30.294: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
Jun 28 08:54:30.295: INFO: ExecWithOptions: Clientset creation
Jun 28 08:54:30.295: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/hostport-1913/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+192.168.11.3+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
[AfterEach] [sig-network] HostPort
  test/e2e/framework/framework.go:187
Jun 28 08:54:35.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostport-1913" for this suite. 06/28/23 08:54:35.749
{"msg":"PASSED [sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]","completed":303,"skipped":5569,"failed":0}
------------------------------
â€¢ [SLOW TEST] [14.438 seconds]
[sig-network] HostPort
test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] HostPort
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:54:21.318
    Jun 28 08:54:21.318: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename hostport 06/28/23 08:54:21.319
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:54:21.331
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:54:21.335
    [BeforeEach] [sig-network] HostPort
      test/e2e/network/hostport.go:49
    [It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
      test/e2e/network/hostport.go:63
    STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 06/28/23 08:54:21.348
    Jun 28 08:54:21.356: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-1913" to be "running and ready"
    Jun 28 08:54:21.360: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.931044ms
    Jun 28 08:54:21.360: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jun 28 08:54:23.366: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.010113922s
    Jun 28 08:54:23.366: INFO: The phase of Pod pod1 is Running (Ready = true)
    Jun 28 08:54:23.366: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 192.168.11.3 on the node which pod1 resides and expect scheduled 06/28/23 08:54:23.366
    Jun 28 08:54:23.372: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-1913" to be "running and ready"
    Jun 28 08:54:23.376: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.854952ms
    Jun 28 08:54:23.376: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jun 28 08:54:25.382: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.009489111s
    Jun 28 08:54:25.382: INFO: The phase of Pod pod2 is Running (Ready = true)
    Jun 28 08:54:25.382: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 192.168.11.3 but use UDP protocol on the node which pod2 resides 06/28/23 08:54:25.382
    Jun 28 08:54:25.388: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-1913" to be "running and ready"
    Jun 28 08:54:25.392: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.275768ms
    Jun 28 08:54:25.392: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
    Jun 28 08:54:27.404: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.015806511s
    Jun 28 08:54:27.404: INFO: The phase of Pod pod3 is Running (Ready = true)
    Jun 28 08:54:27.404: INFO: Pod "pod3" satisfied condition "running and ready"
    Jun 28 08:54:27.417: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-1913" to be "running and ready"
    Jun 28 08:54:27.426: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 8.991294ms
    Jun 28 08:54:27.426: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
    Jun 28 08:54:29.432: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.014355026s
    Jun 28 08:54:29.432: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
    Jun 28 08:54:29.432: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 06/28/23 08:54:29.436
    Jun 28 08:54:29.436: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 192.168.11.3 http://127.0.0.1:54323/hostname] Namespace:hostport-1913 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 28 08:54:29.436: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    Jun 28 08:54:29.436: INFO: ExecWithOptions: Clientset creation
    Jun 28 08:54:29.436: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/hostport-1913/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+192.168.11.3+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.11.3, port: 54323 06/28/23 08:54:29.861
    Jun 28 08:54:29.862: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://192.168.11.3:54323/hostname] Namespace:hostport-1913 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 28 08:54:29.862: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    Jun 28 08:54:29.862: INFO: ExecWithOptions: Clientset creation
    Jun 28 08:54:29.862: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/hostport-1913/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F192.168.11.3%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.11.3, port: 54323 UDP 06/28/23 08:54:30.294
    Jun 28 08:54:30.294: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 192.168.11.3 54323] Namespace:hostport-1913 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 28 08:54:30.294: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    Jun 28 08:54:30.295: INFO: ExecWithOptions: Clientset creation
    Jun 28 08:54:30.295: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/hostport-1913/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+192.168.11.3+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    [AfterEach] [sig-network] HostPort
      test/e2e/framework/framework.go:187
    Jun 28 08:54:35.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "hostport-1913" for this suite. 06/28/23 08:54:35.749
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] Deployment
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:54:35.757
Jun 28 08:54:35.757: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename deployment 06/28/23 08:54:35.757
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:54:35.768
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:54:35.772
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
Jun 28 08:54:35.776: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Jun 28 08:54:35.787: INFO: Pod name sample-pod: Found 0 pods out of 1
Jun 28 08:54:40.795: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 06/28/23 08:54:40.795
Jun 28 08:54:40.795: INFO: Creating deployment "test-rolling-update-deployment"
Jun 28 08:54:40.801: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Jun 28 08:54:40.810: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Jun 28 08:54:42.821: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Jun 28 08:54:42.825: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jun 28 08:54:42.839: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-1403  9846e9ae-71b9-4a60-af9f-5a093ad143b3 84568 1 2023-06-28 08:54:40 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-06-28 08:54:40 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-28 08:54:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0042ecf68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-06-28 08:54:40 +0000 UTC,LastTransitionTime:2023-06-28 08:54:40 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-78f575d8ff" has successfully progressed.,LastUpdateTime:2023-06-28 08:54:41 +0000 UTC,LastTransitionTime:2023-06-28 08:54:40 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jun 28 08:54:42.843: INFO: New ReplicaSet "test-rolling-update-deployment-78f575d8ff" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-78f575d8ff  deployment-1403  f16d3e65-ab1a-4d8c-b70c-dd8c7451be51 84558 1 2023-06-28 08:54:40 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 9846e9ae-71b9-4a60-af9f-5a093ad143b3 0xc0042bd127 0xc0042bd128}] [] [{kube-controller-manager Update apps/v1 2023-06-28 08:54:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9846e9ae-71b9-4a60-af9f-5a093ad143b3\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-28 08:54:41 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 78f575d8ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0042bd208 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jun 28 08:54:42.843: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Jun 28 08:54:42.843: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-1403  aed774e7-6292-46c6-8b1b-cd1c1b1f4c49 84567 2 2023-06-28 08:54:35 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 9846e9ae-71b9-4a60-af9f-5a093ad143b3 0xc0042bcff7 0xc0042bcff8}] [] [{e2e.test Update apps/v1 2023-06-28 08:54:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-28 08:54:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9846e9ae-71b9-4a60-af9f-5a093ad143b3\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-06-28 08:54:41 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0042bd0b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun 28 08:54:42.848: INFO: Pod "test-rolling-update-deployment-78f575d8ff-tnr4g" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-78f575d8ff-tnr4g test-rolling-update-deployment-78f575d8ff- deployment-1403  e78b555e-82a2-46c9-a12e-cfa0f5501474 84557 0 2023-06-28 08:54:40 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[cni.projectcalico.org/containerID:9e0089511a0f05558daefc43e0bce16f019452535a20a7fcbafdf0646719fdd5 cni.projectcalico.org/podIP:172.21.30.97/32 cni.projectcalico.org/podIPs:172.21.30.97/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-78f575d8ff f16d3e65-ab1a-4d8c-b70c-dd8c7451be51 0xc0042bd677 0xc0042bd678}] [] [{kube-controller-manager Update v1 2023-06-28 08:54:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f16d3e65-ab1a-4d8c-b70c-dd8c7451be51\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-06-28 08:54:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-06-28 08:54:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.30.97\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mz4xm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mz4xm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-rhel-749f7d55c8xdd8b6-zxlfv,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:54:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:54:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:54:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:54:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.11.5,PodIP:172.21.30.97,StartTime:2023-06-28 08:54:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-28 08:54:41 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://006bdf93d1c0498202c552ce82b0c2e5a5b7499f58849a5f7631c939df728604,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.21.30.97,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jun 28 08:54:42.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1403" for this suite. 06/28/23 08:54:42.856
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","completed":304,"skipped":5572,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.106 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:54:35.757
    Jun 28 08:54:35.757: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename deployment 06/28/23 08:54:35.757
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:54:35.768
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:54:35.772
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:105
    Jun 28 08:54:35.776: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
    Jun 28 08:54:35.787: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jun 28 08:54:40.795: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 06/28/23 08:54:40.795
    Jun 28 08:54:40.795: INFO: Creating deployment "test-rolling-update-deployment"
    Jun 28 08:54:40.801: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
    Jun 28 08:54:40.810: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
    Jun 28 08:54:42.821: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
    Jun 28 08:54:42.825: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jun 28 08:54:42.839: INFO: Deployment "test-rolling-update-deployment":
    &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-1403  9846e9ae-71b9-4a60-af9f-5a093ad143b3 84568 1 2023-06-28 08:54:40 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-06-28 08:54:40 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-28 08:54:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0042ecf68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-06-28 08:54:40 +0000 UTC,LastTransitionTime:2023-06-28 08:54:40 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-78f575d8ff" has successfully progressed.,LastUpdateTime:2023-06-28 08:54:41 +0000 UTC,LastTransitionTime:2023-06-28 08:54:40 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jun 28 08:54:42.843: INFO: New ReplicaSet "test-rolling-update-deployment-78f575d8ff" of Deployment "test-rolling-update-deployment":
    &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-78f575d8ff  deployment-1403  f16d3e65-ab1a-4d8c-b70c-dd8c7451be51 84558 1 2023-06-28 08:54:40 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 9846e9ae-71b9-4a60-af9f-5a093ad143b3 0xc0042bd127 0xc0042bd128}] [] [{kube-controller-manager Update apps/v1 2023-06-28 08:54:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9846e9ae-71b9-4a60-af9f-5a093ad143b3\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-28 08:54:41 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 78f575d8ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0042bd208 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jun 28 08:54:42.843: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
    Jun 28 08:54:42.843: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-1403  aed774e7-6292-46c6-8b1b-cd1c1b1f4c49 84567 2 2023-06-28 08:54:35 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 9846e9ae-71b9-4a60-af9f-5a093ad143b3 0xc0042bcff7 0xc0042bcff8}] [] [{e2e.test Update apps/v1 2023-06-28 08:54:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-28 08:54:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9846e9ae-71b9-4a60-af9f-5a093ad143b3\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-06-28 08:54:41 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0042bd0b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jun 28 08:54:42.848: INFO: Pod "test-rolling-update-deployment-78f575d8ff-tnr4g" is available:
    &Pod{ObjectMeta:{test-rolling-update-deployment-78f575d8ff-tnr4g test-rolling-update-deployment-78f575d8ff- deployment-1403  e78b555e-82a2-46c9-a12e-cfa0f5501474 84557 0 2023-06-28 08:54:40 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[cni.projectcalico.org/containerID:9e0089511a0f05558daefc43e0bce16f019452535a20a7fcbafdf0646719fdd5 cni.projectcalico.org/podIP:172.21.30.97/32 cni.projectcalico.org/podIPs:172.21.30.97/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-78f575d8ff f16d3e65-ab1a-4d8c-b70c-dd8c7451be51 0xc0042bd677 0xc0042bd678}] [] [{kube-controller-manager Update v1 2023-06-28 08:54:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f16d3e65-ab1a-4d8c-b70c-dd8c7451be51\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-06-28 08:54:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-06-28 08:54:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.30.97\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mz4xm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mz4xm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-rhel-749f7d55c8xdd8b6-zxlfv,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:54:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:54:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:54:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 08:54:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.11.5,PodIP:172.21.30.97,StartTime:2023-06-28 08:54:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-28 08:54:41 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://006bdf93d1c0498202c552ce82b0c2e5a5b7499f58849a5f7631c939df728604,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.21.30.97,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jun 28 08:54:42.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-1403" for this suite. 06/28/23 08:54:42.856
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:54:42.864
Jun 28 08:54:42.864: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename webhook 06/28/23 08:54:42.865
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:54:42.878
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:54:42.883
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 06/28/23 08:54:42.898
STEP: Create role binding to let webhook read extension-apiserver-authentication 06/28/23 08:54:43.2
STEP: Deploying the webhook pod 06/28/23 08:54:43.21
STEP: Wait for the deployment to be ready 06/28/23 08:54:43.228
Jun 28 08:54:43.236: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun 28 08:54:45.250: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 28, 8, 54, 43, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 28, 8, 54, 43, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 28, 8, 54, 43, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 28, 8, 54, 43, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 06/28/23 08:54:47.257
STEP: Verifying the service has paired with the endpoint 06/28/23 08:54:47.266
Jun 28 08:54:48.268: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380
STEP: Setting timeout (1s) shorter than webhook latency (5s) 06/28/23 08:54:48.273
STEP: Registering slow webhook via the AdmissionRegistration API 06/28/23 08:54:48.273
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 06/28/23 08:54:48.377
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 06/28/23 08:54:49.387
STEP: Registering slow webhook via the AdmissionRegistration API 06/28/23 08:54:49.387
STEP: Having no error when timeout is longer than webhook latency 06/28/23 08:54:50.462
STEP: Registering slow webhook via the AdmissionRegistration API 06/28/23 08:54:50.462
STEP: Having no error when timeout is empty (defaulted to 10s in v1) 06/28/23 08:54:55.633
STEP: Registering slow webhook via the AdmissionRegistration API 06/28/23 08:54:55.633
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jun 28 08:55:00.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5911" for this suite. 06/28/23 08:55:00.695
STEP: Destroying namespace "webhook-5911-markers" for this suite. 06/28/23 08:55:00.702
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","completed":305,"skipped":5594,"failed":0}
------------------------------
â€¢ [SLOW TEST] [17.880 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:54:42.864
    Jun 28 08:54:42.864: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename webhook 06/28/23 08:54:42.865
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:54:42.878
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:54:42.883
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 06/28/23 08:54:42.898
    STEP: Create role binding to let webhook read extension-apiserver-authentication 06/28/23 08:54:43.2
    STEP: Deploying the webhook pod 06/28/23 08:54:43.21
    STEP: Wait for the deployment to be ready 06/28/23 08:54:43.228
    Jun 28 08:54:43.236: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jun 28 08:54:45.250: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 28, 8, 54, 43, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 28, 8, 54, 43, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 28, 8, 54, 43, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 28, 8, 54, 43, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 06/28/23 08:54:47.257
    STEP: Verifying the service has paired with the endpoint 06/28/23 08:54:47.266
    Jun 28 08:54:48.268: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should honor timeout [Conformance]
      test/e2e/apimachinery/webhook.go:380
    STEP: Setting timeout (1s) shorter than webhook latency (5s) 06/28/23 08:54:48.273
    STEP: Registering slow webhook via the AdmissionRegistration API 06/28/23 08:54:48.273
    STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 06/28/23 08:54:48.377
    STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 06/28/23 08:54:49.387
    STEP: Registering slow webhook via the AdmissionRegistration API 06/28/23 08:54:49.387
    STEP: Having no error when timeout is longer than webhook latency 06/28/23 08:54:50.462
    STEP: Registering slow webhook via the AdmissionRegistration API 06/28/23 08:54:50.462
    STEP: Having no error when timeout is empty (defaulted to 10s in v1) 06/28/23 08:54:55.633
    STEP: Registering slow webhook via the AdmissionRegistration API 06/28/23 08:54:55.633
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jun 28 08:55:00.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-5911" for this suite. 06/28/23 08:55:00.695
    STEP: Destroying namespace "webhook-5911-markers" for this suite. 06/28/23 08:55:00.702
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:55:00.745
Jun 28 08:55:00.745: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename projected 06/28/23 08:55:00.746
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:55:00.762
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:55:00.766
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77
STEP: Creating projection with secret that has name projected-secret-test-map-51330752-f258-47df-93a2-d758337eb866 06/28/23 08:55:00.772
STEP: Creating a pod to test consume secrets 06/28/23 08:55:00.777
Jun 28 08:55:00.786: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-62fdf7ce-c5f8-4ea5-9dfe-ece023681796" in namespace "projected-3432" to be "Succeeded or Failed"
Jun 28 08:55:00.790: INFO: Pod "pod-projected-secrets-62fdf7ce-c5f8-4ea5-9dfe-ece023681796": Phase="Pending", Reason="", readiness=false. Elapsed: 4.082413ms
Jun 28 08:55:02.796: INFO: Pod "pod-projected-secrets-62fdf7ce-c5f8-4ea5-9dfe-ece023681796": Phase="Running", Reason="", readiness=false. Elapsed: 2.009703889s
Jun 28 08:55:04.795: INFO: Pod "pod-projected-secrets-62fdf7ce-c5f8-4ea5-9dfe-ece023681796": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009506114s
STEP: Saw pod success 06/28/23 08:55:04.795
Jun 28 08:55:04.796: INFO: Pod "pod-projected-secrets-62fdf7ce-c5f8-4ea5-9dfe-ece023681796" satisfied condition "Succeeded or Failed"
Jun 28 08:55:04.800: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod pod-projected-secrets-62fdf7ce-c5f8-4ea5-9dfe-ece023681796 container projected-secret-volume-test: <nil>
STEP: delete the pod 06/28/23 08:55:04.849
Jun 28 08:55:04.860: INFO: Waiting for pod pod-projected-secrets-62fdf7ce-c5f8-4ea5-9dfe-ece023681796 to disappear
Jun 28 08:55:04.863: INFO: Pod pod-projected-secrets-62fdf7ce-c5f8-4ea5-9dfe-ece023681796 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Jun 28 08:55:04.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3432" for this suite. 06/28/23 08:55:04.87
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":306,"skipped":5604,"failed":0}
------------------------------
â€¢ [4.131 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:55:00.745
    Jun 28 08:55:00.745: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename projected 06/28/23 08:55:00.746
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:55:00.762
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:55:00.766
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:77
    STEP: Creating projection with secret that has name projected-secret-test-map-51330752-f258-47df-93a2-d758337eb866 06/28/23 08:55:00.772
    STEP: Creating a pod to test consume secrets 06/28/23 08:55:00.777
    Jun 28 08:55:00.786: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-62fdf7ce-c5f8-4ea5-9dfe-ece023681796" in namespace "projected-3432" to be "Succeeded or Failed"
    Jun 28 08:55:00.790: INFO: Pod "pod-projected-secrets-62fdf7ce-c5f8-4ea5-9dfe-ece023681796": Phase="Pending", Reason="", readiness=false. Elapsed: 4.082413ms
    Jun 28 08:55:02.796: INFO: Pod "pod-projected-secrets-62fdf7ce-c5f8-4ea5-9dfe-ece023681796": Phase="Running", Reason="", readiness=false. Elapsed: 2.009703889s
    Jun 28 08:55:04.795: INFO: Pod "pod-projected-secrets-62fdf7ce-c5f8-4ea5-9dfe-ece023681796": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009506114s
    STEP: Saw pod success 06/28/23 08:55:04.795
    Jun 28 08:55:04.796: INFO: Pod "pod-projected-secrets-62fdf7ce-c5f8-4ea5-9dfe-ece023681796" satisfied condition "Succeeded or Failed"
    Jun 28 08:55:04.800: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod pod-projected-secrets-62fdf7ce-c5f8-4ea5-9dfe-ece023681796 container projected-secret-volume-test: <nil>
    STEP: delete the pod 06/28/23 08:55:04.849
    Jun 28 08:55:04.860: INFO: Waiting for pod pod-projected-secrets-62fdf7ce-c5f8-4ea5-9dfe-ece023681796 to disappear
    Jun 28 08:55:04.863: INFO: Pod pod-projected-secrets-62fdf7ce-c5f8-4ea5-9dfe-ece023681796 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Jun 28 08:55:04.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3432" for this suite. 06/28/23 08:55:04.87
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:55:04.876
Jun 28 08:55:04.876: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename webhook 06/28/23 08:55:04.878
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:55:04.889
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:55:04.893
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 06/28/23 08:55:04.91
STEP: Create role binding to let webhook read extension-apiserver-authentication 06/28/23 08:55:05.344
STEP: Deploying the webhook pod 06/28/23 08:55:05.35
STEP: Wait for the deployment to be ready 06/28/23 08:55:05.362
Jun 28 08:55:05.373: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 06/28/23 08:55:07.388
STEP: Verifying the service has paired with the endpoint 06/28/23 08:55:07.397
Jun 28 08:55:08.398: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 06/28/23 08:55:08.404
STEP: create a namespace for the webhook 06/28/23 08:55:08.509
STEP: create a configmap should be unconditionally rejected by the webhook 06/28/23 08:55:08.517
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jun 28 08:55:08.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2171" for this suite. 06/28/23 08:55:08.548
STEP: Destroying namespace "webhook-2171-markers" for this suite. 06/28/23 08:55:08.554
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","completed":307,"skipped":5608,"failed":0}
------------------------------
â€¢ [3.719 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:55:04.876
    Jun 28 08:55:04.876: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename webhook 06/28/23 08:55:04.878
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:55:04.889
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:55:04.893
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 06/28/23 08:55:04.91
    STEP: Create role binding to let webhook read extension-apiserver-authentication 06/28/23 08:55:05.344
    STEP: Deploying the webhook pod 06/28/23 08:55:05.35
    STEP: Wait for the deployment to be ready 06/28/23 08:55:05.362
    Jun 28 08:55:05.373: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 06/28/23 08:55:07.388
    STEP: Verifying the service has paired with the endpoint 06/28/23 08:55:07.397
    Jun 28 08:55:08.398: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should unconditionally reject operations on fail closed webhook [Conformance]
      test/e2e/apimachinery/webhook.go:238
    STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 06/28/23 08:55:08.404
    STEP: create a namespace for the webhook 06/28/23 08:55:08.509
    STEP: create a configmap should be unconditionally rejected by the webhook 06/28/23 08:55:08.517
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jun 28 08:55:08.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-2171" for this suite. 06/28/23 08:55:08.548
    STEP: Destroying namespace "webhook-2171-markers" for this suite. 06/28/23 08:55:08.554
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] ReplicaSet
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:55:08.595
Jun 28 08:55:08.596: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename replicaset 06/28/23 08:55:08.596
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:55:08.609
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:55:08.614
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
Jun 28 08:55:08.619: INFO: Creating ReplicaSet my-hostname-basic-d75b597d-38a2-4ab1-a83a-a16bfc5d7315
Jun 28 08:55:08.629: INFO: Pod name my-hostname-basic-d75b597d-38a2-4ab1-a83a-a16bfc5d7315: Found 0 pods out of 1
Jun 28 08:55:13.633: INFO: Pod name my-hostname-basic-d75b597d-38a2-4ab1-a83a-a16bfc5d7315: Found 1 pods out of 1
Jun 28 08:55:13.633: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-d75b597d-38a2-4ab1-a83a-a16bfc5d7315" is running
Jun 28 08:55:13.633: INFO: Waiting up to 5m0s for pod "my-hostname-basic-d75b597d-38a2-4ab1-a83a-a16bfc5d7315-psg22" in namespace "replicaset-2848" to be "running"
Jun 28 08:55:13.636: INFO: Pod "my-hostname-basic-d75b597d-38a2-4ab1-a83a-a16bfc5d7315-psg22": Phase="Running", Reason="", readiness=true. Elapsed: 3.452156ms
Jun 28 08:55:13.637: INFO: Pod "my-hostname-basic-d75b597d-38a2-4ab1-a83a-a16bfc5d7315-psg22" satisfied condition "running"
Jun 28 08:55:13.637: INFO: Pod "my-hostname-basic-d75b597d-38a2-4ab1-a83a-a16bfc5d7315-psg22" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-28 08:55:08 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-28 08:55:09 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-28 08:55:09 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-28 08:55:08 +0000 UTC Reason: Message:}])
Jun 28 08:55:13.637: INFO: Trying to dial the pod
Jun 28 08:55:18.740: INFO: Controller my-hostname-basic-d75b597d-38a2-4ab1-a83a-a16bfc5d7315: Got expected result from replica 1 [my-hostname-basic-d75b597d-38a2-4ab1-a83a-a16bfc5d7315-psg22]: "my-hostname-basic-d75b597d-38a2-4ab1-a83a-a16bfc5d7315-psg22", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Jun 28 08:55:18.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-2848" for this suite. 06/28/23 08:55:18.748
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","completed":308,"skipped":5612,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.161 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:55:08.595
    Jun 28 08:55:08.596: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename replicaset 06/28/23 08:55:08.596
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:55:08.609
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:55:08.614
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/replica_set.go:111
    Jun 28 08:55:08.619: INFO: Creating ReplicaSet my-hostname-basic-d75b597d-38a2-4ab1-a83a-a16bfc5d7315
    Jun 28 08:55:08.629: INFO: Pod name my-hostname-basic-d75b597d-38a2-4ab1-a83a-a16bfc5d7315: Found 0 pods out of 1
    Jun 28 08:55:13.633: INFO: Pod name my-hostname-basic-d75b597d-38a2-4ab1-a83a-a16bfc5d7315: Found 1 pods out of 1
    Jun 28 08:55:13.633: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-d75b597d-38a2-4ab1-a83a-a16bfc5d7315" is running
    Jun 28 08:55:13.633: INFO: Waiting up to 5m0s for pod "my-hostname-basic-d75b597d-38a2-4ab1-a83a-a16bfc5d7315-psg22" in namespace "replicaset-2848" to be "running"
    Jun 28 08:55:13.636: INFO: Pod "my-hostname-basic-d75b597d-38a2-4ab1-a83a-a16bfc5d7315-psg22": Phase="Running", Reason="", readiness=true. Elapsed: 3.452156ms
    Jun 28 08:55:13.637: INFO: Pod "my-hostname-basic-d75b597d-38a2-4ab1-a83a-a16bfc5d7315-psg22" satisfied condition "running"
    Jun 28 08:55:13.637: INFO: Pod "my-hostname-basic-d75b597d-38a2-4ab1-a83a-a16bfc5d7315-psg22" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-28 08:55:08 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-28 08:55:09 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-28 08:55:09 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-28 08:55:08 +0000 UTC Reason: Message:}])
    Jun 28 08:55:13.637: INFO: Trying to dial the pod
    Jun 28 08:55:18.740: INFO: Controller my-hostname-basic-d75b597d-38a2-4ab1-a83a-a16bfc5d7315: Got expected result from replica 1 [my-hostname-basic-d75b597d-38a2-4ab1-a83a-a16bfc5d7315-psg22]: "my-hostname-basic-d75b597d-38a2-4ab1-a83a-a16bfc5d7315-psg22", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Jun 28 08:55:18.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-2848" for this suite. 06/28/23 08:55:18.748
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:55:18.757
Jun 28 08:55:18.757: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename gc 06/28/23 08:55:18.758
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:55:18.774
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:55:18.778
[It] should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
STEP: create the rc 06/28/23 08:55:18.783
STEP: delete the rc 06/28/23 08:55:23.795
STEP: wait for all pods to be garbage collected 06/28/23 08:55:23.8
STEP: Gathering metrics 06/28/23 08:55:28.811
W0628 08:55:28.823823      18 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jun 28 08:55:28.823: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Jun 28 08:55:28.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4554" for this suite. 06/28/23 08:55:28.842
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","completed":309,"skipped":5647,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.092 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:55:18.757
    Jun 28 08:55:18.757: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename gc 06/28/23 08:55:18.758
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:55:18.774
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:55:18.778
    [It] should delete pods created by rc when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:312
    STEP: create the rc 06/28/23 08:55:18.783
    STEP: delete the rc 06/28/23 08:55:23.795
    STEP: wait for all pods to be garbage collected 06/28/23 08:55:23.8
    STEP: Gathering metrics 06/28/23 08:55:28.811
    W0628 08:55:28.823823      18 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Jun 28 08:55:28.823: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Jun 28 08:55:28.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-4554" for this suite. 06/28/23 08:55:28.842
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:55:28.851
Jun 28 08:55:28.851: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename crd-publish-openapi 06/28/23 08:55:28.851
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:55:28.868
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:55:28.873
[It] works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 06/28/23 08:55:28.877
Jun 28 08:55:28.878: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 06/28/23 08:55:39.846
Jun 28 08:55:39.846: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
Jun 28 08:55:42.087: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jun 28 08:55:53.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4054" for this suite. 06/28/23 08:55:53.432
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","completed":310,"skipped":5674,"failed":0}
------------------------------
â€¢ [SLOW TEST] [24.591 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:55:28.851
    Jun 28 08:55:28.851: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename crd-publish-openapi 06/28/23 08:55:28.851
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:55:28.868
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:55:28.873
    [It] works for multiple CRDs of same group but different versions [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:308
    STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 06/28/23 08:55:28.877
    Jun 28 08:55:28.878: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 06/28/23 08:55:39.846
    Jun 28 08:55:39.846: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    Jun 28 08:55:42.087: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jun 28 08:55:53.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-4054" for this suite. 06/28/23 08:55:53.432
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance]
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:55:53.443
Jun 28 08:55:53.444: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename ephemeral-containers-test 06/28/23 08:55:53.445
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:55:53.459
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:55:53.466
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/common/node/ephemeral_containers.go:38
[It] will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
STEP: creating a target pod 06/28/23 08:55:53.471
Jun 28 08:55:53.484: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-7982" to be "running and ready"
Jun 28 08:55:53.490: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.073663ms
Jun 28 08:55:53.490: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
Jun 28 08:55:55.496: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.011515547s
Jun 28 08:55:55.496: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
Jun 28 08:55:55.496: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
STEP: adding an ephemeral container 06/28/23 08:55:55.502
Jun 28 08:55:55.519: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-7982" to be "container debugger running"
Jun 28 08:55:55.524: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.802458ms
Jun 28 08:55:57.531: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.011741003s
Jun 28 08:55:59.533: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.014071525s
Jun 28 08:55:59.533: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
STEP: checking pod container endpoints 06/28/23 08:55:59.534
Jun 28 08:55:59.534: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-7982 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 28 08:55:59.534: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
Jun 28 08:55:59.534: INFO: ExecWithOptions: Clientset creation
Jun 28 08:55:59.534: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/ephemeral-containers-test-7982/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
Jun 28 08:55:59.848: INFO: Exec stderr: ""
[AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/framework.go:187
Jun 28 08:55:59.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ephemeral-containers-test-7982" for this suite. 06/28/23 08:55:59.947
{"msg":"PASSED [sig-node] Ephemeral Containers [NodeConformance] will start an ephemeral container in an existing pod [Conformance]","completed":311,"skipped":5696,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.515 seconds]
[sig-node] Ephemeral Containers [NodeConformance]
test/e2e/common/node/framework.go:23
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:55:53.443
    Jun 28 08:55:53.444: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename ephemeral-containers-test 06/28/23 08:55:53.445
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:55:53.459
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:55:53.466
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/common/node/ephemeral_containers.go:38
    [It] will start an ephemeral container in an existing pod [Conformance]
      test/e2e/common/node/ephemeral_containers.go:45
    STEP: creating a target pod 06/28/23 08:55:53.471
    Jun 28 08:55:53.484: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-7982" to be "running and ready"
    Jun 28 08:55:53.490: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.073663ms
    Jun 28 08:55:53.490: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
    Jun 28 08:55:55.496: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.011515547s
    Jun 28 08:55:55.496: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
    Jun 28 08:55:55.496: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
    STEP: adding an ephemeral container 06/28/23 08:55:55.502
    Jun 28 08:55:55.519: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-7982" to be "container debugger running"
    Jun 28 08:55:55.524: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.802458ms
    Jun 28 08:55:57.531: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.011741003s
    Jun 28 08:55:59.533: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.014071525s
    Jun 28 08:55:59.533: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
    STEP: checking pod container endpoints 06/28/23 08:55:59.534
    Jun 28 08:55:59.534: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-7982 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 28 08:55:59.534: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    Jun 28 08:55:59.534: INFO: ExecWithOptions: Clientset creation
    Jun 28 08:55:59.534: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/ephemeral-containers-test-7982/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
    Jun 28 08:55:59.848: INFO: Exec stderr: ""
    [AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/framework.go:187
    Jun 28 08:55:59.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ephemeral-containers-test-7982" for this suite. 06/28/23 08:55:59.947
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:55:59.959
Jun 28 08:55:59.959: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename namespaces 06/28/23 08:55:59.959
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:55:59.984
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:55:59.991
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250
STEP: Creating a test namespace 06/28/23 08:55:59.997
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:56:00.014
STEP: Creating a service in the namespace 06/28/23 08:56:00.018
STEP: Deleting the namespace 06/28/23 08:56:00.033
STEP: Waiting for the namespace to be removed. 06/28/23 08:56:00.043
STEP: Recreating the namespace 06/28/23 08:56:06.048
STEP: Verifying there is no service in the namespace 06/28/23 08:56:06.064
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Jun 28 08:56:06.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-4726" for this suite. 06/28/23 08:56:06.076
STEP: Destroying namespace "nsdeletetest-6182" for this suite. 06/28/23 08:56:06.083
Jun 28 08:56:06.087: INFO: Namespace nsdeletetest-6182 was already deleted
STEP: Destroying namespace "nsdeletetest-3887" for this suite. 06/28/23 08:56:06.087
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","completed":312,"skipped":5714,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.136 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:55:59.959
    Jun 28 08:55:59.959: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename namespaces 06/28/23 08:55:59.959
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:55:59.984
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:55:59.991
    [It] should ensure that all services are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:250
    STEP: Creating a test namespace 06/28/23 08:55:59.997
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:56:00.014
    STEP: Creating a service in the namespace 06/28/23 08:56:00.018
    STEP: Deleting the namespace 06/28/23 08:56:00.033
    STEP: Waiting for the namespace to be removed. 06/28/23 08:56:00.043
    STEP: Recreating the namespace 06/28/23 08:56:06.048
    STEP: Verifying there is no service in the namespace 06/28/23 08:56:06.064
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Jun 28 08:56:06.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-4726" for this suite. 06/28/23 08:56:06.076
    STEP: Destroying namespace "nsdeletetest-6182" for this suite. 06/28/23 08:56:06.083
    Jun 28 08:56:06.087: INFO: Namespace nsdeletetest-6182 was already deleted
    STEP: Destroying namespace "nsdeletetest-3887" for this suite. 06/28/23 08:56:06.087
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] Daemon set [Serial]
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:56:06.095
Jun 28 08:56:06.095: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename daemonsets 06/28/23 08:56:06.096
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:56:06.108
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:56:06.112
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861
STEP: Creating simple DaemonSet "daemon-set" 06/28/23 08:56:06.154
STEP: Check that daemon pods launch on every node of the cluster. 06/28/23 08:56:06.162
Jun 28 08:56:06.174: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 28 08:56:06.174: INFO: Node ske-rhel-749f7d55c8xdd8b6-ct4cp is running 0 daemon pod, expected 1
Jun 28 08:56:07.188: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jun 28 08:56:07.188: INFO: Node ske-rhel-749f7d55c8xdd8b6-ct4cp is running 0 daemon pod, expected 1
Jun 28 08:56:08.193: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jun 28 08:56:08.193: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Getting /status 06/28/23 08:56:08.198
Jun 28 08:56:08.205: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status 06/28/23 08:56:08.205
Jun 28 08:56:08.219: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated 06/28/23 08:56:08.219
Jun 28 08:56:08.223: INFO: Observed &DaemonSet event: ADDED
Jun 28 08:56:08.223: INFO: Observed &DaemonSet event: MODIFIED
Jun 28 08:56:08.223: INFO: Observed &DaemonSet event: MODIFIED
Jun 28 08:56:08.223: INFO: Observed &DaemonSet event: MODIFIED
Jun 28 08:56:08.223: INFO: Observed &DaemonSet event: MODIFIED
Jun 28 08:56:08.223: INFO: Observed &DaemonSet event: MODIFIED
Jun 28 08:56:08.223: INFO: Found daemon set daemon-set in namespace daemonsets-2564 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jun 28 08:56:08.223: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status 06/28/23 08:56:08.223
STEP: watching for the daemon set status to be patched 06/28/23 08:56:08.232
Jun 28 08:56:08.236: INFO: Observed &DaemonSet event: ADDED
Jun 28 08:56:08.236: INFO: Observed &DaemonSet event: MODIFIED
Jun 28 08:56:08.236: INFO: Observed &DaemonSet event: MODIFIED
Jun 28 08:56:08.236: INFO: Observed &DaemonSet event: MODIFIED
Jun 28 08:56:08.236: INFO: Observed &DaemonSet event: MODIFIED
Jun 28 08:56:08.236: INFO: Observed &DaemonSet event: MODIFIED
Jun 28 08:56:08.236: INFO: Observed daemon set daemon-set in namespace daemonsets-2564 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jun 28 08:56:08.237: INFO: Observed &DaemonSet event: MODIFIED
Jun 28 08:56:08.237: INFO: Found daemon set daemon-set in namespace daemonsets-2564 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Jun 28 08:56:08.237: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 06/28/23 08:56:08.241
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2564, will wait for the garbage collector to delete the pods 06/28/23 08:56:08.241
Jun 28 08:56:08.306: INFO: Deleting DaemonSet.extensions daemon-set took: 9.291071ms
Jun 28 08:56:08.406: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.434587ms
Jun 28 08:56:11.012: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 28 08:56:11.012: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jun 28 08:56:11.018: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"85369"},"items":null}

Jun 28 08:56:11.024: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"85369"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Jun 28 08:56:11.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2564" for this suite. 06/28/23 08:56:11.06
{"msg":"PASSED [sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]","completed":313,"skipped":5715,"failed":0}
------------------------------
â€¢ [4.973 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:56:06.095
    Jun 28 08:56:06.095: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename daemonsets 06/28/23 08:56:06.096
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:56:06.108
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:56:06.112
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should verify changes to a daemon set status [Conformance]
      test/e2e/apps/daemon_set.go:861
    STEP: Creating simple DaemonSet "daemon-set" 06/28/23 08:56:06.154
    STEP: Check that daemon pods launch on every node of the cluster. 06/28/23 08:56:06.162
    Jun 28 08:56:06.174: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun 28 08:56:06.174: INFO: Node ske-rhel-749f7d55c8xdd8b6-ct4cp is running 0 daemon pod, expected 1
    Jun 28 08:56:07.188: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jun 28 08:56:07.188: INFO: Node ske-rhel-749f7d55c8xdd8b6-ct4cp is running 0 daemon pod, expected 1
    Jun 28 08:56:08.193: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jun 28 08:56:08.193: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Getting /status 06/28/23 08:56:08.198
    Jun 28 08:56:08.205: INFO: Daemon Set daemon-set has Conditions: []
    STEP: updating the DaemonSet Status 06/28/23 08:56:08.205
    Jun 28 08:56:08.219: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the daemon set status to be updated 06/28/23 08:56:08.219
    Jun 28 08:56:08.223: INFO: Observed &DaemonSet event: ADDED
    Jun 28 08:56:08.223: INFO: Observed &DaemonSet event: MODIFIED
    Jun 28 08:56:08.223: INFO: Observed &DaemonSet event: MODIFIED
    Jun 28 08:56:08.223: INFO: Observed &DaemonSet event: MODIFIED
    Jun 28 08:56:08.223: INFO: Observed &DaemonSet event: MODIFIED
    Jun 28 08:56:08.223: INFO: Observed &DaemonSet event: MODIFIED
    Jun 28 08:56:08.223: INFO: Found daemon set daemon-set in namespace daemonsets-2564 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jun 28 08:56:08.223: INFO: Daemon set daemon-set has an updated status
    STEP: patching the DaemonSet Status 06/28/23 08:56:08.223
    STEP: watching for the daemon set status to be patched 06/28/23 08:56:08.232
    Jun 28 08:56:08.236: INFO: Observed &DaemonSet event: ADDED
    Jun 28 08:56:08.236: INFO: Observed &DaemonSet event: MODIFIED
    Jun 28 08:56:08.236: INFO: Observed &DaemonSet event: MODIFIED
    Jun 28 08:56:08.236: INFO: Observed &DaemonSet event: MODIFIED
    Jun 28 08:56:08.236: INFO: Observed &DaemonSet event: MODIFIED
    Jun 28 08:56:08.236: INFO: Observed &DaemonSet event: MODIFIED
    Jun 28 08:56:08.236: INFO: Observed daemon set daemon-set in namespace daemonsets-2564 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jun 28 08:56:08.237: INFO: Observed &DaemonSet event: MODIFIED
    Jun 28 08:56:08.237: INFO: Found daemon set daemon-set in namespace daemonsets-2564 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
    Jun 28 08:56:08.237: INFO: Daemon set daemon-set has a patched status
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 06/28/23 08:56:08.241
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2564, will wait for the garbage collector to delete the pods 06/28/23 08:56:08.241
    Jun 28 08:56:08.306: INFO: Deleting DaemonSet.extensions daemon-set took: 9.291071ms
    Jun 28 08:56:08.406: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.434587ms
    Jun 28 08:56:11.012: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun 28 08:56:11.012: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jun 28 08:56:11.018: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"85369"},"items":null}

    Jun 28 08:56:11.024: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"85369"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Jun 28 08:56:11.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-2564" for this suite. 06/28/23 08:56:11.06
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:56:11.069
Jun 28 08:56:11.069: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename configmap 06/28/23 08:56:11.07
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:56:11.086
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:56:11.091
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jun 28 08:56:11.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2273" for this suite. 06/28/23 08:56:11.167
{"msg":"PASSED [sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]","completed":314,"skipped":5729,"failed":0}
------------------------------
â€¢ [0.105 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:56:11.069
    Jun 28 08:56:11.069: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename configmap 06/28/23 08:56:11.07
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:56:11.086
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:56:11.091
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/configmap_volume.go:503
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jun 28 08:56:11.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-2273" for this suite. 06/28/23 08:56:11.167
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:56:11.173
Jun 28 08:56:11.174: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename webhook 06/28/23 08:56:11.174
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:56:11.188
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:56:11.193
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 06/28/23 08:56:11.215
STEP: Create role binding to let webhook read extension-apiserver-authentication 06/28/23 08:56:11.412
STEP: Deploying the webhook pod 06/28/23 08:56:11.424
STEP: Wait for the deployment to be ready 06/28/23 08:56:11.442
Jun 28 08:56:11.454: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 06/28/23 08:56:13.47
STEP: Verifying the service has paired with the endpoint 06/28/23 08:56:13.48
Jun 28 08:56:14.481: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655
STEP: Listing all of the created validation webhooks 06/28/23 08:56:14.547
STEP: Creating a configMap that should be mutated 06/28/23 08:56:14.654
STEP: Deleting the collection of validation webhooks 06/28/23 08:56:15.167
STEP: Creating a configMap that should not be mutated 06/28/23 08:56:15.218
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jun 28 08:56:15.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2719" for this suite. 06/28/23 08:56:15.238
STEP: Destroying namespace "webhook-2719-markers" for this suite. 06/28/23 08:56:15.248
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","completed":315,"skipped":5729,"failed":0}
------------------------------
â€¢ [4.116 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:56:11.173
    Jun 28 08:56:11.174: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename webhook 06/28/23 08:56:11.174
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:56:11.188
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:56:11.193
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 06/28/23 08:56:11.215
    STEP: Create role binding to let webhook read extension-apiserver-authentication 06/28/23 08:56:11.412
    STEP: Deploying the webhook pod 06/28/23 08:56:11.424
    STEP: Wait for the deployment to be ready 06/28/23 08:56:11.442
    Jun 28 08:56:11.454: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 06/28/23 08:56:13.47
    STEP: Verifying the service has paired with the endpoint 06/28/23 08:56:13.48
    Jun 28 08:56:14.481: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing mutating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:655
    STEP: Listing all of the created validation webhooks 06/28/23 08:56:14.547
    STEP: Creating a configMap that should be mutated 06/28/23 08:56:14.654
    STEP: Deleting the collection of validation webhooks 06/28/23 08:56:15.167
    STEP: Creating a configMap that should not be mutated 06/28/23 08:56:15.218
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jun 28 08:56:15.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-2719" for this suite. 06/28/23 08:56:15.238
    STEP: Destroying namespace "webhook-2719-markers" for this suite. 06/28/23 08:56:15.248
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:56:15.291
Jun 28 08:56:15.291: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename webhook 06/28/23 08:56:15.291
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:56:15.313
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:56:15.32
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 06/28/23 08:56:15.339
STEP: Create role binding to let webhook read extension-apiserver-authentication 06/28/23 08:56:15.68
STEP: Deploying the webhook pod 06/28/23 08:56:15.686
STEP: Wait for the deployment to be ready 06/28/23 08:56:15.699
Jun 28 08:56:15.713: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 06/28/23 08:56:17.728
STEP: Verifying the service has paired with the endpoint 06/28/23 08:56:17.737
Jun 28 08:56:18.738: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340
Jun 28 08:56:18.743: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3140-crds.webhook.example.com via the AdmissionRegistration API 06/28/23 08:56:19.26
STEP: Creating a custom resource that should be mutated by the webhook 06/28/23 08:56:19.364
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jun 28 08:56:22.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6151" for this suite. 06/28/23 08:56:22.084
STEP: Destroying namespace "webhook-6151-markers" for this suite. 06/28/23 08:56:22.091
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","completed":316,"skipped":5751,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.841 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:56:15.291
    Jun 28 08:56:15.291: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename webhook 06/28/23 08:56:15.291
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:56:15.313
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:56:15.32
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 06/28/23 08:56:15.339
    STEP: Create role binding to let webhook read extension-apiserver-authentication 06/28/23 08:56:15.68
    STEP: Deploying the webhook pod 06/28/23 08:56:15.686
    STEP: Wait for the deployment to be ready 06/28/23 08:56:15.699
    Jun 28 08:56:15.713: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 06/28/23 08:56:17.728
    STEP: Verifying the service has paired with the endpoint 06/28/23 08:56:17.737
    Jun 28 08:56:18.738: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with pruning [Conformance]
      test/e2e/apimachinery/webhook.go:340
    Jun 28 08:56:18.743: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3140-crds.webhook.example.com via the AdmissionRegistration API 06/28/23 08:56:19.26
    STEP: Creating a custom resource that should be mutated by the webhook 06/28/23 08:56:19.364
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jun 28 08:56:22.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-6151" for this suite. 06/28/23 08:56:22.084
    STEP: Destroying namespace "webhook-6151-markers" for this suite. 06/28/23 08:56:22.091
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:56:22.132
Jun 28 08:56:22.132: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename container-probe 06/28/23 08:56:22.133
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:56:22.148
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:56:22.152
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195
STEP: Creating pod liveness-96e23b94-070f-4c48-9ed4-799f9d0d9a3f in namespace container-probe-9474 06/28/23 08:56:22.156
Jun 28 08:56:22.164: INFO: Waiting up to 5m0s for pod "liveness-96e23b94-070f-4c48-9ed4-799f9d0d9a3f" in namespace "container-probe-9474" to be "not pending"
Jun 28 08:56:22.168: INFO: Pod "liveness-96e23b94-070f-4c48-9ed4-799f9d0d9a3f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.84186ms
Jun 28 08:56:24.174: INFO: Pod "liveness-96e23b94-070f-4c48-9ed4-799f9d0d9a3f": Phase="Running", Reason="", readiness=true. Elapsed: 2.010377329s
Jun 28 08:56:24.174: INFO: Pod "liveness-96e23b94-070f-4c48-9ed4-799f9d0d9a3f" satisfied condition "not pending"
Jun 28 08:56:24.174: INFO: Started pod liveness-96e23b94-070f-4c48-9ed4-799f9d0d9a3f in namespace container-probe-9474
STEP: checking the pod's current state and verifying that restartCount is present 06/28/23 08:56:24.174
Jun 28 08:56:24.178: INFO: Initial restart count of pod liveness-96e23b94-070f-4c48-9ed4-799f9d0d9a3f is 0
Jun 28 08:56:44.244: INFO: Restart count of pod container-probe-9474/liveness-96e23b94-070f-4c48-9ed4-799f9d0d9a3f is now 1 (20.065963167s elapsed)
Jun 28 08:57:04.297: INFO: Restart count of pod container-probe-9474/liveness-96e23b94-070f-4c48-9ed4-799f9d0d9a3f is now 2 (40.118849106s elapsed)
Jun 28 08:57:24.351: INFO: Restart count of pod container-probe-9474/liveness-96e23b94-070f-4c48-9ed4-799f9d0d9a3f is now 3 (1m0.172477728s elapsed)
Jun 28 08:57:44.402: INFO: Restart count of pod container-probe-9474/liveness-96e23b94-070f-4c48-9ed4-799f9d0d9a3f is now 4 (1m20.22436739s elapsed)
Jun 28 08:58:58.612: INFO: Restart count of pod container-probe-9474/liveness-96e23b94-070f-4c48-9ed4-799f9d0d9a3f is now 5 (2m34.434383988s elapsed)
STEP: deleting the pod 06/28/23 08:58:58.612
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jun 28 08:58:58.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9474" for this suite. 06/28/23 08:58:58.634
{"msg":"PASSED [sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","completed":317,"skipped":5765,"failed":0}
------------------------------
â€¢ [SLOW TEST] [156.508 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:56:22.132
    Jun 28 08:56:22.132: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename container-probe 06/28/23 08:56:22.133
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:56:22.148
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:56:22.152
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should have monotonically increasing restart count [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:195
    STEP: Creating pod liveness-96e23b94-070f-4c48-9ed4-799f9d0d9a3f in namespace container-probe-9474 06/28/23 08:56:22.156
    Jun 28 08:56:22.164: INFO: Waiting up to 5m0s for pod "liveness-96e23b94-070f-4c48-9ed4-799f9d0d9a3f" in namespace "container-probe-9474" to be "not pending"
    Jun 28 08:56:22.168: INFO: Pod "liveness-96e23b94-070f-4c48-9ed4-799f9d0d9a3f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.84186ms
    Jun 28 08:56:24.174: INFO: Pod "liveness-96e23b94-070f-4c48-9ed4-799f9d0d9a3f": Phase="Running", Reason="", readiness=true. Elapsed: 2.010377329s
    Jun 28 08:56:24.174: INFO: Pod "liveness-96e23b94-070f-4c48-9ed4-799f9d0d9a3f" satisfied condition "not pending"
    Jun 28 08:56:24.174: INFO: Started pod liveness-96e23b94-070f-4c48-9ed4-799f9d0d9a3f in namespace container-probe-9474
    STEP: checking the pod's current state and verifying that restartCount is present 06/28/23 08:56:24.174
    Jun 28 08:56:24.178: INFO: Initial restart count of pod liveness-96e23b94-070f-4c48-9ed4-799f9d0d9a3f is 0
    Jun 28 08:56:44.244: INFO: Restart count of pod container-probe-9474/liveness-96e23b94-070f-4c48-9ed4-799f9d0d9a3f is now 1 (20.065963167s elapsed)
    Jun 28 08:57:04.297: INFO: Restart count of pod container-probe-9474/liveness-96e23b94-070f-4c48-9ed4-799f9d0d9a3f is now 2 (40.118849106s elapsed)
    Jun 28 08:57:24.351: INFO: Restart count of pod container-probe-9474/liveness-96e23b94-070f-4c48-9ed4-799f9d0d9a3f is now 3 (1m0.172477728s elapsed)
    Jun 28 08:57:44.402: INFO: Restart count of pod container-probe-9474/liveness-96e23b94-070f-4c48-9ed4-799f9d0d9a3f is now 4 (1m20.22436739s elapsed)
    Jun 28 08:58:58.612: INFO: Restart count of pod container-probe-9474/liveness-96e23b94-070f-4c48-9ed4-799f9d0d9a3f is now 5 (2m34.434383988s elapsed)
    STEP: deleting the pod 06/28/23 08:58:58.612
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jun 28 08:58:58.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-9474" for this suite. 06/28/23 08:58:58.634
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Secrets
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:58:58.641
Jun 28 08:58:58.641: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename secrets 06/28/23 08:58:58.642
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:58:58.668
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:58:58.673
[It] should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153
STEP: creating a secret 06/28/23 08:58:58.678
STEP: listing secrets in all namespaces to ensure that there are more than zero 06/28/23 08:58:58.683
STEP: patching the secret 06/28/23 08:58:58.688
STEP: deleting the secret using a LabelSelector 06/28/23 08:58:58.698
STEP: listing secrets in all namespaces, searching for label name and value in patch 06/28/23 08:58:58.705
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Jun 28 08:58:58.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9934" for this suite. 06/28/23 08:58:58.719
{"msg":"PASSED [sig-node] Secrets should patch a secret [Conformance]","completed":318,"skipped":5766,"failed":0}
------------------------------
â€¢ [0.085 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:58:58.641
    Jun 28 08:58:58.641: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename secrets 06/28/23 08:58:58.642
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:58:58.668
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:58:58.673
    [It] should patch a secret [Conformance]
      test/e2e/common/node/secrets.go:153
    STEP: creating a secret 06/28/23 08:58:58.678
    STEP: listing secrets in all namespaces to ensure that there are more than zero 06/28/23 08:58:58.683
    STEP: patching the secret 06/28/23 08:58:58.688
    STEP: deleting the secret using a LabelSelector 06/28/23 08:58:58.698
    STEP: listing secrets in all namespaces, searching for label name and value in patch 06/28/23 08:58:58.705
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Jun 28 08:58:58.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-9934" for this suite. 06/28/23 08:58:58.719
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:58:58.726
Jun 28 08:58:58.726: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename webhook 06/28/23 08:58:58.727
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:58:58.74
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:58:58.745
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 06/28/23 08:58:58.764
STEP: Create role binding to let webhook read extension-apiserver-authentication 06/28/23 08:58:59.223
STEP: Deploying the webhook pod 06/28/23 08:58:59.232
STEP: Wait for the deployment to be ready 06/28/23 08:58:59.245
Jun 28 08:58:59.254: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 06/28/23 08:59:01.271
STEP: Verifying the service has paired with the endpoint 06/28/23 08:59:01.283
Jun 28 08:59:02.283: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116
STEP: fetching the /apis discovery document 06/28/23 08:59:02.289
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 06/28/23 08:59:02.292
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 06/28/23 08:59:02.292
STEP: fetching the /apis/admissionregistration.k8s.io discovery document 06/28/23 08:59:02.292
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 06/28/23 08:59:02.294
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 06/28/23 08:59:02.294
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 06/28/23 08:59:02.296
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jun 28 08:59:02.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5386" for this suite. 06/28/23 08:59:02.306
STEP: Destroying namespace "webhook-5386-markers" for this suite. 06/28/23 08:59:02.315
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","completed":319,"skipped":5771,"failed":0}
------------------------------
â€¢ [3.647 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:58:58.726
    Jun 28 08:58:58.726: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename webhook 06/28/23 08:58:58.727
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:58:58.74
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:58:58.745
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 06/28/23 08:58:58.764
    STEP: Create role binding to let webhook read extension-apiserver-authentication 06/28/23 08:58:59.223
    STEP: Deploying the webhook pod 06/28/23 08:58:59.232
    STEP: Wait for the deployment to be ready 06/28/23 08:58:59.245
    Jun 28 08:58:59.254: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 06/28/23 08:59:01.271
    STEP: Verifying the service has paired with the endpoint 06/28/23 08:59:01.283
    Jun 28 08:59:02.283: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should include webhook resources in discovery documents [Conformance]
      test/e2e/apimachinery/webhook.go:116
    STEP: fetching the /apis discovery document 06/28/23 08:59:02.289
    STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 06/28/23 08:59:02.292
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 06/28/23 08:59:02.292
    STEP: fetching the /apis/admissionregistration.k8s.io discovery document 06/28/23 08:59:02.292
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 06/28/23 08:59:02.294
    STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 06/28/23 08:59:02.294
    STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 06/28/23 08:59:02.296
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jun 28 08:59:02.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-5386" for this suite. 06/28/23 08:59:02.306
    STEP: Destroying namespace "webhook-5386-markers" for this suite. 06/28/23 08:59:02.315
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace
  should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1745
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:59:02.374
Jun 28 08:59:02.375: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename kubectl 06/28/23 08:59:02.375
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:59:02.392
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:59:02.399
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1732
[It] should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1745
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 06/28/23 08:59:02.405
Jun 28 08:59:02.405: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-4590 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Jun 28 08:59:02.478: INFO: stderr: ""
Jun 28 08:59:02.478: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running 06/28/23 08:59:02.478
STEP: verifying the pod e2e-test-httpd-pod was created 06/28/23 08:59:07.531
Jun 28 08:59:07.532: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-4590 get pod e2e-test-httpd-pod -o json'
Jun 28 08:59:07.598: INFO: stderr: ""
Jun 28 08:59:07.598: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"587b7030d679e9ba15ac1f56923e573f69abc76cf1f90636fba0b2309ce4d115\",\n            \"cni.projectcalico.org/podIP\": \"172.21.122.53/32\",\n            \"cni.projectcalico.org/podIPs\": \"172.21.122.53/32\"\n        },\n        \"creationTimestamp\": \"2023-06-28T08:59:02Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-4590\",\n        \"resourceVersion\": \"86315\",\n        \"uid\": \"c4ce6a21-59a8-4f30-b618-bd32d1363661\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-vkjtj\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"ske-rhel-749f7d55c8xdd8b6-ct4cp\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-vkjtj\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-06-28T08:59:02Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-06-28T08:59:04Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-06-28T08:59:04Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-06-28T08:59:02Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://79bce861839f2d59d05f0ae465fa0114625a0328b9f1d334b930e601e919c8c0\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-06-28T08:59:03Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.11.3\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.21.122.53\",\n        \"podIPs\": [\n            {\n                \"ip\": \"172.21.122.53\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-06-28T08:59:02Z\"\n    }\n}\n"
STEP: replace the image in the pod 06/28/23 08:59:07.598
Jun 28 08:59:07.599: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-4590 replace -f -'
Jun 28 08:59:08.372: INFO: stderr: ""
Jun 28 08:59:08.372: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-2 06/28/23 08:59:08.372
[AfterEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1736
Jun 28 08:59:08.378: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-4590 delete pods e2e-test-httpd-pod'
Jun 28 08:59:10.191: INFO: stderr: ""
Jun 28 08:59:10.191: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jun 28 08:59:10.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4590" for this suite. 06/28/23 08:59:10.198
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","completed":320,"skipped":5820,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.832 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl replace
  test/e2e/kubectl/kubectl.go:1729
    should update a single-container pod's image  [Conformance]
    test/e2e/kubectl/kubectl.go:1745

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:59:02.374
    Jun 28 08:59:02.375: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename kubectl 06/28/23 08:59:02.375
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:59:02.392
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:59:02.399
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1732
    [It] should update a single-container pod's image  [Conformance]
      test/e2e/kubectl/kubectl.go:1745
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 06/28/23 08:59:02.405
    Jun 28 08:59:02.405: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-4590 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Jun 28 08:59:02.478: INFO: stderr: ""
    Jun 28 08:59:02.478: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod is running 06/28/23 08:59:02.478
    STEP: verifying the pod e2e-test-httpd-pod was created 06/28/23 08:59:07.531
    Jun 28 08:59:07.532: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-4590 get pod e2e-test-httpd-pod -o json'
    Jun 28 08:59:07.598: INFO: stderr: ""
    Jun 28 08:59:07.598: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"587b7030d679e9ba15ac1f56923e573f69abc76cf1f90636fba0b2309ce4d115\",\n            \"cni.projectcalico.org/podIP\": \"172.21.122.53/32\",\n            \"cni.projectcalico.org/podIPs\": \"172.21.122.53/32\"\n        },\n        \"creationTimestamp\": \"2023-06-28T08:59:02Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-4590\",\n        \"resourceVersion\": \"86315\",\n        \"uid\": \"c4ce6a21-59a8-4f30-b618-bd32d1363661\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-vkjtj\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"ske-rhel-749f7d55c8xdd8b6-ct4cp\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-vkjtj\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-06-28T08:59:02Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-06-28T08:59:04Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-06-28T08:59:04Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-06-28T08:59:02Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://79bce861839f2d59d05f0ae465fa0114625a0328b9f1d334b930e601e919c8c0\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-06-28T08:59:03Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.11.3\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.21.122.53\",\n        \"podIPs\": [\n            {\n                \"ip\": \"172.21.122.53\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-06-28T08:59:02Z\"\n    }\n}\n"
    STEP: replace the image in the pod 06/28/23 08:59:07.598
    Jun 28 08:59:07.599: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-4590 replace -f -'
    Jun 28 08:59:08.372: INFO: stderr: ""
    Jun 28 08:59:08.372: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-2 06/28/23 08:59:08.372
    [AfterEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1736
    Jun 28 08:59:08.378: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=kubectl-4590 delete pods e2e-test-httpd-pod'
    Jun 28 08:59:10.191: INFO: stderr: ""
    Jun 28 08:59:10.191: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jun 28 08:59:10.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-4590" for this suite. 06/28/23 08:59:10.198
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 08:59:10.207
Jun 28 08:59:10.208: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename sched-pred 06/28/23 08:59:10.208
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:59:10.222
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:59:10.226
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Jun 28 08:59:10.231: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun 28 08:59:10.246: INFO: Waiting for terminating namespaces to be deleted...
Jun 28 08:59:10.251: INFO: 
Logging pods the apiserver thinks is on node ske-rhel-749f7d55c8xdd8b6-ct4cp before test
Jun 28 08:59:10.265: INFO: calico-node-h6www from kube-system started at 2023-06-28 05:12:45 +0000 UTC (1 container statuses recorded)
Jun 28 08:59:10.265: INFO: 	Container calico-node ready: true, restart count 0
Jun 28 08:59:10.265: INFO: csi-smb-node-7hhpt from kube-system started at 2023-06-28 05:12:45 +0000 UTC (3 container statuses recorded)
Jun 28 08:59:10.265: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 28 08:59:10.265: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jun 28 08:59:10.265: INFO: 	Container smb ready: true, restart count 0
Jun 28 08:59:10.265: INFO: kube-proxy-78kbb from kube-system started at 2023-06-28 05:12:45 +0000 UTC (1 container statuses recorded)
Jun 28 08:59:10.265: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 28 08:59:10.265: INFO: node-exporter-fkq2r from kube-system started at 2023-06-28 05:12:45 +0000 UTC (1 container statuses recorded)
Jun 28 08:59:10.265: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Jun 28 08:59:10.265: INFO: sonobuoy from sonobuoy started at 2023-06-28 07:40:00 +0000 UTC (1 container statuses recorded)
Jun 28 08:59:10.265: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun 28 08:59:10.265: INFO: sonobuoy-systemd-logs-daemon-set-747dcc3d7bcc492c-v77g9 from sonobuoy started at 2023-06-28 07:40:01 +0000 UTC (2 container statuses recorded)
Jun 28 08:59:10.265: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 28 08:59:10.265: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 28 08:59:10.265: INFO: 
Logging pods the apiserver thinks is on node ske-rhel-749f7d55c8xdd8b6-srshq before test
Jun 28 08:59:10.282: INFO: calico-kube-controllers-77cc457ff7-jfwrp from kube-system started at 2023-06-28 05:13:50 +0000 UTC (1 container statuses recorded)
Jun 28 08:59:10.282: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jun 28 08:59:10.282: INFO: calico-node-xkhs5 from kube-system started at 2023-06-28 05:12:45 +0000 UTC (1 container statuses recorded)
Jun 28 08:59:10.282: INFO: 	Container calico-node ready: true, restart count 0
Jun 28 08:59:10.282: INFO: coredns-7b4f76cbb6-gkxll from kube-system started at 2023-06-28 05:13:50 +0000 UTC (1 container statuses recorded)
Jun 28 08:59:10.282: INFO: 	Container coredns ready: true, restart count 0
Jun 28 08:59:10.282: INFO: coredns-7b4f76cbb6-ztck5 from kube-system started at 2023-06-28 05:13:50 +0000 UTC (1 container statuses recorded)
Jun 28 08:59:10.282: INFO: 	Container coredns ready: true, restart count 0
Jun 28 08:59:10.282: INFO: csi-smb-controller-7ffdfbf4b6-pfh4n from kube-system started at 2023-06-28 05:13:50 +0000 UTC (3 container statuses recorded)
Jun 28 08:59:10.282: INFO: 	Container csi-provisioner ready: true, restart count 1
Jun 28 08:59:10.282: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 28 08:59:10.282: INFO: 	Container smb ready: true, restart count 0
Jun 28 08:59:10.282: INFO: csi-smb-node-v64b5 from kube-system started at 2023-06-28 05:12:45 +0000 UTC (3 container statuses recorded)
Jun 28 08:59:10.282: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 28 08:59:10.282: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jun 28 08:59:10.282: INFO: 	Container smb ready: true, restart count 0
Jun 28 08:59:10.282: INFO: kube-proxy-l8rq8 from kube-system started at 2023-06-28 05:12:45 +0000 UTC (1 container statuses recorded)
Jun 28 08:59:10.282: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 28 08:59:10.282: INFO: metrics-server-54cd6dc7f5-8j8ss from kube-system started at 2023-06-28 05:13:50 +0000 UTC (1 container statuses recorded)
Jun 28 08:59:10.282: INFO: 	Container metrics-server ready: true, restart count 0
Jun 28 08:59:10.282: INFO: nfs-subdir-external-provisioner-548dcf4dc4-z87tg from kube-system started at 2023-06-28 05:13:50 +0000 UTC (1 container statuses recorded)
Jun 28 08:59:10.282: INFO: 	Container nfs-subdir-external-provisioner ready: true, restart count 1
Jun 28 08:59:10.282: INFO: node-exporter-hcfhb from kube-system started at 2023-06-28 05:12:45 +0000 UTC (1 container statuses recorded)
Jun 28 08:59:10.282: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Jun 28 08:59:10.282: INFO: vpn-target-bcf545797-bfhjg from kube-system started at 2023-06-28 05:13:50 +0000 UTC (1 container statuses recorded)
Jun 28 08:59:10.282: INFO: 	Container vpn-target ready: true, restart count 0
Jun 28 08:59:10.282: INFO: sonobuoy-systemd-logs-daemon-set-747dcc3d7bcc492c-gmpnw from sonobuoy started at 2023-06-28 07:40:01 +0000 UTC (2 container statuses recorded)
Jun 28 08:59:10.282: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 28 08:59:10.282: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 28 08:59:10.282: INFO: 
Logging pods the apiserver thinks is on node ske-rhel-749f7d55c8xdd8b6-zxlfv before test
Jun 28 08:59:10.295: INFO: calico-node-6xpbr from kube-system started at 2023-06-28 05:12:49 +0000 UTC (1 container statuses recorded)
Jun 28 08:59:10.295: INFO: 	Container calico-node ready: true, restart count 0
Jun 28 08:59:10.295: INFO: csi-smb-node-t6cfj from kube-system started at 2023-06-28 05:12:49 +0000 UTC (3 container statuses recorded)
Jun 28 08:59:10.295: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 28 08:59:10.295: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jun 28 08:59:10.295: INFO: 	Container smb ready: true, restart count 0
Jun 28 08:59:10.295: INFO: kube-proxy-l6xjw from kube-system started at 2023-06-28 05:12:49 +0000 UTC (1 container statuses recorded)
Jun 28 08:59:10.295: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 28 08:59:10.295: INFO: node-exporter-w4bjx from kube-system started at 2023-06-28 05:12:49 +0000 UTC (1 container statuses recorded)
Jun 28 08:59:10.295: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Jun 28 08:59:10.295: INFO: sonobuoy-e2e-job-4078a53074bd4828 from sonobuoy started at 2023-06-28 07:40:01 +0000 UTC (2 container statuses recorded)
Jun 28 08:59:10.295: INFO: 	Container e2e ready: true, restart count 0
Jun 28 08:59:10.295: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 28 08:59:10.295: INFO: sonobuoy-systemd-logs-daemon-set-747dcc3d7bcc492c-wmgvv from sonobuoy started at 2023-06-28 07:40:01 +0000 UTC (2 container statuses recorded)
Jun 28 08:59:10.295: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 28 08:59:10.295: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699
STEP: Trying to launch a pod without a label to get a node which can launch it. 06/28/23 08:59:10.295
Jun 28 08:59:10.304: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-3692" to be "running"
Jun 28 08:59:10.308: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 4.352936ms
Jun 28 08:59:12.314: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.009793458s
Jun 28 08:59:12.314: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 06/28/23 08:59:12.319
STEP: Trying to apply a random label on the found node. 06/28/23 08:59:12.335
STEP: verifying the node has the label kubernetes.io/e2e-776821eb-80b3-4076-84ab-8f60b8fbfb5c 95 06/28/23 08:59:12.348
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 06/28/23 08:59:12.354
Jun 28 08:59:12.360: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-3692" to be "not pending"
Jun 28 08:59:12.363: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.555487ms
Jun 28 08:59:14.368: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.008470688s
Jun 28 08:59:14.368: INFO: Pod "pod4" satisfied condition "not pending"
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 192.168.11.3 on the node which pod4 resides and expect not scheduled 06/28/23 08:59:14.368
Jun 28 08:59:14.376: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-3692" to be "not pending"
Jun 28 08:59:14.381: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.804542ms
Jun 28 08:59:16.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010694633s
Jun 28 08:59:18.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010518568s
Jun 28 08:59:20.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.010040174s
Jun 28 08:59:22.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.01062527s
Jun 28 08:59:24.392: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.015596852s
Jun 28 08:59:26.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.011370253s
Jun 28 08:59:28.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.010282086s
Jun 28 08:59:30.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.010193885s
Jun 28 08:59:32.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.010177541s
Jun 28 08:59:34.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.01003574s
Jun 28 08:59:36.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.010380731s
Jun 28 08:59:38.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.010654719s
Jun 28 08:59:40.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.009908595s
Jun 28 08:59:42.385: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.009154874s
Jun 28 08:59:44.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.009590653s
Jun 28 08:59:46.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.010152875s
Jun 28 08:59:48.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.009602327s
Jun 28 08:59:50.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.009925371s
Jun 28 08:59:52.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.00989288s
Jun 28 08:59:54.425: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.049443108s
Jun 28 08:59:56.443: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.067093784s
Jun 28 08:59:58.446: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.069900327s
Jun 28 09:00:00.456: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.07994851s
Jun 28 09:00:02.442: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.065951521s
Jun 28 09:00:04.477: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.10129021s
Jun 28 09:00:06.388: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.011682891s
Jun 28 09:00:08.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.010017725s
Jun 28 09:00:10.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.010798745s
Jun 28 09:00:12.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.010102564s
Jun 28 09:00:14.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.009956395s
Jun 28 09:00:16.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.010001987s
Jun 28 09:00:18.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.009914719s
Jun 28 09:00:20.388: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.012242357s
Jun 28 09:00:22.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.010084069s
Jun 28 09:00:24.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.009814309s
Jun 28 09:00:26.389: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.012718156s
Jun 28 09:00:28.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.010728092s
Jun 28 09:00:30.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.010072838s
Jun 28 09:00:32.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.010716488s
Jun 28 09:00:34.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.010784954s
Jun 28 09:00:36.388: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.012162262s
Jun 28 09:00:38.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.0100706s
Jun 28 09:00:40.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.010903971s
Jun 28 09:00:42.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.010375009s
Jun 28 09:00:44.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.010423413s
Jun 28 09:00:46.388: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.01177367s
Jun 28 09:00:48.385: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.00951276s
Jun 28 09:00:50.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.0109613s
Jun 28 09:00:52.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.010765452s
Jun 28 09:00:54.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.01132732s
Jun 28 09:00:56.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.009830877s
Jun 28 09:00:58.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.010457028s
Jun 28 09:01:00.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.010567374s
Jun 28 09:01:02.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.010523913s
Jun 28 09:01:04.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.010513893s
Jun 28 09:01:06.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.010635403s
Jun 28 09:01:08.385: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.009514045s
Jun 28 09:01:10.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.01033949s
Jun 28 09:01:12.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.010885333s
Jun 28 09:01:14.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.010395772s
Jun 28 09:01:16.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.011002041s
Jun 28 09:01:18.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.010818965s
Jun 28 09:01:20.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.009553141s
Jun 28 09:01:22.388: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.011703819s
Jun 28 09:01:24.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.009684729s
Jun 28 09:01:26.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.010369026s
Jun 28 09:01:28.385: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.009509846s
Jun 28 09:01:30.390: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.013608442s
Jun 28 09:01:32.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.010216302s
Jun 28 09:01:34.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.01144774s
Jun 28 09:01:36.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.01040486s
Jun 28 09:01:38.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.010564203s
Jun 28 09:01:40.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.010580913s
Jun 28 09:01:42.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.010851568s
Jun 28 09:01:44.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.010541652s
Jun 28 09:01:46.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.010415685s
Jun 28 09:01:48.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.010047587s
Jun 28 09:01:50.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.010688182s
Jun 28 09:01:52.388: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.011699066s
Jun 28 09:01:54.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.010172292s
Jun 28 09:01:56.385: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.009399467s
Jun 28 09:01:58.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.01097327s
Jun 28 09:02:00.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.011422384s
Jun 28 09:02:02.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.010680555s
Jun 28 09:02:04.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.010235897s
Jun 28 09:02:06.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.009558629s
Jun 28 09:02:08.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.010688001s
Jun 28 09:02:10.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.010261435s
Jun 28 09:02:12.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.010756575s
Jun 28 09:02:14.388: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.011801943s
Jun 28 09:02:16.388: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.012050565s
Jun 28 09:02:18.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.010875223s
Jun 28 09:02:20.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.011073113s
Jun 28 09:02:22.388: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.011703575s
Jun 28 09:02:24.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.010411984s
Jun 28 09:02:26.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.010867025s
Jun 28 09:02:28.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.009670821s
Jun 28 09:02:30.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.010187622s
Jun 28 09:02:32.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.010912938s
Jun 28 09:02:34.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.009901816s
Jun 28 09:02:36.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.011143498s
Jun 28 09:02:38.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.010089925s
Jun 28 09:02:40.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.010644171s
Jun 28 09:02:42.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.010435796s
Jun 28 09:02:44.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.011375828s
Jun 28 09:02:46.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.010415576s
Jun 28 09:02:48.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.009868356s
Jun 28 09:02:50.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.009533908s
Jun 28 09:02:52.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.01135067s
Jun 28 09:02:54.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.011168779s
Jun 28 09:02:56.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.011088666s
Jun 28 09:02:58.385: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.009526001s
Jun 28 09:03:00.389: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.012960365s
Jun 28 09:03:02.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.010641025s
Jun 28 09:03:04.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.010841462s
Jun 28 09:03:06.385: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.009430161s
Jun 28 09:03:08.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.010124344s
Jun 28 09:03:10.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.009842743s
Jun 28 09:03:12.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.010604895s
Jun 28 09:03:14.385: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.009410659s
Jun 28 09:03:16.385: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.009317682s
Jun 28 09:03:18.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.00996568s
Jun 28 09:03:20.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.009990245s
Jun 28 09:03:22.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.010218459s
Jun 28 09:03:24.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.010603947s
Jun 28 09:03:26.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.010453279s
Jun 28 09:03:28.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.010169268s
Jun 28 09:03:30.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.010577449s
Jun 28 09:03:32.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.009861207s
Jun 28 09:03:34.388: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.012416155s
Jun 28 09:03:36.385: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.009330394s
Jun 28 09:03:38.388: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.012346314s
Jun 28 09:03:40.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.009629657s
Jun 28 09:03:42.388: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.012419514s
Jun 28 09:03:44.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.011018725s
Jun 28 09:03:46.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.009972453s
Jun 28 09:03:48.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.010235435s
Jun 28 09:03:50.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.009576641s
Jun 28 09:03:52.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.011198031s
Jun 28 09:03:54.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.011168506s
Jun 28 09:03:56.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.009778813s
Jun 28 09:03:58.388: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.011997266s
Jun 28 09:04:00.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.010715986s
Jun 28 09:04:02.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.010273132s
Jun 28 09:04:04.388: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.011557022s
Jun 28 09:04:06.392: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.016309115s
Jun 28 09:04:08.393: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.016580951s
Jun 28 09:04:10.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.009947973s
Jun 28 09:04:12.388: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.012321512s
Jun 28 09:04:14.388: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.012229485s
Jun 28 09:04:14.393: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.016883472s
STEP: removing the label kubernetes.io/e2e-776821eb-80b3-4076-84ab-8f60b8fbfb5c off the node ske-rhel-749f7d55c8xdd8b6-ct4cp 06/28/23 09:04:14.393
STEP: verifying the node doesn't have the label kubernetes.io/e2e-776821eb-80b3-4076-84ab-8f60b8fbfb5c 06/28/23 09:04:14.413
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Jun 28 09:04:14.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-3692" for this suite. 06/28/23 09:04:14.428
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","completed":321,"skipped":5846,"failed":0}
------------------------------
â€¢ [SLOW TEST] [304.227 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 08:59:10.207
    Jun 28 08:59:10.208: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename sched-pred 06/28/23 08:59:10.208
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 08:59:10.222
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 08:59:10.226
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Jun 28 08:59:10.231: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jun 28 08:59:10.246: INFO: Waiting for terminating namespaces to be deleted...
    Jun 28 08:59:10.251: INFO: 
    Logging pods the apiserver thinks is on node ske-rhel-749f7d55c8xdd8b6-ct4cp before test
    Jun 28 08:59:10.265: INFO: calico-node-h6www from kube-system started at 2023-06-28 05:12:45 +0000 UTC (1 container statuses recorded)
    Jun 28 08:59:10.265: INFO: 	Container calico-node ready: true, restart count 0
    Jun 28 08:59:10.265: INFO: csi-smb-node-7hhpt from kube-system started at 2023-06-28 05:12:45 +0000 UTC (3 container statuses recorded)
    Jun 28 08:59:10.265: INFO: 	Container liveness-probe ready: true, restart count 0
    Jun 28 08:59:10.265: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Jun 28 08:59:10.265: INFO: 	Container smb ready: true, restart count 0
    Jun 28 08:59:10.265: INFO: kube-proxy-78kbb from kube-system started at 2023-06-28 05:12:45 +0000 UTC (1 container statuses recorded)
    Jun 28 08:59:10.265: INFO: 	Container kube-proxy ready: true, restart count 0
    Jun 28 08:59:10.265: INFO: node-exporter-fkq2r from kube-system started at 2023-06-28 05:12:45 +0000 UTC (1 container statuses recorded)
    Jun 28 08:59:10.265: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
    Jun 28 08:59:10.265: INFO: sonobuoy from sonobuoy started at 2023-06-28 07:40:00 +0000 UTC (1 container statuses recorded)
    Jun 28 08:59:10.265: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jun 28 08:59:10.265: INFO: sonobuoy-systemd-logs-daemon-set-747dcc3d7bcc492c-v77g9 from sonobuoy started at 2023-06-28 07:40:01 +0000 UTC (2 container statuses recorded)
    Jun 28 08:59:10.265: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jun 28 08:59:10.265: INFO: 	Container systemd-logs ready: true, restart count 0
    Jun 28 08:59:10.265: INFO: 
    Logging pods the apiserver thinks is on node ske-rhel-749f7d55c8xdd8b6-srshq before test
    Jun 28 08:59:10.282: INFO: calico-kube-controllers-77cc457ff7-jfwrp from kube-system started at 2023-06-28 05:13:50 +0000 UTC (1 container statuses recorded)
    Jun 28 08:59:10.282: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Jun 28 08:59:10.282: INFO: calico-node-xkhs5 from kube-system started at 2023-06-28 05:12:45 +0000 UTC (1 container statuses recorded)
    Jun 28 08:59:10.282: INFO: 	Container calico-node ready: true, restart count 0
    Jun 28 08:59:10.282: INFO: coredns-7b4f76cbb6-gkxll from kube-system started at 2023-06-28 05:13:50 +0000 UTC (1 container statuses recorded)
    Jun 28 08:59:10.282: INFO: 	Container coredns ready: true, restart count 0
    Jun 28 08:59:10.282: INFO: coredns-7b4f76cbb6-ztck5 from kube-system started at 2023-06-28 05:13:50 +0000 UTC (1 container statuses recorded)
    Jun 28 08:59:10.282: INFO: 	Container coredns ready: true, restart count 0
    Jun 28 08:59:10.282: INFO: csi-smb-controller-7ffdfbf4b6-pfh4n from kube-system started at 2023-06-28 05:13:50 +0000 UTC (3 container statuses recorded)
    Jun 28 08:59:10.282: INFO: 	Container csi-provisioner ready: true, restart count 1
    Jun 28 08:59:10.282: INFO: 	Container liveness-probe ready: true, restart count 0
    Jun 28 08:59:10.282: INFO: 	Container smb ready: true, restart count 0
    Jun 28 08:59:10.282: INFO: csi-smb-node-v64b5 from kube-system started at 2023-06-28 05:12:45 +0000 UTC (3 container statuses recorded)
    Jun 28 08:59:10.282: INFO: 	Container liveness-probe ready: true, restart count 0
    Jun 28 08:59:10.282: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Jun 28 08:59:10.282: INFO: 	Container smb ready: true, restart count 0
    Jun 28 08:59:10.282: INFO: kube-proxy-l8rq8 from kube-system started at 2023-06-28 05:12:45 +0000 UTC (1 container statuses recorded)
    Jun 28 08:59:10.282: INFO: 	Container kube-proxy ready: true, restart count 0
    Jun 28 08:59:10.282: INFO: metrics-server-54cd6dc7f5-8j8ss from kube-system started at 2023-06-28 05:13:50 +0000 UTC (1 container statuses recorded)
    Jun 28 08:59:10.282: INFO: 	Container metrics-server ready: true, restart count 0
    Jun 28 08:59:10.282: INFO: nfs-subdir-external-provisioner-548dcf4dc4-z87tg from kube-system started at 2023-06-28 05:13:50 +0000 UTC (1 container statuses recorded)
    Jun 28 08:59:10.282: INFO: 	Container nfs-subdir-external-provisioner ready: true, restart count 1
    Jun 28 08:59:10.282: INFO: node-exporter-hcfhb from kube-system started at 2023-06-28 05:12:45 +0000 UTC (1 container statuses recorded)
    Jun 28 08:59:10.282: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
    Jun 28 08:59:10.282: INFO: vpn-target-bcf545797-bfhjg from kube-system started at 2023-06-28 05:13:50 +0000 UTC (1 container statuses recorded)
    Jun 28 08:59:10.282: INFO: 	Container vpn-target ready: true, restart count 0
    Jun 28 08:59:10.282: INFO: sonobuoy-systemd-logs-daemon-set-747dcc3d7bcc492c-gmpnw from sonobuoy started at 2023-06-28 07:40:01 +0000 UTC (2 container statuses recorded)
    Jun 28 08:59:10.282: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jun 28 08:59:10.282: INFO: 	Container systemd-logs ready: true, restart count 0
    Jun 28 08:59:10.282: INFO: 
    Logging pods the apiserver thinks is on node ske-rhel-749f7d55c8xdd8b6-zxlfv before test
    Jun 28 08:59:10.295: INFO: calico-node-6xpbr from kube-system started at 2023-06-28 05:12:49 +0000 UTC (1 container statuses recorded)
    Jun 28 08:59:10.295: INFO: 	Container calico-node ready: true, restart count 0
    Jun 28 08:59:10.295: INFO: csi-smb-node-t6cfj from kube-system started at 2023-06-28 05:12:49 +0000 UTC (3 container statuses recorded)
    Jun 28 08:59:10.295: INFO: 	Container liveness-probe ready: true, restart count 0
    Jun 28 08:59:10.295: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Jun 28 08:59:10.295: INFO: 	Container smb ready: true, restart count 0
    Jun 28 08:59:10.295: INFO: kube-proxy-l6xjw from kube-system started at 2023-06-28 05:12:49 +0000 UTC (1 container statuses recorded)
    Jun 28 08:59:10.295: INFO: 	Container kube-proxy ready: true, restart count 0
    Jun 28 08:59:10.295: INFO: node-exporter-w4bjx from kube-system started at 2023-06-28 05:12:49 +0000 UTC (1 container statuses recorded)
    Jun 28 08:59:10.295: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
    Jun 28 08:59:10.295: INFO: sonobuoy-e2e-job-4078a53074bd4828 from sonobuoy started at 2023-06-28 07:40:01 +0000 UTC (2 container statuses recorded)
    Jun 28 08:59:10.295: INFO: 	Container e2e ready: true, restart count 0
    Jun 28 08:59:10.295: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jun 28 08:59:10.295: INFO: sonobuoy-systemd-logs-daemon-set-747dcc3d7bcc492c-wmgvv from sonobuoy started at 2023-06-28 07:40:01 +0000 UTC (2 container statuses recorded)
    Jun 28 08:59:10.295: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jun 28 08:59:10.295: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
      test/e2e/scheduling/predicates.go:699
    STEP: Trying to launch a pod without a label to get a node which can launch it. 06/28/23 08:59:10.295
    Jun 28 08:59:10.304: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-3692" to be "running"
    Jun 28 08:59:10.308: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 4.352936ms
    Jun 28 08:59:12.314: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.009793458s
    Jun 28 08:59:12.314: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 06/28/23 08:59:12.319
    STEP: Trying to apply a random label on the found node. 06/28/23 08:59:12.335
    STEP: verifying the node has the label kubernetes.io/e2e-776821eb-80b3-4076-84ab-8f60b8fbfb5c 95 06/28/23 08:59:12.348
    STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 06/28/23 08:59:12.354
    Jun 28 08:59:12.360: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-3692" to be "not pending"
    Jun 28 08:59:12.363: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.555487ms
    Jun 28 08:59:14.368: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.008470688s
    Jun 28 08:59:14.368: INFO: Pod "pod4" satisfied condition "not pending"
    STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 192.168.11.3 on the node which pod4 resides and expect not scheduled 06/28/23 08:59:14.368
    Jun 28 08:59:14.376: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-3692" to be "not pending"
    Jun 28 08:59:14.381: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.804542ms
    Jun 28 08:59:16.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010694633s
    Jun 28 08:59:18.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010518568s
    Jun 28 08:59:20.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.010040174s
    Jun 28 08:59:22.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.01062527s
    Jun 28 08:59:24.392: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.015596852s
    Jun 28 08:59:26.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.011370253s
    Jun 28 08:59:28.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.010282086s
    Jun 28 08:59:30.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.010193885s
    Jun 28 08:59:32.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.010177541s
    Jun 28 08:59:34.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.01003574s
    Jun 28 08:59:36.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.010380731s
    Jun 28 08:59:38.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.010654719s
    Jun 28 08:59:40.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.009908595s
    Jun 28 08:59:42.385: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.009154874s
    Jun 28 08:59:44.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.009590653s
    Jun 28 08:59:46.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.010152875s
    Jun 28 08:59:48.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.009602327s
    Jun 28 08:59:50.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.009925371s
    Jun 28 08:59:52.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.00989288s
    Jun 28 08:59:54.425: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.049443108s
    Jun 28 08:59:56.443: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.067093784s
    Jun 28 08:59:58.446: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.069900327s
    Jun 28 09:00:00.456: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.07994851s
    Jun 28 09:00:02.442: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.065951521s
    Jun 28 09:00:04.477: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.10129021s
    Jun 28 09:00:06.388: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.011682891s
    Jun 28 09:00:08.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.010017725s
    Jun 28 09:00:10.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.010798745s
    Jun 28 09:00:12.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.010102564s
    Jun 28 09:00:14.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.009956395s
    Jun 28 09:00:16.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.010001987s
    Jun 28 09:00:18.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.009914719s
    Jun 28 09:00:20.388: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.012242357s
    Jun 28 09:00:22.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.010084069s
    Jun 28 09:00:24.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.009814309s
    Jun 28 09:00:26.389: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.012718156s
    Jun 28 09:00:28.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.010728092s
    Jun 28 09:00:30.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.010072838s
    Jun 28 09:00:32.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.010716488s
    Jun 28 09:00:34.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.010784954s
    Jun 28 09:00:36.388: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.012162262s
    Jun 28 09:00:38.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.0100706s
    Jun 28 09:00:40.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.010903971s
    Jun 28 09:00:42.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.010375009s
    Jun 28 09:00:44.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.010423413s
    Jun 28 09:00:46.388: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.01177367s
    Jun 28 09:00:48.385: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.00951276s
    Jun 28 09:00:50.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.0109613s
    Jun 28 09:00:52.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.010765452s
    Jun 28 09:00:54.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.01132732s
    Jun 28 09:00:56.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.009830877s
    Jun 28 09:00:58.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.010457028s
    Jun 28 09:01:00.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.010567374s
    Jun 28 09:01:02.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.010523913s
    Jun 28 09:01:04.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.010513893s
    Jun 28 09:01:06.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.010635403s
    Jun 28 09:01:08.385: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.009514045s
    Jun 28 09:01:10.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.01033949s
    Jun 28 09:01:12.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.010885333s
    Jun 28 09:01:14.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.010395772s
    Jun 28 09:01:16.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.011002041s
    Jun 28 09:01:18.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.010818965s
    Jun 28 09:01:20.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.009553141s
    Jun 28 09:01:22.388: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.011703819s
    Jun 28 09:01:24.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.009684729s
    Jun 28 09:01:26.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.010369026s
    Jun 28 09:01:28.385: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.009509846s
    Jun 28 09:01:30.390: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.013608442s
    Jun 28 09:01:32.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.010216302s
    Jun 28 09:01:34.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.01144774s
    Jun 28 09:01:36.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.01040486s
    Jun 28 09:01:38.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.010564203s
    Jun 28 09:01:40.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.010580913s
    Jun 28 09:01:42.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.010851568s
    Jun 28 09:01:44.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.010541652s
    Jun 28 09:01:46.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.010415685s
    Jun 28 09:01:48.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.010047587s
    Jun 28 09:01:50.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.010688182s
    Jun 28 09:01:52.388: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.011699066s
    Jun 28 09:01:54.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.010172292s
    Jun 28 09:01:56.385: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.009399467s
    Jun 28 09:01:58.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.01097327s
    Jun 28 09:02:00.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.011422384s
    Jun 28 09:02:02.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.010680555s
    Jun 28 09:02:04.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.010235897s
    Jun 28 09:02:06.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.009558629s
    Jun 28 09:02:08.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.010688001s
    Jun 28 09:02:10.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.010261435s
    Jun 28 09:02:12.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.010756575s
    Jun 28 09:02:14.388: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.011801943s
    Jun 28 09:02:16.388: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.012050565s
    Jun 28 09:02:18.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.010875223s
    Jun 28 09:02:20.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.011073113s
    Jun 28 09:02:22.388: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.011703575s
    Jun 28 09:02:24.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.010411984s
    Jun 28 09:02:26.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.010867025s
    Jun 28 09:02:28.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.009670821s
    Jun 28 09:02:30.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.010187622s
    Jun 28 09:02:32.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.010912938s
    Jun 28 09:02:34.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.009901816s
    Jun 28 09:02:36.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.011143498s
    Jun 28 09:02:38.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.010089925s
    Jun 28 09:02:40.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.010644171s
    Jun 28 09:02:42.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.010435796s
    Jun 28 09:02:44.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.011375828s
    Jun 28 09:02:46.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.010415576s
    Jun 28 09:02:48.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.009868356s
    Jun 28 09:02:50.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.009533908s
    Jun 28 09:02:52.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.01135067s
    Jun 28 09:02:54.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.011168779s
    Jun 28 09:02:56.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.011088666s
    Jun 28 09:02:58.385: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.009526001s
    Jun 28 09:03:00.389: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.012960365s
    Jun 28 09:03:02.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.010641025s
    Jun 28 09:03:04.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.010841462s
    Jun 28 09:03:06.385: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.009430161s
    Jun 28 09:03:08.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.010124344s
    Jun 28 09:03:10.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.009842743s
    Jun 28 09:03:12.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.010604895s
    Jun 28 09:03:14.385: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.009410659s
    Jun 28 09:03:16.385: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.009317682s
    Jun 28 09:03:18.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.00996568s
    Jun 28 09:03:20.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.009990245s
    Jun 28 09:03:22.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.010218459s
    Jun 28 09:03:24.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.010603947s
    Jun 28 09:03:26.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.010453279s
    Jun 28 09:03:28.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.010169268s
    Jun 28 09:03:30.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.010577449s
    Jun 28 09:03:32.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.009861207s
    Jun 28 09:03:34.388: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.012416155s
    Jun 28 09:03:36.385: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.009330394s
    Jun 28 09:03:38.388: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.012346314s
    Jun 28 09:03:40.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.009629657s
    Jun 28 09:03:42.388: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.012419514s
    Jun 28 09:03:44.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.011018725s
    Jun 28 09:03:46.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.009972453s
    Jun 28 09:03:48.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.010235435s
    Jun 28 09:03:50.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.009576641s
    Jun 28 09:03:52.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.011198031s
    Jun 28 09:03:54.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.011168506s
    Jun 28 09:03:56.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.009778813s
    Jun 28 09:03:58.388: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.011997266s
    Jun 28 09:04:00.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.010715986s
    Jun 28 09:04:02.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.010273132s
    Jun 28 09:04:04.388: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.011557022s
    Jun 28 09:04:06.392: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.016309115s
    Jun 28 09:04:08.393: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.016580951s
    Jun 28 09:04:10.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.009947973s
    Jun 28 09:04:12.388: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.012321512s
    Jun 28 09:04:14.388: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.012229485s
    Jun 28 09:04:14.393: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.016883472s
    STEP: removing the label kubernetes.io/e2e-776821eb-80b3-4076-84ab-8f60b8fbfb5c off the node ske-rhel-749f7d55c8xdd8b6-ct4cp 06/28/23 09:04:14.393
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-776821eb-80b3-4076-84ab-8f60b8fbfb5c 06/28/23 09:04:14.413
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Jun 28 09:04:14.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-3692" for this suite. 06/28/23 09:04:14.428
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-instrumentation] Events
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 09:04:14.435
Jun 28 09:04:14.435: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename events 06/28/23 09:04:14.44
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 09:04:14.454
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 09:04:14.457
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
STEP: Create set of events 06/28/23 09:04:14.462
Jun 28 09:04:14.470: INFO: created test-event-1
Jun 28 09:04:14.475: INFO: created test-event-2
Jun 28 09:04:14.480: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace 06/28/23 09:04:14.48
STEP: delete collection of events 06/28/23 09:04:14.484
Jun 28 09:04:14.484: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 06/28/23 09:04:14.502
Jun 28 09:04:14.502: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:187
Jun 28 09:04:14.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-5632" for this suite. 06/28/23 09:04:14.514
{"msg":"PASSED [sig-instrumentation] Events should delete a collection of events [Conformance]","completed":322,"skipped":5848,"failed":0}
------------------------------
â€¢ [0.086 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 09:04:14.435
    Jun 28 09:04:14.435: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename events 06/28/23 09:04:14.44
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 09:04:14.454
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 09:04:14.457
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/core_events.go:175
    STEP: Create set of events 06/28/23 09:04:14.462
    Jun 28 09:04:14.470: INFO: created test-event-1
    Jun 28 09:04:14.475: INFO: created test-event-2
    Jun 28 09:04:14.480: INFO: created test-event-3
    STEP: get a list of Events with a label in the current namespace 06/28/23 09:04:14.48
    STEP: delete collection of events 06/28/23 09:04:14.484
    Jun 28 09:04:14.484: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 06/28/23 09:04:14.502
    Jun 28 09:04:14.502: INFO: requesting list of events to confirm quantity
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:187
    Jun 28 09:04:14.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-5632" for this suite. 06/28/23 09:04:14.514
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 09:04:14.523
Jun 28 09:04:14.523: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename projected 06/28/23 09:04:14.524
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 09:04:14.538
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 09:04:14.543
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192
STEP: Creating a pod to test downward API volume plugin 06/28/23 09:04:14.549
Jun 28 09:04:14.560: INFO: Waiting up to 5m0s for pod "downwardapi-volume-68c5b446-a926-4986-8fea-83cfdcbaaa5b" in namespace "projected-3035" to be "Succeeded or Failed"
Jun 28 09:04:14.565: INFO: Pod "downwardapi-volume-68c5b446-a926-4986-8fea-83cfdcbaaa5b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.208761ms
Jun 28 09:04:16.570: INFO: Pod "downwardapi-volume-68c5b446-a926-4986-8fea-83cfdcbaaa5b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010383101s
Jun 28 09:04:18.570: INFO: Pod "downwardapi-volume-68c5b446-a926-4986-8fea-83cfdcbaaa5b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010156962s
STEP: Saw pod success 06/28/23 09:04:18.57
Jun 28 09:04:18.570: INFO: Pod "downwardapi-volume-68c5b446-a926-4986-8fea-83cfdcbaaa5b" satisfied condition "Succeeded or Failed"
Jun 28 09:04:18.576: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-zxlfv pod downwardapi-volume-68c5b446-a926-4986-8fea-83cfdcbaaa5b container client-container: <nil>
STEP: delete the pod 06/28/23 09:04:18.59
Jun 28 09:04:18.601: INFO: Waiting for pod downwardapi-volume-68c5b446-a926-4986-8fea-83cfdcbaaa5b to disappear
Jun 28 09:04:18.605: INFO: Pod downwardapi-volume-68c5b446-a926-4986-8fea-83cfdcbaaa5b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jun 28 09:04:18.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3035" for this suite. 06/28/23 09:04:18.614
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","completed":323,"skipped":5904,"failed":0}
------------------------------
â€¢ [4.097 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 09:04:14.523
    Jun 28 09:04:14.523: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename projected 06/28/23 09:04:14.524
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 09:04:14.538
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 09:04:14.543
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:192
    STEP: Creating a pod to test downward API volume plugin 06/28/23 09:04:14.549
    Jun 28 09:04:14.560: INFO: Waiting up to 5m0s for pod "downwardapi-volume-68c5b446-a926-4986-8fea-83cfdcbaaa5b" in namespace "projected-3035" to be "Succeeded or Failed"
    Jun 28 09:04:14.565: INFO: Pod "downwardapi-volume-68c5b446-a926-4986-8fea-83cfdcbaaa5b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.208761ms
    Jun 28 09:04:16.570: INFO: Pod "downwardapi-volume-68c5b446-a926-4986-8fea-83cfdcbaaa5b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010383101s
    Jun 28 09:04:18.570: INFO: Pod "downwardapi-volume-68c5b446-a926-4986-8fea-83cfdcbaaa5b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010156962s
    STEP: Saw pod success 06/28/23 09:04:18.57
    Jun 28 09:04:18.570: INFO: Pod "downwardapi-volume-68c5b446-a926-4986-8fea-83cfdcbaaa5b" satisfied condition "Succeeded or Failed"
    Jun 28 09:04:18.576: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-zxlfv pod downwardapi-volume-68c5b446-a926-4986-8fea-83cfdcbaaa5b container client-container: <nil>
    STEP: delete the pod 06/28/23 09:04:18.59
    Jun 28 09:04:18.601: INFO: Waiting for pod downwardapi-volume-68c5b446-a926-4986-8fea-83cfdcbaaa5b to disappear
    Jun 28 09:04:18.605: INFO: Pod downwardapi-volume-68c5b446-a926-4986-8fea-83cfdcbaaa5b no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jun 28 09:04:18.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3035" for this suite. 06/28/23 09:04:18.614
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 09:04:18.621
Jun 28 09:04:18.621: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename emptydir 06/28/23 09:04:18.622
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 09:04:18.636
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 09:04:18.641
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156
STEP: Creating a pod to test emptydir volume type on node default medium 06/28/23 09:04:18.645
Jun 28 09:04:18.653: INFO: Waiting up to 5m0s for pod "pod-ab5520fa-5698-472a-b7e2-25189957a145" in namespace "emptydir-2405" to be "Succeeded or Failed"
Jun 28 09:04:18.658: INFO: Pod "pod-ab5520fa-5698-472a-b7e2-25189957a145": Phase="Pending", Reason="", readiness=false. Elapsed: 4.409199ms
Jun 28 09:04:20.663: INFO: Pod "pod-ab5520fa-5698-472a-b7e2-25189957a145": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009528758s
Jun 28 09:04:22.663: INFO: Pod "pod-ab5520fa-5698-472a-b7e2-25189957a145": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009622274s
STEP: Saw pod success 06/28/23 09:04:22.663
Jun 28 09:04:22.663: INFO: Pod "pod-ab5520fa-5698-472a-b7e2-25189957a145" satisfied condition "Succeeded or Failed"
Jun 28 09:04:22.667: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod pod-ab5520fa-5698-472a-b7e2-25189957a145 container test-container: <nil>
STEP: delete the pod 06/28/23 09:04:22.68
Jun 28 09:04:22.690: INFO: Waiting for pod pod-ab5520fa-5698-472a-b7e2-25189957a145 to disappear
Jun 28 09:04:22.695: INFO: Pod pod-ab5520fa-5698-472a-b7e2-25189957a145 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jun 28 09:04:22.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2405" for this suite. 06/28/23 09:04:22.702
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","completed":324,"skipped":5928,"failed":0}
------------------------------
â€¢ [4.088 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 09:04:18.621
    Jun 28 09:04:18.621: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename emptydir 06/28/23 09:04:18.622
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 09:04:18.636
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 09:04:18.641
    [It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:156
    STEP: Creating a pod to test emptydir volume type on node default medium 06/28/23 09:04:18.645
    Jun 28 09:04:18.653: INFO: Waiting up to 5m0s for pod "pod-ab5520fa-5698-472a-b7e2-25189957a145" in namespace "emptydir-2405" to be "Succeeded or Failed"
    Jun 28 09:04:18.658: INFO: Pod "pod-ab5520fa-5698-472a-b7e2-25189957a145": Phase="Pending", Reason="", readiness=false. Elapsed: 4.409199ms
    Jun 28 09:04:20.663: INFO: Pod "pod-ab5520fa-5698-472a-b7e2-25189957a145": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009528758s
    Jun 28 09:04:22.663: INFO: Pod "pod-ab5520fa-5698-472a-b7e2-25189957a145": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009622274s
    STEP: Saw pod success 06/28/23 09:04:22.663
    Jun 28 09:04:22.663: INFO: Pod "pod-ab5520fa-5698-472a-b7e2-25189957a145" satisfied condition "Succeeded or Failed"
    Jun 28 09:04:22.667: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod pod-ab5520fa-5698-472a-b7e2-25189957a145 container test-container: <nil>
    STEP: delete the pod 06/28/23 09:04:22.68
    Jun 28 09:04:22.690: INFO: Waiting for pod pod-ab5520fa-5698-472a-b7e2-25189957a145 to disappear
    Jun 28 09:04:22.695: INFO: Pod pod-ab5520fa-5698-472a-b7e2-25189957a145 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jun 28 09:04:22.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-2405" for this suite. 06/28/23 09:04:22.702
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 09:04:22.709
Jun 28 09:04:22.709: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename deployment 06/28/23 09:04:22.71
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 09:04:22.722
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 09:04:22.726
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
STEP: creating a Deployment 06/28/23 09:04:22.737
Jun 28 09:04:22.737: INFO: Creating simple deployment test-deployment-tctdh
Jun 28 09:04:22.753: INFO: new replicaset for deployment "test-deployment-tctdh" is yet to be created
STEP: Getting /status 06/28/23 09:04:24.772
Jun 28 09:04:24.777: INFO: Deployment test-deployment-tctdh has Conditions: [{Available True 2023-06-28 09:04:23 +0000 UTC 2023-06-28 09:04:23 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-06-28 09:04:23 +0000 UTC 2023-06-28 09:04:22 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-tctdh-777898ffcc" has successfully progressed.}]
STEP: updating Deployment Status 06/28/23 09:04:24.777
Jun 28 09:04:24.789: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.June, 28, 9, 4, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 28, 9, 4, 23, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 28, 9, 4, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 28, 9, 4, 22, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-tctdh-777898ffcc\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated 06/28/23 09:04:24.789
Jun 28 09:04:24.792: INFO: Observed &Deployment event: ADDED
Jun 28 09:04:24.792: INFO: Observed Deployment test-deployment-tctdh in namespace deployment-4660 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-28 09:04:22 +0000 UTC 2023-06-28 09:04:22 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-tctdh-777898ffcc"}
Jun 28 09:04:24.792: INFO: Observed &Deployment event: MODIFIED
Jun 28 09:04:24.792: INFO: Observed Deployment test-deployment-tctdh in namespace deployment-4660 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-28 09:04:22 +0000 UTC 2023-06-28 09:04:22 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-tctdh-777898ffcc"}
Jun 28 09:04:24.792: INFO: Observed Deployment test-deployment-tctdh in namespace deployment-4660 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-06-28 09:04:22 +0000 UTC 2023-06-28 09:04:22 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jun 28 09:04:24.792: INFO: Observed &Deployment event: MODIFIED
Jun 28 09:04:24.792: INFO: Observed Deployment test-deployment-tctdh in namespace deployment-4660 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-06-28 09:04:22 +0000 UTC 2023-06-28 09:04:22 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jun 28 09:04:24.792: INFO: Observed Deployment test-deployment-tctdh in namespace deployment-4660 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-28 09:04:22 +0000 UTC 2023-06-28 09:04:22 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-tctdh-777898ffcc" is progressing.}
Jun 28 09:04:24.792: INFO: Observed &Deployment event: MODIFIED
Jun 28 09:04:24.793: INFO: Observed Deployment test-deployment-tctdh in namespace deployment-4660 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-06-28 09:04:23 +0000 UTC 2023-06-28 09:04:23 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jun 28 09:04:24.793: INFO: Observed Deployment test-deployment-tctdh in namespace deployment-4660 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-28 09:04:23 +0000 UTC 2023-06-28 09:04:22 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-tctdh-777898ffcc" has successfully progressed.}
Jun 28 09:04:24.793: INFO: Observed &Deployment event: MODIFIED
Jun 28 09:04:24.793: INFO: Observed Deployment test-deployment-tctdh in namespace deployment-4660 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-06-28 09:04:23 +0000 UTC 2023-06-28 09:04:23 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jun 28 09:04:24.793: INFO: Observed Deployment test-deployment-tctdh in namespace deployment-4660 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-28 09:04:23 +0000 UTC 2023-06-28 09:04:22 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-tctdh-777898ffcc" has successfully progressed.}
Jun 28 09:04:24.793: INFO: Found Deployment test-deployment-tctdh in namespace deployment-4660 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jun 28 09:04:24.793: INFO: Deployment test-deployment-tctdh has an updated status
STEP: patching the Statefulset Status 06/28/23 09:04:24.793
Jun 28 09:04:24.793: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jun 28 09:04:24.801: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched 06/28/23 09:04:24.801
Jun 28 09:04:24.804: INFO: Observed &Deployment event: ADDED
Jun 28 09:04:24.804: INFO: Observed deployment test-deployment-tctdh in namespace deployment-4660 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-28 09:04:22 +0000 UTC 2023-06-28 09:04:22 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-tctdh-777898ffcc"}
Jun 28 09:04:24.804: INFO: Observed &Deployment event: MODIFIED
Jun 28 09:04:24.805: INFO: Observed deployment test-deployment-tctdh in namespace deployment-4660 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-28 09:04:22 +0000 UTC 2023-06-28 09:04:22 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-tctdh-777898ffcc"}
Jun 28 09:04:24.805: INFO: Observed deployment test-deployment-tctdh in namespace deployment-4660 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-06-28 09:04:22 +0000 UTC 2023-06-28 09:04:22 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jun 28 09:04:24.805: INFO: Observed &Deployment event: MODIFIED
Jun 28 09:04:24.805: INFO: Observed deployment test-deployment-tctdh in namespace deployment-4660 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-06-28 09:04:22 +0000 UTC 2023-06-28 09:04:22 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jun 28 09:04:24.805: INFO: Observed deployment test-deployment-tctdh in namespace deployment-4660 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-28 09:04:22 +0000 UTC 2023-06-28 09:04:22 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-tctdh-777898ffcc" is progressing.}
Jun 28 09:04:24.805: INFO: Observed &Deployment event: MODIFIED
Jun 28 09:04:24.805: INFO: Observed deployment test-deployment-tctdh in namespace deployment-4660 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-06-28 09:04:23 +0000 UTC 2023-06-28 09:04:23 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jun 28 09:04:24.805: INFO: Observed deployment test-deployment-tctdh in namespace deployment-4660 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-28 09:04:23 +0000 UTC 2023-06-28 09:04:22 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-tctdh-777898ffcc" has successfully progressed.}
Jun 28 09:04:24.806: INFO: Observed &Deployment event: MODIFIED
Jun 28 09:04:24.806: INFO: Observed deployment test-deployment-tctdh in namespace deployment-4660 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-06-28 09:04:23 +0000 UTC 2023-06-28 09:04:23 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jun 28 09:04:24.806: INFO: Observed deployment test-deployment-tctdh in namespace deployment-4660 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-28 09:04:23 +0000 UTC 2023-06-28 09:04:22 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-tctdh-777898ffcc" has successfully progressed.}
Jun 28 09:04:24.806: INFO: Observed deployment test-deployment-tctdh in namespace deployment-4660 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jun 28 09:04:24.806: INFO: Observed &Deployment event: MODIFIED
Jun 28 09:04:24.806: INFO: Found deployment test-deployment-tctdh in namespace deployment-4660 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Jun 28 09:04:24.806: INFO: Deployment test-deployment-tctdh has a patched status
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jun 28 09:04:24.810: INFO: Deployment "test-deployment-tctdh":
&Deployment{ObjectMeta:{test-deployment-tctdh  deployment-4660  8aaf4cad-dc0c-4243-b18f-2bc7f73f6095 87602 1 2023-06-28 09:04:22 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-06-28 09:04:22 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-06-28 09:04:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-06-28 09:04:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004d40198 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-tctdh-777898ffcc",LastUpdateTime:2023-06-28 09:04:24 +0000 UTC,LastTransitionTime:2023-06-28 09:04:24 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jun 28 09:04:24.814: INFO: New ReplicaSet "test-deployment-tctdh-777898ffcc" of Deployment "test-deployment-tctdh":
&ReplicaSet{ObjectMeta:{test-deployment-tctdh-777898ffcc  deployment-4660  23f36c11-1a7b-4719-a747-571519e4854a 87597 1 2023-06-28 09:04:22 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-tctdh 8aaf4cad-dc0c-4243-b18f-2bc7f73f6095 0xc00308cae0 0xc00308cae1}] [] [{kube-controller-manager Update apps/v1 2023-06-28 09:04:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8aaf4cad-dc0c-4243-b18f-2bc7f73f6095\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-28 09:04:23 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 777898ffcc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00308cb88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jun 28 09:04:24.819: INFO: Pod "test-deployment-tctdh-777898ffcc-gwdv2" is available:
&Pod{ObjectMeta:{test-deployment-tctdh-777898ffcc-gwdv2 test-deployment-tctdh-777898ffcc- deployment-4660  32b181d0-527f-4d07-86bb-0759791b2ce2 87596 0 2023-06-28 09:04:22 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[cni.projectcalico.org/containerID:57e35782172f23c06bb65c15733d0cc4f326d7ae12a3e542d8e5ee134d9cb2c7 cni.projectcalico.org/podIP:172.21.122.42/32 cni.projectcalico.org/podIPs:172.21.122.42/32] [{apps/v1 ReplicaSet test-deployment-tctdh-777898ffcc 23f36c11-1a7b-4719-a747-571519e4854a 0xc004d40f10 0xc004d40f11}] [] [{kube-controller-manager Update v1 2023-06-28 09:04:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"23f36c11-1a7b-4719-a747-571519e4854a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-06-28 09:04:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-06-28 09:04:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.122.42\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-z7dwn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-z7dwn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-rhel-749f7d55c8xdd8b6-ct4cp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 09:04:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 09:04:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 09:04:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 09:04:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.11.3,PodIP:172.21.122.42,StartTime:2023-06-28 09:04:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-28 09:04:23 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://78db365aece064f738012421d72f95039bf693881e56eeba2af3fc34576f64a0,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.21.122.42,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jun 28 09:04:24.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4660" for this suite. 06/28/23 09:04:24.826
{"msg":"PASSED [sig-apps] Deployment should validate Deployment Status endpoints [Conformance]","completed":325,"skipped":5943,"failed":0}
------------------------------
â€¢ [2.123 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 09:04:22.709
    Jun 28 09:04:22.709: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename deployment 06/28/23 09:04:22.71
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 09:04:22.722
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 09:04:22.726
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should validate Deployment Status endpoints [Conformance]
      test/e2e/apps/deployment.go:479
    STEP: creating a Deployment 06/28/23 09:04:22.737
    Jun 28 09:04:22.737: INFO: Creating simple deployment test-deployment-tctdh
    Jun 28 09:04:22.753: INFO: new replicaset for deployment "test-deployment-tctdh" is yet to be created
    STEP: Getting /status 06/28/23 09:04:24.772
    Jun 28 09:04:24.777: INFO: Deployment test-deployment-tctdh has Conditions: [{Available True 2023-06-28 09:04:23 +0000 UTC 2023-06-28 09:04:23 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-06-28 09:04:23 +0000 UTC 2023-06-28 09:04:22 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-tctdh-777898ffcc" has successfully progressed.}]
    STEP: updating Deployment Status 06/28/23 09:04:24.777
    Jun 28 09:04:24.789: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.June, 28, 9, 4, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 28, 9, 4, 23, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 28, 9, 4, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 28, 9, 4, 22, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-tctdh-777898ffcc\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Deployment status to be updated 06/28/23 09:04:24.789
    Jun 28 09:04:24.792: INFO: Observed &Deployment event: ADDED
    Jun 28 09:04:24.792: INFO: Observed Deployment test-deployment-tctdh in namespace deployment-4660 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-28 09:04:22 +0000 UTC 2023-06-28 09:04:22 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-tctdh-777898ffcc"}
    Jun 28 09:04:24.792: INFO: Observed &Deployment event: MODIFIED
    Jun 28 09:04:24.792: INFO: Observed Deployment test-deployment-tctdh in namespace deployment-4660 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-28 09:04:22 +0000 UTC 2023-06-28 09:04:22 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-tctdh-777898ffcc"}
    Jun 28 09:04:24.792: INFO: Observed Deployment test-deployment-tctdh in namespace deployment-4660 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-06-28 09:04:22 +0000 UTC 2023-06-28 09:04:22 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jun 28 09:04:24.792: INFO: Observed &Deployment event: MODIFIED
    Jun 28 09:04:24.792: INFO: Observed Deployment test-deployment-tctdh in namespace deployment-4660 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-06-28 09:04:22 +0000 UTC 2023-06-28 09:04:22 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jun 28 09:04:24.792: INFO: Observed Deployment test-deployment-tctdh in namespace deployment-4660 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-28 09:04:22 +0000 UTC 2023-06-28 09:04:22 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-tctdh-777898ffcc" is progressing.}
    Jun 28 09:04:24.792: INFO: Observed &Deployment event: MODIFIED
    Jun 28 09:04:24.793: INFO: Observed Deployment test-deployment-tctdh in namespace deployment-4660 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-06-28 09:04:23 +0000 UTC 2023-06-28 09:04:23 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jun 28 09:04:24.793: INFO: Observed Deployment test-deployment-tctdh in namespace deployment-4660 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-28 09:04:23 +0000 UTC 2023-06-28 09:04:22 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-tctdh-777898ffcc" has successfully progressed.}
    Jun 28 09:04:24.793: INFO: Observed &Deployment event: MODIFIED
    Jun 28 09:04:24.793: INFO: Observed Deployment test-deployment-tctdh in namespace deployment-4660 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-06-28 09:04:23 +0000 UTC 2023-06-28 09:04:23 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jun 28 09:04:24.793: INFO: Observed Deployment test-deployment-tctdh in namespace deployment-4660 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-28 09:04:23 +0000 UTC 2023-06-28 09:04:22 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-tctdh-777898ffcc" has successfully progressed.}
    Jun 28 09:04:24.793: INFO: Found Deployment test-deployment-tctdh in namespace deployment-4660 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jun 28 09:04:24.793: INFO: Deployment test-deployment-tctdh has an updated status
    STEP: patching the Statefulset Status 06/28/23 09:04:24.793
    Jun 28 09:04:24.793: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Jun 28 09:04:24.801: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Deployment status to be patched 06/28/23 09:04:24.801
    Jun 28 09:04:24.804: INFO: Observed &Deployment event: ADDED
    Jun 28 09:04:24.804: INFO: Observed deployment test-deployment-tctdh in namespace deployment-4660 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-28 09:04:22 +0000 UTC 2023-06-28 09:04:22 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-tctdh-777898ffcc"}
    Jun 28 09:04:24.804: INFO: Observed &Deployment event: MODIFIED
    Jun 28 09:04:24.805: INFO: Observed deployment test-deployment-tctdh in namespace deployment-4660 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-28 09:04:22 +0000 UTC 2023-06-28 09:04:22 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-tctdh-777898ffcc"}
    Jun 28 09:04:24.805: INFO: Observed deployment test-deployment-tctdh in namespace deployment-4660 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-06-28 09:04:22 +0000 UTC 2023-06-28 09:04:22 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jun 28 09:04:24.805: INFO: Observed &Deployment event: MODIFIED
    Jun 28 09:04:24.805: INFO: Observed deployment test-deployment-tctdh in namespace deployment-4660 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-06-28 09:04:22 +0000 UTC 2023-06-28 09:04:22 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jun 28 09:04:24.805: INFO: Observed deployment test-deployment-tctdh in namespace deployment-4660 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-28 09:04:22 +0000 UTC 2023-06-28 09:04:22 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-tctdh-777898ffcc" is progressing.}
    Jun 28 09:04:24.805: INFO: Observed &Deployment event: MODIFIED
    Jun 28 09:04:24.805: INFO: Observed deployment test-deployment-tctdh in namespace deployment-4660 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-06-28 09:04:23 +0000 UTC 2023-06-28 09:04:23 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jun 28 09:04:24.805: INFO: Observed deployment test-deployment-tctdh in namespace deployment-4660 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-28 09:04:23 +0000 UTC 2023-06-28 09:04:22 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-tctdh-777898ffcc" has successfully progressed.}
    Jun 28 09:04:24.806: INFO: Observed &Deployment event: MODIFIED
    Jun 28 09:04:24.806: INFO: Observed deployment test-deployment-tctdh in namespace deployment-4660 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-06-28 09:04:23 +0000 UTC 2023-06-28 09:04:23 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jun 28 09:04:24.806: INFO: Observed deployment test-deployment-tctdh in namespace deployment-4660 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-28 09:04:23 +0000 UTC 2023-06-28 09:04:22 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-tctdh-777898ffcc" has successfully progressed.}
    Jun 28 09:04:24.806: INFO: Observed deployment test-deployment-tctdh in namespace deployment-4660 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jun 28 09:04:24.806: INFO: Observed &Deployment event: MODIFIED
    Jun 28 09:04:24.806: INFO: Found deployment test-deployment-tctdh in namespace deployment-4660 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
    Jun 28 09:04:24.806: INFO: Deployment test-deployment-tctdh has a patched status
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jun 28 09:04:24.810: INFO: Deployment "test-deployment-tctdh":
    &Deployment{ObjectMeta:{test-deployment-tctdh  deployment-4660  8aaf4cad-dc0c-4243-b18f-2bc7f73f6095 87602 1 2023-06-28 09:04:22 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-06-28 09:04:22 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-06-28 09:04:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-06-28 09:04:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004d40198 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-tctdh-777898ffcc",LastUpdateTime:2023-06-28 09:04:24 +0000 UTC,LastTransitionTime:2023-06-28 09:04:24 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jun 28 09:04:24.814: INFO: New ReplicaSet "test-deployment-tctdh-777898ffcc" of Deployment "test-deployment-tctdh":
    &ReplicaSet{ObjectMeta:{test-deployment-tctdh-777898ffcc  deployment-4660  23f36c11-1a7b-4719-a747-571519e4854a 87597 1 2023-06-28 09:04:22 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-tctdh 8aaf4cad-dc0c-4243-b18f-2bc7f73f6095 0xc00308cae0 0xc00308cae1}] [] [{kube-controller-manager Update apps/v1 2023-06-28 09:04:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8aaf4cad-dc0c-4243-b18f-2bc7f73f6095\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-28 09:04:23 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 777898ffcc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00308cb88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jun 28 09:04:24.819: INFO: Pod "test-deployment-tctdh-777898ffcc-gwdv2" is available:
    &Pod{ObjectMeta:{test-deployment-tctdh-777898ffcc-gwdv2 test-deployment-tctdh-777898ffcc- deployment-4660  32b181d0-527f-4d07-86bb-0759791b2ce2 87596 0 2023-06-28 09:04:22 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[cni.projectcalico.org/containerID:57e35782172f23c06bb65c15733d0cc4f326d7ae12a3e542d8e5ee134d9cb2c7 cni.projectcalico.org/podIP:172.21.122.42/32 cni.projectcalico.org/podIPs:172.21.122.42/32] [{apps/v1 ReplicaSet test-deployment-tctdh-777898ffcc 23f36c11-1a7b-4719-a747-571519e4854a 0xc004d40f10 0xc004d40f11}] [] [{kube-controller-manager Update v1 2023-06-28 09:04:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"23f36c11-1a7b-4719-a747-571519e4854a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-06-28 09:04:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-06-28 09:04:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.21.122.42\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-z7dwn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-z7dwn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-rhel-749f7d55c8xdd8b6-ct4cp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 09:04:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 09:04:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 09:04:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 09:04:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.11.3,PodIP:172.21.122.42,StartTime:2023-06-28 09:04:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-28 09:04:23 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://78db365aece064f738012421d72f95039bf693881e56eeba2af3fc34576f64a0,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.21.122.42,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jun 28 09:04:24.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-4660" for this suite. 06/28/23 09:04:24.826
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Pods
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 09:04:24.833
Jun 28 09:04:24.833: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename pods 06/28/23 09:04:24.833
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 09:04:24.846
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 09:04:24.853
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343
STEP: creating the pod 06/28/23 09:04:24.857
STEP: submitting the pod to kubernetes 06/28/23 09:04:24.858
Jun 28 09:04:24.867: INFO: Waiting up to 5m0s for pod "pod-update-d2620c58-bb83-42c4-b22f-d9347a180819" in namespace "pods-8088" to be "running and ready"
Jun 28 09:04:24.872: INFO: Pod "pod-update-d2620c58-bb83-42c4-b22f-d9347a180819": Phase="Pending", Reason="", readiness=false. Elapsed: 4.765247ms
Jun 28 09:04:24.872: INFO: The phase of Pod pod-update-d2620c58-bb83-42c4-b22f-d9347a180819 is Pending, waiting for it to be Running (with Ready = true)
Jun 28 09:04:26.877: INFO: Pod "pod-update-d2620c58-bb83-42c4-b22f-d9347a180819": Phase="Running", Reason="", readiness=true. Elapsed: 2.01011438s
Jun 28 09:04:26.877: INFO: The phase of Pod pod-update-d2620c58-bb83-42c4-b22f-d9347a180819 is Running (Ready = true)
Jun 28 09:04:26.877: INFO: Pod "pod-update-d2620c58-bb83-42c4-b22f-d9347a180819" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 06/28/23 09:04:26.882
STEP: updating the pod 06/28/23 09:04:26.886
Jun 28 09:04:27.403: INFO: Successfully updated pod "pod-update-d2620c58-bb83-42c4-b22f-d9347a180819"
Jun 28 09:04:27.403: INFO: Waiting up to 5m0s for pod "pod-update-d2620c58-bb83-42c4-b22f-d9347a180819" in namespace "pods-8088" to be "running"
Jun 28 09:04:27.408: INFO: Pod "pod-update-d2620c58-bb83-42c4-b22f-d9347a180819": Phase="Running", Reason="", readiness=true. Elapsed: 4.527581ms
Jun 28 09:04:27.408: INFO: Pod "pod-update-d2620c58-bb83-42c4-b22f-d9347a180819" satisfied condition "running"
STEP: verifying the updated pod is in kubernetes 06/28/23 09:04:27.408
Jun 28 09:04:27.412: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jun 28 09:04:27.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8088" for this suite. 06/28/23 09:04:27.42
{"msg":"PASSED [sig-node] Pods should be updated [NodeConformance] [Conformance]","completed":326,"skipped":5944,"failed":0}
------------------------------
â€¢ [2.594 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 09:04:24.833
    Jun 28 09:04:24.833: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename pods 06/28/23 09:04:24.833
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 09:04:24.846
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 09:04:24.853
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:343
    STEP: creating the pod 06/28/23 09:04:24.857
    STEP: submitting the pod to kubernetes 06/28/23 09:04:24.858
    Jun 28 09:04:24.867: INFO: Waiting up to 5m0s for pod "pod-update-d2620c58-bb83-42c4-b22f-d9347a180819" in namespace "pods-8088" to be "running and ready"
    Jun 28 09:04:24.872: INFO: Pod "pod-update-d2620c58-bb83-42c4-b22f-d9347a180819": Phase="Pending", Reason="", readiness=false. Elapsed: 4.765247ms
    Jun 28 09:04:24.872: INFO: The phase of Pod pod-update-d2620c58-bb83-42c4-b22f-d9347a180819 is Pending, waiting for it to be Running (with Ready = true)
    Jun 28 09:04:26.877: INFO: Pod "pod-update-d2620c58-bb83-42c4-b22f-d9347a180819": Phase="Running", Reason="", readiness=true. Elapsed: 2.01011438s
    Jun 28 09:04:26.877: INFO: The phase of Pod pod-update-d2620c58-bb83-42c4-b22f-d9347a180819 is Running (Ready = true)
    Jun 28 09:04:26.877: INFO: Pod "pod-update-d2620c58-bb83-42c4-b22f-d9347a180819" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 06/28/23 09:04:26.882
    STEP: updating the pod 06/28/23 09:04:26.886
    Jun 28 09:04:27.403: INFO: Successfully updated pod "pod-update-d2620c58-bb83-42c4-b22f-d9347a180819"
    Jun 28 09:04:27.403: INFO: Waiting up to 5m0s for pod "pod-update-d2620c58-bb83-42c4-b22f-d9347a180819" in namespace "pods-8088" to be "running"
    Jun 28 09:04:27.408: INFO: Pod "pod-update-d2620c58-bb83-42c4-b22f-d9347a180819": Phase="Running", Reason="", readiness=true. Elapsed: 4.527581ms
    Jun 28 09:04:27.408: INFO: Pod "pod-update-d2620c58-bb83-42c4-b22f-d9347a180819" satisfied condition "running"
    STEP: verifying the updated pod is in kubernetes 06/28/23 09:04:27.408
    Jun 28 09:04:27.412: INFO: Pod update OK
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jun 28 09:04:27.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-8088" for this suite. 06/28/23 09:04:27.42
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:906
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 09:04:27.427
Jun 28 09:04:27.427: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename statefulset 06/28/23 09:04:27.428
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 09:04:27.441
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 09:04:27.445
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-6101 06/28/23 09:04:27.45
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:906
Jun 28 09:04:27.466: INFO: Found 0 stateful pods, waiting for 1
Jun 28 09:04:37.472: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet 06/28/23 09:04:37.48
W0628 09:04:37.488725      18 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Jun 28 09:04:37.496: INFO: Found 1 stateful pods, waiting for 2
Jun 28 09:04:47.516: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 28 09:04:47.516: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets 06/28/23 09:04:47.53
STEP: Delete all of the StatefulSets 06/28/23 09:04:47.536
STEP: Verify that StatefulSets have been deleted 06/28/23 09:04:47.546
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jun 28 09:04:47.556: INFO: Deleting all statefulset in ns statefulset-6101
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jun 28 09:04:47.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6101" for this suite. 06/28/23 09:04:47.584
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]","completed":327,"skipped":5949,"failed":0}
------------------------------
â€¢ [SLOW TEST] [20.164 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should list, patch and delete a collection of StatefulSets [Conformance]
    test/e2e/apps/statefulset.go:906

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 09:04:27.427
    Jun 28 09:04:27.427: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename statefulset 06/28/23 09:04:27.428
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 09:04:27.441
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 09:04:27.445
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-6101 06/28/23 09:04:27.45
    [It] should list, patch and delete a collection of StatefulSets [Conformance]
      test/e2e/apps/statefulset.go:906
    Jun 28 09:04:27.466: INFO: Found 0 stateful pods, waiting for 1
    Jun 28 09:04:37.472: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: patching the StatefulSet 06/28/23 09:04:37.48
    W0628 09:04:37.488725      18 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Jun 28 09:04:37.496: INFO: Found 1 stateful pods, waiting for 2
    Jun 28 09:04:47.516: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Jun 28 09:04:47.516: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Listing all StatefulSets 06/28/23 09:04:47.53
    STEP: Delete all of the StatefulSets 06/28/23 09:04:47.536
    STEP: Verify that StatefulSets have been deleted 06/28/23 09:04:47.546
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jun 28 09:04:47.556: INFO: Deleting all statefulset in ns statefulset-6101
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jun 28 09:04:47.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-6101" for this suite. 06/28/23 09:04:47.584
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 09:04:47.591
Jun 28 09:04:47.591: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename namespaces 06/28/23 09:04:47.592
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 09:04:47.607
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 09:04:47.619
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242
STEP: Creating a test namespace 06/28/23 09:04:47.623
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 09:04:47.639
STEP: Creating a pod in the namespace 06/28/23 09:04:47.648
STEP: Waiting for the pod to have running status 06/28/23 09:04:47.669
Jun 28 09:04:47.669: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-9621" to be "running"
Jun 28 09:04:47.676: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.549021ms
Jun 28 09:04:49.682: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.013177936s
Jun 28 09:04:49.682: INFO: Pod "test-pod" satisfied condition "running"
STEP: Deleting the namespace 06/28/23 09:04:49.682
STEP: Waiting for the namespace to be removed. 06/28/23 09:04:49.688
STEP: Recreating the namespace 06/28/23 09:05:00.694
STEP: Verifying there are no pods in the namespace 06/28/23 09:05:00.708
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Jun 28 09:05:00.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-8045" for this suite. 06/28/23 09:05:00.723
STEP: Destroying namespace "nsdeletetest-9621" for this suite. 06/28/23 09:05:00.73
Jun 28 09:05:00.734: INFO: Namespace nsdeletetest-9621 was already deleted
STEP: Destroying namespace "nsdeletetest-9127" for this suite. 06/28/23 09:05:00.734
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","completed":328,"skipped":5957,"failed":0}
------------------------------
â€¢ [SLOW TEST] [13.149 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 09:04:47.591
    Jun 28 09:04:47.591: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename namespaces 06/28/23 09:04:47.592
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 09:04:47.607
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 09:04:47.619
    [It] should ensure that all pods are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:242
    STEP: Creating a test namespace 06/28/23 09:04:47.623
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 09:04:47.639
    STEP: Creating a pod in the namespace 06/28/23 09:04:47.648
    STEP: Waiting for the pod to have running status 06/28/23 09:04:47.669
    Jun 28 09:04:47.669: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-9621" to be "running"
    Jun 28 09:04:47.676: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.549021ms
    Jun 28 09:04:49.682: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.013177936s
    Jun 28 09:04:49.682: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Deleting the namespace 06/28/23 09:04:49.682
    STEP: Waiting for the namespace to be removed. 06/28/23 09:04:49.688
    STEP: Recreating the namespace 06/28/23 09:05:00.694
    STEP: Verifying there are no pods in the namespace 06/28/23 09:05:00.708
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Jun 28 09:05:00.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-8045" for this suite. 06/28/23 09:05:00.723
    STEP: Destroying namespace "nsdeletetest-9621" for this suite. 06/28/23 09:05:00.73
    Jun 28 09:05:00.734: INFO: Namespace nsdeletetest-9621 was already deleted
    STEP: Destroying namespace "nsdeletetest-9127" for this suite. 06/28/23 09:05:00.734
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 09:05:00.741
Jun 28 09:05:00.741: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename webhook 06/28/23 09:05:00.741
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 09:05:00.755
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 09:05:00.76
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 06/28/23 09:05:00.778
STEP: Create role binding to let webhook read extension-apiserver-authentication 06/28/23 09:05:01.136
STEP: Deploying the webhook pod 06/28/23 09:05:01.151
STEP: Wait for the deployment to be ready 06/28/23 09:05:01.167
Jun 28 09:05:01.178: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 06/28/23 09:05:03.194
STEP: Verifying the service has paired with the endpoint 06/28/23 09:05:03.205
Jun 28 09:05:04.206: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220
Jun 28 09:05:04.211: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Registering the custom resource webhook via the AdmissionRegistration API 06/28/23 09:05:04.725
STEP: Creating a custom resource that should be denied by the webhook 06/28/23 09:05:04.83
STEP: Creating a custom resource whose deletion would be denied by the webhook 06/28/23 09:05:06.952
STEP: Updating the custom resource with disallowed data should be denied 06/28/23 09:05:07.005
STEP: Deleting the custom resource should be denied 06/28/23 09:05:07.059
STEP: Remove the offending key and value from the custom resource data 06/28/23 09:05:07.111
STEP: Deleting the updated custom resource should be successful 06/28/23 09:05:07.167
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jun 28 09:05:07.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5608" for this suite. 06/28/23 09:05:07.748
STEP: Destroying namespace "webhook-5608-markers" for this suite. 06/28/23 09:05:07.755
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","completed":329,"skipped":5959,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.066 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 09:05:00.741
    Jun 28 09:05:00.741: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename webhook 06/28/23 09:05:00.741
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 09:05:00.755
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 09:05:00.76
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 06/28/23 09:05:00.778
    STEP: Create role binding to let webhook read extension-apiserver-authentication 06/28/23 09:05:01.136
    STEP: Deploying the webhook pod 06/28/23 09:05:01.151
    STEP: Wait for the deployment to be ready 06/28/23 09:05:01.167
    Jun 28 09:05:01.178: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 06/28/23 09:05:03.194
    STEP: Verifying the service has paired with the endpoint 06/28/23 09:05:03.205
    Jun 28 09:05:04.206: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny custom resource creation, update and deletion [Conformance]
      test/e2e/apimachinery/webhook.go:220
    Jun 28 09:05:04.211: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Registering the custom resource webhook via the AdmissionRegistration API 06/28/23 09:05:04.725
    STEP: Creating a custom resource that should be denied by the webhook 06/28/23 09:05:04.83
    STEP: Creating a custom resource whose deletion would be denied by the webhook 06/28/23 09:05:06.952
    STEP: Updating the custom resource with disallowed data should be denied 06/28/23 09:05:07.005
    STEP: Deleting the custom resource should be denied 06/28/23 09:05:07.059
    STEP: Remove the offending key and value from the custom resource data 06/28/23 09:05:07.111
    STEP: Deleting the updated custom resource should be successful 06/28/23 09:05:07.167
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jun 28 09:05:07.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-5608" for this suite. 06/28/23 09:05:07.748
    STEP: Destroying namespace "webhook-5608-markers" for this suite. 06/28/23 09:05:07.755
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 09:05:07.808
Jun 28 09:05:07.808: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename container-probe 06/28/23 09:05:07.809
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 09:05:07.825
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 09:05:07.83
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jun 28 09:06:07.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8495" for this suite. 06/28/23 09:06:07.861
{"msg":"PASSED [sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","completed":330,"skipped":5973,"failed":0}
------------------------------
â€¢ [SLOW TEST] [60.059 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 09:05:07.808
    Jun 28 09:05:07.808: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename container-probe 06/28/23 09:05:07.809
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 09:05:07.825
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 09:05:07.83
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:104
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jun 28 09:06:07.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-8495" for this suite. 06/28/23 09:06:07.861
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 09:06:07.869
Jun 28 09:06:07.869: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename var-expansion 06/28/23 09:06:07.87
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 09:06:07.883
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 09:06:07.888
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151
Jun 28 09:06:07.902: INFO: Waiting up to 2m0s for pod "var-expansion-f66ff0dd-fd20-4473-b86f-67cc4c264767" in namespace "var-expansion-8487" to be "container 0 failed with reason CreateContainerConfigError"
Jun 28 09:06:07.907: INFO: Pod "var-expansion-f66ff0dd-fd20-4473-b86f-67cc4c264767": Phase="Pending", Reason="", readiness=false. Elapsed: 4.49336ms
Jun 28 09:06:09.913: INFO: Pod "var-expansion-f66ff0dd-fd20-4473-b86f-67cc4c264767": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010875185s
Jun 28 09:06:09.913: INFO: Pod "var-expansion-f66ff0dd-fd20-4473-b86f-67cc4c264767" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Jun 28 09:06:09.913: INFO: Deleting pod "var-expansion-f66ff0dd-fd20-4473-b86f-67cc4c264767" in namespace "var-expansion-8487"
Jun 28 09:06:09.922: INFO: Wait up to 5m0s for pod "var-expansion-f66ff0dd-fd20-4473-b86f-67cc4c264767" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jun 28 09:06:13.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8487" for this suite. 06/28/23 09:06:13.95
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]","completed":331,"skipped":5996,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.088 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 09:06:07.869
    Jun 28 09:06:07.869: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename var-expansion 06/28/23 09:06:07.87
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 09:06:07.883
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 09:06:07.888
    [It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
      test/e2e/common/node/expansion.go:151
    Jun 28 09:06:07.902: INFO: Waiting up to 2m0s for pod "var-expansion-f66ff0dd-fd20-4473-b86f-67cc4c264767" in namespace "var-expansion-8487" to be "container 0 failed with reason CreateContainerConfigError"
    Jun 28 09:06:07.907: INFO: Pod "var-expansion-f66ff0dd-fd20-4473-b86f-67cc4c264767": Phase="Pending", Reason="", readiness=false. Elapsed: 4.49336ms
    Jun 28 09:06:09.913: INFO: Pod "var-expansion-f66ff0dd-fd20-4473-b86f-67cc4c264767": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010875185s
    Jun 28 09:06:09.913: INFO: Pod "var-expansion-f66ff0dd-fd20-4473-b86f-67cc4c264767" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Jun 28 09:06:09.913: INFO: Deleting pod "var-expansion-f66ff0dd-fd20-4473-b86f-67cc4c264767" in namespace "var-expansion-8487"
    Jun 28 09:06:09.922: INFO: Wait up to 5m0s for pod "var-expansion-f66ff0dd-fd20-4473-b86f-67cc4c264767" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jun 28 09:06:13.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-8487" for this suite. 06/28/23 09:06:13.95
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 09:06:13.957
Jun 28 09:06:13.957: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename configmap 06/28/23 09:06:13.958
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 09:06:13.972
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 09:06:13.976
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108
STEP: Creating configMap with name configmap-test-volume-map-93d17e8f-7cab-4d50-b2bc-93747d059e60 06/28/23 09:06:13.981
STEP: Creating a pod to test consume configMaps 06/28/23 09:06:13.998
Jun 28 09:06:14.007: INFO: Waiting up to 5m0s for pod "pod-configmaps-63f4e5c0-4509-4f89-83d8-7b902e87c6a5" in namespace "configmap-2621" to be "Succeeded or Failed"
Jun 28 09:06:14.011: INFO: Pod "pod-configmaps-63f4e5c0-4509-4f89-83d8-7b902e87c6a5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.140418ms
Jun 28 09:06:16.018: INFO: Pod "pod-configmaps-63f4e5c0-4509-4f89-83d8-7b902e87c6a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010397811s
Jun 28 09:06:18.018: INFO: Pod "pod-configmaps-63f4e5c0-4509-4f89-83d8-7b902e87c6a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010363316s
STEP: Saw pod success 06/28/23 09:06:18.018
Jun 28 09:06:18.018: INFO: Pod "pod-configmaps-63f4e5c0-4509-4f89-83d8-7b902e87c6a5" satisfied condition "Succeeded or Failed"
Jun 28 09:06:18.021: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod pod-configmaps-63f4e5c0-4509-4f89-83d8-7b902e87c6a5 container agnhost-container: <nil>
STEP: delete the pod 06/28/23 09:06:18.071
Jun 28 09:06:18.082: INFO: Waiting for pod pod-configmaps-63f4e5c0-4509-4f89-83d8-7b902e87c6a5 to disappear
Jun 28 09:06:18.085: INFO: Pod pod-configmaps-63f4e5c0-4509-4f89-83d8-7b902e87c6a5 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jun 28 09:06:18.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2621" for this suite. 06/28/23 09:06:18.095
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","completed":332,"skipped":5998,"failed":0}
------------------------------
â€¢ [4.145 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 09:06:13.957
    Jun 28 09:06:13.957: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename configmap 06/28/23 09:06:13.958
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 09:06:13.972
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 09:06:13.976
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:108
    STEP: Creating configMap with name configmap-test-volume-map-93d17e8f-7cab-4d50-b2bc-93747d059e60 06/28/23 09:06:13.981
    STEP: Creating a pod to test consume configMaps 06/28/23 09:06:13.998
    Jun 28 09:06:14.007: INFO: Waiting up to 5m0s for pod "pod-configmaps-63f4e5c0-4509-4f89-83d8-7b902e87c6a5" in namespace "configmap-2621" to be "Succeeded or Failed"
    Jun 28 09:06:14.011: INFO: Pod "pod-configmaps-63f4e5c0-4509-4f89-83d8-7b902e87c6a5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.140418ms
    Jun 28 09:06:16.018: INFO: Pod "pod-configmaps-63f4e5c0-4509-4f89-83d8-7b902e87c6a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010397811s
    Jun 28 09:06:18.018: INFO: Pod "pod-configmaps-63f4e5c0-4509-4f89-83d8-7b902e87c6a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010363316s
    STEP: Saw pod success 06/28/23 09:06:18.018
    Jun 28 09:06:18.018: INFO: Pod "pod-configmaps-63f4e5c0-4509-4f89-83d8-7b902e87c6a5" satisfied condition "Succeeded or Failed"
    Jun 28 09:06:18.021: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod pod-configmaps-63f4e5c0-4509-4f89-83d8-7b902e87c6a5 container agnhost-container: <nil>
    STEP: delete the pod 06/28/23 09:06:18.071
    Jun 28 09:06:18.082: INFO: Waiting for pod pod-configmaps-63f4e5c0-4509-4f89-83d8-7b902e87c6a5 to disappear
    Jun 28 09:06:18.085: INFO: Pod pod-configmaps-63f4e5c0-4509-4f89-83d8-7b902e87c6a5 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jun 28 09:06:18.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-2621" for this suite. 06/28/23 09:06:18.095
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 09:06:18.104
Jun 28 09:06:18.104: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename deployment 06/28/23 09:06:18.105
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 09:06:18.12
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 09:06:18.125
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
Jun 28 09:06:18.129: INFO: Creating deployment "test-recreate-deployment"
Jun 28 09:06:18.136: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Jun 28 09:06:18.147: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Jun 28 09:06:20.159: INFO: Waiting deployment "test-recreate-deployment" to complete
Jun 28 09:06:20.163: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Jun 28 09:06:20.174: INFO: Updating deployment test-recreate-deployment
Jun 28 09:06:20.174: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jun 28 09:06:20.235: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-4043  9544c811-dfe0-49e5-8ae3-2d4b10369827 88459 2 2023-06-28 09:06:18 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-06-28 09:06:20 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-28 09:06:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003bb4bd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-06-28 09:06:20 +0000 UTC,LastTransitionTime:2023-06-28 09:06:20 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-9d58999df" is progressing.,LastUpdateTime:2023-06-28 09:06:20 +0000 UTC,LastTransitionTime:2023-06-28 09:06:18 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Jun 28 09:06:20.239: INFO: New ReplicaSet "test-recreate-deployment-9d58999df" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-9d58999df  deployment-4043  b7145e20-b804-4819-8b54-a0fdfecbcf35 88457 1 2023-06-28 09:06:20 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 9544c811-dfe0-49e5-8ae3-2d4b10369827 0xc00365aca0 0xc00365aca1}] [] [{kube-controller-manager Update apps/v1 2023-06-28 09:06:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9544c811-dfe0-49e5-8ae3-2d4b10369827\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-28 09:06:20 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 9d58999df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00365ad38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun 28 09:06:20.239: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Jun 28 09:06:20.239: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-7d8b6f647f  deployment-4043  9e7d8495-b352-4a16-95c1-5f549e74b22b 88447 2 2023-06-28 09:06:18 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 9544c811-dfe0-49e5-8ae3-2d4b10369827 0xc00365ab87 0xc00365ab88}] [] [{kube-controller-manager Update apps/v1 2023-06-28 09:06:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9544c811-dfe0-49e5-8ae3-2d4b10369827\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-28 09:06:20 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d8b6f647f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00365ac38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun 28 09:06:20.243: INFO: Pod "test-recreate-deployment-9d58999df-btlkz" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-9d58999df-btlkz test-recreate-deployment-9d58999df- deployment-4043  666a4577-d673-452e-8e84-242bda95877c 88458 0 2023-06-28 09:06:20 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [{apps/v1 ReplicaSet test-recreate-deployment-9d58999df b7145e20-b804-4819-8b54-a0fdfecbcf35 0xc003bb5850 0xc003bb5851}] [] [{kube-controller-manager Update v1 2023-06-28 09:06:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b7145e20-b804-4819-8b54-a0fdfecbcf35\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-28 09:06:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d6jc7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d6jc7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-rhel-749f7d55c8xdd8b6-ct4cp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 09:06:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 09:06:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 09:06:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 09:06:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.11.3,PodIP:,StartTime:2023-06-28 09:06:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jun 28 09:06:20.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4043" for this suite. 06/28/23 09:06:20.251
{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","completed":333,"skipped":6042,"failed":0}
------------------------------
â€¢ [2.153 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 09:06:18.104
    Jun 28 09:06:18.104: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename deployment 06/28/23 09:06:18.105
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 09:06:18.12
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 09:06:18.125
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RecreateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:113
    Jun 28 09:06:18.129: INFO: Creating deployment "test-recreate-deployment"
    Jun 28 09:06:18.136: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
    Jun 28 09:06:18.147: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
    Jun 28 09:06:20.159: INFO: Waiting deployment "test-recreate-deployment" to complete
    Jun 28 09:06:20.163: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
    Jun 28 09:06:20.174: INFO: Updating deployment test-recreate-deployment
    Jun 28 09:06:20.174: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jun 28 09:06:20.235: INFO: Deployment "test-recreate-deployment":
    &Deployment{ObjectMeta:{test-recreate-deployment  deployment-4043  9544c811-dfe0-49e5-8ae3-2d4b10369827 88459 2 2023-06-28 09:06:18 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-06-28 09:06:20 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-28 09:06:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003bb4bd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-06-28 09:06:20 +0000 UTC,LastTransitionTime:2023-06-28 09:06:20 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-9d58999df" is progressing.,LastUpdateTime:2023-06-28 09:06:20 +0000 UTC,LastTransitionTime:2023-06-28 09:06:18 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

    Jun 28 09:06:20.239: INFO: New ReplicaSet "test-recreate-deployment-9d58999df" of Deployment "test-recreate-deployment":
    &ReplicaSet{ObjectMeta:{test-recreate-deployment-9d58999df  deployment-4043  b7145e20-b804-4819-8b54-a0fdfecbcf35 88457 1 2023-06-28 09:06:20 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 9544c811-dfe0-49e5-8ae3-2d4b10369827 0xc00365aca0 0xc00365aca1}] [] [{kube-controller-manager Update apps/v1 2023-06-28 09:06:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9544c811-dfe0-49e5-8ae3-2d4b10369827\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-28 09:06:20 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 9d58999df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00365ad38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jun 28 09:06:20.239: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
    Jun 28 09:06:20.239: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-7d8b6f647f  deployment-4043  9e7d8495-b352-4a16-95c1-5f549e74b22b 88447 2 2023-06-28 09:06:18 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 9544c811-dfe0-49e5-8ae3-2d4b10369827 0xc00365ab87 0xc00365ab88}] [] [{kube-controller-manager Update apps/v1 2023-06-28 09:06:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9544c811-dfe0-49e5-8ae3-2d4b10369827\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-28 09:06:20 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d8b6f647f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00365ac38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jun 28 09:06:20.243: INFO: Pod "test-recreate-deployment-9d58999df-btlkz" is not available:
    &Pod{ObjectMeta:{test-recreate-deployment-9d58999df-btlkz test-recreate-deployment-9d58999df- deployment-4043  666a4577-d673-452e-8e84-242bda95877c 88458 0 2023-06-28 09:06:20 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [{apps/v1 ReplicaSet test-recreate-deployment-9d58999df b7145e20-b804-4819-8b54-a0fdfecbcf35 0xc003bb5850 0xc003bb5851}] [] [{kube-controller-manager Update v1 2023-06-28 09:06:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b7145e20-b804-4819-8b54-a0fdfecbcf35\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-28 09:06:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d6jc7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d6jc7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ske-rhel-749f7d55c8xdd8b6-ct4cp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 09:06:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 09:06:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 09:06:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-28 09:06:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.11.3,PodIP:,StartTime:2023-06-28 09:06:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jun 28 09:06:20.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-4043" for this suite. 06/28/23 09:06:20.251
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Secrets
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 09:06:20.257
Jun 28 09:06:20.257: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename secrets 06/28/23 09:06:20.258
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 09:06:20.269
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 09:06:20.273
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124
STEP: Creating secret with name secret-test-bdd86da5-05af-4dba-8443-c00f9d6a296e 06/28/23 09:06:20.277
STEP: Creating a pod to test consume secrets 06/28/23 09:06:20.283
Jun 28 09:06:20.291: INFO: Waiting up to 5m0s for pod "pod-secrets-294a3081-77a7-4a3c-8495-e7a090108479" in namespace "secrets-9603" to be "Succeeded or Failed"
Jun 28 09:06:20.296: INFO: Pod "pod-secrets-294a3081-77a7-4a3c-8495-e7a090108479": Phase="Pending", Reason="", readiness=false. Elapsed: 4.519598ms
Jun 28 09:06:22.302: INFO: Pod "pod-secrets-294a3081-77a7-4a3c-8495-e7a090108479": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010594412s
Jun 28 09:06:24.301: INFO: Pod "pod-secrets-294a3081-77a7-4a3c-8495-e7a090108479": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009699446s
STEP: Saw pod success 06/28/23 09:06:24.301
Jun 28 09:06:24.301: INFO: Pod "pod-secrets-294a3081-77a7-4a3c-8495-e7a090108479" satisfied condition "Succeeded or Failed"
Jun 28 09:06:24.305: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-zxlfv pod pod-secrets-294a3081-77a7-4a3c-8495-e7a090108479 container secret-volume-test: <nil>
STEP: delete the pod 06/28/23 09:06:24.354
Jun 28 09:06:24.366: INFO: Waiting for pod pod-secrets-294a3081-77a7-4a3c-8495-e7a090108479 to disappear
Jun 28 09:06:24.369: INFO: Pod pod-secrets-294a3081-77a7-4a3c-8495-e7a090108479 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jun 28 09:06:24.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9603" for this suite. 06/28/23 09:06:24.376
{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","completed":334,"skipped":6046,"failed":0}
------------------------------
â€¢ [4.125 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 09:06:20.257
    Jun 28 09:06:20.257: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename secrets 06/28/23 09:06:20.258
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 09:06:20.269
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 09:06:20.273
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:124
    STEP: Creating secret with name secret-test-bdd86da5-05af-4dba-8443-c00f9d6a296e 06/28/23 09:06:20.277
    STEP: Creating a pod to test consume secrets 06/28/23 09:06:20.283
    Jun 28 09:06:20.291: INFO: Waiting up to 5m0s for pod "pod-secrets-294a3081-77a7-4a3c-8495-e7a090108479" in namespace "secrets-9603" to be "Succeeded or Failed"
    Jun 28 09:06:20.296: INFO: Pod "pod-secrets-294a3081-77a7-4a3c-8495-e7a090108479": Phase="Pending", Reason="", readiness=false. Elapsed: 4.519598ms
    Jun 28 09:06:22.302: INFO: Pod "pod-secrets-294a3081-77a7-4a3c-8495-e7a090108479": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010594412s
    Jun 28 09:06:24.301: INFO: Pod "pod-secrets-294a3081-77a7-4a3c-8495-e7a090108479": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009699446s
    STEP: Saw pod success 06/28/23 09:06:24.301
    Jun 28 09:06:24.301: INFO: Pod "pod-secrets-294a3081-77a7-4a3c-8495-e7a090108479" satisfied condition "Succeeded or Failed"
    Jun 28 09:06:24.305: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-zxlfv pod pod-secrets-294a3081-77a7-4a3c-8495-e7a090108479 container secret-volume-test: <nil>
    STEP: delete the pod 06/28/23 09:06:24.354
    Jun 28 09:06:24.366: INFO: Waiting for pod pod-secrets-294a3081-77a7-4a3c-8495-e7a090108479 to disappear
    Jun 28 09:06:24.369: INFO: Pod pod-secrets-294a3081-77a7-4a3c-8495-e7a090108479 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jun 28 09:06:24.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-9603" for this suite. 06/28/23 09:06:24.376
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 09:06:24.384
Jun 28 09:06:24.384: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename dns 06/28/23 09:06:24.385
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 09:06:24.396
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 09:06:24.4
[It] should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
STEP: Creating a test headless service 06/28/23 09:06:24.404
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-9700.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-9700.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
 06/28/23 09:06:24.41
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-9700.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-9700.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
 06/28/23 09:06:24.41
STEP: creating a pod to probe DNS 06/28/23 09:06:24.41
STEP: submitting the pod to kubernetes 06/28/23 09:06:24.41
Jun 28 09:06:24.419: INFO: Waiting up to 15m0s for pod "dns-test-c74cbf6b-bdda-499d-8d3e-570d12ffbbfd" in namespace "dns-9700" to be "running"
Jun 28 09:06:24.422: INFO: Pod "dns-test-c74cbf6b-bdda-499d-8d3e-570d12ffbbfd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.954446ms
Jun 28 09:06:26.428: INFO: Pod "dns-test-c74cbf6b-bdda-499d-8d3e-570d12ffbbfd": Phase="Running", Reason="", readiness=true. Elapsed: 2.009630083s
Jun 28 09:06:26.428: INFO: Pod "dns-test-c74cbf6b-bdda-499d-8d3e-570d12ffbbfd" satisfied condition "running"
STEP: retrieving the pod 06/28/23 09:06:26.428
STEP: looking for the results for each expected name from probers 06/28/23 09:06:26.432
Jun 28 09:06:26.582: INFO: DNS probes using dns-9700/dns-test-c74cbf6b-bdda-499d-8d3e-570d12ffbbfd succeeded

STEP: deleting the pod 06/28/23 09:06:26.583
STEP: deleting the test headless service 06/28/23 09:06:26.597
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jun 28 09:06:26.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9700" for this suite. 06/28/23 09:06:26.616
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [Conformance]","completed":335,"skipped":6124,"failed":0}
------------------------------
â€¢ [2.239 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 09:06:24.384
    Jun 28 09:06:24.384: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename dns 06/28/23 09:06:24.385
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 09:06:24.396
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 09:06:24.4
    [It] should provide DNS for pods for Hostname [Conformance]
      test/e2e/network/dns.go:248
    STEP: Creating a test headless service 06/28/23 09:06:24.404
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-9700.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-9700.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
     06/28/23 09:06:24.41
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-9700.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-9700.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
     06/28/23 09:06:24.41
    STEP: creating a pod to probe DNS 06/28/23 09:06:24.41
    STEP: submitting the pod to kubernetes 06/28/23 09:06:24.41
    Jun 28 09:06:24.419: INFO: Waiting up to 15m0s for pod "dns-test-c74cbf6b-bdda-499d-8d3e-570d12ffbbfd" in namespace "dns-9700" to be "running"
    Jun 28 09:06:24.422: INFO: Pod "dns-test-c74cbf6b-bdda-499d-8d3e-570d12ffbbfd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.954446ms
    Jun 28 09:06:26.428: INFO: Pod "dns-test-c74cbf6b-bdda-499d-8d3e-570d12ffbbfd": Phase="Running", Reason="", readiness=true. Elapsed: 2.009630083s
    Jun 28 09:06:26.428: INFO: Pod "dns-test-c74cbf6b-bdda-499d-8d3e-570d12ffbbfd" satisfied condition "running"
    STEP: retrieving the pod 06/28/23 09:06:26.428
    STEP: looking for the results for each expected name from probers 06/28/23 09:06:26.432
    Jun 28 09:06:26.582: INFO: DNS probes using dns-9700/dns-test-c74cbf6b-bdda-499d-8d3e-570d12ffbbfd succeeded

    STEP: deleting the pod 06/28/23 09:06:26.583
    STEP: deleting the test headless service 06/28/23 09:06:26.597
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jun 28 09:06:26.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-9700" for this suite. 06/28/23 09:06:26.616
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 09:06:26.623
Jun 28 09:06:26.623: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename projected 06/28/23 09:06:26.624
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 09:06:26.637
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 09:06:26.641
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118
STEP: Creating secret with name projected-secret-test-43c9e18e-4192-42bd-88c1-6fbebfced8e9 06/28/23 09:06:26.646
STEP: Creating a pod to test consume secrets 06/28/23 09:06:26.653
Jun 28 09:06:26.663: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-1af5a746-4aa1-4dc6-83e0-394c0c27c7d2" in namespace "projected-8757" to be "Succeeded or Failed"
Jun 28 09:06:26.668: INFO: Pod "pod-projected-secrets-1af5a746-4aa1-4dc6-83e0-394c0c27c7d2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.539946ms
Jun 28 09:06:28.673: INFO: Pod "pod-projected-secrets-1af5a746-4aa1-4dc6-83e0-394c0c27c7d2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009890184s
Jun 28 09:06:30.674: INFO: Pod "pod-projected-secrets-1af5a746-4aa1-4dc6-83e0-394c0c27c7d2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010500051s
STEP: Saw pod success 06/28/23 09:06:30.674
Jun 28 09:06:30.674: INFO: Pod "pod-projected-secrets-1af5a746-4aa1-4dc6-83e0-394c0c27c7d2" satisfied condition "Succeeded or Failed"
Jun 28 09:06:30.678: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod pod-projected-secrets-1af5a746-4aa1-4dc6-83e0-394c0c27c7d2 container secret-volume-test: <nil>
STEP: delete the pod 06/28/23 09:06:30.688
Jun 28 09:06:30.701: INFO: Waiting for pod pod-projected-secrets-1af5a746-4aa1-4dc6-83e0-394c0c27c7d2 to disappear
Jun 28 09:06:30.707: INFO: Pod pod-projected-secrets-1af5a746-4aa1-4dc6-83e0-394c0c27c7d2 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Jun 28 09:06:30.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8757" for this suite. 06/28/23 09:06:30.715
{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","completed":336,"skipped":6134,"failed":0}
------------------------------
â€¢ [4.099 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 09:06:26.623
    Jun 28 09:06:26.623: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename projected 06/28/23 09:06:26.624
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 09:06:26.637
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 09:06:26.641
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:118
    STEP: Creating secret with name projected-secret-test-43c9e18e-4192-42bd-88c1-6fbebfced8e9 06/28/23 09:06:26.646
    STEP: Creating a pod to test consume secrets 06/28/23 09:06:26.653
    Jun 28 09:06:26.663: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-1af5a746-4aa1-4dc6-83e0-394c0c27c7d2" in namespace "projected-8757" to be "Succeeded or Failed"
    Jun 28 09:06:26.668: INFO: Pod "pod-projected-secrets-1af5a746-4aa1-4dc6-83e0-394c0c27c7d2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.539946ms
    Jun 28 09:06:28.673: INFO: Pod "pod-projected-secrets-1af5a746-4aa1-4dc6-83e0-394c0c27c7d2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009890184s
    Jun 28 09:06:30.674: INFO: Pod "pod-projected-secrets-1af5a746-4aa1-4dc6-83e0-394c0c27c7d2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010500051s
    STEP: Saw pod success 06/28/23 09:06:30.674
    Jun 28 09:06:30.674: INFO: Pod "pod-projected-secrets-1af5a746-4aa1-4dc6-83e0-394c0c27c7d2" satisfied condition "Succeeded or Failed"
    Jun 28 09:06:30.678: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod pod-projected-secrets-1af5a746-4aa1-4dc6-83e0-394c0c27c7d2 container secret-volume-test: <nil>
    STEP: delete the pod 06/28/23 09:06:30.688
    Jun 28 09:06:30.701: INFO: Waiting for pod pod-projected-secrets-1af5a746-4aa1-4dc6-83e0-394c0c27c7d2 to disappear
    Jun 28 09:06:30.707: INFO: Pod pod-projected-secrets-1af5a746-4aa1-4dc6-83e0-394c0c27c7d2 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Jun 28 09:06:30.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-8757" for this suite. 06/28/23 09:06:30.715
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] RuntimeClass
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 09:06:30.722
Jun 28 09:06:30.722: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename runtimeclass 06/28/23 09:06:30.723
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 09:06:30.735
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 09:06:30.739
[It]  should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
STEP: getting /apis 06/28/23 09:06:30.744
STEP: getting /apis/node.k8s.io 06/28/23 09:06:30.748
STEP: getting /apis/node.k8s.io/v1 06/28/23 09:06:30.75
STEP: creating 06/28/23 09:06:30.752
STEP: watching 06/28/23 09:06:30.771
Jun 28 09:06:30.771: INFO: starting watch
STEP: getting 06/28/23 09:06:30.779
STEP: listing 06/28/23 09:06:30.783
STEP: patching 06/28/23 09:06:30.787
STEP: updating 06/28/23 09:06:30.794
Jun 28 09:06:30.799: INFO: waiting for watch events with expected annotations
STEP: deleting 06/28/23 09:06:30.8
STEP: deleting a collection 06/28/23 09:06:30.815
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Jun 28 09:06:30.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-4301" for this suite. 06/28/23 09:06:30.84
{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","completed":337,"skipped":6139,"failed":0}
------------------------------
â€¢ [0.125 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 09:06:30.722
    Jun 28 09:06:30.722: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename runtimeclass 06/28/23 09:06:30.723
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 09:06:30.735
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 09:06:30.739
    [It]  should support RuntimeClasses API operations [Conformance]
      test/e2e/common/node/runtimeclass.go:189
    STEP: getting /apis 06/28/23 09:06:30.744
    STEP: getting /apis/node.k8s.io 06/28/23 09:06:30.748
    STEP: getting /apis/node.k8s.io/v1 06/28/23 09:06:30.75
    STEP: creating 06/28/23 09:06:30.752
    STEP: watching 06/28/23 09:06:30.771
    Jun 28 09:06:30.771: INFO: starting watch
    STEP: getting 06/28/23 09:06:30.779
    STEP: listing 06/28/23 09:06:30.783
    STEP: patching 06/28/23 09:06:30.787
    STEP: updating 06/28/23 09:06:30.794
    Jun 28 09:06:30.799: INFO: waiting for watch events with expected annotations
    STEP: deleting 06/28/23 09:06:30.8
    STEP: deleting a collection 06/28/23 09:06:30.815
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Jun 28 09:06:30.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-4301" for this suite. 06/28/23 09:06:30.84
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 09:06:30.848
Jun 28 09:06:30.848: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename webhook 06/28/23 09:06:30.849
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 09:06:30.862
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 09:06:30.868
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 06/28/23 09:06:30.887
STEP: Create role binding to let webhook read extension-apiserver-authentication 06/28/23 09:06:31.12
STEP: Deploying the webhook pod 06/28/23 09:06:31.128
STEP: Wait for the deployment to be ready 06/28/23 09:06:31.14
Jun 28 09:06:31.149: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 06/28/23 09:06:33.162
STEP: Verifying the service has paired with the endpoint 06/28/23 09:06:33.172
Jun 28 09:06:34.172: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507
STEP: Creating a mutating webhook configuration 06/28/23 09:06:34.178
STEP: Updating a mutating webhook configuration's rules to not include the create operation 06/28/23 09:06:34.288
STEP: Creating a configMap that should not be mutated 06/28/23 09:06:34.296
STEP: Patching a mutating webhook configuration's rules to include the create operation 06/28/23 09:06:34.311
STEP: Creating a configMap that should be mutated 06/28/23 09:06:34.319
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jun 28 09:06:34.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3752" for this suite. 06/28/23 09:06:34.486
STEP: Destroying namespace "webhook-3752-markers" for this suite. 06/28/23 09:06:34.493
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","completed":338,"skipped":6164,"failed":0}
------------------------------
â€¢ [3.693 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 09:06:30.848
    Jun 28 09:06:30.848: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename webhook 06/28/23 09:06:30.849
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 09:06:30.862
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 09:06:30.868
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 06/28/23 09:06:30.887
    STEP: Create role binding to let webhook read extension-apiserver-authentication 06/28/23 09:06:31.12
    STEP: Deploying the webhook pod 06/28/23 09:06:31.128
    STEP: Wait for the deployment to be ready 06/28/23 09:06:31.14
    Jun 28 09:06:31.149: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 06/28/23 09:06:33.162
    STEP: Verifying the service has paired with the endpoint 06/28/23 09:06:33.172
    Jun 28 09:06:34.172: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a mutating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:507
    STEP: Creating a mutating webhook configuration 06/28/23 09:06:34.178
    STEP: Updating a mutating webhook configuration's rules to not include the create operation 06/28/23 09:06:34.288
    STEP: Creating a configMap that should not be mutated 06/28/23 09:06:34.296
    STEP: Patching a mutating webhook configuration's rules to include the create operation 06/28/23 09:06:34.311
    STEP: Creating a configMap that should be mutated 06/28/23 09:06:34.319
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jun 28 09:06:34.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-3752" for this suite. 06/28/23 09:06:34.486
    STEP: Destroying namespace "webhook-3752-markers" for this suite. 06/28/23 09:06:34.493
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 09:06:34.542
Jun 28 09:06:34.542: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename downward-api 06/28/23 09:06:34.542
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 09:06:34.561
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 09:06:34.565
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67
STEP: Creating a pod to test downward API volume plugin 06/28/23 09:06:34.571
Jun 28 09:06:34.579: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1ce70289-1110-4da7-a3e6-7df862d2d975" in namespace "downward-api-8154" to be "Succeeded or Failed"
Jun 28 09:06:34.584: INFO: Pod "downwardapi-volume-1ce70289-1110-4da7-a3e6-7df862d2d975": Phase="Pending", Reason="", readiness=false. Elapsed: 4.288068ms
Jun 28 09:06:36.591: INFO: Pod "downwardapi-volume-1ce70289-1110-4da7-a3e6-7df862d2d975": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01164127s
Jun 28 09:06:38.591: INFO: Pod "downwardapi-volume-1ce70289-1110-4da7-a3e6-7df862d2d975": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011471736s
STEP: Saw pod success 06/28/23 09:06:38.591
Jun 28 09:06:38.591: INFO: Pod "downwardapi-volume-1ce70289-1110-4da7-a3e6-7df862d2d975" satisfied condition "Succeeded or Failed"
Jun 28 09:06:38.595: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod downwardapi-volume-1ce70289-1110-4da7-a3e6-7df862d2d975 container client-container: <nil>
STEP: delete the pod 06/28/23 09:06:38.606
Jun 28 09:06:38.619: INFO: Waiting for pod downwardapi-volume-1ce70289-1110-4da7-a3e6-7df862d2d975 to disappear
Jun 28 09:06:38.623: INFO: Pod downwardapi-volume-1ce70289-1110-4da7-a3e6-7df862d2d975 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jun 28 09:06:38.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8154" for this suite. 06/28/23 09:06:38.631
{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","completed":339,"skipped":6204,"failed":0}
------------------------------
â€¢ [4.097 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 09:06:34.542
    Jun 28 09:06:34.542: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename downward-api 06/28/23 09:06:34.542
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 09:06:34.561
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 09:06:34.565
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:67
    STEP: Creating a pod to test downward API volume plugin 06/28/23 09:06:34.571
    Jun 28 09:06:34.579: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1ce70289-1110-4da7-a3e6-7df862d2d975" in namespace "downward-api-8154" to be "Succeeded or Failed"
    Jun 28 09:06:34.584: INFO: Pod "downwardapi-volume-1ce70289-1110-4da7-a3e6-7df862d2d975": Phase="Pending", Reason="", readiness=false. Elapsed: 4.288068ms
    Jun 28 09:06:36.591: INFO: Pod "downwardapi-volume-1ce70289-1110-4da7-a3e6-7df862d2d975": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01164127s
    Jun 28 09:06:38.591: INFO: Pod "downwardapi-volume-1ce70289-1110-4da7-a3e6-7df862d2d975": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011471736s
    STEP: Saw pod success 06/28/23 09:06:38.591
    Jun 28 09:06:38.591: INFO: Pod "downwardapi-volume-1ce70289-1110-4da7-a3e6-7df862d2d975" satisfied condition "Succeeded or Failed"
    Jun 28 09:06:38.595: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod downwardapi-volume-1ce70289-1110-4da7-a3e6-7df862d2d975 container client-container: <nil>
    STEP: delete the pod 06/28/23 09:06:38.606
    Jun 28 09:06:38.619: INFO: Waiting for pod downwardapi-volume-1ce70289-1110-4da7-a3e6-7df862d2d975 to disappear
    Jun 28 09:06:38.623: INFO: Pod downwardapi-volume-1ce70289-1110-4da7-a3e6-7df862d2d975 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jun 28 09:06:38.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-8154" for this suite. 06/28/23 09:06:38.631
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 09:06:38.639
Jun 28 09:06:38.639: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename downward-api 06/28/23 09:06:38.639
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 09:06:38.652
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 09:06:38.657
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83
STEP: Creating a pod to test downward API volume plugin 06/28/23 09:06:38.661
Jun 28 09:06:38.672: INFO: Waiting up to 5m0s for pod "downwardapi-volume-01852e2b-bf49-4982-b47f-8237073c31da" in namespace "downward-api-3955" to be "Succeeded or Failed"
Jun 28 09:06:38.677: INFO: Pod "downwardapi-volume-01852e2b-bf49-4982-b47f-8237073c31da": Phase="Pending", Reason="", readiness=false. Elapsed: 5.608601ms
Jun 28 09:06:40.686: INFO: Pod "downwardapi-volume-01852e2b-bf49-4982-b47f-8237073c31da": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013956009s
Jun 28 09:06:42.683: INFO: Pod "downwardapi-volume-01852e2b-bf49-4982-b47f-8237073c31da": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011667973s
STEP: Saw pod success 06/28/23 09:06:42.683
Jun 28 09:06:42.683: INFO: Pod "downwardapi-volume-01852e2b-bf49-4982-b47f-8237073c31da" satisfied condition "Succeeded or Failed"
Jun 28 09:06:42.688: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod downwardapi-volume-01852e2b-bf49-4982-b47f-8237073c31da container client-container: <nil>
STEP: delete the pod 06/28/23 09:06:42.738
Jun 28 09:06:42.749: INFO: Waiting for pod downwardapi-volume-01852e2b-bf49-4982-b47f-8237073c31da to disappear
Jun 28 09:06:42.753: INFO: Pod downwardapi-volume-01852e2b-bf49-4982-b47f-8237073c31da no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jun 28 09:06:42.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3955" for this suite. 06/28/23 09:06:42.761
{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","completed":340,"skipped":6214,"failed":0}
------------------------------
â€¢ [4.130 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 09:06:38.639
    Jun 28 09:06:38.639: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename downward-api 06/28/23 09:06:38.639
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 09:06:38.652
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 09:06:38.657
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:83
    STEP: Creating a pod to test downward API volume plugin 06/28/23 09:06:38.661
    Jun 28 09:06:38.672: INFO: Waiting up to 5m0s for pod "downwardapi-volume-01852e2b-bf49-4982-b47f-8237073c31da" in namespace "downward-api-3955" to be "Succeeded or Failed"
    Jun 28 09:06:38.677: INFO: Pod "downwardapi-volume-01852e2b-bf49-4982-b47f-8237073c31da": Phase="Pending", Reason="", readiness=false. Elapsed: 5.608601ms
    Jun 28 09:06:40.686: INFO: Pod "downwardapi-volume-01852e2b-bf49-4982-b47f-8237073c31da": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013956009s
    Jun 28 09:06:42.683: INFO: Pod "downwardapi-volume-01852e2b-bf49-4982-b47f-8237073c31da": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011667973s
    STEP: Saw pod success 06/28/23 09:06:42.683
    Jun 28 09:06:42.683: INFO: Pod "downwardapi-volume-01852e2b-bf49-4982-b47f-8237073c31da" satisfied condition "Succeeded or Failed"
    Jun 28 09:06:42.688: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod downwardapi-volume-01852e2b-bf49-4982-b47f-8237073c31da container client-container: <nil>
    STEP: delete the pod 06/28/23 09:06:42.738
    Jun 28 09:06:42.749: INFO: Waiting for pod downwardapi-volume-01852e2b-bf49-4982-b47f-8237073c31da to disappear
    Jun 28 09:06:42.753: INFO: Pod downwardapi-volume-01852e2b-bf49-4982-b47f-8237073c31da no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jun 28 09:06:42.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-3955" for this suite. 06/28/23 09:06:42.761
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:737
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 09:06:42.769
Jun 28 09:06:42.769: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename statefulset 06/28/23 09:06:42.77
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 09:06:42.783
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 09:06:42.788
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-2650 06/28/23 09:06:42.793
[It] Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:737
STEP: Looking for a node to schedule stateful set and pod 06/28/23 09:06:42.798
STEP: Creating pod with conflicting port in namespace statefulset-2650 06/28/23 09:06:42.806
STEP: Waiting until pod test-pod will start running in namespace statefulset-2650 06/28/23 09:06:42.815
Jun 28 09:06:42.815: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-2650" to be "running"
Jun 28 09:06:42.819: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.324374ms
Jun 28 09:06:44.824: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009526425s
Jun 28 09:06:44.824: INFO: Pod "test-pod" satisfied condition "running"
STEP: Creating statefulset with conflicting port in namespace statefulset-2650 06/28/23 09:06:44.824
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-2650 06/28/23 09:06:44.831
Jun 28 09:06:44.843: INFO: Observed stateful pod in namespace: statefulset-2650, name: ss-0, uid: fb0b4fce-56df-4a46-9d1b-e301d2670c29, status phase: Pending. Waiting for statefulset controller to delete.
Jun 28 09:06:44.856: INFO: Observed stateful pod in namespace: statefulset-2650, name: ss-0, uid: fb0b4fce-56df-4a46-9d1b-e301d2670c29, status phase: Failed. Waiting for statefulset controller to delete.
Jun 28 09:06:44.864: INFO: Observed stateful pod in namespace: statefulset-2650, name: ss-0, uid: fb0b4fce-56df-4a46-9d1b-e301d2670c29, status phase: Failed. Waiting for statefulset controller to delete.
Jun 28 09:06:44.866: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-2650
STEP: Removing pod with conflicting port in namespace statefulset-2650 06/28/23 09:06:44.866
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-2650 and will be in running state 06/28/23 09:06:44.88
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jun 28 09:06:46.892: INFO: Deleting all statefulset in ns statefulset-2650
Jun 28 09:06:46.896: INFO: Scaling statefulset ss to 0
Jun 28 09:06:56.917: INFO: Waiting for statefulset status.replicas updated to 0
Jun 28 09:06:56.922: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jun 28 09:06:56.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2650" for this suite. 06/28/23 09:06:56.945
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","completed":341,"skipped":6215,"failed":0}
------------------------------
â€¢ [SLOW TEST] [14.184 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Should recreate evicted statefulset [Conformance]
    test/e2e/apps/statefulset.go:737

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 09:06:42.769
    Jun 28 09:06:42.769: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename statefulset 06/28/23 09:06:42.77
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 09:06:42.783
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 09:06:42.788
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-2650 06/28/23 09:06:42.793
    [It] Should recreate evicted statefulset [Conformance]
      test/e2e/apps/statefulset.go:737
    STEP: Looking for a node to schedule stateful set and pod 06/28/23 09:06:42.798
    STEP: Creating pod with conflicting port in namespace statefulset-2650 06/28/23 09:06:42.806
    STEP: Waiting until pod test-pod will start running in namespace statefulset-2650 06/28/23 09:06:42.815
    Jun 28 09:06:42.815: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-2650" to be "running"
    Jun 28 09:06:42.819: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.324374ms
    Jun 28 09:06:44.824: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009526425s
    Jun 28 09:06:44.824: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Creating statefulset with conflicting port in namespace statefulset-2650 06/28/23 09:06:44.824
    STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-2650 06/28/23 09:06:44.831
    Jun 28 09:06:44.843: INFO: Observed stateful pod in namespace: statefulset-2650, name: ss-0, uid: fb0b4fce-56df-4a46-9d1b-e301d2670c29, status phase: Pending. Waiting for statefulset controller to delete.
    Jun 28 09:06:44.856: INFO: Observed stateful pod in namespace: statefulset-2650, name: ss-0, uid: fb0b4fce-56df-4a46-9d1b-e301d2670c29, status phase: Failed. Waiting for statefulset controller to delete.
    Jun 28 09:06:44.864: INFO: Observed stateful pod in namespace: statefulset-2650, name: ss-0, uid: fb0b4fce-56df-4a46-9d1b-e301d2670c29, status phase: Failed. Waiting for statefulset controller to delete.
    Jun 28 09:06:44.866: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-2650
    STEP: Removing pod with conflicting port in namespace statefulset-2650 06/28/23 09:06:44.866
    STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-2650 and will be in running state 06/28/23 09:06:44.88
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jun 28 09:06:46.892: INFO: Deleting all statefulset in ns statefulset-2650
    Jun 28 09:06:46.896: INFO: Scaling statefulset ss to 0
    Jun 28 09:06:56.917: INFO: Waiting for statefulset status.replicas updated to 0
    Jun 28 09:06:56.922: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jun 28 09:06:56.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-2650" for this suite. 06/28/23 09:06:56.945
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Pods Extended Pods Set QOS Class
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
[BeforeEach] [sig-node] Pods Extended
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 09:06:56.953
Jun 28 09:06:56.953: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename pods 06/28/23 09:06:56.954
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 09:06:56.968
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 09:06:56.973
[BeforeEach] Pods Set QOS Class
  test/e2e/node/pods.go:152
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
STEP: creating the pod 06/28/23 09:06:56.977
STEP: submitting the pod to kubernetes 06/28/23 09:06:56.977
STEP: verifying QOS class is set on the pod 06/28/23 09:06:56.985
[AfterEach] [sig-node] Pods Extended
  test/e2e/framework/framework.go:187
Jun 28 09:06:56.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4527" for this suite. 06/28/23 09:06:56.996
{"msg":"PASSED [sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","completed":342,"skipped":6216,"failed":0}
------------------------------
â€¢ [0.049 seconds]
[sig-node] Pods Extended
test/e2e/node/framework.go:23
  Pods Set QOS Class
  test/e2e/node/pods.go:150
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    test/e2e/node/pods.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods Extended
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 09:06:56.953
    Jun 28 09:06:56.953: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename pods 06/28/23 09:06:56.954
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 09:06:56.968
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 09:06:56.973
    [BeforeEach] Pods Set QOS Class
      test/e2e/node/pods.go:152
    [It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
      test/e2e/node/pods.go:161
    STEP: creating the pod 06/28/23 09:06:56.977
    STEP: submitting the pod to kubernetes 06/28/23 09:06:56.977
    STEP: verifying QOS class is set on the pod 06/28/23 09:06:56.985
    [AfterEach] [sig-node] Pods Extended
      test/e2e/framework/framework.go:187
    Jun 28 09:06:56.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-4527" for this suite. 06/28/23 09:06:56.996
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 09:06:57.002
Jun 28 09:06:57.002: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename init-container 06/28/23 09:06:57.003
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 09:06:57.016
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 09:06:57.02
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333
STEP: creating the pod 06/28/23 09:06:57.025
Jun 28 09:06:57.025: INFO: PodSpec: initContainers in spec.initContainers
Jun 28 09:07:40.404: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-c3135e76-3900-44ce-a75a-d798bfb959c4", GenerateName:"", Namespace:"init-container-8366", SelfLink:"", UID:"2dc6fe1b-7b53-4545-9ef6-a18e8178b549", ResourceVersion:"89177", Generation:0, CreationTimestamp:time.Date(2023, time.June, 28, 9, 6, 57, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"25187381"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"2e69bd0803ef18a5eb5af8898e9ecbec2ea74b72a9720bed7d069ca19439583d", "cni.projectcalico.org/podIP":"172.21.122.37/32", "cni.projectcalico.org/podIPs":"172.21.122.37/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.June, 28, 9, 6, 57, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0009e4600), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.June, 28, 9, 6, 57, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0009e4630), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.June, 28, 9, 7, 40, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0009e4660), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-985hd", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc000b792e0), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-985hd", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-985hd", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.8", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-985hd", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0014e6898), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ske-rhel-749f7d55c8xdd8b6-ct4cp", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc003947c70), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0014e6910)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0014e6930)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0014e6938), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0014e693c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc004a5a160), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.June, 28, 9, 6, 57, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.June, 28, 9, 6, 57, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.June, 28, 9, 6, 57, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.June, 28, 9, 6, 57, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.11.3", PodIP:"172.21.122.37", PodIPs:[]v1.PodIP{v1.PodIP{IP:"172.21.122.37"}}, StartTime:time.Date(2023, time.June, 28, 9, 6, 57, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc003947d50)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc003947dc0)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:c318242786b139d18676b1c09a0ad7f15fc17f8f16a5b2e625cd0dc8c9703daf", ContainerID:"containerd://74985bfce8f94b114dca2ff5219edd8d8ce80f5c44e1cdfc0c1c7d941a74729e", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc000b79480), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc000b793a0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.8", ImageID:"", ContainerID:"", Started:(*bool)(0xc0014e69cf)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Jun 28 09:07:40.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-8366" for this suite. 06/28/23 09:07:40.412
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","completed":343,"skipped":6220,"failed":0}
------------------------------
â€¢ [SLOW TEST] [43.416 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 09:06:57.002
    Jun 28 09:06:57.002: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename init-container 06/28/23 09:06:57.003
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 09:06:57.016
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 09:06:57.02
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:333
    STEP: creating the pod 06/28/23 09:06:57.025
    Jun 28 09:06:57.025: INFO: PodSpec: initContainers in spec.initContainers
    Jun 28 09:07:40.404: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-c3135e76-3900-44ce-a75a-d798bfb959c4", GenerateName:"", Namespace:"init-container-8366", SelfLink:"", UID:"2dc6fe1b-7b53-4545-9ef6-a18e8178b549", ResourceVersion:"89177", Generation:0, CreationTimestamp:time.Date(2023, time.June, 28, 9, 6, 57, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"25187381"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"2e69bd0803ef18a5eb5af8898e9ecbec2ea74b72a9720bed7d069ca19439583d", "cni.projectcalico.org/podIP":"172.21.122.37/32", "cni.projectcalico.org/podIPs":"172.21.122.37/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.June, 28, 9, 6, 57, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0009e4600), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.June, 28, 9, 6, 57, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0009e4630), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.June, 28, 9, 7, 40, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0009e4660), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-985hd", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc000b792e0), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-985hd", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-985hd", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.8", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-985hd", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0014e6898), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ske-rhel-749f7d55c8xdd8b6-ct4cp", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc003947c70), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0014e6910)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0014e6930)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0014e6938), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0014e693c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc004a5a160), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.June, 28, 9, 6, 57, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.June, 28, 9, 6, 57, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.June, 28, 9, 6, 57, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.June, 28, 9, 6, 57, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.11.3", PodIP:"172.21.122.37", PodIPs:[]v1.PodIP{v1.PodIP{IP:"172.21.122.37"}}, StartTime:time.Date(2023, time.June, 28, 9, 6, 57, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc003947d50)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc003947dc0)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:c318242786b139d18676b1c09a0ad7f15fc17f8f16a5b2e625cd0dc8c9703daf", ContainerID:"containerd://74985bfce8f94b114dca2ff5219edd8d8ce80f5c44e1cdfc0c1c7d941a74729e", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc000b79480), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc000b793a0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.8", ImageID:"", ContainerID:"", Started:(*bool)(0xc0014e69cf)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Jun 28 09:07:40.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-8366" for this suite. 06/28/23 09:07:40.412
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] ReplicaSet
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 09:07:40.418
Jun 28 09:07:40.418: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename replicaset 06/28/23 09:07:40.419
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 09:07:40.432
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 09:07:40.436
[It] Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
Jun 28 09:07:40.452: INFO: Pod name sample-pod: Found 0 pods out of 1
Jun 28 09:07:45.457: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 06/28/23 09:07:45.457
STEP: Scaling up "test-rs" replicaset  06/28/23 09:07:45.457
Jun 28 09:07:45.469: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet 06/28/23 09:07:45.469
W0628 09:07:45.476172      18 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Jun 28 09:07:45.478: INFO: observed ReplicaSet test-rs in namespace replicaset-5639 with ReadyReplicas 1, AvailableReplicas 1
Jun 28 09:07:45.487: INFO: observed ReplicaSet test-rs in namespace replicaset-5639 with ReadyReplicas 1, AvailableReplicas 1
Jun 28 09:07:45.497: INFO: observed ReplicaSet test-rs in namespace replicaset-5639 with ReadyReplicas 1, AvailableReplicas 1
Jun 28 09:07:45.503: INFO: observed ReplicaSet test-rs in namespace replicaset-5639 with ReadyReplicas 1, AvailableReplicas 1
Jun 28 09:07:46.445: INFO: observed ReplicaSet test-rs in namespace replicaset-5639 with ReadyReplicas 2, AvailableReplicas 2
Jun 28 09:07:46.560: INFO: observed Replicaset test-rs in namespace replicaset-5639 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Jun 28 09:07:46.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-5639" for this suite. 06/28/23 09:07:46.568
{"msg":"PASSED [sig-apps] ReplicaSet Replace and Patch tests [Conformance]","completed":344,"skipped":6220,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.157 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 09:07:40.418
    Jun 28 09:07:40.418: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename replicaset 06/28/23 09:07:40.419
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 09:07:40.432
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 09:07:40.436
    [It] Replace and Patch tests [Conformance]
      test/e2e/apps/replica_set.go:154
    Jun 28 09:07:40.452: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jun 28 09:07:45.457: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 06/28/23 09:07:45.457
    STEP: Scaling up "test-rs" replicaset  06/28/23 09:07:45.457
    Jun 28 09:07:45.469: INFO: Updating replica set "test-rs"
    STEP: patching the ReplicaSet 06/28/23 09:07:45.469
    W0628 09:07:45.476172      18 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Jun 28 09:07:45.478: INFO: observed ReplicaSet test-rs in namespace replicaset-5639 with ReadyReplicas 1, AvailableReplicas 1
    Jun 28 09:07:45.487: INFO: observed ReplicaSet test-rs in namespace replicaset-5639 with ReadyReplicas 1, AvailableReplicas 1
    Jun 28 09:07:45.497: INFO: observed ReplicaSet test-rs in namespace replicaset-5639 with ReadyReplicas 1, AvailableReplicas 1
    Jun 28 09:07:45.503: INFO: observed ReplicaSet test-rs in namespace replicaset-5639 with ReadyReplicas 1, AvailableReplicas 1
    Jun 28 09:07:46.445: INFO: observed ReplicaSet test-rs in namespace replicaset-5639 with ReadyReplicas 2, AvailableReplicas 2
    Jun 28 09:07:46.560: INFO: observed Replicaset test-rs in namespace replicaset-5639 with ReadyReplicas 3 found true
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Jun 28 09:07:46.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-5639" for this suite. 06/28/23 09:07:46.568
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 09:07:46.576
Jun 28 09:07:46.576: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename downward-api 06/28/23 09:07:46.576
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 09:07:46.589
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 09:07:46.593
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192
STEP: Creating a pod to test downward API volume plugin 06/28/23 09:07:46.598
Jun 28 09:07:46.607: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f860dfcc-2858-4844-b637-366cbdd8588a" in namespace "downward-api-7189" to be "Succeeded or Failed"
Jun 28 09:07:46.612: INFO: Pod "downwardapi-volume-f860dfcc-2858-4844-b637-366cbdd8588a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.237342ms
Jun 28 09:07:48.620: INFO: Pod "downwardapi-volume-f860dfcc-2858-4844-b637-366cbdd8588a": Phase="Running", Reason="", readiness=false. Elapsed: 2.01305571s
Jun 28 09:07:50.620: INFO: Pod "downwardapi-volume-f860dfcc-2858-4844-b637-366cbdd8588a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012644314s
STEP: Saw pod success 06/28/23 09:07:50.62
Jun 28 09:07:50.620: INFO: Pod "downwardapi-volume-f860dfcc-2858-4844-b637-366cbdd8588a" satisfied condition "Succeeded or Failed"
Jun 28 09:07:50.624: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod downwardapi-volume-f860dfcc-2858-4844-b637-366cbdd8588a container client-container: <nil>
STEP: delete the pod 06/28/23 09:07:50.634
Jun 28 09:07:50.645: INFO: Waiting for pod downwardapi-volume-f860dfcc-2858-4844-b637-366cbdd8588a to disappear
Jun 28 09:07:50.649: INFO: Pod downwardapi-volume-f860dfcc-2858-4844-b637-366cbdd8588a no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jun 28 09:07:50.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7189" for this suite. 06/28/23 09:07:50.66
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","completed":345,"skipped":6220,"failed":0}
------------------------------
â€¢ [4.090 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 09:07:46.576
    Jun 28 09:07:46.576: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename downward-api 06/28/23 09:07:46.576
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 09:07:46.589
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 09:07:46.593
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:192
    STEP: Creating a pod to test downward API volume plugin 06/28/23 09:07:46.598
    Jun 28 09:07:46.607: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f860dfcc-2858-4844-b637-366cbdd8588a" in namespace "downward-api-7189" to be "Succeeded or Failed"
    Jun 28 09:07:46.612: INFO: Pod "downwardapi-volume-f860dfcc-2858-4844-b637-366cbdd8588a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.237342ms
    Jun 28 09:07:48.620: INFO: Pod "downwardapi-volume-f860dfcc-2858-4844-b637-366cbdd8588a": Phase="Running", Reason="", readiness=false. Elapsed: 2.01305571s
    Jun 28 09:07:50.620: INFO: Pod "downwardapi-volume-f860dfcc-2858-4844-b637-366cbdd8588a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012644314s
    STEP: Saw pod success 06/28/23 09:07:50.62
    Jun 28 09:07:50.620: INFO: Pod "downwardapi-volume-f860dfcc-2858-4844-b637-366cbdd8588a" satisfied condition "Succeeded or Failed"
    Jun 28 09:07:50.624: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod downwardapi-volume-f860dfcc-2858-4844-b637-366cbdd8588a container client-container: <nil>
    STEP: delete the pod 06/28/23 09:07:50.634
    Jun 28 09:07:50.645: INFO: Waiting for pod downwardapi-volume-f860dfcc-2858-4844-b637-366cbdd8588a to disappear
    Jun 28 09:07:50.649: INFO: Pod downwardapi-volume-f860dfcc-2858-4844-b637-366cbdd8588a no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jun 28 09:07:50.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-7189" for this suite. 06/28/23 09:07:50.66
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 09:07:50.668
Jun 28 09:07:50.669: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename secrets 06/28/23 09:07:50.669
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 09:07:50.681
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 09:07:50.685
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67
STEP: Creating secret with name secret-test-352137b1-6ef0-42ef-939e-5eaf68cf9420 06/28/23 09:07:50.689
STEP: Creating a pod to test consume secrets 06/28/23 09:07:50.695
Jun 28 09:07:50.704: INFO: Waiting up to 5m0s for pod "pod-secrets-cd1158d5-fa5d-45e2-b352-ab22711b3433" in namespace "secrets-4016" to be "Succeeded or Failed"
Jun 28 09:07:50.708: INFO: Pod "pod-secrets-cd1158d5-fa5d-45e2-b352-ab22711b3433": Phase="Pending", Reason="", readiness=false. Elapsed: 3.881626ms
Jun 28 09:07:52.713: INFO: Pod "pod-secrets-cd1158d5-fa5d-45e2-b352-ab22711b3433": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009305398s
Jun 28 09:07:54.715: INFO: Pod "pod-secrets-cd1158d5-fa5d-45e2-b352-ab22711b3433": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010670764s
STEP: Saw pod success 06/28/23 09:07:54.715
Jun 28 09:07:54.715: INFO: Pod "pod-secrets-cd1158d5-fa5d-45e2-b352-ab22711b3433" satisfied condition "Succeeded or Failed"
Jun 28 09:07:54.720: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod pod-secrets-cd1158d5-fa5d-45e2-b352-ab22711b3433 container secret-volume-test: <nil>
STEP: delete the pod 06/28/23 09:07:54.77
Jun 28 09:07:54.783: INFO: Waiting for pod pod-secrets-cd1158d5-fa5d-45e2-b352-ab22711b3433 to disappear
Jun 28 09:07:54.787: INFO: Pod pod-secrets-cd1158d5-fa5d-45e2-b352-ab22711b3433 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jun 28 09:07:54.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4016" for this suite. 06/28/23 09:07:54.795
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","completed":346,"skipped":6284,"failed":0}
------------------------------
â€¢ [4.134 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 09:07:50.668
    Jun 28 09:07:50.669: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename secrets 06/28/23 09:07:50.669
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 09:07:50.681
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 09:07:50.685
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:67
    STEP: Creating secret with name secret-test-352137b1-6ef0-42ef-939e-5eaf68cf9420 06/28/23 09:07:50.689
    STEP: Creating a pod to test consume secrets 06/28/23 09:07:50.695
    Jun 28 09:07:50.704: INFO: Waiting up to 5m0s for pod "pod-secrets-cd1158d5-fa5d-45e2-b352-ab22711b3433" in namespace "secrets-4016" to be "Succeeded or Failed"
    Jun 28 09:07:50.708: INFO: Pod "pod-secrets-cd1158d5-fa5d-45e2-b352-ab22711b3433": Phase="Pending", Reason="", readiness=false. Elapsed: 3.881626ms
    Jun 28 09:07:52.713: INFO: Pod "pod-secrets-cd1158d5-fa5d-45e2-b352-ab22711b3433": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009305398s
    Jun 28 09:07:54.715: INFO: Pod "pod-secrets-cd1158d5-fa5d-45e2-b352-ab22711b3433": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010670764s
    STEP: Saw pod success 06/28/23 09:07:54.715
    Jun 28 09:07:54.715: INFO: Pod "pod-secrets-cd1158d5-fa5d-45e2-b352-ab22711b3433" satisfied condition "Succeeded or Failed"
    Jun 28 09:07:54.720: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod pod-secrets-cd1158d5-fa5d-45e2-b352-ab22711b3433 container secret-volume-test: <nil>
    STEP: delete the pod 06/28/23 09:07:54.77
    Jun 28 09:07:54.783: INFO: Waiting for pod pod-secrets-cd1158d5-fa5d-45e2-b352-ab22711b3433 to disappear
    Jun 28 09:07:54.787: INFO: Pod pod-secrets-cd1158d5-fa5d-45e2-b352-ab22711b3433 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jun 28 09:07:54.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-4016" for this suite. 06/28/23 09:07:54.795
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 09:07:54.802
Jun 28 09:07:54.802: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename var-expansion 06/28/23 09:07:54.803
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 09:07:54.816
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 09:07:54.82
[It] should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111
STEP: Creating a pod to test substitution in volume subpath 06/28/23 09:07:54.824
Jun 28 09:07:54.834: INFO: Waiting up to 5m0s for pod "var-expansion-36ad46fc-e5b1-49b8-8054-44fa7e96e84c" in namespace "var-expansion-6493" to be "Succeeded or Failed"
Jun 28 09:07:54.839: INFO: Pod "var-expansion-36ad46fc-e5b1-49b8-8054-44fa7e96e84c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.697887ms
Jun 28 09:07:56.845: INFO: Pod "var-expansion-36ad46fc-e5b1-49b8-8054-44fa7e96e84c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011452063s
Jun 28 09:07:58.844: INFO: Pod "var-expansion-36ad46fc-e5b1-49b8-8054-44fa7e96e84c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010003184s
STEP: Saw pod success 06/28/23 09:07:58.844
Jun 28 09:07:58.844: INFO: Pod "var-expansion-36ad46fc-e5b1-49b8-8054-44fa7e96e84c" satisfied condition "Succeeded or Failed"
Jun 28 09:07:58.848: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod var-expansion-36ad46fc-e5b1-49b8-8054-44fa7e96e84c container dapi-container: <nil>
STEP: delete the pod 06/28/23 09:07:58.858
Jun 28 09:07:58.870: INFO: Waiting for pod var-expansion-36ad46fc-e5b1-49b8-8054-44fa7e96e84c to disappear
Jun 28 09:07:58.874: INFO: Pod var-expansion-36ad46fc-e5b1-49b8-8054-44fa7e96e84c no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jun 28 09:07:58.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6493" for this suite. 06/28/23 09:07:58.882
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]","completed":347,"skipped":6288,"failed":0}
------------------------------
â€¢ [4.085 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 09:07:54.802
    Jun 28 09:07:54.802: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename var-expansion 06/28/23 09:07:54.803
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 09:07:54.816
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 09:07:54.82
    [It] should allow substituting values in a volume subpath [Conformance]
      test/e2e/common/node/expansion.go:111
    STEP: Creating a pod to test substitution in volume subpath 06/28/23 09:07:54.824
    Jun 28 09:07:54.834: INFO: Waiting up to 5m0s for pod "var-expansion-36ad46fc-e5b1-49b8-8054-44fa7e96e84c" in namespace "var-expansion-6493" to be "Succeeded or Failed"
    Jun 28 09:07:54.839: INFO: Pod "var-expansion-36ad46fc-e5b1-49b8-8054-44fa7e96e84c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.697887ms
    Jun 28 09:07:56.845: INFO: Pod "var-expansion-36ad46fc-e5b1-49b8-8054-44fa7e96e84c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011452063s
    Jun 28 09:07:58.844: INFO: Pod "var-expansion-36ad46fc-e5b1-49b8-8054-44fa7e96e84c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010003184s
    STEP: Saw pod success 06/28/23 09:07:58.844
    Jun 28 09:07:58.844: INFO: Pod "var-expansion-36ad46fc-e5b1-49b8-8054-44fa7e96e84c" satisfied condition "Succeeded or Failed"
    Jun 28 09:07:58.848: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod var-expansion-36ad46fc-e5b1-49b8-8054-44fa7e96e84c container dapi-container: <nil>
    STEP: delete the pod 06/28/23 09:07:58.858
    Jun 28 09:07:58.870: INFO: Waiting for pod var-expansion-36ad46fc-e5b1-49b8-8054-44fa7e96e84c to disappear
    Jun 28 09:07:58.874: INFO: Pod var-expansion-36ad46fc-e5b1-49b8-8054-44fa7e96e84c no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jun 28 09:07:58.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-6493" for this suite. 06/28/23 09:07:58.882
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3210
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 09:07:58.888
Jun 28 09:07:58.888: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename services 06/28/23 09:07:58.889
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 09:07:58.9
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 09:07:58.904
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3210
STEP: creating an Endpoint 06/28/23 09:07:58.913
STEP: waiting for available Endpoint 06/28/23 09:07:58.919
STEP: listing all Endpoints 06/28/23 09:07:58.922
STEP: updating the Endpoint 06/28/23 09:07:58.929
STEP: fetching the Endpoint 06/28/23 09:07:58.939
STEP: patching the Endpoint 06/28/23 09:07:58.943
STEP: fetching the Endpoint 06/28/23 09:07:58.953
STEP: deleting the Endpoint by Collection 06/28/23 09:07:58.957
STEP: waiting for Endpoint deletion 06/28/23 09:07:58.965
STEP: fetching the Endpoint 06/28/23 09:07:58.968
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jun 28 09:07:58.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9375" for this suite. 06/28/23 09:07:58.978
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","completed":348,"skipped":6302,"failed":0}
------------------------------
â€¢ [0.097 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3210

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 09:07:58.888
    Jun 28 09:07:58.888: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename services 06/28/23 09:07:58.889
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 09:07:58.9
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 09:07:58.904
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should test the lifecycle of an Endpoint [Conformance]
      test/e2e/network/service.go:3210
    STEP: creating an Endpoint 06/28/23 09:07:58.913
    STEP: waiting for available Endpoint 06/28/23 09:07:58.919
    STEP: listing all Endpoints 06/28/23 09:07:58.922
    STEP: updating the Endpoint 06/28/23 09:07:58.929
    STEP: fetching the Endpoint 06/28/23 09:07:58.939
    STEP: patching the Endpoint 06/28/23 09:07:58.943
    STEP: fetching the Endpoint 06/28/23 09:07:58.953
    STEP: deleting the Endpoint by Collection 06/28/23 09:07:58.957
    STEP: waiting for Endpoint deletion 06/28/23 09:07:58.965
    STEP: fetching the Endpoint 06/28/23 09:07:58.968
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jun 28 09:07:58.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-9375" for this suite. 06/28/23 09:07:58.978
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 09:07:58.986
Jun 28 09:07:58.986: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename pod-network-test 06/28/23 09:07:58.987
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 09:07:58.998
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 09:07:59.002
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
STEP: Performing setup for networking test in namespace pod-network-test-5794 06/28/23 09:07:59.006
STEP: creating a selector 06/28/23 09:07:59.006
STEP: Creating the service pods in kubernetes 06/28/23 09:07:59.006
Jun 28 09:07:59.006: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jun 28 09:07:59.039: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-5794" to be "running and ready"
Jun 28 09:07:59.045: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 5.29336ms
Jun 28 09:07:59.045: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun 28 09:08:01.050: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.010978448s
Jun 28 09:08:01.050: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 28 09:08:03.051: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.011151158s
Jun 28 09:08:03.051: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 28 09:08:05.050: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.010110894s
Jun 28 09:08:05.050: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 28 09:08:07.051: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.011655469s
Jun 28 09:08:07.051: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 28 09:08:09.050: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.010865006s
Jun 28 09:08:09.050: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 28 09:08:11.051: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.012019883s
Jun 28 09:08:11.051: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jun 28 09:08:11.051: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jun 28 09:08:11.056: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-5794" to be "running and ready"
Jun 28 09:08:11.060: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 4.183964ms
Jun 28 09:08:11.060: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jun 28 09:08:11.060: INFO: Pod "netserver-1" satisfied condition "running and ready"
Jun 28 09:08:11.064: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-5794" to be "running and ready"
Jun 28 09:08:11.069: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 4.623659ms
Jun 28 09:08:11.069: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Jun 28 09:08:11.069: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 06/28/23 09:08:11.073
Jun 28 09:08:11.086: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-5794" to be "running"
Jun 28 09:08:11.091: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.466959ms
Jun 28 09:08:13.097: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.010822665s
Jun 28 09:08:13.097: INFO: Pod "test-container-pod" satisfied condition "running"
Jun 28 09:08:13.101: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-5794" to be "running"
Jun 28 09:08:13.105: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.262452ms
Jun 28 09:08:13.105: INFO: Pod "host-test-container-pod" satisfied condition "running"
Jun 28 09:08:13.110: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Jun 28 09:08:13.110: INFO: Going to poll 172.21.122.41 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Jun 28 09:08:13.114: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.21.122.41:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5794 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 28 09:08:13.114: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
Jun 28 09:08:13.114: INFO: ExecWithOptions: Clientset creation
Jun 28 09:08:13.114: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/pod-network-test-5794/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.21.122.41%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jun 28 09:08:13.460: INFO: Found all 1 expected endpoints: [netserver-0]
Jun 28 09:08:13.460: INFO: Going to poll 172.21.122.102 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Jun 28 09:08:13.464: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.21.122.102:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5794 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 28 09:08:13.464: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
Jun 28 09:08:13.465: INFO: ExecWithOptions: Clientset creation
Jun 28 09:08:13.465: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/pod-network-test-5794/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.21.122.102%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jun 28 09:08:13.743: INFO: Found all 1 expected endpoints: [netserver-1]
Jun 28 09:08:13.743: INFO: Going to poll 172.21.30.67 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Jun 28 09:08:13.747: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.21.30.67:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5794 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 28 09:08:13.747: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
Jun 28 09:08:13.748: INFO: ExecWithOptions: Clientset creation
Jun 28 09:08:13.748: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/pod-network-test-5794/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.21.30.67%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jun 28 09:08:14.100: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Jun 28 09:08:14.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5794" for this suite. 06/28/23 09:08:14.109
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","completed":349,"skipped":6314,"failed":0}
------------------------------
â€¢ [SLOW TEST] [15.129 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 09:07:58.986
    Jun 28 09:07:58.986: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename pod-network-test 06/28/23 09:07:58.987
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 09:07:58.998
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 09:07:59.002
    [It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:105
    STEP: Performing setup for networking test in namespace pod-network-test-5794 06/28/23 09:07:59.006
    STEP: creating a selector 06/28/23 09:07:59.006
    STEP: Creating the service pods in kubernetes 06/28/23 09:07:59.006
    Jun 28 09:07:59.006: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jun 28 09:07:59.039: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-5794" to be "running and ready"
    Jun 28 09:07:59.045: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 5.29336ms
    Jun 28 09:07:59.045: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jun 28 09:08:01.050: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.010978448s
    Jun 28 09:08:01.050: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 28 09:08:03.051: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.011151158s
    Jun 28 09:08:03.051: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 28 09:08:05.050: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.010110894s
    Jun 28 09:08:05.050: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 28 09:08:07.051: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.011655469s
    Jun 28 09:08:07.051: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 28 09:08:09.050: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.010865006s
    Jun 28 09:08:09.050: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 28 09:08:11.051: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.012019883s
    Jun 28 09:08:11.051: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jun 28 09:08:11.051: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jun 28 09:08:11.056: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-5794" to be "running and ready"
    Jun 28 09:08:11.060: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 4.183964ms
    Jun 28 09:08:11.060: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jun 28 09:08:11.060: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Jun 28 09:08:11.064: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-5794" to be "running and ready"
    Jun 28 09:08:11.069: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 4.623659ms
    Jun 28 09:08:11.069: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Jun 28 09:08:11.069: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 06/28/23 09:08:11.073
    Jun 28 09:08:11.086: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-5794" to be "running"
    Jun 28 09:08:11.091: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.466959ms
    Jun 28 09:08:13.097: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.010822665s
    Jun 28 09:08:13.097: INFO: Pod "test-container-pod" satisfied condition "running"
    Jun 28 09:08:13.101: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-5794" to be "running"
    Jun 28 09:08:13.105: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.262452ms
    Jun 28 09:08:13.105: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Jun 28 09:08:13.110: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Jun 28 09:08:13.110: INFO: Going to poll 172.21.122.41 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Jun 28 09:08:13.114: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.21.122.41:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5794 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 28 09:08:13.114: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    Jun 28 09:08:13.114: INFO: ExecWithOptions: Clientset creation
    Jun 28 09:08:13.114: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/pod-network-test-5794/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.21.122.41%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jun 28 09:08:13.460: INFO: Found all 1 expected endpoints: [netserver-0]
    Jun 28 09:08:13.460: INFO: Going to poll 172.21.122.102 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Jun 28 09:08:13.464: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.21.122.102:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5794 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 28 09:08:13.464: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    Jun 28 09:08:13.465: INFO: ExecWithOptions: Clientset creation
    Jun 28 09:08:13.465: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/pod-network-test-5794/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.21.122.102%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jun 28 09:08:13.743: INFO: Found all 1 expected endpoints: [netserver-1]
    Jun 28 09:08:13.743: INFO: Going to poll 172.21.30.67 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Jun 28 09:08:13.747: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.21.30.67:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5794 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 28 09:08:13.747: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    Jun 28 09:08:13.748: INFO: ExecWithOptions: Clientset creation
    Jun 28 09:08:13.748: INFO: ExecWithOptions: execute(POST https://172.20.0.1:443/api/v1/namespaces/pod-network-test-5794/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.21.30.67%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jun 28 09:08:14.100: INFO: Found all 1 expected endpoints: [netserver-2]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Jun 28 09:08:14.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-5794" for this suite. 06/28/23 09:08:14.109
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:585
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 09:08:14.119
Jun 28 09:08:14.119: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename statefulset 06/28/23 09:08:14.12
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 09:08:14.133
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 09:08:14.137
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-1911 06/28/23 09:08:14.142
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:585
STEP: Initializing watcher for selector baz=blah,foo=bar 06/28/23 09:08:14.147
STEP: Creating stateful set ss in namespace statefulset-1911 06/28/23 09:08:14.152
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-1911 06/28/23 09:08:14.159
Jun 28 09:08:14.163: INFO: Found 0 stateful pods, waiting for 1
Jun 28 09:08:24.168: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 06/28/23 09:08:24.168
Jun 28 09:08:24.173: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=statefulset-1911 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 28 09:08:24.583: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 28 09:08:24.583: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 28 09:08:24.583: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 28 09:08:24.587: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jun 28 09:08:34.595: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun 28 09:08:34.595: INFO: Waiting for statefulset status.replicas updated to 0
Jun 28 09:08:34.616: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999711s
Jun 28 09:08:35.621: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.994936632s
Jun 28 09:08:36.626: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.989518959s
Jun 28 09:08:37.631: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.985121532s
Jun 28 09:08:38.635: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.979982826s
Jun 28 09:08:39.641: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.975265945s
Jun 28 09:08:40.646: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.969406819s
Jun 28 09:08:41.652: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.96439855s
Jun 28 09:08:42.658: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.958089979s
Jun 28 09:08:43.665: INFO: Verifying statefulset ss doesn't scale past 1 for another 951.41827ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-1911 06/28/23 09:08:44.666
Jun 28 09:08:44.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=statefulset-1911 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 09:08:45.105: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun 28 09:08:45.105: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 28 09:08:45.105: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 28 09:08:45.110: INFO: Found 1 stateful pods, waiting for 3
Jun 28 09:08:55.118: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 28 09:08:55.118: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 28 09:08:55.118: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order 06/28/23 09:08:55.118
STEP: Scale down will halt with unhealthy stateful pod 06/28/23 09:08:55.118
Jun 28 09:08:55.129: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=statefulset-1911 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 28 09:08:55.562: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 28 09:08:55.562: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 28 09:08:55.562: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 28 09:08:55.562: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=statefulset-1911 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 28 09:08:55.993: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 28 09:08:55.993: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 28 09:08:55.993: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 28 09:08:55.993: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=statefulset-1911 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 28 09:08:56.411: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 28 09:08:56.411: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 28 09:08:56.411: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 28 09:08:56.411: INFO: Waiting for statefulset status.replicas updated to 0
Jun 28 09:08:56.416: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Jun 28 09:09:06.429: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun 28 09:09:06.429: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jun 28 09:09:06.429: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jun 28 09:09:06.447: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999663s
Jun 28 09:09:07.453: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.994603572s
Jun 28 09:09:08.460: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.987531933s
Jun 28 09:09:09.467: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.980823709s
Jun 28 09:09:10.473: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.974789721s
Jun 28 09:09:11.479: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.96854293s
Jun 28 09:09:12.485: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.96189093s
Jun 28 09:09:13.492: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.955984759s
Jun 28 09:09:14.497: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.949613112s
Jun 28 09:09:15.504: INFO: Verifying statefulset ss doesn't scale past 3 for another 943.833447ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-1911 06/28/23 09:09:16.504
Jun 28 09:09:16.511: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=statefulset-1911 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 09:09:16.906: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun 28 09:09:16.906: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 28 09:09:16.906: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 28 09:09:16.906: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=statefulset-1911 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 09:09:17.347: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun 28 09:09:17.347: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 28 09:09:17.347: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 28 09:09:17.347: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=statefulset-1911 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 28 09:09:17.782: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun 28 09:09:17.782: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 28 09:09:17.782: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 28 09:09:17.782: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order 06/28/23 09:09:27.803
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jun 28 09:09:27.803: INFO: Deleting all statefulset in ns statefulset-1911
Jun 28 09:09:27.808: INFO: Scaling statefulset ss to 0
Jun 28 09:09:27.822: INFO: Waiting for statefulset status.replicas updated to 0
Jun 28 09:09:27.826: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jun 28 09:09:27.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1911" for this suite. 06/28/23 09:09:27.848
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","completed":350,"skipped":6431,"failed":0}
------------------------------
â€¢ [SLOW TEST] [73.737 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    test/e2e/apps/statefulset.go:585

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 09:08:14.119
    Jun 28 09:08:14.119: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename statefulset 06/28/23 09:08:14.12
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 09:08:14.133
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 09:08:14.137
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-1911 06/28/23 09:08:14.142
    [It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
      test/e2e/apps/statefulset.go:585
    STEP: Initializing watcher for selector baz=blah,foo=bar 06/28/23 09:08:14.147
    STEP: Creating stateful set ss in namespace statefulset-1911 06/28/23 09:08:14.152
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-1911 06/28/23 09:08:14.159
    Jun 28 09:08:14.163: INFO: Found 0 stateful pods, waiting for 1
    Jun 28 09:08:24.168: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 06/28/23 09:08:24.168
    Jun 28 09:08:24.173: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=statefulset-1911 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jun 28 09:08:24.583: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jun 28 09:08:24.583: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jun 28 09:08:24.583: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jun 28 09:08:24.587: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Jun 28 09:08:34.595: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jun 28 09:08:34.595: INFO: Waiting for statefulset status.replicas updated to 0
    Jun 28 09:08:34.616: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999711s
    Jun 28 09:08:35.621: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.994936632s
    Jun 28 09:08:36.626: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.989518959s
    Jun 28 09:08:37.631: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.985121532s
    Jun 28 09:08:38.635: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.979982826s
    Jun 28 09:08:39.641: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.975265945s
    Jun 28 09:08:40.646: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.969406819s
    Jun 28 09:08:41.652: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.96439855s
    Jun 28 09:08:42.658: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.958089979s
    Jun 28 09:08:43.665: INFO: Verifying statefulset ss doesn't scale past 1 for another 951.41827ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-1911 06/28/23 09:08:44.666
    Jun 28 09:08:44.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=statefulset-1911 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jun 28 09:08:45.105: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jun 28 09:08:45.105: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jun 28 09:08:45.105: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jun 28 09:08:45.110: INFO: Found 1 stateful pods, waiting for 3
    Jun 28 09:08:55.118: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Jun 28 09:08:55.118: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Jun 28 09:08:55.118: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Verifying that stateful set ss was scaled up in order 06/28/23 09:08:55.118
    STEP: Scale down will halt with unhealthy stateful pod 06/28/23 09:08:55.118
    Jun 28 09:08:55.129: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=statefulset-1911 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jun 28 09:08:55.562: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jun 28 09:08:55.562: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jun 28 09:08:55.562: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jun 28 09:08:55.562: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=statefulset-1911 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jun 28 09:08:55.993: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jun 28 09:08:55.993: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jun 28 09:08:55.993: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jun 28 09:08:55.993: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=statefulset-1911 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jun 28 09:08:56.411: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jun 28 09:08:56.411: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jun 28 09:08:56.411: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jun 28 09:08:56.411: INFO: Waiting for statefulset status.replicas updated to 0
    Jun 28 09:08:56.416: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
    Jun 28 09:09:06.429: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jun 28 09:09:06.429: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Jun 28 09:09:06.429: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Jun 28 09:09:06.447: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999663s
    Jun 28 09:09:07.453: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.994603572s
    Jun 28 09:09:08.460: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.987531933s
    Jun 28 09:09:09.467: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.980823709s
    Jun 28 09:09:10.473: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.974789721s
    Jun 28 09:09:11.479: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.96854293s
    Jun 28 09:09:12.485: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.96189093s
    Jun 28 09:09:13.492: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.955984759s
    Jun 28 09:09:14.497: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.949613112s
    Jun 28 09:09:15.504: INFO: Verifying statefulset ss doesn't scale past 3 for another 943.833447ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-1911 06/28/23 09:09:16.504
    Jun 28 09:09:16.511: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=statefulset-1911 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jun 28 09:09:16.906: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jun 28 09:09:16.906: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jun 28 09:09:16.906: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jun 28 09:09:16.906: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=statefulset-1911 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jun 28 09:09:17.347: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jun 28 09:09:17.347: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jun 28 09:09:17.347: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jun 28 09:09:17.347: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3061215203 --namespace=statefulset-1911 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jun 28 09:09:17.782: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jun 28 09:09:17.782: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jun 28 09:09:17.782: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jun 28 09:09:17.782: INFO: Scaling statefulset ss to 0
    STEP: Verifying that stateful set ss was scaled down in reverse order 06/28/23 09:09:27.803
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jun 28 09:09:27.803: INFO: Deleting all statefulset in ns statefulset-1911
    Jun 28 09:09:27.808: INFO: Scaling statefulset ss to 0
    Jun 28 09:09:27.822: INFO: Waiting for statefulset status.replicas updated to 0
    Jun 28 09:09:27.826: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jun 28 09:09:27.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-1911" for this suite. 06/28/23 09:09:27.848
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 09:09:27.857
Jun 28 09:09:27.857: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename projected 06/28/23 09:09:27.858
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 09:09:27.872
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 09:09:27.879
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129
STEP: Creating the pod 06/28/23 09:09:27.883
Jun 28 09:09:27.891: INFO: Waiting up to 5m0s for pod "labelsupdate8d934c82-6812-4a4f-95ee-5c7971257227" in namespace "projected-4106" to be "running and ready"
Jun 28 09:09:27.895: INFO: Pod "labelsupdate8d934c82-6812-4a4f-95ee-5c7971257227": Phase="Pending", Reason="", readiness=false. Elapsed: 4.058752ms
Jun 28 09:09:27.895: INFO: The phase of Pod labelsupdate8d934c82-6812-4a4f-95ee-5c7971257227 is Pending, waiting for it to be Running (with Ready = true)
Jun 28 09:09:29.901: INFO: Pod "labelsupdate8d934c82-6812-4a4f-95ee-5c7971257227": Phase="Running", Reason="", readiness=true. Elapsed: 2.0093663s
Jun 28 09:09:29.901: INFO: The phase of Pod labelsupdate8d934c82-6812-4a4f-95ee-5c7971257227 is Running (Ready = true)
Jun 28 09:09:29.901: INFO: Pod "labelsupdate8d934c82-6812-4a4f-95ee-5c7971257227" satisfied condition "running and ready"
Jun 28 09:09:30.430: INFO: Successfully updated pod "labelsupdate8d934c82-6812-4a4f-95ee-5c7971257227"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jun 28 09:09:34.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4106" for this suite. 06/28/23 09:09:34.475
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","completed":351,"skipped":6436,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.626 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 09:09:27.857
    Jun 28 09:09:27.857: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename projected 06/28/23 09:09:27.858
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 09:09:27.872
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 09:09:27.879
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:129
    STEP: Creating the pod 06/28/23 09:09:27.883
    Jun 28 09:09:27.891: INFO: Waiting up to 5m0s for pod "labelsupdate8d934c82-6812-4a4f-95ee-5c7971257227" in namespace "projected-4106" to be "running and ready"
    Jun 28 09:09:27.895: INFO: Pod "labelsupdate8d934c82-6812-4a4f-95ee-5c7971257227": Phase="Pending", Reason="", readiness=false. Elapsed: 4.058752ms
    Jun 28 09:09:27.895: INFO: The phase of Pod labelsupdate8d934c82-6812-4a4f-95ee-5c7971257227 is Pending, waiting for it to be Running (with Ready = true)
    Jun 28 09:09:29.901: INFO: Pod "labelsupdate8d934c82-6812-4a4f-95ee-5c7971257227": Phase="Running", Reason="", readiness=true. Elapsed: 2.0093663s
    Jun 28 09:09:29.901: INFO: The phase of Pod labelsupdate8d934c82-6812-4a4f-95ee-5c7971257227 is Running (Ready = true)
    Jun 28 09:09:29.901: INFO: Pod "labelsupdate8d934c82-6812-4a4f-95ee-5c7971257227" satisfied condition "running and ready"
    Jun 28 09:09:30.430: INFO: Successfully updated pod "labelsupdate8d934c82-6812-4a4f-95ee-5c7971257227"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jun 28 09:09:34.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4106" for this suite. 06/28/23 09:09:34.475
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Secrets
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 09:09:34.483
Jun 28 09:09:34.483: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename secrets 06/28/23 09:09:34.484
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 09:09:34.498
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 09:09:34.502
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94
STEP: creating secret secrets-2695/secret-test-55673b53-85d3-4a1f-b6f9-2842de1efcb0 06/28/23 09:09:34.507
STEP: Creating a pod to test consume secrets 06/28/23 09:09:34.512
Jun 28 09:09:34.520: INFO: Waiting up to 5m0s for pod "pod-configmaps-b1d8763f-a1f7-42c4-adaf-d62466e52b63" in namespace "secrets-2695" to be "Succeeded or Failed"
Jun 28 09:09:34.525: INFO: Pod "pod-configmaps-b1d8763f-a1f7-42c4-adaf-d62466e52b63": Phase="Pending", Reason="", readiness=false. Elapsed: 5.200746ms
Jun 28 09:09:36.531: INFO: Pod "pod-configmaps-b1d8763f-a1f7-42c4-adaf-d62466e52b63": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010738137s
Jun 28 09:09:38.531: INFO: Pod "pod-configmaps-b1d8763f-a1f7-42c4-adaf-d62466e52b63": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011569719s
STEP: Saw pod success 06/28/23 09:09:38.531
Jun 28 09:09:38.531: INFO: Pod "pod-configmaps-b1d8763f-a1f7-42c4-adaf-d62466e52b63" satisfied condition "Succeeded or Failed"
Jun 28 09:09:38.537: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-zxlfv pod pod-configmaps-b1d8763f-a1f7-42c4-adaf-d62466e52b63 container env-test: <nil>
STEP: delete the pod 06/28/23 09:09:38.551
Jun 28 09:09:38.563: INFO: Waiting for pod pod-configmaps-b1d8763f-a1f7-42c4-adaf-d62466e52b63 to disappear
Jun 28 09:09:38.567: INFO: Pod pod-configmaps-b1d8763f-a1f7-42c4-adaf-d62466e52b63 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Jun 28 09:09:38.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2695" for this suite. 06/28/23 09:09:38.576
{"msg":"PASSED [sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]","completed":352,"skipped":6436,"failed":0}
------------------------------
â€¢ [4.102 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 09:09:34.483
    Jun 28 09:09:34.483: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename secrets 06/28/23 09:09:34.484
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 09:09:34.498
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 09:09:34.502
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:94
    STEP: creating secret secrets-2695/secret-test-55673b53-85d3-4a1f-b6f9-2842de1efcb0 06/28/23 09:09:34.507
    STEP: Creating a pod to test consume secrets 06/28/23 09:09:34.512
    Jun 28 09:09:34.520: INFO: Waiting up to 5m0s for pod "pod-configmaps-b1d8763f-a1f7-42c4-adaf-d62466e52b63" in namespace "secrets-2695" to be "Succeeded or Failed"
    Jun 28 09:09:34.525: INFO: Pod "pod-configmaps-b1d8763f-a1f7-42c4-adaf-d62466e52b63": Phase="Pending", Reason="", readiness=false. Elapsed: 5.200746ms
    Jun 28 09:09:36.531: INFO: Pod "pod-configmaps-b1d8763f-a1f7-42c4-adaf-d62466e52b63": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010738137s
    Jun 28 09:09:38.531: INFO: Pod "pod-configmaps-b1d8763f-a1f7-42c4-adaf-d62466e52b63": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011569719s
    STEP: Saw pod success 06/28/23 09:09:38.531
    Jun 28 09:09:38.531: INFO: Pod "pod-configmaps-b1d8763f-a1f7-42c4-adaf-d62466e52b63" satisfied condition "Succeeded or Failed"
    Jun 28 09:09:38.537: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-zxlfv pod pod-configmaps-b1d8763f-a1f7-42c4-adaf-d62466e52b63 container env-test: <nil>
    STEP: delete the pod 06/28/23 09:09:38.551
    Jun 28 09:09:38.563: INFO: Waiting for pod pod-configmaps-b1d8763f-a1f7-42c4-adaf-d62466e52b63 to disappear
    Jun 28 09:09:38.567: INFO: Pod pod-configmaps-b1d8763f-a1f7-42c4-adaf-d62466e52b63 no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Jun 28 09:09:38.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-2695" for this suite. 06/28/23 09:09:38.576
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:97
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 09:09:38.586
Jun 28 09:09:38.586: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename container-lifecycle-hook 06/28/23 09:09:38.587
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 09:09:38.6
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 09:09:38.605
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 06/28/23 09:09:38.618
Jun 28 09:09:38.627: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-3712" to be "running and ready"
Jun 28 09:09:38.631: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 4.355841ms
Jun 28 09:09:38.631: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jun 28 09:09:40.643: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.015849372s
Jun 28 09:09:40.643: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jun 28 09:09:40.643: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:97
STEP: create the pod with lifecycle hook 06/28/23 09:09:40.648
Jun 28 09:09:40.654: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-3712" to be "running and ready"
Jun 28 09:09:40.659: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 4.989759ms
Jun 28 09:09:40.659: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jun 28 09:09:42.665: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.010631267s
Jun 28 09:09:42.665: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
Jun 28 09:09:42.665: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
STEP: check poststart hook 06/28/23 09:09:42.669
STEP: delete the pod with lifecycle hook 06/28/23 09:09:42.718
Jun 28 09:09:42.726: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 28 09:09:42.731: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 28 09:09:44.731: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 28 09:09:44.736: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 28 09:09:46.731: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 28 09:09:46.736: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Jun 28 09:09:46.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3712" for this suite. 06/28/23 09:09:46.744
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","completed":353,"skipped":6460,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.166 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 09:09:38.586
    Jun 28 09:09:38.586: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename container-lifecycle-hook 06/28/23 09:09:38.587
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 09:09:38.6
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 09:09:38.605
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 06/28/23 09:09:38.618
    Jun 28 09:09:38.627: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-3712" to be "running and ready"
    Jun 28 09:09:38.631: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 4.355841ms
    Jun 28 09:09:38.631: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jun 28 09:09:40.643: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.015849372s
    Jun 28 09:09:40.643: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jun 28 09:09:40.643: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:97
    STEP: create the pod with lifecycle hook 06/28/23 09:09:40.648
    Jun 28 09:09:40.654: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-3712" to be "running and ready"
    Jun 28 09:09:40.659: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 4.989759ms
    Jun 28 09:09:40.659: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Jun 28 09:09:42.665: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.010631267s
    Jun 28 09:09:42.665: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
    Jun 28 09:09:42.665: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
    STEP: check poststart hook 06/28/23 09:09:42.669
    STEP: delete the pod with lifecycle hook 06/28/23 09:09:42.718
    Jun 28 09:09:42.726: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Jun 28 09:09:42.731: INFO: Pod pod-with-poststart-exec-hook still exists
    Jun 28 09:09:44.731: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Jun 28 09:09:44.736: INFO: Pod pod-with-poststart-exec-hook still exists
    Jun 28 09:09:46.731: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Jun 28 09:09:46.736: INFO: Pod pod-with-poststart-exec-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Jun 28 09:09:46.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-3712" for this suite. 06/28/23 09:09:46.744
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Pods
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 09:09:46.753
Jun 28 09:09:46.753: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename pods 06/28/23 09:09:46.754
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 09:09:46.768
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 09:09:46.772
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617
Jun 28 09:09:46.777: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: creating the pod 06/28/23 09:09:46.777
STEP: submitting the pod to kubernetes 06/28/23 09:09:46.777
Jun 28 09:09:46.787: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-5a6be22c-a9c4-4c15-9188-ecf6e69ada56" in namespace "pods-5393" to be "running and ready"
Jun 28 09:09:46.791: INFO: Pod "pod-logs-websocket-5a6be22c-a9c4-4c15-9188-ecf6e69ada56": Phase="Pending", Reason="", readiness=false. Elapsed: 4.583906ms
Jun 28 09:09:46.791: INFO: The phase of Pod pod-logs-websocket-5a6be22c-a9c4-4c15-9188-ecf6e69ada56 is Pending, waiting for it to be Running (with Ready = true)
Jun 28 09:09:48.798: INFO: Pod "pod-logs-websocket-5a6be22c-a9c4-4c15-9188-ecf6e69ada56": Phase="Running", Reason="", readiness=true. Elapsed: 2.010924641s
Jun 28 09:09:48.798: INFO: The phase of Pod pod-logs-websocket-5a6be22c-a9c4-4c15-9188-ecf6e69ada56 is Running (Ready = true)
Jun 28 09:09:48.798: INFO: Pod "pod-logs-websocket-5a6be22c-a9c4-4c15-9188-ecf6e69ada56" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jun 28 09:09:48.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5393" for this suite. 06/28/23 09:09:48.838
{"msg":"PASSED [sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","completed":354,"skipped":6470,"failed":0}
------------------------------
â€¢ [2.093 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 09:09:46.753
    Jun 28 09:09:46.753: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename pods 06/28/23 09:09:46.754
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 09:09:46.768
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 09:09:46.772
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:617
    Jun 28 09:09:46.777: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: creating the pod 06/28/23 09:09:46.777
    STEP: submitting the pod to kubernetes 06/28/23 09:09:46.777
    Jun 28 09:09:46.787: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-5a6be22c-a9c4-4c15-9188-ecf6e69ada56" in namespace "pods-5393" to be "running and ready"
    Jun 28 09:09:46.791: INFO: Pod "pod-logs-websocket-5a6be22c-a9c4-4c15-9188-ecf6e69ada56": Phase="Pending", Reason="", readiness=false. Elapsed: 4.583906ms
    Jun 28 09:09:46.791: INFO: The phase of Pod pod-logs-websocket-5a6be22c-a9c4-4c15-9188-ecf6e69ada56 is Pending, waiting for it to be Running (with Ready = true)
    Jun 28 09:09:48.798: INFO: Pod "pod-logs-websocket-5a6be22c-a9c4-4c15-9188-ecf6e69ada56": Phase="Running", Reason="", readiness=true. Elapsed: 2.010924641s
    Jun 28 09:09:48.798: INFO: The phase of Pod pod-logs-websocket-5a6be22c-a9c4-4c15-9188-ecf6e69ada56 is Running (Ready = true)
    Jun 28 09:09:48.798: INFO: Pod "pod-logs-websocket-5a6be22c-a9c4-4c15-9188-ecf6e69ada56" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jun 28 09:09:48.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-5393" for this suite. 06/28/23 09:09:48.838
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 09:09:48.847
Jun 28 09:09:48.847: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename cronjob 06/28/23 09:09:48.848
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 09:09:48.862
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 09:09:48.866
[It] should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
STEP: Creating a cronjob 06/28/23 09:09:48.87
STEP: Ensuring more than one job is running at a time 06/28/23 09:09:48.877
STEP: Ensuring at least two running jobs exists by listing jobs explicitly 06/28/23 09:11:00.882
STEP: Removing cronjob 06/28/23 09:11:00.886
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Jun 28 09:11:00.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-9045" for this suite. 06/28/23 09:11:00.9
{"msg":"PASSED [sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]","completed":355,"skipped":6520,"failed":0}
------------------------------
â€¢ [SLOW TEST] [72.059 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 09:09:48.847
    Jun 28 09:09:48.847: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename cronjob 06/28/23 09:09:48.848
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 09:09:48.862
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 09:09:48.866
    [It] should schedule multiple jobs concurrently [Conformance]
      test/e2e/apps/cronjob.go:69
    STEP: Creating a cronjob 06/28/23 09:09:48.87
    STEP: Ensuring more than one job is running at a time 06/28/23 09:09:48.877
    STEP: Ensuring at least two running jobs exists by listing jobs explicitly 06/28/23 09:11:00.882
    STEP: Removing cronjob 06/28/23 09:11:00.886
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Jun 28 09:11:00.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-9045" for this suite. 06/28/23 09:11:00.9
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 09:11:00.911
Jun 28 09:11:00.911: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename crd-publish-openapi 06/28/23 09:11:00.913
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 09:11:00.924
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 09:11:00.929
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 06/28/23 09:11:00.933
Jun 28 09:11:00.933: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
Jun 28 09:11:03.003: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jun 28 09:11:12.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8924" for this suite. 06/28/23 09:11:12.892
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","completed":356,"skipped":6537,"failed":0}
------------------------------
â€¢ [SLOW TEST] [11.988 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 09:11:00.911
    Jun 28 09:11:00.911: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename crd-publish-openapi 06/28/23 09:11:00.913
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 09:11:00.924
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 09:11:00.929
    [It] works for multiple CRDs of same group and version but different kinds [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:356
    STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 06/28/23 09:11:00.933
    Jun 28 09:11:00.933: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    Jun 28 09:11:03.003: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jun 28 09:11:12.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-8924" for this suite. 06/28/23 09:11:12.892
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 09:11:12.9
Jun 28 09:11:12.900: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename projected 06/28/23 09:11:12.901
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 09:11:12.915
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 09:11:12.92
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73
STEP: Creating configMap with name projected-configmap-test-volume-d8cea236-c878-4ebe-a354-14f664769be3 06/28/23 09:11:12.923
STEP: Creating a pod to test consume configMaps 06/28/23 09:11:12.928
Jun 28 09:11:12.937: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5f0faf02-71a7-4c7b-820a-c665d69bfeea" in namespace "projected-1220" to be "Succeeded or Failed"
Jun 28 09:11:12.942: INFO: Pod "pod-projected-configmaps-5f0faf02-71a7-4c7b-820a-c665d69bfeea": Phase="Pending", Reason="", readiness=false. Elapsed: 4.79898ms
Jun 28 09:11:14.948: INFO: Pod "pod-projected-configmaps-5f0faf02-71a7-4c7b-820a-c665d69bfeea": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010651363s
Jun 28 09:11:16.949: INFO: Pod "pod-projected-configmaps-5f0faf02-71a7-4c7b-820a-c665d69bfeea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011536966s
STEP: Saw pod success 06/28/23 09:11:16.949
Jun 28 09:11:16.949: INFO: Pod "pod-projected-configmaps-5f0faf02-71a7-4c7b-820a-c665d69bfeea" satisfied condition "Succeeded or Failed"
Jun 28 09:11:16.953: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod pod-projected-configmaps-5f0faf02-71a7-4c7b-820a-c665d69bfeea container agnhost-container: <nil>
STEP: delete the pod 06/28/23 09:11:16.961
Jun 28 09:11:16.971: INFO: Waiting for pod pod-projected-configmaps-5f0faf02-71a7-4c7b-820a-c665d69bfeea to disappear
Jun 28 09:11:16.975: INFO: Pod pod-projected-configmaps-5f0faf02-71a7-4c7b-820a-c665d69bfeea no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jun 28 09:11:16.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1220" for this suite. 06/28/23 09:11:16.982
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","completed":357,"skipped":6565,"failed":0}
------------------------------
â€¢ [4.087 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 09:11:12.9
    Jun 28 09:11:12.900: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename projected 06/28/23 09:11:12.901
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 09:11:12.915
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 09:11:12.92
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:73
    STEP: Creating configMap with name projected-configmap-test-volume-d8cea236-c878-4ebe-a354-14f664769be3 06/28/23 09:11:12.923
    STEP: Creating a pod to test consume configMaps 06/28/23 09:11:12.928
    Jun 28 09:11:12.937: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5f0faf02-71a7-4c7b-820a-c665d69bfeea" in namespace "projected-1220" to be "Succeeded or Failed"
    Jun 28 09:11:12.942: INFO: Pod "pod-projected-configmaps-5f0faf02-71a7-4c7b-820a-c665d69bfeea": Phase="Pending", Reason="", readiness=false. Elapsed: 4.79898ms
    Jun 28 09:11:14.948: INFO: Pod "pod-projected-configmaps-5f0faf02-71a7-4c7b-820a-c665d69bfeea": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010651363s
    Jun 28 09:11:16.949: INFO: Pod "pod-projected-configmaps-5f0faf02-71a7-4c7b-820a-c665d69bfeea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011536966s
    STEP: Saw pod success 06/28/23 09:11:16.949
    Jun 28 09:11:16.949: INFO: Pod "pod-projected-configmaps-5f0faf02-71a7-4c7b-820a-c665d69bfeea" satisfied condition "Succeeded or Failed"
    Jun 28 09:11:16.953: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod pod-projected-configmaps-5f0faf02-71a7-4c7b-820a-c665d69bfeea container agnhost-container: <nil>
    STEP: delete the pod 06/28/23 09:11:16.961
    Jun 28 09:11:16.971: INFO: Waiting for pod pod-projected-configmaps-5f0faf02-71a7-4c7b-820a-c665d69bfeea to disappear
    Jun 28 09:11:16.975: INFO: Pod pod-projected-configmaps-5f0faf02-71a7-4c7b-820a-c665d69bfeea no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jun 28 09:11:16.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1220" for this suite. 06/28/23 09:11:16.982
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] ResourceQuota
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 09:11:16.988
Jun 28 09:11:16.988: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename resourcequota 06/28/23 09:11:16.989
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 09:11:17.002
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 09:11:17.007
[It] should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874
STEP: Creating a ResourceQuota 06/28/23 09:11:17.012
STEP: Getting a ResourceQuota 06/28/23 09:11:17.018
STEP: Updating a ResourceQuota 06/28/23 09:11:17.022
STEP: Verifying a ResourceQuota was modified 06/28/23 09:11:17.027
STEP: Deleting a ResourceQuota 06/28/23 09:11:17.03
STEP: Verifying the deleted ResourceQuota 06/28/23 09:11:17.035
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jun 28 09:11:17.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8504" for this suite. 06/28/23 09:11:17.046
{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","completed":358,"skipped":6566,"failed":0}
------------------------------
â€¢ [0.064 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 09:11:16.988
    Jun 28 09:11:16.988: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename resourcequota 06/28/23 09:11:16.989
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 09:11:17.002
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 09:11:17.007
    [It] should be able to update and delete ResourceQuota. [Conformance]
      test/e2e/apimachinery/resource_quota.go:874
    STEP: Creating a ResourceQuota 06/28/23 09:11:17.012
    STEP: Getting a ResourceQuota 06/28/23 09:11:17.018
    STEP: Updating a ResourceQuota 06/28/23 09:11:17.022
    STEP: Verifying a ResourceQuota was modified 06/28/23 09:11:17.027
    STEP: Deleting a ResourceQuota 06/28/23 09:11:17.03
    STEP: Verifying the deleted ResourceQuota 06/28/23 09:11:17.035
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jun 28 09:11:17.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-8504" for this suite. 06/28/23 09:11:17.046
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 09:11:17.054
Jun 28 09:11:17.054: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename configmap 06/28/23 09:11:17.055
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 09:11:17.066
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 09:11:17.07
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123
STEP: Creating configMap with name configmap-test-upd-c3782ce0-7827-477d-adee-1651538f476d 06/28/23 09:11:17.08
STEP: Creating the pod 06/28/23 09:11:17.085
Jun 28 09:11:17.093: INFO: Waiting up to 5m0s for pod "pod-configmaps-14aeddf8-4bfe-4584-a351-378ed5015a0e" in namespace "configmap-1149" to be "running and ready"
Jun 28 09:11:17.097: INFO: Pod "pod-configmaps-14aeddf8-4bfe-4584-a351-378ed5015a0e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.574815ms
Jun 28 09:11:17.097: INFO: The phase of Pod pod-configmaps-14aeddf8-4bfe-4584-a351-378ed5015a0e is Pending, waiting for it to be Running (with Ready = true)
Jun 28 09:11:19.102: INFO: Pod "pod-configmaps-14aeddf8-4bfe-4584-a351-378ed5015a0e": Phase="Running", Reason="", readiness=true. Elapsed: 2.008571504s
Jun 28 09:11:19.102: INFO: The phase of Pod pod-configmaps-14aeddf8-4bfe-4584-a351-378ed5015a0e is Running (Ready = true)
Jun 28 09:11:19.102: INFO: Pod "pod-configmaps-14aeddf8-4bfe-4584-a351-378ed5015a0e" satisfied condition "running and ready"
STEP: Updating configmap configmap-test-upd-c3782ce0-7827-477d-adee-1651538f476d 06/28/23 09:11:19.154
STEP: waiting to observe update in volume 06/28/23 09:11:19.16
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jun 28 09:12:27.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1149" for this suite. 06/28/23 09:12:27.833
{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","completed":359,"skipped":6655,"failed":0}
------------------------------
â€¢ [SLOW TEST] [70.785 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 09:11:17.054
    Jun 28 09:11:17.054: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename configmap 06/28/23 09:11:17.055
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 09:11:17.066
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 09:11:17.07
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:123
    STEP: Creating configMap with name configmap-test-upd-c3782ce0-7827-477d-adee-1651538f476d 06/28/23 09:11:17.08
    STEP: Creating the pod 06/28/23 09:11:17.085
    Jun 28 09:11:17.093: INFO: Waiting up to 5m0s for pod "pod-configmaps-14aeddf8-4bfe-4584-a351-378ed5015a0e" in namespace "configmap-1149" to be "running and ready"
    Jun 28 09:11:17.097: INFO: Pod "pod-configmaps-14aeddf8-4bfe-4584-a351-378ed5015a0e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.574815ms
    Jun 28 09:11:17.097: INFO: The phase of Pod pod-configmaps-14aeddf8-4bfe-4584-a351-378ed5015a0e is Pending, waiting for it to be Running (with Ready = true)
    Jun 28 09:11:19.102: INFO: Pod "pod-configmaps-14aeddf8-4bfe-4584-a351-378ed5015a0e": Phase="Running", Reason="", readiness=true. Elapsed: 2.008571504s
    Jun 28 09:11:19.102: INFO: The phase of Pod pod-configmaps-14aeddf8-4bfe-4584-a351-378ed5015a0e is Running (Ready = true)
    Jun 28 09:11:19.102: INFO: Pod "pod-configmaps-14aeddf8-4bfe-4584-a351-378ed5015a0e" satisfied condition "running and ready"
    STEP: Updating configmap configmap-test-upd-c3782ce0-7827-477d-adee-1651538f476d 06/28/23 09:11:19.154
    STEP: waiting to observe update in volume 06/28/23 09:11:19.16
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jun 28 09:12:27.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-1149" for this suite. 06/28/23 09:12:27.833
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/28/23 09:12:27.84
Jun 28 09:12:27.840: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
STEP: Building a namespace api object, basename configmap 06/28/23 09:12:27.841
STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 09:12:27.852
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 09:12:27.856
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98
STEP: Creating configMap with name configmap-test-volume-map-6241b8bc-df7e-47a3-832b-094be368a641 06/28/23 09:12:27.86
STEP: Creating a pod to test consume configMaps 06/28/23 09:12:27.864
Jun 28 09:12:27.872: INFO: Waiting up to 5m0s for pod "pod-configmaps-1262d452-1e95-4097-9fad-3547b0547768" in namespace "configmap-8470" to be "Succeeded or Failed"
Jun 28 09:12:27.876: INFO: Pod "pod-configmaps-1262d452-1e95-4097-9fad-3547b0547768": Phase="Pending", Reason="", readiness=false. Elapsed: 3.782838ms
Jun 28 09:12:29.881: INFO: Pod "pod-configmaps-1262d452-1e95-4097-9fad-3547b0547768": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009492609s
Jun 28 09:12:31.882: INFO: Pod "pod-configmaps-1262d452-1e95-4097-9fad-3547b0547768": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009660878s
STEP: Saw pod success 06/28/23 09:12:31.882
Jun 28 09:12:31.882: INFO: Pod "pod-configmaps-1262d452-1e95-4097-9fad-3547b0547768" satisfied condition "Succeeded or Failed"
Jun 28 09:12:31.886: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod pod-configmaps-1262d452-1e95-4097-9fad-3547b0547768 container agnhost-container: <nil>
STEP: delete the pod 06/28/23 09:12:31.895
Jun 28 09:12:31.905: INFO: Waiting for pod pod-configmaps-1262d452-1e95-4097-9fad-3547b0547768 to disappear
Jun 28 09:12:31.909: INFO: Pod pod-configmaps-1262d452-1e95-4097-9fad-3547b0547768 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jun 28 09:12:31.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8470" for this suite. 06/28/23 09:12:31.916
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":360,"skipped":6661,"failed":0}
------------------------------
â€¢ [4.082 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/28/23 09:12:27.84
    Jun 28 09:12:27.840: INFO: >>> kubeConfig: /tmp/kubeconfig-3061215203
    STEP: Building a namespace api object, basename configmap 06/28/23 09:12:27.841
    STEP: Waiting for a default service account to be provisioned in namespace 06/28/23 09:12:27.852
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/28/23 09:12:27.856
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:98
    STEP: Creating configMap with name configmap-test-volume-map-6241b8bc-df7e-47a3-832b-094be368a641 06/28/23 09:12:27.86
    STEP: Creating a pod to test consume configMaps 06/28/23 09:12:27.864
    Jun 28 09:12:27.872: INFO: Waiting up to 5m0s for pod "pod-configmaps-1262d452-1e95-4097-9fad-3547b0547768" in namespace "configmap-8470" to be "Succeeded or Failed"
    Jun 28 09:12:27.876: INFO: Pod "pod-configmaps-1262d452-1e95-4097-9fad-3547b0547768": Phase="Pending", Reason="", readiness=false. Elapsed: 3.782838ms
    Jun 28 09:12:29.881: INFO: Pod "pod-configmaps-1262d452-1e95-4097-9fad-3547b0547768": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009492609s
    Jun 28 09:12:31.882: INFO: Pod "pod-configmaps-1262d452-1e95-4097-9fad-3547b0547768": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009660878s
    STEP: Saw pod success 06/28/23 09:12:31.882
    Jun 28 09:12:31.882: INFO: Pod "pod-configmaps-1262d452-1e95-4097-9fad-3547b0547768" satisfied condition "Succeeded or Failed"
    Jun 28 09:12:31.886: INFO: Trying to get logs from node ske-rhel-749f7d55c8xdd8b6-ct4cp pod pod-configmaps-1262d452-1e95-4097-9fad-3547b0547768 container agnhost-container: <nil>
    STEP: delete the pod 06/28/23 09:12:31.895
    Jun 28 09:12:31.905: INFO: Waiting for pod pod-configmaps-1262d452-1e95-4097-9fad-3547b0547768 to disappear
    Jun 28 09:12:31.909: INFO: Pod pod-configmaps-1262d452-1e95-4097-9fad-3547b0547768 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jun 28 09:12:31.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-8470" for this suite. 06/28/23 09:12:31.916
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:87
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:87
{"msg":"Test Suite completed","completed":360,"skipped":6706,"failed":0}
Jun 28 09:12:31.924: INFO: Running AfterSuite actions on all nodes
Jun 28 09:12:31.924: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func20.2
Jun 28 09:12:31.924: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func10.2
Jun 28 09:12:31.924: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func9.2
Jun 28 09:12:31.924: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
Jun 28 09:12:31.924: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
Jun 28 09:12:31.924: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
Jun 28 09:12:31.924: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:87
Jun 28 09:12:31.924: INFO: Running AfterSuite actions on node 1
Jun 28 09:12:31.924: INFO: Skipping dumping logs from cluster
------------------------------
[SynchronizedAfterSuite] PASSED [0.000 seconds]
[SynchronizedAfterSuite] 
test/e2e/e2e.go:87

  Begin Captured GinkgoWriter Output >>
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:87
    Jun 28 09:12:31.924: INFO: Running AfterSuite actions on all nodes
    Jun 28 09:12:31.924: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func20.2
    Jun 28 09:12:31.924: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func10.2
    Jun 28 09:12:31.924: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func9.2
    Jun 28 09:12:31.924: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
    Jun 28 09:12:31.924: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
    Jun 28 09:12:31.924: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
    Jun 28 09:12:31.924: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:87
    Jun 28 09:12:31.924: INFO: Running AfterSuite actions on node 1
    Jun 28 09:12:31.924: INFO: Skipping dumping logs from cluster
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:146
[ReportAfterSuite] TOP-LEVEL
  test/e2e/e2e_test.go:146
------------------------------
[ReportAfterSuite] PASSED [0.000 seconds]
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:146

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/e2e_test.go:146
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:559
[ReportAfterSuite] TOP-LEVEL
  test/e2e/framework/test_context.go:559
------------------------------
[ReportAfterSuite] PASSED [0.050 seconds]
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:559

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/framework/test_context.go:559
  << End Captured GinkgoWriter Output
------------------------------

Ran 360 of 7066 Specs in 5549.193 seconds
SUCCESS! -- 360 Passed | 0 Failed | 0 Pending | 6706 Skipped
PASS

Ginkgo ran 1 suite in 1h32m29.512956616s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.1.6[0m

